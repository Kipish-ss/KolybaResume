Category,Resume
data engineer,"Industrialized an existing scikit-learn prototype using Spark (Scala) to deploy a system for predictive maintenance of trains. 15 binary resume_classifier predicting the probability of failure of each type of physical equipment of 200 Paris regional trains. This allows French railways to detect potential failures and proactively replace a train, which is more cost-effective than fixing a train failing during a passenger circulation.

Deployed an image classification system, based on satellite images (100 megapixel, 1 meter resolution) from Sentinel satellite of EU Copernicus program. System is used for ice segmentation in the Arctic Ocean, improving safety of ships navigation.

Was teaching 4 introductory 1-week courses on Machine Learning / Deep Learning to the students of Pierre-Marie Curie university in Paris.

Created 2-million node graph of transportation network (roads, railways) of 100% territory of France, using OpenStreetMap data. Implemented Markov Models to project data of mobile network events onto the generated graph. This map-matching system, via both LeafletJS visualization and aggregated geo data, allowed mobile operators to improve the accuracy of their geo-statistics solutions.

In a team of 4, built a hackathon winning Flask/Leaflet web application: animation of map-matched trajectories on the transport network of France.

Built a centralized system for storing all marketing costs (25 channels/vendors) of a NYSE public company. Reports based on this data are more stable, allowing Business Intelligence, Strategy, Marketing teams to plan marketing campaigns more accurately.

Participated in French Robotics Cup 2016.
The main part of my IT career was in France. After some Java internship, got my first Data Science job in 2015. As a consultant, participated in several projects for transport, energy, manufacturing, retail. Then, in 2017-2020, had several Data Science/Data Engineering jobs, covering a wide range of tasks: from research/prototyping to data engineering and devops activities for deploying ML resume_classifier.

In 2021-2022 I had a role of AI Team Lead, delivering various AI applications to the clients, with hands-on experience as well as team and people management activities.

Originally from Ukraine, i lived in Paris in 2011-2019 and in Berlin in 2019-2020. This gave me a good experience in working with European clients and a lot of foreign languages practice (especially English and French). In 2021 i moved back to Ukraine.

I also like teaching: in 2018-19 gave several 1-week introductory courses on Machine Learning / Deep Learning to the students of Pierre-Marie Curie university in Paris.

Interested in designing/building/deploying Machine Learning systems. A mix of a solid mathematical background (theoretical physics MSc in Ukraine and France) and a software engineering education make me relevant for a wide range of Data Science/Machine Learning/Deep Learning positions, from prototyping ideas to deploying resume_classifier in production.
Machine Learning/Deep Learning/Computer Vision projects, with resume_classifier running in production (not just prototypes). Perfect case: ""bridge"" between Data Science and Data Engineering.

Want to be a part of a strong Machine Learning/Data Engineering team, being able to constantly learn and grow. 

Most interested in domains like computer vision, medical, transport, ""social good"", ecology. Less interested in marketing, retail.

Social aspect is very important to me: knowing that the results of my work help to improve people's lives.

Agile methodology is preferred, with very clear communication. 

In case of an international collaboration, the company could also benefit from my knowledge of foreign languages and almost 10 years of observing the culture/mentality of Western Europe."
data engineer,"
4 years of experience in Data Analytics / BI.

Programming: Python (Pandas, Numpy, SqlAlchemy, skicit-learn, BeautifulSoup) ,OOP   
Visualization: Power BI (PBI Desktop, PBI Service, Data Flows, Development Pipelines,Automated Refresh, ETL,DAX query Language, Power Query)
SQL: MS SQL (T-SQL), Oracle, PostgreSQL,SnowFlake
MS BI: SSAS, SSIS 
Jira (Agile Scrum) 
Git
"
data engineer,"
For the past 6 years, I have worked in the IT industry, with two years of experience in database development and maintenance, two years in business intelligence (ETL, data visualization/transformation), and two+ years in AWS cloud development (data oriented). 
My current job involves working with large datasets, writing Lambdas using Python, creating infrastructure using CFN templates and boto3 methods (Python SDK for AWS), monitoring on AWS via Cloudwatch, configuring and maintaining databases and DWHs (RDS and Redshift databases), S3 for storage, Athena for data reading, CLI for a wide range of AWS tasks. 
Furthermore, I have had direct contact with customers and enjoy it. I am also knowledgeable in designing relational databases as well as ETL pipelines using Python to store information from various sources.
I'm interested in projects that align with my expertise"
data engineer,"
Development of AWS ETL pipelines and solutions using serverless stack. Migration to a cloud-native architecture.

Financial reporting automation, ELT pipelines and validations using an enterprise reporting platform. Performance optimisation of data-intensive scripts.
BI / dashboards"
data engineer,"Exam 70-761 Querying Data with T-SQL
Microsoft Certified: Azure Fundamentals
Microsoft Certified: Azure Data Engineer Associate
Developed DB structure and ETL processes.
Bulding ETL Data Migration process (to copy data from MSQL to Redshift) and destination structure  
Lambda/Boto3 using Python
AWS Glue job development with spark/Python shell
Step functions as orchestration of ETL process
Development reports in AWS QuickSight 
Building Redshifts structure as DWH storage from zero 
Using Terraform as solution to deploy IaaC from dev env to other stages

Full lifecycle of design and development SQL Databases (SQL Server, Azure SQL)

DB/DWH Developer with more than 10 years of development experience

MS SQL Database Performance optimization

SSRS, SSIS, and Azure Data Factory experience development 

Leading the team members
A new challenging project with AWS stack that allows me to grow and improve my professional and soft skills.
Project (OLTP or DWH) with AWS Cloud technologies.
I am looking for a remote job or a part-time job."
data engineer,"
I'm a product minded, result oriented software engineer with 11+ years of experience in building B2B, B2C systems and mobile apps. I’m keen on implementing and optimising software engineering practices and processes to help products grow.

At the current moment I'm working as a Data Engineering Consultant for a major Pharmacology producer.

Living in Warsaw, Poland
"
data engineer,"I can proud of my participation in each project I have worked for. I believe that Customers I have worked for can say that Products I developed absolutely met their expectations.

A lot of software products were developed and commissioned into Production during 21 years. Many legacy Software decommissioned and thrown into trash can.

N.B. Please!!! See detailed info in attached resume or on LinkedIn
N.B. Please!!! See detailed info in attached resume or on LinkedIn

Summary
--------------------------------------------------------------
• 21 years as software engineer(backend). Cloud and on-premise application development.
• 10 years as Technical Lead of software development team

• Certified in Python 3.4 
• AWS Certified Solutions Architect – Associate

Developer skills in Cloud solutions:
----
• AWS developer: Lambda, SNS, SQS, API Getaway, RDS Postgresql, DynamoDB, Timestream DB, S3/Athena, Glue, DMS, Step Functions, Batch
• AWS DevOps: VPC, CDK/SAM, ECR, Fargate and Batch
• AWS IoT: Core, SiteWise, IMC

Other developer skills:
----
• Python 3.5/3.8, sync and async programming
• Flask and Sanic frameworks: RESTful API micro service development
• Using of Docker virtualization
• Python + Celery, Reddis, Rabbitmq, Kafka, 
• SQL DB (PostgreSQL), NoSQL(Cassandra), Apache Spark
• SQLAlchemy / ORM / Raw T-SQL
• Data base developer(PostgreSQL, Oracle PL/SQL, MS SQL)

Projects:
------
Jan 2022 - now

Project: mining company. AWS IoT solution to support research of zero-emission truck development
Role: AWS Data Engineer
Tech details: AWS: IMC, IoT Greengrass/Core/SiteWise, Lambda, Kinesis, Glue, S3/Athena

Nov 2021 - Jan 2022

Project: food&beverage production. Research of solution for BigData processing, storage and reporting
Role: AWS Data Engineer
Tech details: AWS: CDK/SAM, Lambda, Timestream DB, Dynamo DB, S3/Athena, Glue

Aug 2021 - Nov 2021

Project: traders consulting. Migration of on-premise solution to AWS
Role: AWS Data Engineer
Tech details: AWS: CDK/SAM, VPC, Lambda, DMS, RDS, Glue, Step Function

Oct 2016 - Aug 2021

Project: Medical Insurance, USA. Corporate WEB application built on micro service platform
Position: Technical Lead / Senior Software Developer
Tech details: Python, WEB, Flask/Sanic, REST API, microservices, Kafka, RabbitMQ, Celery, Postgres PL/SQL, Spark

2014-2016

Project: Banking and Financial, Germany. Bank Data Warehouse based on Oracle technologies
Position: Software Engineer
Tech details: Oracle Pl/SQL, Python

2000-2014(more than 10 projects)

Projects: Banking, Metallurgy, Heavy industry. ERP, Billing systems based on self-developed and third party Software
Position: Senior Software Engineer, Technical Lead
Tech details: Oracle Pl/SQL, Postgres PL/SQL
1. I see myself as AWS Python Data Engineer/Developer with Big Data specialization in future.

2. Project is a long-term. Project technical stack is a really huge; a lot of different AWS services are used.

3. Roles and responsibilities of each project member are clearly defined.

4. Office is located in Dnipro or full remote

5. Project road map exists and project follow this map. Project goals are understood for all team. Project goals are reachable."
data engineer,"experienced in Azure stack of data engineering technologies, such as Data Factory pipelines, data flows, data transformations using Databricks notebooks, data warehousing capabilities in Azure Synapse Analytics.
IT professional with 18 years of experience in various positions, including customer service, Cybersecurity, back-end development (.NET) and Data Engineering (Analytics). Currently holding 5 Azure certifications.
Слава Україні!
Opportunity to relocate to Europe and work with a multi-ethnic and energetic team, to learn new technology stacks and meet new challenges."
data engineer,"During my work in Deloitte I started with Azure Cloud Fundamentals and achieved to get Azure Administrator Associate and Data Engineer Associate certifications. These accomplishments is the evidence of my dedication to continuous professional development.
Over the past eight months, I have applied my theoretical knowledge into practical experience through an internship within Deloitte Consulting team in Germany in the Cloud Transformation department. We had a project in which the client possessed various data sources (SAP, SQL Server, Excel). We employed an ETL process utilizing the Data Vault 2.0 model on the Azure Cloud platform (including Data Lake, Data Factory, and SQL Database), followed by visualization in Power BI.
"
data engineer,"
Data Engineer experienced in developing, supporting, and optimizing data systems (OLTP, DW, BI, ETL) for different domains.

- Got multicultural working experience with clients (USA, EU, UK, Ukraine) and was a part of local and abroad development teams (out staff and outsource).
- Got solid experience in working with Microsoft-developed data products (Azure, MS SQL Server, SSIS, SSRS, Power BI) and Azure Databricks.
- Worked side-by-side with Business Analytics, Product Owners, Product Managers, and Delivery Managers with software development teams or as a separate and independent developer on delivering quality data products.
- Worked both in Waterfall and Agile methodologies.
- Worked in such domains as healthcare, agribusiness, cybersecurity, oil & gas, and others.
- Passed certifications 70-461, 70-462 needed for MCSA: SQL Server 2012/2014. 
- Passed certification for Databricks Certified Associate Developer for Apache Spark 3.0.
I'm looking for a nice, friendly, and healthy working environment. Teammates who you can call friends. And a company that values the great sacrifice of the AFU."
data engineer,"Worked closely with the client teams to analyze and clarify technical and business requirements. 
Leaded estimation activities reported and negotiated task completion terms with the client’s reps. 
Drove architecture definition for both application and cloud infrastructure and refinement processes.
Took part in quality management, testing with high code coverage (90+% for the application server with ~200 endpoints).
Was responsible for the development of application servers, event-based scripts, data pipelines, and cloud infrastructure.
Supervised new versions deployment.
Leadership, mentorship, knowledge sharing, researching.
More than 5 years of experience in designing, developing, testing, and maintaining cloud-based web services with a focus on software quality, i.e. application design patterns & approaches, cloud architecture patterns, refactoring techniques, different types of automated testing, ensuring performance and fault tolerance.

As a Backend Software Engineer:
The main domain is E-commerce. Main responsibilities: development of backend application and infrastructure. Main technologies used: Python, Tornado, MongoDB, Redis, Stripe, AWS, Docker.

As a Data Engineer:
The main domain is Corporate Systems. Main responsibilities: development of data pipelines and infrastructure. Main technologies used: Python, PySpark, SQL, Databricks, AWS.
I expect from offers great teamwork on an interesting project, where completed tasks lead to visible results. Clear career advancement opportunities, the opportunity to gain experience in leadership and hard skills will be highly appreciated."
data engineer,"
Data Engineer/Power BI Developer
October 2021 – Present
Project – is a cloud-based solution that enables companies to successfully launch, manage, and transform into a subscription business. It provides a suite of products for recurring billing, collections, quoting, revenue recognition, and subscription metrics. 
Responsibilities:
	data modeling
	creation of interactive reports/dashboards
	multi-pages report, bookmarks
	drill-through functionality to the detail level
	Writing complex measures, performance improvements of existing measures
	Reports optimization
	snowflake querying 
	Data analysis 
	preparing documentation(instructions) 
	Deployments process
	data validation and preparation
	incremental refresh configuration
	Import Mode/DQ mode data loading configuration
Tools and technologies used: Microsoft Power BI, DAX Studio, ALM Toolkit, Report Builder, Tabular Editor, Performance Analyzer, DBT, Snowflake, Git, GitHub

Mede/Analitycs
BI/Data Engineer
July 2018 – October 2021
Project
Operated as a BI engineer implementing healthcare analytics software upgrades and net new SAAS implementations using proprietary software for hospitals and healthcare systems.        
Project under NDA
Responsibilities:
	Ensured security in the database. Database backup and restore
	Ensured the integrity of databases, Beta testing of product components
	Preparation, review and verification of client documentation
	Connection, DB configuration (Relational, Analytical) into the application
	ETL – process
	Historical data preparation
	Data Analysis
	Design and built client customization (fields, measures, dimensions, reports)
	Individual client configuration (based on client requirements) and implementation
	Data testing
	Migration from SQL DB to Vertica – data loading process using product functionality
	Production preparation and deployments processing 
	Performance Analysis, Optimization ETL process
	New Hire Education Training 
	Deep involvement in the developing process of implementation
	Deployment process through an internal application

Tools and technologies used: T-SQL, Vertica, MSSMS, DBeaver, SAAS, PAAS

I want to develop as a Power BI Developer. Also, I am interested in Python and PySpark, to work with Data using these technologies.
"
data engineer,"Current project after the implementation of the BI solution, the quality date allowed engineers to reduce the analysis time of their application from one day to 2 hours.

Past project with GCP I have been developing ETL using python library – Airflow and Google Cloud Services, I got from BA SQL scripts or looked business logic in tableau and rewrite these logics to Airflow and BigQuery. That helped company save cost in GCP and facilitated data transformations for reports.

Past project with AWS I have been developing ETL using Lambda Function (Python libraries), Step Function as orchestration, Aurora DB as DWH and S3 for storing configuration files.
More than 5 years of experience development Bisiness Intelligence with skills data solutions.
Strong problem solving skills and knowledge in:
- Google Cloud Platform stack technologies including BigQuery and cloud storage;
- Microsoft stack technologies including Microsoft Azure, MS SQL Server and Power BI.
- AWS stack technologies including Lambda Function, MySQL, Secret Manager, S3 Bucket, SSN, Layers, Step Function and Cloud Formation.
Has experience with Oracle SQL Developer, Apache Airflow, Python.
Full application life cycle experience: writing storeg procedure and jobs, development of ETL packages, development optimal datasets  (sharding, partitioning, clustering), reports modeling and development using Power BI, SAP BO and Microstrategy.
Communication, responsible, determined, good at problem solving, well organized and building client relations.
"
data engineer,"Development and support of a web application for storing and controlling the maintenance of internal documentation.
Creation of database 
Database optimization
Stored functions and procedures
Testing
Creation of documentation
Data analysis
EXCEL
Power BI 
Creation different types of reports
I would like to work in a position related to databases and analytics, where I can get new practical knowledge using my knowledge and skills."
data engineer,"- Was participating in deployment pipeline automation for Power BI
- Was mentoring and launching BI development team in company
•	Knowledge of SQL, T-SQL, DAX, MDX, Python.
•	Knowledge of Power BI, Google Data Studio, PowerQuery, PowerPivot, Sisense, Tableau, Jaspersoft Studio;
•	Development of procedures, views, transactions, etc.
•	Knowledge of Google Analytics;
•	Knowledge of MS Office products (Word, Excel, Power Point etc.)
•	Work experience with CRM system (Salesforce);
•	ETL development using Azure Data Factory, SSIS, SQL, Python.
•	Work experience with SSMS, DBeaver, DataGrip, VSCode;
•	Work experience with OLAP;
•	CI/CD;
•	Work experience with Azure, MS SQL, PostgreSQL, Greenplum, MongoDB (basics) databases;
•	Python.
I think this can be challenging work with interesting tasks which can give me more experience in solution of different tasks."
data engineer,"
I’m a BI developer with 4 years of experience in the Banking and Fintech field and eagerness to develop complex reporting systems, including the development of data pipelines, data modeling and visualizing. 

Skills 
SQL: MS SQL (T-SQL), PostgreSQL, MySQL 
Power BI: PBI desktop, PBI online, DAX, Power Query 
MS BI stack (SSMS, SSRS, SSIS) 
Python
MS Excel (for data analysis and reporting) + VBA for automation, MS Office stack 

Key projects completed
Company #1 (a FinTech startup)
 - Development and maintenance of the reporting for the company’s TOP and middle management in Power BI (from scratch) - including clarification of business requirements, the research of company's DWH, development of SQL procedures and datamarts, data modeling in Power BI desktop and visualizing in Power BI desktop and online. 
 - Automation of Incentive calculation process for Contact center operators and supervisors - helped to decrease the total time of calculation from ~ 1.5 hours down to 5 minutes, and the time of calculation from users’ perspective from ~ 30 minutes down to less than 1 minute. Also, within this project, I developed a module for automated handling of the data anomalies related to Incentive calculation that helped us to decrease the volume of manual work from ~ 10 hours per month to ~ 2 hours per month. 
- Data quality control (developed a set of alerts to catch data anomalies in the DWH). 
Company #2 (A contact center of a foreign bank operating in Ukraine)
 - I’ve optimized the method of fuzzy data search (using so called “fuzzy lookup”) that helped to minimize the number of sales missing in the CC agents’ Incentive calculation. 
 - I’ve developed a system for logging and tracking of historical manual activities in order to avoid so called “spamming” of bank’s clients with marketing ads.
I’m happy to work in multinational and multicultural teams, as well as to learn new tools and technologies. Am looking for a long-term cooperation with challenging tasks. 
Remote work is preferred. 
Flexible schedule would be a big advantage."
data engineer,"- Developed dashboards and pixel-perfect reports (Tableau and Pentaho)
- Automation of the process of creating aggregated tables
for dashboards and reports
- Creation ETL pipelines
On the last project (population census project in Saudi Arabia)  I worked as Senior BI Developer. My main responsibilities on these projects are development from scratch dashboards, analytics and pixel-perfect reports in accordance with customer requirements. Automation of the data preparation process for dashboards and reports. 
Before that during 10 year I worked as Senior Field Engineer in Hitachi Vantara where I was engaged in solving problems of varying complexity arising from customers (hardware and software).
I'd like to take a role as BI Developer or Data Engineer. I enjoy when raw data becomes valuable information which helps businesses make decisions."
data engineer,"Designed and Implemented a real time data pipeline using Azure Stream Analytics and Events Hub to process telemetry data and Sales Order Delivery data from SAP and other 3rd party ERP Solutions into Azure Data Lake Storage Gen2 and Azure Synapse Analytics.
Maintained data pipeline uptime of 99.8% while ingesting streaming and transactional data across 8 different primary data sources using SSIS, Azure Data Factory and Php.
Designed and implemented Enterprise Information Management (EIM) using Microsoft Master Data Services (MDS) a nd Data Quality Services (DQS) to integrate master data assets across various ERP systems within Sales and Marketing, HR, Supply Chain and Finance improving Enterprise BI Reporting accuracy and credibility by about 90%
Designed and Implemented Enterprise Data Warehouse (EDW) using Microsoft BI Stack to integrate different business datasets, build analytics resume_classifier in Analysis Services to enable enterprise and Self Service BI Reporting across all units of the business
• Successfully installed and configured Power BI Report Server for in house reporting and migrated over 350 reports from Power BI Service to the inhouse report server helping to eliminate cost of purchasing Power BI PRO Licenses for users
• Authored different Reports and Dashboards using Microsoft Power BI to help business decision makers derive insights from their business data and make better decisions
• Developed a daily sales volume and revenue tracking dashboard to help Sales team and management monitor sales at zonal and regional levels
• Commended for ensuring consistent adherence to proper protocols and procedures according to the standards of practice of the organization
• Played major role in ensuring integrity of multi million worth of equipment avoiding downtime, maintained visual aspects of site and enhanced the safety of members
Ayobami is an experienced BI Developer, Data Engineer and Analyst. I have over 4 years work experience as a Data Warehouse and BI Developer and Analyst using the Microsoft BI Stack (SQL Server Integration Services (SSIS), SQL Server Analysis Services (SSAS),  SQL Server Reporting Services (SSRS) and Power BI) to design and build enterprise reports and dashboards across Banking, Financial Services Industry to Investments, Retail and Manufacturing Industries.  I'm also a Microsoft Certified Data Analyst and Data Engineer Associate with years of experience working with SQL Server and Power BI.  I possess a Microsoft Certified Solutions Associate in SQL 2016 Business Intelligence Development certification and I'm equally Microsoft Azure, Azure Data fundamentals and Microsoft Certified Trainer certified. His core competencies are DAta Integration, Database Development and Modelling, Business Intelligence Development and Reporting, ETL Development, Data Analysis and Visualization.

Projects:
- Business Intelligence Solution – Implementation of On-premises Enterprise Datawarehouse
- Implementation of Azure Enterprise Data Warehouse
- Operational data store migration from on-premise to Azure
- Periodic Performance Reports on Power BI Desktop

Role - BI Developer/Analyst
• Design and build ETL pipelines to automate ingestion of data from various sources.
• Maintain and optimize existing ETL workflows, data management and data query components
• Maintained business intelligence resume_classifier to design, develop and generate both standard and ad-hoc reports.
• Generated reports and dashboards for internal and external customers for business performance monitoring and business decision making.
• Participated in project planning sessions with project managers, business analysts and team members to analyze business requirements and outline the proposed solution.
• Worked with Project Managers to develop and execute project plans within assigned schedule and timeline.
• Managed the development, maintenance, and integration of enterprise data warehouse and data reporting solutions.
• Update business intelligence and data warehousing solutions to meet changing business needs.
• Coordinated ETL processes to retrieve data from various data sources.
"
data engineer,"
•6 years experience in data analytics
• 4 years experience in project management and team leading (telecommunication); 
• Strong analytical and problem solving skills
• Processing large massive of information
• Languages: SQL
• Experience with Oracle PL/SQL Database architecture/design (volume data >120TB)
• Strong skills in developing and design Data Warehouse (DWH) and BI system 
• Experience with development billing system
• Ability to work under pressure and meet deadlines
• Excellent communication and interpersonal skills
"
data engineer,"
5 years of experience in SQL Development. DWH managing MS SQL Server, Snowflake, Amazon Redshift, Azure SQL DB.
Knowledge and work experience with T-SQL, query writing and optimizing, views, stored procedures, functions, indexes, jobs. DB backup and restore.
DB Change Management: Sqitch, Flyway.
BI: modeling and developing dashboards. Work with Looker, Tableau, Power BI. OLAP.
AWS: Snowflake, Redshift, S3.
Matillion.
Version Control System: CVS, SVN, Git.
Expert in MS Excel.
Able to identify, investigate and resolve data issues.
Learn and improve new skills quickly.
I'm always excited to work on new technologies. Would be great to improve my skills, learn smth new. Of course help new team with my experience and expertise."
data engineer,"Supported significant blocks of procedures and ETL, had experience in communicating with the customer and fulfilling non-standard requests for creating metrics and logics on OLAP, worked in the field of medicine. Mentor for New BI Engineers
Expirience with MS SQL Server, T-SQL, VSQL, DBeaver, DAX, MDX, SSIS, SSAS, OLAP, SSRS
Knowledge of Python, PowerBI, Tableau, Snowflake
Query writing and optimization
Creation of views, stored procedures, functions, indexes.
Ready to get acquainted with different offers"
data engineer,"Certified Azure Data Engineer
Java, Scala, Node Js, Hadoop, DataScience, R, RDBMS, Machine Learning, Functional Programming, MySQL, Linux, HTML,
NO RELOCATION 
B2B only 
Please doublecheck the Location"
data engineer,"I have experience in civil aviation, tourism, e-comerce, Amazon(Advertising, Sellers centre, Product Advertising), startups and working with machine learning projects
Full Stack developer 
InStandart
June 2020- now
Kherson Region, Ukraine 

Duties :
- full circle of software development - from planning to deployment 
- creation backend using Django and UI using Angular 
- particular DevOps tasks 

Middle Python Developer 
Hotkey
February 2020 - June 2020
Kherson Region, Ukraine 

Duties :
- create and deploy chatbots for Telegramm and Viber and deploy them to Azure
- build and integrate web app into different CRM systems
- build new and rebuild current websites(front and back)
- integrate different payment services into applications and web shops
- create and maintain analytics systems for ip telephony 

Android Developer
Flexen
May 2019 – December 2020
Kherson Region, Ukraine

Duties:
- creating UI interface in collaborating with designer and customer
- collaborating with backend developers
- planning application architecture and task delegation
- making Android


Full stack mobile
Develop8tors
August 2018 – April 2019
Bucharest, Romania

Duties:
- making apps using Flutter framework and native Android tools 

- creating backend for this app(server side on Aqueduct and database on Postgresql)
"
data engineer,"
I've been working as a Big data engineer at Sekai (IOT with logic patterns, Spark (streaming/batch)/Scala/Kafka/Prefect.io(Python a little bit)/TimescaleDB/Kubernetes/Camel/Graphana/Prometheus/Loki) for almost 2 years and at Propzmedia (Offline retail bigdata in Brazil, Spark/Scala/Jenkins/Airflow(little bit)/AWS(S3,EMR,EC2)/GCP/Azure/Terraform) for almost 4 years. I also had an experience with Cloudera HBase/HDFS/Jira/Redis/Vertica in Creara (ads exchange platforms). 16 years as software developer in various companies overall (Scala/Java).
"
data engineer,"I am quick learner and team member with good analytical skills and physical/mathematical background. 
I have practical experience working on big data project.
Java 8
Scala
Hadoop (Yarn, HDFS, MapR)
Kafka
Spark (Spark Streaming)
I'd prefer working on long-play project. 

Passionate about new technologies. 

Preferable technologies: Hadoop ecosystem + Spark"
data engineer,"Improved Airflow DAGs by adding parametrized prioritization. Created service to load data with constantly changing schema to data lake for further processing. Together with Data Science team created fraud detection solution for customer using AWS. Created dockerized Python client to load data from customer's resources for further processing (previously it was done manually).
Big Data engineer for retailer company. Tasks: took part in DWH design, created analytic Spark jobs, Python services to load data from different sources. Created fraud detection solution on AWS Stack: Glue, Lambda, Kinesis, Jupiter, Athena, Spark, Python, Java, Jenkins, Kubernetes

  Big data engineer on fintech project. Tsks: create ingestion and analytics jobs, Airflow DAGs. Designed microservice monitoring/notification solution in GCP. Stack: Spark, GCP, Kubernetes, Spring Boot, Scala, Java, Python

  Big data engineer on oil company project. Tasks: create Hive ETL jobs, Spark analytic jobs, Spark ML job to categorize Text data, Oozie workflows. Stack: Hive, Spark, Spark ML, Oozie, Java, Scala
Would prefer to work for a product company. Would be great to have small team and new technologies. Prefer not to work with Azure cloud"
data engineer,"- Conducted proof-of-concept testing of internal tools, effectively optimizing Snowflake performance.
- Reduced the team's time spent on support by 30%;
Data Engineer specializing in levaraging and optimization of Snowflake and Redshift data warehouses. 
Proficient in orchestrating complex data workflows using Airflow and enhancing data transformation processes with dbt. 
Experienced in the deployment and automation of systems using DevOps tools such as Terraform and Kubernetes, including the creation of CI/CD pipelines.

I have had the privilege of collaborating with diverse clients, which include a US-based delivery company and an S&P 500 listed company.
Intrigued by the challenges of building large-scale distributive systems and harnessing the power of data.  
Bring a strong interest in building large distributed systems, particularly using Spark, and designing data warehouses on AWS."
data engineer,"
Results-driven Big Data Engineer with over 8 years of experience in developing and implementing data solutions. Skilled in utilizing HDFS, MapReduce, Spark, Hive, Kafka, Java, and Python. Strong knowledge of AWS Cloud and a passion for learning innovative technologies. Proven expertise in software development, system implementation, debugging, testing, and maintenance. Experienced in research, development, and implementing proof of concept (POC) projects.
Challenging Projects: Seek complex data engineering tasks to expand skills.
Learning Opportunities: Value continuous learning. Cutting-Edge Technologies: Look for a company that stays up to date with the latest data engineering tools to stay relevant and explore new approaches. Professional Growth: Ensure a clear career path with opportunities for promotion. Feedback and Reviews: Expect regular performance assessments and constructive feedback."
data engineer,"Upgraded project from python 2.* to python 3.*
Experience in scraping and parsing with selenium, beautifulSoup, lxml
Created GUI with pyqt
Created new and updated existing data pipelines with dataflow, orchestrate a jobs with airflow and other data lake related job.
Worked as a PHP developer on freelance. 
1 year of experience in web scraping, in office, using Web Query Language. Experience in Jira and SVN.
Last 5 years work with python, 2 of them as a GCP Data engineer in data lake.
Certified Google data engineer.
"
data engineer,"
Development of ETL  processes for related databases. 
Writing SQL queries 
Troubleshooting and support existing systems 
Creating pipelines based on AWS Step Functions
Writing PySpark scripts for AWS Glue and Azure Databricks
"
data engineer,"In one of project I have optimised data pipeline and reduce time to process data in 4x. I have good experience with relative new tool - DataHub. Recently I have gotten AWS Certified Data Analytics - Specialty (03.02.2022-03.02.2025)
1. Creating Data Platform with using DataHub as the metadata storage and visualisation.

Responsibilities:
- Collect metadata and fill DataHub, build data lineage.
- Creating and maintaining airflow pipelines to fill RedShift data warehouse and datalake.
- Develop IaC using Terraform

2. Creating airflow pipelines to fill RedShift data warehouse and datalake.
Responsibilities:
- Creating and maintaining airflow pipelines to fill RedShift data warehouse and datalake.
- Creating AWS Glue job
- Debugging and bug fixing

3. ETL systems. Data analysis and conversion. Creating Data Marts based on the data.
Responsibilities:
- Creating serverless aplication on AWS (AWS Lambda, AWS Glue)
- Increased code quality(integrated sonar, decomposition complex area of codes) 
- Rump-up of new members.
- Creating stage table using AWS Crawler.
- Covering with tests
4. Large enterprise project with microservice architectureintended to optimize calculating the rebates, chargeback and
other types of fees that are used in the USA marketing. The
complex platform for business data management and
analysis aim is to assists customers in the delivery of managed services for the health care industry.

Responsibilities:
- Increase code quality(added docstrings, integrated sonar,decomposition some complex area of codes).
- Implement inside tools for parsing Title of commit and sync with JIRA ticket between different release.
- Fix bugs and noted root causes of bugs.
"
data engineer,"Helped the customer to enter the market in another region faster by building an Ml platform in a short time
1) Development of a service for automatic detection of anomalies in the fuel system. 
Responsibilities:
Creating data lake and MLOps process
Development of PySpark pipelines for data preparation for training resume_classifier
Implementation of the orchestration of model training and prediction pipelines using Airflow 
Mentoring interns and knowledge transfer to new project participants during their onboarding 
2) Development of an Ml platform for a pharmaceutical company.
Responsibilities:
Development of Terraform scripts
Development of pipelines for model training
Leading a team of 4 people
Discussion of requirements with the customer
Conducting a demo
3) Development of a data governance solution for all internal services of the company. 
Responsibilities:
Development of connectors for data ingestion from various internal services
Development of data access differentiation
Architecture development
4) Development of a recommendation system for e-commerce
Responsibilities:
Building pipelines for data processing
Delta Lake development
Building pipelines for setting ML resume_classifier for production
Interested in any complex tasks related to the construction of highly loaded pipelines"
data engineer,"1. Significant performance enhancements for different clients.
2. Expenditure reductions.
3. Participated in a few large customers acquisition for the company.
1. Development of different infrastructures and processes
for Big Data purposes (bare metal, cloud, k8s) with
different stack types (Spark, Kafka, Hive, HBase, Yarn, Spark Standalone, Spark on k8s, MongoDB, Mysql, some
specific NoSql and SQL databases, Livy)

2. Cloud system development

3. DevOps related tasks (large K8S clusters setup and
administrating, CI process management with Jenkins, development, and management of internal infrastructure
project on Ansible, networks management), docker images
refactoring, helm charts development
Involving in apache products improvements
QA-related tasks (performance testing, setting up the
the testing process)

4. Teams management

5. Projects audit and performance enhancement

6. Presale activities (presentations, communications with clients, etc.)
I want to be engaged in near to life project that makes some real stuff with a lot of benefits for real world"
data engineer,"- project to parse Amazon data using Tor network of free proxies as an alternative to paid services (client could decrease monthly costs by a few thousand US dollars)

- background before switching to IT - economics & finance (passed 2 out of 3 levels of Chartered Financial Analyst [CFA] qualification), data analytics roles in various Ukraine-based western-capital top-tier banks / investment & insurance companies.
- 4th commercial project (~6 months) - global privacy protection-based search engine (DuckDuckGo alternative). I defined architecture and built integrated and automated parsing of Wikipedia articles big data dumps within AWS Serverless infrastructure (AWS Step Functions, Lambda, Glue/PySpark, DynamoDB, AWS SAM) - to use it for displaying Wikipedia widget in a search engine results.

- 3rd commercial project (~3 months) - NFT-market based project in transition from Flask to Django. Writing basic Django replica (basic fullstack Django/Bootstrap) app from scratch with functionality of existing project in Flask (using docker & docker compose, consisting of Django web app, nginx, celery, Postgres, Redis, Rabbitmq), celery jobs of parsing of data from Opensea, endpoints and views, user dashboards & authentication etc.) 

- 2nd commercial project (~14 months) - US e-commerce platform for managing products marketing on Amazon - new functionality, performance & speed improvements, debugging, Pytest tests from scratch, ad hoc scripts.

- 1st commercial project (~7 months) - US e-commerce platform for managing logistics & goods deliveries costs & efficiency for e-commerce traders - improvements and debugging.

- pet-project (~6 months) - e-commerce platform for managing drop-shipping products sales in Ukraine - myself full-stack building from scratch to MVP.
Focus on back-end.

However, I have some practical experience with integrating back-end with HTML / Bootstrap / CSS / Javascript / JQuery / Ajax / Jinja."
data engineer,"7+ experience years.
-saved 200k $ for the company
-deployed service 2 months faster than expected
-report speaker
6+ commercial dev experience.
-Scala(Certified)
-Spark/Hadoop (Certified)
-Hive, MS SQL, MongoDB, Redis
-RabbitMQ, Kafka
-Microservices
-Highload
-DevOps
TDD, BDD, DRY, KISS, SOLID, GoF
living in the USA"
data engineer,"I've been engaged in ETL projects holding responsibilities of setting up fault-tolerant and reliable environments with Docker containers, extracting and delivering data from various sources for further processing using Kafka and Spark, building scalable and automated data pipelines with Airflow, and analyzing data using Kibana.
As a Big Data Engineer, I utilized different tools such as Java, Go, Scala, Python, Pandas, Spark, TimescaleDB, Postgres, Kafka, Azure Cloud, Amazon Cloud, and NodeRed to develop and maintain big data solutions.

- Involved in various projects, including real-time message processing, end-to-end metrics analysis platforms, and building an IoT device analytics platform.
- Gained experience in designing, deploying, and managing big data solutions in cloud environments, resulting in improved efficiency and cost savings for the organization.
- Collaborated with cross-functional teams including Data Science, AQA, Backend, Frontend, and BI to deliver end-to-end big data solutions.
- Participated in interviewing and training junior specialists, contributing to the growth and development of the team.
I have a strong passion to improve business and operational processes by leveraging data, and creating reliable and scalable Big Data Platforms using technologies like Amazon Web Services (AWS), Spark, Scala, Airflow, and many more."
data engineer,"
BigData developer, with more than 10 years experience in e-commerce development. More than 3 years in Big data development, 7 as a QC Automation Engineer. Experienced in ETL pipelines implementation (Big Data stack: Apache Spark, Apache Hadoop, Apache Kafka + AWS).
  	Experienced in team leading, designing of automation frameworks, implementation and maintenance of complex ecosystems and QC frameworks based on Java stack.
 	Experienced in long term onsite-offshore team effort coordination with success.
"
data engineer,"Migrated to cloud the full scale multistack system for geospatial data processing in real time. The system consists from a bunch of ETL processes, both batch and streaming, Geoserver as backend, React app as frontend and Acumulo as distributed database, continuously debugging and upgraded the system.
Big Data Engineer at N-IX
February 2021
Data Engineering activities  - developing, maintaining and troubleshooting big data pipelines (ETL, ELT) using Spark, Hadoop, Airflow. 
Experinced with AWS cloud tech: EMR, EC2, S3, Kinesis, Lambda, S3, SES, Athena, Redshift.
Get known with Scala and JS, working with geospatial data.
Tech Stack:
Python/Scala/JS/Spark/Hadoop/Airflow/AWS/Git/Acumulo/Geomesa/Geoserver

Database Researcher at AllStarsIT 
October 2020 - Junuary 2021
Research new types of database vulnerabilities, applying best practices and official security policies to the on-prem and cloud databases (vSphere, AWS, Azure). Developing bash scripts for OS security checks, python parsers and other internal tools, sql-queries. DBA activities, tuning databases according to the best security practices. 
Bash/Python/SQL/DBA/Linux/Windows Server/AWS/Azure  

Data Analyst/Research Engineer  at Ring Ukraine  April 2018 - September 2020   
Implemented anomaly detection applications, developed web applications for Edge devices performance analytics and Data Collection Process. Dashboards developing. developed analytics applications for computer vision algorithms (ex: people/vehicle detection/tracking) based on Cloud platform AWS). Data Collection on real world conditions. Applying Machine Learning and Deep Learning techniques. 
Python/Linux/SQL/AWS Athena/Redshift/Scikit-learn/Pytorch   

Data Annotation Specialist  at Ring Ukraine  
October 2017 - April 2018   
Data Annotation and Data Quality control using internal tools. Data Entry activities. 
 Mac OS/JSON
Azure cloud"
data engineer,"Confluent Certified Developer for Apache Kafka, 2020
Big Data Engineer with more than 8 years of commercial experience with a key
focus on big data and streaming technologies (Apache Kafka, Apache Spark, Hadoop),
cloud (AWS), and containerization (Docker, Kubernetes). Expertise in working as a part
of distributed teams with customers from the US and EU. Participated in projects of such domains as banking, IOT, healthcare, oil, energy.

Worked with Data lakes, DWHs, ETL pipelines, Streaming processing. 

As engineer, my main responsibilities included:
 Requirements gathering
 Evaluation of available solutions on the market
 Software solutions design
 POC development
 Implementing batch & event-driven streaming data pipelines
 Development and support of data migration solutions.
Not interested in positions that involve lots of SQL-scripts development."
data engineer,"
Big Data Software Engineer 
- Creating and maintaining BigQuery views.
- Leading on sharding, partitioning, clustering optimization in BigQuery
- Denormalization of structures into BigQuery
- Creating dashboards into DataStudio
- Optimizing DataFlow batch/streaming jobs
- Developing ETL pipelines using Apache Airflow & Python
- Optimizing scheduled queries into BigQuery
- Configuring AirFlow DAGs
"
data engineer,"
Dec 2021 - present:
Big Data engineer, GlobalLogic
Project overview:
Data platform that provides a capability for hospitality industry businesses to get an
unified view of their customers.
Key technologies:
Microsoft Azure Cloud (ADLS2, Data Factory), Databricks, Delta Lake, Spark, Scala,
Elasticsearch

May 2021 - Dec 2021:
Big Data engineer, Luxoft
Project overview:
Data quality solution for corporate Big Data platform.
Key technologies:
Kafka, Apache Flink, AWS, Java 11, Python, Gitlab CI/CD, Kubernetes

June 2020 – May 2021
Big Data engineer, Global Logic
Project overview:
Datalake migration from on-premises environments to AWS.
Key technologies:
Airflow, AWS (EMR, S3, DynamoDB, Glue), Python

Oct 2019 – June 2020
Big Data engineer, EPAM Systems
Project overview:
Migration of data warehouses from on-premises environments to GCP.
Key technologies:
Google Dataflow, Airflow, Apache Beam, Google Cloud Storage,
BigQuery, Java, Avro, Scala, Apache Spark.

Aug 2019 - Oct 2019
Software engineer, Global Logic
Project overview:
Medical-domain project. Web application that allows patient to submit
observations and predict which type of treatment is the most suitable.
Key technologies:
Spring Boot, Spring Data, REST, MySQL, Java 9.

May 2018 - Aug 2019
Software engineer, Global Logic
Project overview:
Security analytic platform, analysis of incoming stream of event logs,
detection of potential threats. Comprises streaming part and micro-services.
Key technologies:
Apache Spark, Google Dataflow, Bigtable, Google Cloud Storage,
Kafka, Apache Airflow, NiFi, Spring Boot, Spring Data, Elasticsearch,
MySQL, Java, Python.

Feb 2015 - Apr 2018
Software engineer, Nix Solutions
Project overview:
Web project (medical domain).
Key technologies:
Spring, Hibernate, jsp, MSSql, BackboneJS, REST
"
data engineer,"·	Designed, developed, and migrated ETL and ELT processes in Snowflake.
·	Designed solutions for complex data processing from the AWS layer to Snowflake
Developing an application that drives End-to-End Global Supply Chain Performance with batch processing, sensitive data, and orchestration of all data processing. Creating Glue jobs. Developing Snowpark jobs. Troubleshooting and fixing complex defects. Firefighting during deployment processes
"
data engineer,"
Experienced Big Data Engineer/Data Engineer with MLOps skills. Using Scala, Python, Spark, and AWS. Have team-leading experience in a small DE team.
My current role is Big Data Engineer/MLOps.

On a current project, I am developing infrastructure for ML and NLP resume_classifier(deployment, API, feature engineering etc.) and data storage, also working on the integration of customers' internal services.

On a previous project, I developed an engineering part(API, data storage, spark jobs, ci/cd) for the NLP model.
Also last year I lead an engineering team and was a mentor for new team members. As a team leader, I planned and designed new features with customer stakeholders and my team successfully completed the development.
"
data engineer,"AWS Developer Certificate
Oracle Java Certificate
Hello! 

Are you looking for a qualified specialist?
I've quite a good experience working in different domains. On practice always try to follow best software design and development practices. Such approach allows to provide the best quality product and value for the customers.

Can't find skill or tech in my bio? 
Just write me to clarify.

Compensation looks like too high? :) 
Lets discuss it :) 

Data Engineer 
1. Prepare DataLake for Fin-tech customer. 
2. Build automatic medical adverse event detection. 
3. Bioinformatics project for efficient storing genetic data

Software Engineer
1. Web-page crawler for Anti-piracy project.
2. Platform for learning languages online.
3. CRM for finance/investment managers. 

Tech: Java/Scala/Python
Airflow
PostgreSQL, MongoDB, Snowflake
AWS EMR/Glue/ECS
Hadoop Ecosystem, Docker
SQL/NoSQL DBs
Spring Framework
JUnit/Spock Frameworks
Interesting long-running project with qualified team/management."
data engineer,"
* Hi I'm Big Data Enginner from Tbilisi, Georgia.
 Proficiency in  Oracle database Ms SQL, MySQL, PostgreSQL, Big Data technologies.

* I've strong functional knowledge of IT and I'm looking for 
  challenging projects to help you build something great.

* Hourly rate also depends on the complexity and duration of the job, so the rate may be higher or lower.

* Reporting an ETL Developer over 3 years of exprerience.

* PowerBI / DAX Developer with 2 years of experience.

* Big Data Enginner with 2 years of experience.

* Experiences with Banking for 4 Years.

* Experiences with telecommunications for 3 Years.
"
data engineer,"
I have almost 2 years of experience in Big Data and 3 years in software development. I'm a Big Data Engineer with knowledge of OOP principles, Java 11, Python, RDBMS and SQL. I have experience in developing modern lakehouse solutions. Also, I am deeply familiar
with the ETL process, worked with various sources - structured and semi-structured data for different purposes. Had an experience with AI, including ML&DL. Worked with modern cloud solutions providers such as GCP and Azure. Scrum and Kanban as process methodologies were used to manage product development and other knowledge work in projects where I was involved.
I’m result-oriented, embrace critical thinking and have a growth mindset
"
data engineer,"
I have been working in the field of Big Data for 3 years. Have hands-on experience with Spark, Hive, HDFS, YARN, Apache Livy, HBase, HiFi, Airflow, Docker, Kubernetes, MQTT, and more. Most often use Python, SQL, and Scala programming languages, also have a basis knowledge of C/C++, Go, Java, Kotlin, JS, and R. Have been using AWS, Digital Ocean, and Databricks cloud providers.
I have practical experience in all phases of product implementation: initiation, research, requirement definition, design, establishment of milestones, development, integration, testing and further support. Have experience in firefighting with a highly loaded data processing system.
I’m playing an active and constructive role within the team. After some time I become a good connoisseur of business model and business sphere, which allows me to share knowledge among team members.
For 1.5 years was working in a telecommunication company; have experience working with IoT events. Currently work in the Game Industry as a Big Data Engineer.
"
data engineer,"
I have experience as Python Developer, Machine Learning Engineer and Big Data Engineer. Work on startup project as a Python Developer for almost 1 year. Next 1 year worked as a freelancer Machine Learning Engineer. I was a part of team, we worked on different projects. Our biggest project was an Extract Summarization for lawyer documents. Worked as a Big Data Engineer in Grid Dynamics on big commercial project for 1 year. Now a Big Data Engineer in Trinetix.
"
data engineer,"
I have experience in developing Big Data infrastructues, troubleshooting the errors. I got the experience of leading a small team of developers. Gain the two AWS certifications. I'm passionate to learn new technologies and to solve engineering problems.
"
data engineer,"
I have experience with Big Data. I participated in the creation of the new data process and the improvement of existing ones, as well as in the bug fix. I have experience in organizing teamwork.  Punctual, polite, plodding, attentive to details, who likes to work in a team, help teammates and learn new technologies.
"
data engineer,"Detailed list of my achievements available in resume.
Mainly:
- developed complex systems from scratch
- broke existing monolithic apps to microservices
- proposed changes that led to revenue increase
- leaded a team
I have extensive experience of work in startups with frequent requirement changes. Mostly my experience related to Big Data and Full Stack development areas.
Main technologies in my tech stack are: Java + Spring, BigData (Spark, Flink, Hadoop), Kafka, Vert.x
I have experience of team leadership.
More details about my achievements on various projects are available in my resume.
I want to work on interesting project with new technologies (at least > Java 8). If it's migration from legacy to new technologies I might consider this.
I prefer to work with product companies.
I don't like a lot of bureaucracy."
data engineer,"Microsoft Certified: Azure Fundamentals
Microsoft Certified: Azure Data Fundamentals
I'm a junior big data engineer with a great interest in the data world. I started my career as a python engineer but quickly became fascinated with big data and clouds, so I started exploring them. 

I have successfully passed two Azure certifications (as well as multiple online courses) and I’m preparing for the next one: “Data Engineering on Microsoft Azure”. After that, I’m planning to pass “Databricks Lakehouse Fundamentals”. I’m constantly improving my skills in Python, SQL, and clouds.
I am interested in learning Python, SQL, clouds for software engineering development and data analysis."
data engineer,"
In my current role as a data engineer, I've been involved in a variety of projects and tasks that revolve around data processing and pipeline development.

I've also been responsible for designing and maintaining ETL (Extract, Transform, Load) pipelines, primarily using Python and Apache Spark, to process large volumes of data from various sources and load it into our data warehouse. This has involved optimizing the performance of Spark jobs and ensuring data quality and consistency.
In my new data engineering role, I have several expectations:

1. Challenging projects.
2. Cutting-edge technologies.
3. Collaborative environment.
4. Professional growth.
5. Clear goals and expectations.

Overall, I'm excited to bring my data engineering skills and enthusiasm to my new role, and I look forward to contributing to the success of the organization while advancing my own career in the field of data engineering."
data engineer,"In current position I worked on project called Notification. In that project We find interests of the users by their activities and used that interest for advertising. And another thing that I am proud of is that during simple analysis of logs I found that one of the clients have interest on other our product, after that our product manager prepare additional offer to that client, and our company made extra money for that quarter.
**Jan 2022 - present Epam Data Analyst**
-Analyzing on SQL logs
- Monitoring tables
-Refactoring and engineering data based on cloud platform called palantir platform (Swiss Reinsurance Company)
-Analyzing and engineering data based on cloud platforms like snowflake, AWS and Teradata for Data Analytics and Engineering team (Nike)

**Nov 2020 - Jan 2022   BigData Analyst in telecom company. 
**
Projects:
- Fraud detection algorithm
- Business Notification
- Client support
- Verification/Scoring

**Nov 2017 - Nov 2020 Self Employed 
**
**June 2016 - Aug 2017 Teacher of programming and robotics classes**
In new position I am expecting interesting analytical tasks, like 
- engineering cloud platforms
- analyzing root sources and table structure"
data engineer,"- Optimized Spark pipelines that led to x3 time reduction.
- Improved data ingestion logic that allowed the user to reload data hourly instead of daily.
- Refactored architecture of CDC pipelines that allowed schema evolution.
- Led the migration from NiFi to Airflow that reduced data indigestion infra costs by x15 times.
- Led the migration from NiFi to Airflow that reduced data indigestion infra costs by x15 times.
- Silver and Gold layer processing with 700+ TB of data.
- Dropshipping Framework for top worldwide markets (Amazon, eBay, Fnac, Rakuten)
Product, complex problems, modern technologies"
data engineer,"Last achievements I’m proud of:
- designed and drove a migration of large business-critical system
- passed GCP Professional Data Engineer certification
Most important projects:
1. Telecom - contributed into in-memory data processing system: Java, Kafka, Ignite
2. Sports - developed data warehousing system based on cassandra, spark streaming(scala) and kafka
3. Financial technologies - developed ETL pipelines for data marts based on GCP PubSub, Dataflow and BigQuery

Current responsibilities:
Driving migration of large system from MR stack to Spark from design doc to working prototype. Stack: scala, spark, hive
Perfect opportunity:
It would be perfect to find an interesting big data project with opportunities to grow in Data Science area."
data engineer,"A lot of successful projects, up to production deployment, using most powerful development practices and Agile methodology.
Programming languages: Python, Java, SQL
Relational databases: Oracle, MSSQL, MySQL, PostgreSQL, Sybase, Teradata, SAP HANA, Greenplum
Big data/NoSQL: Hadoop, Hive, HBase, MongoDB, Redshift
Cloud: AWS(EC2, RDS, S3)
ETL: Apache Airflow, Informatica
Version control tools: Git, GitHub, Bitbucket

Responsibility areas: Scope defining, requirements definition, database design, data analysis, ETL pipelines, data migration, data integration, DWH design
I'm Interesting to develop complex products using modern cloud technologies, AWS is preferable. It would be great to work within the team of talented developers with high tech/soft skills level."
data engineer,"7+ years experience in software and data development on various projects
Hands on infrastructure and development experience in building distributed and scalable solutions
Steaming and batching processing data
Data orchestration experience using Airflow
Kubernetes Spark and Airflow operators
Expert in Python
Good experience in Scala
Experience with building ETL
Experience in Data Science (Image recognition using OpenCV)
DevOps practical experience (Ansible, K8s)
Participant in opensource projects (Airflow Spark Operator on top of K8s)
- Python(6+ years)
- Scala(1+ year)
- Spark, Hadoop, HDFS, Hive, Presto, Airflow
- Kafka/RabbitMQ
- NoSQL
- ELK
- RDBMS
- BI(Pentaho, Jasper)
- Docker, Docker Swarm, Kubernetes, Ansible
- AWS/GCP
take a job on project to build scalable platform;
project from scratch is a big plus;

Don't like working with REST API, web frameworks and extracting data using just native libraries of any programing languages."
data engineer,"Created system for keeping databases up to date:
-scrapping open web sources;
- data cleaning and transforming;
- upload data

Automated flow for data extraction and insights creation:
- extraction from DB;
- data processing with business requirements;
- sending to requester
Retail project:
Creation of ETL pipelines for Data Lake.
Performance improvements for Spark jobs.
Mobile advertising project:
Creation of web scrapping tools, data cleaning and verification, ETL processes.Data processing with decision trees and multiprocessing. Creation of PySpark applications for huge data processing. Producing data insights for more business value.
"
data engineer,"- Designed architecture for the Big Data project that was successfully deployed and working on production, that helps clients to explore telemetric data and they can see possible ways on how to improve performance of some specific part of work to spend less money and at the same time earn more
- Improved performance of SQL queries execution on 30%
- Implemented a new async service to enrich information about entities that were added to the production
- Implemented a script to transfer specific data from one environment to another to avoid errors that can occur due to constraints or triggers on DB. This allows us to quickly start working with the system in a new environment, with the data that was set up in the old environment
Software Big Data Engineer with 3 years in software development. Proficient in the back-end development of applications based on microservice architecture. Provided performance improvements of both API and Database (PostgreSQL). Worked with documentation like software requirements specifications, tech design, and implementation notes for project features. I'm working with the frameworks and technologies used in Big Data: HDFS, MapReduce, PySpark, Hive, Apache Airflow.
I have experience in designing the architecture for Big Data projects, most of the time I use AWS to deploy the system. 
Currently, I'm team lead for the Big Data engineers team.
"
data engineer,"
Software developer with over 5 years of experience of the design and development of products.
Main experience with Python, PySpark, Azure, AWS, GIT in Telecom, Web, and Financials areas. Involved in more than 9 projects of various scales and size, as a part of a team with 5-10 members.
Experience working according to the Scrum process as a part of XFT (cross functional team) structure.
Additionally, learn new technologies such as GraphQL, Redis and try to always be familiar with newness. About myself: flexible/adaptable, acting as a team member, ability to learn fast, competitive.
Technical skills
 Programming languages: Python, JavaScript, HTML, CSS, PLEX, PLEX-M.
 Frameworks: Django, Flask, Bootstrap, DRF, GraphQL.
 Platforms: Mac OS X, Windows XP, Linux.
 Development tools: Spark, Hive, Hadoop, Airflow, ADF, Celery, Redis, RabbitMQ, SnowFlake,
SnowPark.
 Core libraries: Pandas, NumPy, Sklearn.
 CI/CD: GitLab, GitHub actions
 Databases: MySQL, PostgreSQL, MongoDB, Redis, Redshift, Aurora.
 Issue Tracking: Jira, Azure DevOps.
 Cloud platforms: AWS, Azure
"
data engineer,"
The vast majority of my experience in the speciality was gained on university courses, that is the place where mistakes and skills were gained. I have used Python as the main language, worked with relational and NoSQL databases, asynchronous libraries such as aiohttp, asyncpg, aiogram. Also used numpy, pandas, scipy, plotly, Jupyter Notebooks for data analysis, optimization methods studying and visualisation of results. I mostly use Docker for apps and have successfully deployed some of the apps on Heroku. Studied parallel computations (mostly on OpenMP and QT). Participated in programming olympiads. Also, I have a Telegram Gmail Bot as a pet project, which uses Google API and pub/sub notifications to forward emails from Gmail to Telegram chat.
"
data engineer,"The two startups I've worked for are now unicorns with an overall evaluation of ~10B.
T-Shaped Product Engineer having 5 years of Machine Learning, DataScience, and 12 years of Software Engineering. Took part in a variety of Data Science projects for e-commerce and advertising companies. My work covered both Engineering and Product parts. On the engineering side, I did R&Ds, prepared system design specs with recommendations, and of cause built solutions. On the Product side I worked with stakeholders to formalize requirements and present results.
I'm looking for rather engineering kind of work such as MLOps, Machine Learning, Big Data, then related to the theoretical part of DS. Companies with strong Data Science/Engineering cultures or solid revenue streams are preferred.     
Sorry, I'm not considering any Data Science or Analytics positions any longer"
data engineer,"
Various projects at previous clients in Luxembourg includes:
- Creating in-house framework to transition in different curated stage of the company datalake. Reducing license costs and dependency to exterior data provider.Reduced manpower through automatization and deployment of open-source technologies (Apache Trino, Apache Spark, HDFS, MariaDB ...).
-Building a data-vault to ensure data consistency and adapt to GDPR compliance. Design the layout and define keys relationships between entities.
-Ingest massive amount of critical data on a daily basis, responsible for pipeline deployment and maintenance (Apache Airflow) and curation related tasks (Python/Pyspark). Testing deployment on DevOps technologies (Kubernetes, Docker, Kind)
Part-time teaching at the ESGF-Finance as a Big Data Professor for Master 2 Students and at ECE for DevOps modules (Kubernetes, Docker, Ansible, Vagrant...).
"
data engineer,"Getting aws certified and surviving 3 hour-long retros)
Worked in a huge team as a developer whose duty was to implement scalable data pipelines, which extract, cleanse, transform various types of data in the data lake on AWS. Besides, created a bunch of API’s for data consumers and other teams to approach different parts of the lake. At the moment, I’m looking forward to be a part of cloud-based project, preferably AWS, but am open to other similar opportunities.
I’d love to work in a nice and friendly team. Ability to grow is a must, having some learning budget (for e.g. for certifications/courses) is a plus."
data engineer,"Certified cloud engineer (AWS Data Analytics Specialty and Azure Data Engineer Associate).
Worked in an international team as a key developer whose duty was to implement highly scalable data pipelines, which produce and extract various types of data. Besides, integrated the solution with Azure Data Factory orchestration. Have commercial experience with Hadoop, Kafka and Spark (java and scala). Currently, I’m looking forward to be a part of AWS-based project, but am open to other similar opportunities.
I want to improve my skills in Data Engineering field, especially in the cloud."
data engineer,"Google Cloud Certified Professiondal Data Engineer
Solid experience with Spark, Hive, Hadoop(HDP), Hbase, Redshift, Firehose, Lambda.
Wrote spark ETL jobs, optimized sql queries, wrote tests.
"
data engineer,"
C#, F#, .NET, ASP.NET, MVC, SQL, Entity, Framework, Web, API, HTML, 5, CSS, JavaScript, Architecture, MongoDB
Ability to grow professionaly"
data engineer,"
13+ years of software engineering experience

Interested in: Big Data, high load, distributed systems, stream processing.

Primary skills: Spark, Scala, Kafka, Cassandra.
"
data engineer,"Languages skills: English - Upper Intermediate (speaking and writing). French – conversational. Ukrainian -
native language.
Technical skills:
 SAS: SAS Management Console, SAS Information Map Studio, SAS Real-Time Decision Manager, SAS
Marketing Automation (SAS MA), SAS DIS (beginner);
 ETL with SAS;
 SQL: Oracle SQL Developer; MS SQL Server(beginner);
 Python(beginner);
 Kannel (SMPP);
 Kafka Console Utilities (beginner);
 Mongo DB (beginner);
 Evam: Evam Designer, Evam Console;
 Linux CMD, Linux Server administration;
 WinScp/Putty;
 Grafana/Graphite;
 Tableau;
 MS Office (Word, Excel, Power Point, Visio, Teams, Access) ;
 JIRA/ Knowledgebase;
 Outlook;
 IBM Lotus;
 Mathlab(beginner);
 Mathcad(beginner);
 Altium designer (beginner);
 SDTM; ADaM (beginner)
Besides that, i have experience completing technical documentation, negotiating with Business or CRM department, Linux and 
database administrators, stakeholders.
Passed training courses:
 freecodecamp.org (HTML courses),
 moonexcel.com.ua (SQL Lessons)
WORK EXPERIENCE
Big Data Engineer
Jan 2021 – Dec 2022
• Administrating of a real-time marketing communications system (more than 50 nodes) based on 
SAS Real-Time Decision Manager & EVAM products.
• Data lifecycle management (processes for obtaining, hosting, storing, distributing, migrating, archiving and
deleting data).
• Solving business problems when planning, testing and conducting marketing campaigns.
• Ensuring correct work with third parties (external or internal systems) - development of methods and
regulations in order to process large volumes of data (data streams) from heterogeneous data sources

Associate Statistical Programmer Intern
Mar 2020 – Jun 2020
• Created data tables and graphs according to the required specification for clinical, pharmacokinetic and
statistical reports.
• Programmed on SAS and related languages (e.g. SQL)to present and analyze clinical trial data.
"
data engineer,"Publications:
• Multistage SVR-RBF-Based Model for Heart Rate Prediction of Individuals
• SGD-Based Cascade Scheme for Higher Degrees Wiener Polynomial Approximation of Large Biomedical Datasets

Completed courses:
• Google Cloud Skill Boost:
  - App Deployment, Debugging, and Performance
  - Securing and Integrating Components of your Application
  - Google Cloud Fundamentals: Core Infrastructure
• Data Warehouse and Business Intelligence
• SQL for Data Science
• Using Git for Distributed Development
• Design Patterns

Hackathons:
• BEST Hackathon
• AI Hackathon
Bachelor of Computer Science in Artificial Intelligence with more than two years of experience in Python, including 1.5 years in the Data Engineering field.
I also worked with Clouds and Big Data technologies such as Spark, Kafka, Hadoop, etc. I have completed a wide range of courses in my specialization and keep staying motivated to improve myself constantly.
Can be either a good team player or individual contributor and become an initiative leader if necessary.
Challenging position and opportunity to influence and deep dive into product development.
Potentially interested in Data Science and ML-oriented projects.

Desired stack: Python + GCP/Azure + (Big) Data Engineering

Open to new technologies if the learning environment is available. Intent getting cloud certification."
data engineer,"I took an active role in designing and building complex web platforms based on microservice architecture. Also, I build highly intensive streaming data pipelines and complex batch processes. I gained two GCP certifications: GCP Professional Cloud Architect and Data Engineer.
I have experience working as a web backend developer with Python Flask, FastAPI, Sanic, Sqlalchemy PostgreSQL, and others. Currently, I'm working as a BigData Engineer with the following technologies GCP (BigQuery, Dataflow, Pub/Sub, Big table, Composer, GCS), PySpark, Apache Beam, Apache Airflow, and others.
"
data engineer,"
Big Data Engineer with 6 years of experience in software development of high load distributed systems. My current responsibilities includes design and implementation of the new data pipelines for mobility intelligence platform, developing data integrity tools, work closely with data science team, mentoring newcomers etc. Current stack: Scala, Spark, Delta Lake, Airflow, Kafka, DynamoDB, Python, S3, EMR, Athena.
Would prefer to work for a product or outstaff company"
data engineer,"Create and maintain big data solutions using Spark, Spark streaming, Kafka, Hive, Solr. I have some pilot projects with technologies such Kafka, Cassandra, Scala. I have some works in machine learning fields (random forests, logistic regression, sentimental analysis and etc). Work with twitter streaming (Spark streaming, sentimental analysis).
Big data: Hadoop, Spark, Flume, Kafka, Impala, Cassandra, Hive, Solr, Zookeeper, Ambari server
languages: Scala, Python, Golang
Development platforms: Linux, Windows.
Development tools: InteliJ.
Framework: Play
Database: MySQL, PostgreSQL
NoSQL: Redis, Memcache, MongoDB
Other: Google API, Elasticsearch, Sphinx, Kibana, Logstash, Filebeat, Ether Blockchain, Bitcoin Blockchain
I am looking for any big data and data science opportunities in general and Spark+Scala opportunities in partial."
data engineer,"
Scala, Java,  Python

Apache Flink(Flink SQL, Datastream)

Apache Kafka using both Java and Scala APIs (Confluent platform,  Schema registry, KSQL, Streams API, Processor API,Kafka Connect)

Apache Spark (RDD,Datasets, Dataframes, SQL, Streaming,Structured Streaming)

Apache Airflow (DataProc, Spark, Python, BigQuery and other operators)

Apache Hadoop (Google cloud Dataproc, Amazon EMR, HDP 2.5)
Athena(Presto), Glue, Hive
Apache Beam (Java and Scala API, Dataflow, Flink, Scio)

AWS, Google Cloud platforms
Akka (Actors, Http, Play)
Google Pub/Sub, AWS MSK, RabbitMQ 
Apache Avro, Parquet
DB: Google BigQuery, 
BigTable(Hbase), Apache Phoenix,  MySQL, MS SQL Server,

Development tools: IntelliJ IDEA, Git, Maven, SBT, Docker/ Docker Compose
Agile development approach, Confluence, Jira
A product company, stock options plan, real-time, high load"
data engineer,"Was involved to developing in all parts of project. Perpetually took part in fixing bugs and architecting new functionality.
ETL Developer, Software Developer

Developing ETL processes in PDI. Creating reports for Business. Creating dashboards in Tableau. Implementing and ongoing support our application ClevaDesk (frontend and backend). Creating Excel and Word templates. Creating BPMN diagrams and logic for approve.

Technology: Pentaho Data Integration, SQL(PostgreSQL, MariaDB), DB Schema, JS, Python, Lua, Domino Notes Formula, ClevaDesk, BPMN, Tableau
__________________________________________________
HCM Developer (Oracle Cloud)

Implementing Oracle Cloud HCM. Ongoing support for existing Oracle HCM clients. Processes analysis and customization of the system according to customer’s needs. Development and producing high level reports according to the technical design. OBIEE\OTBI high-level knowledge.  Developing ETL processes in Oracle HCM.

Technology: Oracle Cloud HCM, Oracle Reports , Fast Formula, XML / XPath / XSLT, SQL(PL SQL), Java, dot-Project, Redmine, BI Publisher
I'm looking for an interesting project in which I can use all my skills. I would admire to be a part of result-oriented professional team."
data engineer,"Key responsibilities and results:
I manage Team progress (tickets prioretisation, review blockers, thinking proactively to prevent issues)
I manage documentation process and BI workflow and change it if necessary to improve client expectations and visibility
I have managed process for Demo Site creation for one of our products (180 days of daily data, 3 semi-additive data sets, 10 data sets total, complex joins between different data sets) from zero till the site is ready
I work with client's requests and delivery their requests with suitable quality to Production asap
I continuously work with different type of risks and I have redused count of incidents up to 90% in my responsibility area
I create shoort-term plans for my Team (3-6 months) and create tasks based on it
I plan tasks for BI engineers to make sure that we have enough capacity to complete required tasks
I provide teams meetings facilitation for develop best team desicions
I have an experience with Job Interviews and assessment
I have mentored BI engineers and BI interns (~20 BI's for all time)
I work on improving Cross Team Communications in my responsibility area
I have created training tasks for junior BI's and trainees
I have worked on ETL processes and reduced ETL execution time for several clients
I have built new complicated logic for clients in new and existing summary and detail reports
I have 4.5 years of experience as a BI developer and 2 years of experience as a BI Tech/Team Lead
Pleviously I have worked as a testing engineer for 10 years (non-IT area, testing for compliance EU safety and EMC directives ). I've completed testing equipment and supported all test methods in my responsibility for ISO/IEC requirements

I am interested in developing my career as a Tech/Team Lead
Currently learn AWS (Data Analytics speciality) and planning to finish my certification at the nearest time

Stack: MSSQL, SSAS, SSIS, Vertica, Mondrian, Python
I am ready for new challenges and new types of tasks. I am investing a lot of time and money into my skills. I have completed several long-term courses and trainings for teamleads and apply it on my current job. I will apply it on my new position with a big pleasure"
data engineer,"Reduced infrastructure costs by $150K per year.
Grew engineering team located in 3 offices to 15 people.
I am a Software Architect with extensive experience in Back-end development and Data Engineering.

Projects I have delivered over the last several years:
- Migrated one of Europe's biggest tech startups from API-based cross-service communication and batch data processing to event-driven communication and real-time data streaming. Built company's data infrastructure from the ground up - event messaging, stream-processing, data warehousing, analytics, data replication, data lake. Improved and optimized company's infrastructure and reduced its cost by around $150K per year
- Implemented several cloud-native services for one of the public cloud providers: software architecture, security, storage, high availability.
- Open-source software contributor
- Author of several articles on western blogs

I have experience in end-to-end software development lifecycle management: requirements collection, software design, software implementation, provisioning of infrastructure, software deployment and maintenance. 

Fields I am interested in: designing and developing scalable back-end services, stream processing, real-time data.
I am looking for Director of Engineering / Principal Engineer (Back-end or Data) role in a company that is working on its product"
data engineer,"
Consulting in Microsoft’s projects. Design and implement ETL solutions between multiple datasets, on-premises servers and Azure Synapse using technologies such as Azure Synapse Studio, Azure Data Factory, Azure Data Lake Storage, Self-Hosted IR. Designing CD / CI mechanisms in operation on Azure DevOps and Git. Migration of all sized (1M – 1B rows) Oracle DBMS tables to Azure Synapse using SSMS, Python and Parquet files. Recreation of Oracle views with complex logic by converting Oracle SQL dialect/CTE/built-in functions to T-SQL. Recreation of Oracle ETL tool (Oracle Data Integrator) packages, mappings, procedures in Azure Synapse. Integration of several data sources such as Vertica, Salesforce, Oracle OBI, Oracle PMDB, SFTP Server, On-premises SQL Server Migration SSAS to Azure Analysis Services. Repointing data source from Oracle to Azure synapse. Design LogicApp workflows for processing AAS resume_classifier. Modification of existing Databricks notebooks using PySpark. Writing CSV files from AWS S3 to Snowflake using delta lake architecture.

Certifications: 
Microsoft Certified Azure Data Engineer Associate
Microsoft Certified Professional
98-364: Microsoft Database Fundamentals
70-761: Querying Data with Transact-SQL
DP -900: Azure Data Fundamentals
DP - 203: Azure  Data Engineering
Senior Azure Data Engineer"
data engineer,"English: C2 Upper-Advanced (CPE) 

Certifications:
SEI Software Architecture Professional 
AWS Certified Data Analytics Specialty
Databricks Certified Associate Developer for Apache Spark 3.0
Microsoft Certified: DevOps Engineer Expert
Microsoft Certified: Azure Solutions Architect Expert
Microsoft Azure Data Engineer Associate
Microsoft Azure Data Scientist Associate
Microsoft Certified: Azure IoT Developer Specialty
Google Cloud Platform - Professional Data Engineer
Google Cloud Platform - Professional Machine Learning Engineer
Oracle Certified Professional Java SE 11 Developer
Oracle Certified Associate Java SE 8 Programmer
Oracle Advanced PL/SQL Developer Certified Professional
Oracle Database 11g Performance Tuning Expert
Oracle Cloud Infrastructure 2019 Certified Architect Professional
Oracle Autonomous Database Cloud 2019 Certified Specialist
Oracle Cloud Infrastructure Developer 2020 Certified Associate
Oracle Application Server 10g Administrator Certified Professional
Oracle Database 9i/10g/11g Certified Professional
Oracle Database 10g Real Application Clusters Administrator Expert
The Linux Foundation Certified Kubernetes Application Developer
The Linux Foundation Certified Kubernetes Administrator
The Linux Foundation Certified System Administrator
Scrum Alliance Certified ScrumMaster
ITIL Foundation
Candidate of Sciences, 2010, comparable to the Doctor of Philosophy (Ph.D.)
speciality ""System analysis and optimum solutions theory""
thesis ""Multivariate risk resume_classifier in enterprise decision support systems""

Master of Computer Science, Institute for Applied System Analysis, NTUU ""KPI""
Senior Data Engineer at Firebolt. 

PhD in System Analysis (probability and statistics).

Before:
* Development Team Lead. Databricks. 
* DevOps in a Data Science Delivery Platform that is based on Hadoop ecosystem deployed in Azure Cloud: Spark, HBase, Hive, DataLake, DataFactory. 
* Lead DB Developer/IT Architect in a Cloud Engineering team. Clouds: Amazon AWS, Microsoft Azure. DBMS: Oracle, Redshift. 
* Development DBA of 80TB+ databases on Exadata and sharded on AWS RDS.

* 11 years as a Senior DBA. Administered about 40 instances of Oracle Database (8/8i/9i/10g/11g/12c), application servers and CloudControl. Platfroms: x86, Power7, zSeries.
OSs: AIX, RHEL/SLES, Windows Server, zLinux 
High Availability solutions: Oracle Real Application Cluster,  ASM, DataGuard, IBM HACMP. 
Backup and Recovery: IBM TSM, Oracle Secure Backup, RMAN, DataPump. 

* 2 years as a developer: PL/SQL, APEX, С++ in pattern recognition.
"
data engineer,"- open source perfectionist
- committed in Python itself
- maintaining ~20 open source projects
- led a team of 30+ members
- 10K+ r/s systems
- 200M+ downloads apps
- excellent communication skills
- 15+ years in the industry
I've been in the industry for 15+ years, I have had various experience since 2006 (except banking software)

Started with PHP websites and e-commerce
Switched to ISP billing systems with Linux and Python
Spent a couple of years as a freelancer (from NodeJS to Golang)
Was working in the biggest CIS gaming/gambling companies with all underlying aspects there
Worked remotely for small startups, leading a small team
Led series A startup from R&D prototype to 30+ developers
Excellent software engineering experience for enterprise automotive
Architect role in world-known venture AI startup with 200M downloads 

As my next step, I would like to work with top-notch modern technologies for all possible kinds of data processing
I am looking for tech-related vacancies, closer to the development, less bureaucracy, new challenging problems and nonstandard projects.

As for me, startup culture and a healthy atmosphere are the best way to go, especially for R&D!

I am interested in crawling, API, data processing, analytics, pipelines, real-time applications, DWH, ETL and of course AI/ML."
data engineer,"
Customer Support Engineer (BS SQL Engineer)

Experience in the migration of data from Excel, Flat file and XML to MS SQL Server by using conversions and DTS utility
Experience with Power BI 
Developing ad-hoc queries for end users
Creating and maintaining jobs
Assisting in transferring databases, including TempDB to new servers 
Modify SQL Server stored procedures using SQL
Verifying XML
Proficient in extracting, transforming and loading of data using SQL Server DTS and SSIS
Day to Day included manipulation of XML, CSV and XSL files
Managed SQL Server Agent JobsCreation of complex database queries
Supporting SQL Server database for our Software as a Service platform
Create detailed documentation, including diagrams of database infrastructure
Monitor, maintain database systems and troubleshoot problems that may arise
Maintain data integrity and security (manage roles and permissions of database users)
Proactive housekeeping/archiving and shrinking of databases
Trained business and end users to create ad-hoc queries that decreased dependency on IT

Experience: Chrome dev tool · PowerShell (basic Knowlege) · Microsoft Power BI · Microsoft SQL Server · MultiDimensional eXpres
"
data engineer,"One of the dashboard I built in July last year was valued at $25,000 as of December 2021. The dashboard tracks transactions across all our platforms. It was also said to have saved the company a little over $70,000 between September and December 2021. The dashboard has also helped in engaging clients better which turned out to help increase profit for the year by keeping and acquiring new clients. Currently working on a similar dashboard but with enhanced features and performance.
I have worked on data migration from Spotfire to PowerBI, Tableau to Power BI as well as build new reports from scratch. I have created more than 100 dashboards for my current company eTranzact where I work as a BI developer/Business intelligence Analyst. I have worked on SQL, Power BI, Power query, Power Pivot, SSRS, SSIS, Snowflakes, MongoDB, DBeaver, Azure, SSMS, SSAS, Power Apps, APIs, ETL tools and Python (basic) and also taking up classes to end 2nd week in June. I web scrape for data, also worked on data across different sector (Agriculture, Financial, health care, telecommunications, retail and manufacturing). Between 2019 and 14th of January 2022 I have created over 307 dashboards. I’m willing to transition into data engineering as I am fascinated about how my data is been housed with more focus on improving data quality and integrity. I started a paid course on the 19th of December 2021.
"
data engineer,"
SQL 5years, PL\SQL, MS SQL. Python, HTML/CSS basics. 
Designing and developing dynamic dashboards, reports, procedures. Data analysis, data cleaning.
Professional growth, challenging tasks."
data engineer,"practical experience in verification and replacement PC devices; programming language: Delphі; modeling language:  UML; database language: SQL, T-SQL; Microsoft Office;
practical experience in AllFusion Modeling Suit: AllFusion Process Modeler (BPwin); AllFusion ERwin Data Modeler (ERwin), MathCad, VBA, Visual Studio 2005/ 2008, Statistica packet, NetCracker;
experience as administrator of tourism speleology base and speleology excursion guide;
2012-2019 chief hostel administrator in Lviv: manage social network pages, the site creation/maintenance with cms Drupal.
February – December 2014 – CallTech Outsourcing – System administrator in the research company, the duty was to set up VoIP communication on worker's computers, network troubleshooting, remote technical support
Windows 7/XP/2003/2000 / Linux installing, administration, setting, troubleshooting, MS Virtual PC, VMware, network troubleshooting, set up VoIP communication.
August 2019 - now StartUs Insights -  Innovation Researcher - manage and maintain data sets on the European startup; manage analytics and research projects that identify emerging technologies, startups and trends, clean and update data accumulated in the databases (e.g. Excel). Identify relevant target companies and contacts (able to execute effective account exploration and mining tactics); execute Internet research to enrich our datasets; maintain and manage all information, ensuring all communication activities are entered into the system, information is up to date and accurate; work collaboratively with Sales and Marketing to develop and grow the pipeline
"
data engineer,"
Designed and migrated QlikSense workflows using KNIME with savings up to $1049 for each month.  
	Analysed datasets and provided demo presentations explaining workflows to the customers.
	Organised knowledge base sessions for non-technical users and supervised new customer trainees.  
	Proposed and implemented open source ETL tool that generated $5195 in savings per user in 2021.  
	Provided free KNIME training to the organisation with savings up to $89.99 per person        - Implemented fraud prevention rulesets and configured 7 main transaction channels in antifraud tool IBM Safer Payment (IRIS) during 8 months.
	Analysed and developed antifraud parameters in IRIS for Risk and Analysis team. Detected fraudulent transactions increased from 73.1% (2021) to 93.9% 
	Analysed and prepared financial datasets to identify fraud attacks. 
	Controlled correctness/flow of data in DWH and in IBM Safer Payment (IRIS).
BeCloud, Almaty, Kazakhstan
Data Engineer                                                                                                                           	Used Pentaho Data Integration/Kettle to design all ETL processes to extract data from various sources including live system and external files, cleanse and then load the data into the target data warehouse.
	Created transformations that involve configuring the different functionalities of ETL Tools (Pentaho and KNIME). 
	Developed complex SQL queries to generate re
"
data engineer,"
Experience with Postgres database. Writing simple scrapers using bs4 library. Other kinds of scripts for reducing manual work with help of basic python libraries. Reading and understanding documentation for working with external API. Excel spreadsheets. Working with remote servers
"
data engineer,"• Build Pipeline systems with unique UI jobs monitoring , high loaded systems;
• Build Data transfers with detailed user's behaviour data from GA/Firebase to Bigquery;• Establish reporting schema and main principles. • Develop and monitor KPI’s calculation schema, logic and provide reports (in Tableau and Excel). • Apply advanced analytics and machine learning algorithms to reveal insights and build forecasts (with Python and Excel). • Develop statistical resume_classifier to measure company performance (Tableau). • Product strategy development • Coordinate of work of development team Tools used: Python, Tableau, SQL, MySQL,  AWS, POstgresql.
- Build analytics systems and data pipelines from zero, using Python Flask  as back end  and huge variants of libraries for data visualization and analysis;
- Develop a unique system of influencers' marketing analysis from own idea to currently working produce. 
 - Programming languages: Python
- Statistic languages: Python (middle+) ;
- Machine learning; - Neural networks (middle): - DB: Mysql (advanced), PostgresSQL, Amazon Redshift; - BI - Tableau (advanced)
- Experience with AWS, ETL construnction - Machine learning (K-means , C-means clustering, logistic regression, predictive analytics). -Project management; - Grow up the project from idea to 500 000 active users and retention of day 30 around 50 % - Establish CRM system from 0 to production - Establish analytics department from 0
Looking for project of team, who results oritnted, have a huge development speed without lasy persons , who want to launch a lot of AB tests, take passion from insights and orient on client and monetizaion of product"
data engineer,"- 3-times growth of e-commerce website traffic and reaching the #1 Google search results in the niche;
- Successful work with multiregional projects with millions of pages in the Google Search index;
- Numerous successful withdrawal from website traffic fall after Google Search updates;
- Experience in full-cycle SEO analytics.
Experienced Data Analyst / SEO Specialist. Worked with various Ukrainian, Russian, and international projects.
Conduct marketing analytics for various projects from all over the world.
Have advanced knowledge of R, JavaScript, HTML A-parser, Zennoposter. 
Experience of work with CSS, Python, SQL, Gephi, Git
- Professional development perspectives;
- Working on the technical aspects of website optimization;
- Team of qualified specialists;
- Flexible working schedule;
- Decent Salary."
data engineer,"
Car Safety Framework: detection of driver's drowsiness, smoking behavior, using smartphone while driving, mask wearing detection.

Microarray analysis. Testing of ML algorithms performance on choosing feature selection to propose a solution to cancer cure.
"
data engineer,"
Currently, I am working on a full-time basis as an ‘Oracle Database Analyst’ at Unisys supporting projects of European Central Bank in Budapest, Hungary:
• Maintained call center database by collecting and recording
information.
• Provided basic troubleshooting of database related issues for the
client.
• Initial support and classification, correlating events and
performing initial fault diagnostic for European Central Bank.
• Provided L1/L2 support and escalate tickets to L3 as needed

Before Unisys, I worked at SAP Hungary as an Application Support intern:
• Managed all communication with customers and partners
required to resolve support tickets. 
• Resolved support tickets within the limits of contractual Service
Level Agreements (SLA). 
• Coordinated and tracked the escalation of tickets


Being very eager and enthusiastic to gain practical and applied programming experience during my Bachelor's studies, and to continue my work experience in database development, I obtained part-time employment, through a competitive process, at GT Solutions as an Oracle SQL Developer supporting projects at the Central Bank of Azerbaijan; GT Solutions is one of the leading IT companies in Azerbaijan. I have also gained the Associate Oracle Database SQL certification from the Oracle Company:
• Perform data analysis and validation checks on corporate events.
• Prepared of daily and monthly data at Central Bank of Azerbaijan
in Oracle Business Intelligence, Oracle APEX using Oracle Data
Integration tool.
• Optimized SQL Queries in order to achieve better processing time.
• Tracked the designing, testing, support and debugging of new and
existing ETL & reporting processes.

Sincerely,
Javid Mahmudov
"
data engineer,"AWS Certified Solutions Architect - Associate,
AWS Data Analyst Certificate
Data Analysis and Reporting using SQL, Tableau, Python, Manage Infrustructe on AWS, Support for internals users. ETL. Manage Projects. Manage external providers. BigData Systems and Cloud Systems Manager AWS in Global Operations Support Group
"
data engineer,"Education: 
Lviv Polytechnic National University (Computer Science and Information Technologies): September 2021 - June 2025.
Completed trainings and courses:
- EPAM: Data Quality Engineering Training
- Udemy: Beginning C++ Programming - From Beginner to Beyond
- LinkedIn Learning: Learning Cloud Computing: Core Concepts
- LinkedIn Learning: Using SQL with Python
- LinkedIn Learning: Learning SQL Programming
- Udemy: Excel for Beginners
EPAM: Trainee Data Quality Engineer: July 2022 - October 2022.
- Technologies: Python(NumPy/Pandas), T-SQL, PostgreSQL, Amazon Web Services, Google Cloud Platform, Microsoft Azure.
- Tasks: Learning all the necessary technologies related to Data Engineering  & Analytics; Building simple ETL processes;  Working with cloud services: Amazon Web Services, Google Cloud Platform, Microsoft Azure. Experience in building ETL pipelines using SSIS; 
- Self-Development: Interested in continuous improvement in the field of Data Engineering & Analytics, exploring new technologies related to Data Science & Machine Learning.
"
data engineer,"Building shaping of an effective teams from scratch.
Constant self learning and motivation to try and use new tech.
~100% success rate of conversion and upload for multiple data migration projects.
I have 10+ years of combined experience with development, data migration and analytics, systems and business analysis on projects for companies in the range from Forbes Top-100 list to small local businesses.

Areas of expertise: 
data engineering (collection, preprocessing, normalization, storage), data analytics, data management and data quality, business analysis, collection of requirements, functional and technical design, data scraping, data visualization, team management, mentoring, writing of documentation.

Tools & Technologies: 
Python, Pandas, AWS (certified associate developer), NumPy, SciPy, scikit-learn, SQL, Selenium, CSV, JSON, HTML, SAP ERP, SAP LSMW, ETL tools, pyplot, seaborn, Jupyter Notebook, Tableau, Power BI

Experience:
Python: 3+ years
ETL: 4+ years
Business/systems analysis: 3+ years
Data management and quality: 5+ years
Team management: 3+ years
"
data engineer,"
I see myself as Data Engineer, designing schemas, data modelling. Implementing
various ML projects for my own experience I found that it is becoming my hobby and
passion. My aim is to gain more experience in the Data Science area.
"
data engineer,"
Student of NTUU “Igor Sikorsky Kyiv Polytechnic Institute”
Faculty of Informatics and Computer Science.
I have no commercial experience, but I like to work with data and I'm ready to develop my skills
"
data engineer,"Passed a technical interview with SQL/Business Intelligence/Data Integration in September 2022, currently a student at EPAM's internal Data Analytics/Engineering lab.
Studied SQL as part of the EPAM Data Analytics/Engineering Summer Program (complex queries, DDL, DML, DQL, DCL, CTE, indexes, triggers, functions, views, window functions). In the internal laboratory, he was engaged in data analysis in Business Intelligence tools Tableau/Power BI, automation of ETL processes in AWS Lambda and Azure Data Factory.
"
data engineer,"I was one of 120 laureates of Ukrainian Presidential Scholarship .
 I was in top-3 among 40 best participants from all the region of Ukrainian Olympiad in Informatics ,
math.
 Absolute first place on the UNESCO Center “Junior Academy of Science of Ukraine” Research
Contest .
Basic knowledge in SQL, Python (numpy, pandas, sklearn,  matplotlib), Tableau, Linux, HTML/CSS, JS, math statistics, linear algebra, A/B tests.
English Upper-Intermediate.
Give a lot of attention to details and can work hard in stressful situations.
I have completed several online courses in Python, Tableau, ML, Big Data, statistics, and SQL (DataCamp, Kaggle, Coursera). I have a strong background in statistical analysis, including the application of descriptive statistics and various statistical tests such as t-test, z-test, correlation, linear regression, multiple regression, and logistic regression. 
I wrote SQL requests for the analysis of statistics and analysis of product metrics.
"
data engineer,"Automated reports and data-check process, which can be run from Terminal, which helps avoid mistakes of manual coding and overlooking important data deviations, while also saving a lot of time.
-- I've worked as a Data Analyst for a telecom company for almost a year. My day-to-day tasks included designing and delivering regular and ad-hoc reports. What I mostly enjoyed was reporting-process automation (SQL procedures and jobs, Power BI dashboards (some DAX), integrated with RDBMS and SQL time-intelligence functions (dateadd(),add_months() etc.). 
-- My next employment was with a web-analytics company where I provided a wide variety of standardized and custom marketing reports, while also ensuring the quality of the delivered data. I mostly used such tools as SQL (analytic/window functions), Python (pandas, NumPY), AWS (Athena, databricks, S3), Excel. While working there I got some experience with Terminal (cmd), Jira, GitLab and Tableau, worked with large datasets.
-- I currently work as a Product Analyst with a company, which offers tools for personal digital security and computer performance enhancement. I develop regular and ad hoc Tableau reports (Tableau Desktop + Tableau Online), derive insights from AB tests and other data-driven investigations in order to improve funnel, user journey and drive revenue and retention. One of my projects is about collecting customer feedback results (in-house surveys + web scaring) and performing Sentiment Analysis (Python).
I've also done various data-preparation process automations, such as LTV predictions automation (Python). In terms of data preparation I've also worked with Google API, cron jobs, used some AWK (logs analysis).
One of my work pet projects is related to introducing version management process within BI team (GitLab).
I've realised that Data Engineering is my area of interest and I'm highly motivated to switch to this position.
Using English on a daily basis would be a great plus.
I highly value friendly working environment. It doesn't matter if the project is big or small, but I want to be able to work with large datasets, Python, preferably cloud technologies (AWS Services) and to be using English on a daily basis. I'm not interested in Product Analyst positions as I want to switch to Data Engineer position and focus on data pipelines rather than on product-related insights. However, I don't mind if some of my work routine includes deriving insights from the data, especially at the beginning."
data engineer,"Built a financial reporting system, where I had to combine multiple data sources using API and write the basic logic of calculation, specify the reliability requirements and implement quality control levers. And I'm still working on the code to make this system even better

Implemented integration with Amazon Kinesis to pull, process, and store in a database for further analysis of this data. The main challenge for me was to implement fast data reading and processing (because there is a lot of data and Kinesis has its own specifics of writing data to stream), so I had to implement parallel processing
-Monitoring of key product indicators, and quickly respond to deviations
-Creating a financial reporting system
-Data visualisation in Redash
-Creating an alerting and notification system
-Evaluation of AB tests
-Working with API (Google Analytics, Amazon S3, Google Ad Manager, Slack, etc.)
-Building streaming data pipelines with bunny CDN, Amazon Kinesis
-Writing project documentation
-Mentoring of other analysts in a team

I use Python and SQL mostly in my work. First of all I want to continue improve my Python skills, I also want to advance in the evaluation of AB tests (now I have a basic knowledge in this field). One day I would like to manage a team of analysts so that we can reach new heights and learn new technologies together.
I prefer not to work on boring reports, especially in Excel"
data engineer,"ORACLE DATABASE SQL CERTIFIED ASSOCIATE – Oracle University 
JAVA SE PROGRAMMER 1 (PASSING EXAM 1Z0-815) – Oracle University 
GOETHE ZERTIFIKAT C1 – Goethe Institut
Good at Critical thinking and problem solving and able to collaborate with team members and colleagues from other departments. Interested in learning new technical skills that enable to increase productivity. Dedicated and technically skilled professional with a technical support skill set developed through experience. Performing the transformation, filtering and aggregation of raw data into concise, accurate and focused data marts or client specific data resume_classifier using SQL and Python skills. Years of experience in building, tuning, evaluating data resume_classifier and presenting data insights to make business decisions. Proven success at creating, developing BI reports.

Using my sql, pls/sql and python skills I have worked on many financial and IT projects.
"
data engineer,"Throughout my career, I have always approached challenges with a ""can-do"" attitude and a determination to succeed. I am the kind of person who is always willing to take risks, to go above and beyond, and to do whatever it takes to achieve a goal.

While I cannot disclose the specific details of my work supporting new deals and renewals, I can say that my approach has always been results-oriented and driven by a desire to exceed expectations. My investments in my colleagues and peers have made a significant impact, helping many individuals to grow and develop in their desired direction.
Hi there! With 10 years of experience in the IT industry, I have gained invaluable skills and exp that I am eager to apply to a new role. My work journey can be divided into two parts. Firstly, I spent just over 6 years in tech support (including NOC experience) at various seniority levels. During this time, I worked across all possible support tiers, resolving a wide range of technical issues. Secondly, I spent more than 3 years focused on resolving various data-related issues, working closely with both internal and external stakeholders.

For the past two years, I have served as a Data Engineer/Analyst, building robust data pipelines to support analytics, research, and new features. My responsibilities have included analyzing data quality and conducting ad-hoc analytics, ranging from simple statistics collection to incubating and prototyping new features. In addition, I have optimized existing workflows to increase performance and reduce costs, leveraging my solid experience working with large data sets.

Please refer to the skills section to view my current tech stack. Thank you for your consideration, and I look forward to discussing how I can contribute to your team.
I believe that people are the cornerstone of any successful organization. That's why I am actively seeking a great working environment that is full of professionals who share my passion for learning and helping others grow. For me, a positive and inclusive workplace culture is essential for achieving high levels of productivity and overall well-being.

I am convinced that a supportive and welcoming atmosphere can make a crucial difference to the quality of our working lives, and I am committed to finding a team that shares this belief. By working alongside like-minded colleagues, I am confident that I can both learn from and contribute to a dynamic and collaborative environment."
data engineer,"
I currently work as a data analyst in the banking industry. I have developed complex SQL scripts to put every necessary data element together and build business-friendly dashboards for stakeholders. I'm also responsible to develop and maintain a potential lead customer database for a specific product. I mostly use Oracle SQL and MS SQL for data preparation, for visualization I use Tableau, for testing and automation purposes I use python, and Jupyter notebooks. I want to improve my business understanding skills more, and I want to use and improve my machine learning skills.  

I have developed a python scraping script to collect data from
specific websites using the Dataiku platform, building
an ETL pipeline to transform and store data in the DWH, making a dashboard on the Tableau Desktop and Tableau Server
"
data engineer,"
Senior Data analyst, Data engineer 2019 - 2022

Macroeconomic data analysis:

- extract macroeconomic data (gdp, cpi, ppi, fof) releases from provider web site,
- convert to custom format using Python scripts,
- check for mistakes, anomalies in data on test environment, launch appropriate reports,
- load data to platform.

Market (financial) data analysis:

- analyze structure, types of data, relations, define metadata/time series fields and measures of market datasets (export/import commodity data, ownership, equity, financial statements). 
Use SQL, Python (depends on source data).
- discuss with client results of analysis and provide it to business analysts.

Senior Analyst/Consultant, 2 years

7 years of experience in marketing research, FMCG, consumer insights, consulting. 
•	Areas of experience include CPG and Retail.
•	Participated in:
o	Forecasting projects
o	Innovation strategy
o	Business Hierarchies
o	Price&Promo
market overview, Category and need states analysis, opportunity spaces prioritization, new hierarchies’ creation

Research analyst / key account manager, 6 years
•	Prepare reports and analytical presentations, present them to clients;
•	Analyze arrays of data;
•	Responsible for FMCG market analysis and reports for Top Retail Chains in Ukraine;
•	Support clients and build up partnership relationships with current and prospective customers;
•	Negotiate conditions of collaboration, provide training sessions (methodology, software); 
•	Actively use Nielsen’s software (Advisor, Answers, interpreting Nielsen data and facts);
"
data engineer,"
I am an Erasmus Mundus Graduate in Big Data Management and Analytics, jointly organized by TU Berlin, ULB, and ULB as a scholarship holder. The focus of the program is on Scalable Data Analytics, Advanced Databases Systems, Data Warehouse, Semantic Data Management, Cloud Computing,  Big Data Analytics, Business Intelligence, OLTP and OLAP systems, Machine learning, and Stream Data Management. In the past 18 months, I have been focusing on designing data pipelines, performing ETL jobs on Spark, applying OLTP and OLAP queries to relational and NoSQL databases, building data lakes and data warehouses on Google Cloud Platform and Amazon Web Services, evaluating the quality of classification algorithms across open-source ML systems, experimenting with novel ML testing approaches, and analyzing frequent ML bugs

My thesis topic is ""On the Problem of Software Quality in ML Systems"". The goal is to evaluate the quality of classification algorithms from open-source ML systems to detect ML bugs at an early stage with model pre-train tests, model post-train tests, applying data and model validations to detect concept and data drifts, and experimenting with smoke testing, metamorphic testing, and model behavioral testing to reproduce or find new bugs for image classification and NLP resume_classifier. In addition, we applied popular static analysis tools to evaluate the quality of four open-source ML software repositories.

I have a bachelor's degree in In from Mekelle Univesity, where talent meets challenges. Owing to my outstanding academic performance and valuable leadership skills, I was offered the role of Assistant Lecturer and Database Developer at Adigrat University in Ethiopia. I taught Database systems, Advanced Programming with Java and Python, Data Structures, and Data Mining with Python for 3 years to about 50 students. Furthermore, I contributed to the University Registration System as a database programmer for SQL Server. Moreover, I worked as a professional Junior Software Developer at Aelaf Technologies, Ethiopia, where I worked mainly on the development of data-intensive pipelines employing relational and non-relational databases. It was a fantastic experience that allowed me to apply cutting-edge technologies to data management problems.

As I advance in my career, It is crucial for me to improve my technical and leadership skills. It is my firm belief that continuing working as a Data Engineer with your brilliant team will unleash my creative potential.
"
data engineer,"
Recent achievements:
• Developed an automated analytics solution via Python and Excel to predict optimal values of machine parameters. Increased the production speed of the machines by 18%
• Built a system to classify thousands of products into a few categories based on their particle-size distribution ranges. Improved decision-making process in production planning
• Analyzed thousands of products to gain insight into the most significant ones and modified their spectrum values. Obtained 8 tonnes less scrap out of 184 tonnes of production
• Conducted a statistical data analysis to determine the target throughput values of each machine. Improved troubleshooting process by enabling early recognition of non-optimal actual speeds
"
data engineer,"
Data analyst with strong probability and statistics background and two years of academic experience using data analysis and visualization software such as Sql and PowerBI, as well as data wrangling experience in Python and Microsoft Excel.
"
data engineer,"
0.5 year as a Data Engineer on Data Migration project with Power BI, pyspark, MS Azure
0.5 year as Data Analyst doing calculations for engineering purposes with R, Python, MS Excel
5 years of experienced with Python, 2 years with SQL, 0.5 with C++
"
data engineer,"
Skills: 
- Google Analytics (UA, 4), Firebase.
- Google Tag Manager, Google Optimize, A/B testing.
- Google Ads, Double Click, Bing, Facebook.
- Google Data Studio, Microsoft Power BI, Redash.
- Jupyter Notebook, Pandas.
- Voluum, Binom, Keitaro, Branch.

- Google App Script (Sheets, Docs, Gmail etc.).
- Google Ads API, Scripts.
- Google Analytics APIs.
- Bing Ads Scripts.

- BigQuery, MongoDB, MySQL, PostgreSQL.
- HTML, CSS. 
- JavaScript, Python, PHP.
- Apache NIFI, Git, Bash.
- Google Cloud, Amazon AWS, Linux, Windows.

- SEO, SEM, SMM, Advertising.
- Affiliate Marketing.
- Crypto, Banking, Ecommerce.
- Advanced Google sheets, Excel.
- Strong data visualization skills.
- ETL/ELT processing.
- Mobile Analytics.
- Jira.

Language: 
- English. 
- Polish.
- Ukrainian.
"
data engineer,"
I worked for Bank from 2019 to 2022 as a Lead Analyst.
I have experience of creating scorecardes, econometric forecasting model, vintage analysis, stress testing, credit risk calculation and loan portfolio analysis.
I have:
- knowledge SQL, knowledge of data processing and normalization, working with databases;
- knowledge of Python;
- knowledge Git (Github);
- Advanced knowledge of Excel;
- visualization in Power BI.
I would like to extend my expertise in analysis and I'm opened to new instruments for this goal.
Analyzing is a part of my life in any situation.

“The goal is to turn data into information, and information into insight.”
"
data engineer,"23.06.2021. Received a distinction from the National Academy of Sciences of Ukraine for young scientists.

Building analytics from scratch (architecture creation and full automation of processes (using GCP)).
Analyst with 3 years of experience. I am proficient in the following tools: 
• Excel (Power Query, Power Pivot, DAX);
• BigQuery, MSSQL, MySQL,  MariaDB, DBeaver;
• Looker Studio, Power BI, Zendesk;
• Python (Pandas, Numpy, Selenium, Request);
• Docker;
• Terraform;
• GCP (Cloud Storage, Cloud Run (jobs), Artifact Registry, BigQuery, Data Transfer, Pub/Sub);
• Git;
• Google analytics (GA4), Google Ads, Facebooke (developer, pixel), Admitad;
• Vertex AI.


WORK EXPERIENCE:
Data Analyst
hotline.finance, Kyiv / January 2023 – present time
•	Development and support of the data platform (GCP, Cloud Run, BigQuery, Python, Docker, Terraform).
•	Building analytics from scratch (architecture creation and full automation of processes (using GCP)). (Work with google ads, Facebook, Admitad, Google Analytics (GA4)).
•	The full cycle of creating dashboards (from 0, from prototyping to implementation and support, automation and upgrade). (Looker Pro, Looker Studio (Data Studio))
•	Calculation of client metrics LTV, ARPPU/ARPU, MAU, DAU, CAC/CPA (for app and web).

Inventory Analyst
Fora, Kyiv / March 2022 – December 2022
•	Report automation (using Python (Numpy, Pandas, Pyodbc), R (dplyr, tidyr, plotly), SQL.

Data Analyst
Fozzy Group, Kyiv / January 2021 – March 2022
•	Traffic forecast (calls, letters, chats) by month, day, hour, 15 minutes. Calculation of the required number of staff.
•	Development of various reports (using Power Query, DAX, Excel, Power Pivot, Zendesk, Power BI, SQL).
•	Monthly reports (calculation of operators' salaries (according to KPI), key indicators of Call Center, etc.).

Research Engineer
Bakul Institute of Superhard Materials of NASU, Kyiv / November 2019 – December 2020
"
data engineer,"- Finished It-school with final project - Text multiclass classificator by Neural network. (Definition of Text genre)
- Took part in 4 competitions  on the kaggle  
- Basic of programing on R - Certificate with honors
- Data analysis with R - Certificate with honors. 
- Basic of statistic
- Reports for Marketing, Customer support, Customer Success departments (Marketing funnels, Sales funnels, different conversion rates, retention, LTV etc.) Product researching for different hypothesis.
- Python Scripts for automatic creation report in google sheets, Google Data Studio, Power BI.  Import and export data to/from different sources (MySQL, Google Big Query etc). ETL data from different API (Google Analytics, Google Ads,  Amplitude, Stripe, Slack, Helpscout, Canny).
- Using Selenium for scraping data from WEB pages
- Familiar with Machine Learning ( Regression, Random Forest, Decision trees, SVM, k-mean,  Neural networks)
-  SQL queries to DWH,  SQL Server Integration Services (SSIS) 
- Dashboards and other visualizations in Power BI
- Orchestration in AirFlow
Difficult and therefore interesting tasks, Work-life balance"
data engineer,"Starting data stream at two products, where stakeholder management made it possible to get the high NPS in two quarters. It involved:
1. getting known quickly with a new stack
2. setting up processes, like PR, release in accompany with other streams, like backend/frontend/other data teams
3. creating an onboarding program to have a fast and productive journey for newcomers
4. technical debt management and planning data storage system changes smoothly to support its scalability
5. Initiating data quality architecture roadmap and successful up-sales
I am an Architect / Lead Big Data Engineer, a certified Google Cloud Data Engineer, and a Professional Architect with a 7-year of experience. 
My expertise is ETL, analytics, and backend applications development, designing architecture on GCP and managing a group of 3 engineers + 4 indirect teammates coordination. 
I built ETL + DWH for Product Launches, data enrichment with AI services, advanced analytics with enterprise system integration, and PoCs for new initiatives. In most of my involvements, I focus on solutions as to how things could be improved, i.e. CI/CD, data quality, and architecture design.
Low-level management and unclear business goals aren't so sweet."
data engineer,"- end to end machine learning projects 
- end to end a/b platform for machine learning projects
- scala/python/java
- spark
- machine learning ecosystem, tools and libraries (python, xgboost, sklearn, numpy, tensorflow)
- postgresql/cassandra/clickhouse/aerospike
- erlang
10+ years in software development
- direct communication
- data driven culture"
data engineer,"
10+ year expedience as database developer (mainly Oracle)
"
data engineer,"
Put it in my CV
I want to use my skills to solve problems. 
It will be great if I have an opportunity to use my creativity in work."
data engineer,"
I am a motivated Database Developer with well developed skills and experience in  banking sector. I have experience analysis complex business requirements , Writing ad-hoc SQL queries , Creating DB applications, Analysis of large datasets to improve process flow and quality of data

Used technology:  Oracle SQL , PL/SQL , Sybase , Tibco Spotfire , Python,  Excel
"
data engineer,"Personal Initiatives and achievements:
1) Database documentation using SSRS and Extended Properties.
2) Elimination of complex data integrity and consistency issues.
3) Refactoring and performance optimization;
4) Testing automation for SQL stored procedures;
5) Improvements for CI/CD;
6) Various processes automation;
7) Enhance project documentation;
8) Took on the role of a SCRUM master for a while;
9) Data quality and profiling.
More then10 years in IT;
Over 5 years of close work with SQL Server and SSIS;
Certified Azure Data Engineer;

Projects:
1) Design and development of international investment bank strategic platform for regulatory reporting and analytics;
2) Transfer pricing reporting system for external regulators;
3) Medical information system including the billing function and reference system;
4) Web archive for national bank;
5) Prototyping of the automated field(mine);
6) MES design and programming, material and energy balances for Iron and Steel Works;
7) MES design, material balances for mining and metallurgical Company;
8) Asset management and visualization. Pilot upstream oil;
9) Project coordinator for internal CRM project;
10) Project coordinator for CRM project. SoftServe's SalesWorks;
Official employment in the Czech Republic. 
Professional and career growth."
data engineer,"Microsoft Certified Solutions Associate: SQL Server 2012/2014
(date of achievement: 21/01/2021, certification number: H648-0083)
Exam CLF-C01: AWS Certified Cloud Practitioner
(date completed: 14/08/2020, registration number: 379967610)
Google Cloud Certified – Associate Cloud Engineer
(date completed: 16/10/2021, certification ID: eTLWhY)
Google Cloud Certified – Professional Data Engineer
(date completed: 14/01/2022, certification ID: rAncEr)
1) Sustainability Reporting
The Sustainability Reporting project was launched with the intent of being able to accurately measure carbon emissions. 
Key to the success of this project is to correctly identify the data required for including in Company Global Carbon Footprint – this includes identifying complementary data (and owners) that is used to make sense of base data. This needs to be done in conjunction with the departments who use/input the data, but we also need to identify data owners and the system owners so that ingestion methods can be agreed ad set up.

Data Engineer
-	Solution implementation
-	Development environment setup (local & remote)
-	Define, develop, or request solution infrastructure setup
-	Low-level design based on high-level design
-	POC development
-	Test/Sample data preparation and loading
-	Support in collection, eliciting, defining, and improving requirements
-	Application development
-	Testing
-	Releasing
-	Post-release support
-	Bug fixing & Troubleshooting
-	Tickets management
-	Supportive documentation (HLD, developers guide, tickets, confluence, etc.)
-	Support in the development of DDE platform

2) Trade project
Within the GCP DDE, there was a need to pay down prioritized technical debt, implement DPM driven business deliverables and fix minor issues. Also, there was a need to maintain two keys on premise SQL Server databases and their related integration pipelines (implemented in SSIS).

Database Engineer
-	Resolving incidents on the DDE platform, RIS (MS SQL Server) or the RDD DW (MS SQL Server) 
-	Work with Sprokit tool
-	Performance optimization of ETL processes

3) Project in the sphere of health protection
It is an application which provides a secure and reliable platform for collaborative Just-In-Time (JIT) and Low-Unit-of-Measure (LUM) inventory management and supply chain forecasting. It helps eliminate many manual, non-value-added activities that are part of traditional processes for managing supplies within the healthcare organization.

Database Engineer
-	Migration MS SQL to MySQL (schema, data, stored procedures, functions)
-	Optimizing Indexes (MS SQL Server, MySQL)
-	Performance optimization (MS SQL Server, MySQL)
-	Diagnose, resolve, and report on unplanned incidents
-	SSIS Packages (OLE DB Source, ADO NET Destination, Derived Column), ZappySys (JSON Source (REST API or File))
-	Configuration and using SQL Full Text search
-	Replication monitoring (MS SQL Server)
-	Established replication for Aurora
I am looking for an interesting, stable and promising project."
data engineer,"Data Warehouse migration from MSSQL database to GCP BigQuery/PostgresSQL
I performed the production data warehouse migration from MSSQL database to GCP BigQuery and client-sensitive data from MSSQL to PostgreSQL to increase performance of real time data representation. Refactored all existing tables, queries and code to support and actively use of BigQuery features. Redesigned and developed new streaming and batch ETL processes. Optimized existing analytical reports for BigQuery.
Data Engineer at Softserve (2018 - Present)

• Developed and maintained 30+ high-performance data pipelines to support complex data integration.
• Sourced, processed, validated, transformed, aggregated, and distributed data from source databases.
• Use MSSQL, PostgreSQL to store processed and aggregated data.
• Optimized data, queries, and data pipeline architecture.
• Designed highly performed analytical reports using SSRS.
• Migrated Data Warehouse from MSSQL database to the GCP BigQuery.
• Redesigned ETL\ELT SSIS project to Cloud Dataflow pipelines.
• Performed database administration activities such as storage management, backup, recovery, and performance tuning.
Looking for job with plenty of new features and opportunity for development. Ready to learn new features."
data engineer,"
DATA ENCODER 

Data input and manipulation in excel spread
sheets and access databases.
- Ensuring the data protection act is adhered to all
times.
-Ensuring the safety and security of the
company's data systems.
-Occasionally carrying out various administration
tasks(scanning,printing,etc)

ENGAGMENT SPECIALIST
-Follow up with a new service provider and
promptly react to any inquiries.
-Create a new marketing strategy based on
customer perspective.
-Perform daily activity as assigned by operation
team lead.
-Run overall process which including onboarding
a service provider and accepting calls from
regular customers and directing them to the
appropriate service providers, as well as
collecting and analyzing any feed back.
HR ASSISTANT
Best industrial park,Hawassa
-Provide clerical support for all employees.
-Maintaining records & information.
-Substantiates applicant's skills by administering
and scoring tests.
-Schedule examinations by coordinating
appointments

     So, I am  successful data entry specialist with an enormous knowledge in
1,  invoicing and inventory systems
2, Word processing
3, Document storage and cloud based systems

Possessed by a lofty skill of correcting  deficiencies  and errors of different data with full contemplation and recreate the  compatibility within the very least time. I am confined  with marvelous experiences in the data entry work center.
"
data engineer,"team player in projects with wide technology stack - debugging and refactoring legacy code, researching and developing new features; strong analytic skills
applied math background, Java/Scala,  Postgresql,  Python+R, bash, Linux
wanna work in a professional team, in  projects with tasks that need the deep research activities (data science), mostly server-side / data engineering"
data engineer,"
Building and maintaining an organization’s centralized data 
warehouses (on-premises, cloud, and hybrid), performing the 
DataOps role (data-specific workflow automation, logging, 
data profiling), enhancing data quality and reliability.
I am looking for the opportunity to work with an innovative and 
successful company. I look forward to utilizing the experience I have in technology to help the company become even more successful."
data engineer,"Designed, built, and supported 20+ data pipelines
Implement Business Intelligence/ETL solutions to integrate disparate data sources 
Collected requirements and reported the progress
Performed source system analysis, data profiling, and design mapping logic between source systems of 15+ plants and data warehouse
Created data extraction, cleansing, and load programs to move data from source systems to data warehouse
Created and presented reports, analyses to executives
Developed scripts for analyzing logs of launched campaigns and incidents 
Developed pipelines for regular cleaning/updating tables in Oracle (using SAS Management Console, SAS Data Integration Studio)
Created CI/CD workflows
Developing scripts using SAS Enterprise Guide for team usage
Installing releases/hot-fixes from our contractor (SAS Institute)
Developing technical reports (using SAS and SQL)
Familiarity with SAS Customer Intelligence Studio and Oracle
Analyzing logs of launched campaigns and incidents (using Python)
Developing jobs for regular cleaning/updating tables in Oracle (using SAS Management Console, SAS Data Integration Studio)
Managing CI/CD workflows (Jenkins, Ansible)
Building and maintenance of data warehouses, data marts (Oracle, Vertica)
Developing ETL-processes using DataStage and SQL
Query optimization with execution plans
Troubleshooting and monitoring of jobs

Software engineer experienced in working across the different stages of the data pipeline, including ingestion, and integration.
I am willing to relocate anywhere globally or continue working remotely
"
data engineer,"
Experienced Data Engineer with more than 10 years of experience in implementing and testing multiple software projects in various domains. Key skills include SQL programming, software integrations and data migration. Strong communication skills, attention to details, team player and very eager to learn new things.
"
data engineer,"
hello there
My Name is Erick. I am a junior data engineer with experience in AWS glue,aws EMR, aws athena big data technologies such as pyspark and Hadoop, and both SQL and NOSQL database. Over the years I have been practicing machine learning and am working as a volunteer developer at ivy ltd. Here we are unifying machine learning frameworks. I have been over a long period of time working as a data engineer freelancer at Upwork and fiver where I have been designing and implementing data pipelines for my clients. I have also been migrating databases for my clients from RDBs to dynamoDb.
"
data engineer,"
LLC ""Data Solutions-BI""
Junior Data Engineer
Dec 2021 - May 2022 (6 month)
I was involved in the design and implementation of data pipelines using ETL and ELT processes. I utilized my expertise in PostgreSQL, Python, and SQL to extract, transform, and load data into data warehouses. I also collaborated with the team to create reports and visualizations for informed business decisions. Additionally, I used BigQuery for large-scale data analysis to support the decision-making process.

Assisted in the design and implementation of data pipelines using ETL and ELT processes.
Worked with PostgreSQL, Python, and SQL to extract, transform, and load data into data warehouses.
Collaborated with the team to create reports and visualizations for business decisions.
Utilized BigQuery for large-scale data analysis.


Integration hub 
Integration Specialist/Data Analyst
May 2022 - Present (10 month)
I was responsible for developing custom transformations and rechecks for clients' data. I assisted in the design and implementation of ETL data pipelines utilizing technologies such as PostgreSQL, Python, and SQL to extract, transform, and load data into data warehouses. Additionally, I collaborated with my team to create reports and visualizations to support informed business decisions. To perform large-scale data analysis, I utilized BigQuery.

Developing custom transformations for clients data.
Developing custom rechecks for clients data.
Assisted in the design and implementation of data pipelines using ETL and ELT processes.
Worked with PostgreSQL, Python, and SQL to extract, transform, and load data into data warehouses.
Collaborated with the team to create reports and visualizations for business decisions.
Utilized BigQuery for large-scale data analysis.
"
data engineer,"
Skill Highlights
• Experience in SQL (PostgreSQL, Oracle, MySQL)
• PL/SQL (Procedures, Functions, Packages, Cursors, Triggers)
• Data warehouse design (Kimball , Inmon Approach)
• Have some experience using Python • Manual ETL
• Basic in AWS (S3, Athena, EC2, Redshift )
• PowerBI (reports, DAX basics)
• Upper-Intermediate English level 
• IBM Cognos tools

Experience

BI Developer – December 2021 – October 2022
VEO Worldwide Services 
• Part of a of 5-person team that is responsible for designing and creating data pipelines for company products
Working mostly with IBM services namely: 
• IBM Cognos Analytics (creations, managing and maintaining dashboards, reports)
• IBM Cognos Transformer (creation of a multi-dimensional model: a business presentation of the information in one or more different data sources that share common data)
• IBM Cognos Framework Manager (creation of the business model of metadata derived from one or more data sources)
• IBM Cognos PowerPlay (cubes creation based on a multi-dimensional model)

Education

EPAM Data Engineer internship – June 2021 – September 2021 
• Four months of full-time lab training with lectures and daily tasks.
• Intense training covering all essential tools and Data Engineer practices.
• Data warehouse design (Star schema basics , Dimension and Facts basics, ETL).
• Oracle DB for DWH and ETL (Oracle Join Methods, Partitioning, Merge, PL/SQL).
• Python basics. 
• AWS basics (S3, EC2, Redshift, Athena, DynamoDB, Cloud Formation, Cloud Watch).
• Power BI (reports best practice, DAX basics).
• Working with Jira.
"
data engineer,"
Skills
Python, SQL
 HDFS, Git, S3 
English: upper intermediate

Sturtup project under NDA
Role: Python developer
Responsibilities: Add new functionality to API, WebUI, S3 integration, unit testing, load testing, graph storage investigation, bugfixing  

Intellias
Role: Python developer
Responsibilities: Add new functionality to testing framework, modify, support and debug existing framework functionality, investigate and fixing MapReduse tasks failures
"
data engineer,"
1. Internship in IT company as big data engineer during 3 months.
2. Working experience as big data engineer during 6 months. 
3. Working experience - internship as data analyst in the French product company.
4. Working experience as PM, BDM including managing BDMs and teams of specialists (devs, QA) for around 5 years.
5. Experience in writing custom API connectors for PowerBI.
6. Experience in creating the PowerBI and Tableau dashboards.
7. English-fluent both speaking and writing skills, French B2, German - basic, very good in the past.
8. Experience in building ML resume_classifier (classification and regression), using R, Python, Pandas, numPy, sklearn, etc. for building the resume_classifier (during education).
9. Experience in data preprocessing/treatement/ ETL using Python.
9. Understanding of development processes of mobile and web applications, general knowledge of server-side architecture
10. Master degree in 1) Management and administration 2) Statistics and data Science 3) information systems 
11. Experience with Azure services, GCP
12. Experience with Azure data factory, Azure function, Kafka, Airflow.
13. Well versed with GIT
"
data engineer,"Developing data pipeline for biggest Ukrainian food retailer company. (Frontend on OLTP, ETL, DWH modelling, Creating reports)
Modeling and managind central DWH for one of the biggest food delivery company in EU
2014-2016 - Oracle developer (OLTP)
2016-2021 - Oracle data engineer. (ETL - Oracle Data Integrator, DWH - Oracle RMA, Orcale BI, BI publisher)
2021-now - Data Engineer (Amazon Redshirt, Apache Airflow, Glue, DMS, BQ)
Legacy code support"
data engineer,"
2+ years of experience
Main tools: Python, AWS;
DB`s: PostgreSQL, MySQL;

Duties and responsibilities:
• reporting and automation;
• DWH development;
• business process automation;
• analysis and visualization of data for decision making;
• preparation, cleaning and consolidation of data from different systems and sources;
• ETL processing;
• data scraping/parsing;
• api integration;
"
data engineer,"
7 years of professional expertise in the Information Technology industry;
 Experience in AWS Services: EC2, S3, RDS, IAM, Athena, Lambda, Amazon Aurora, DMS, SCT, Redshift, EMR, Glue,
VPC, ELB, CodeCommit, CodeBuild,… etc;
 Experience in CI/CD (Jenkins, Liquibase, Git, AWS CodeCommit, AWS CodeDeploy, AWS CodeBuild)
 Rich experience in Database/ETL/DWH;
 Amazon Redshift
 AWS Services
 Solid expertise in development for Oracle, PostgreSQL;
 Experience in programming and query languages: Java, Python, SQL, PL/SQL, PL/pgSQL, T-SQL;
 Data integration and ETL, ELT process design (SSIS, ODI);
 Business Intelligence (OBIEE, SSRS);
 Experience in Dimensional Modeling such as star schema, snowflake schema, creating facts, dimensions and
measures;
 Experience in DB heterogeneous/homogeneous migration;
 High level of theoretical knowledge and utilization expertise for the following development methodologies &
techniques: Agile/ Scrum/ Kanban, TDD (Test Driven Development)
"
data engineer,"- Built mathematical model for the influence of light in the greenhouse on plants growth
- Anomaly detection in radiation data
- Built BI reports for all products metrics
- Built prediction for KPI metrics
- Built ETL pipeline for more than 1TB of average daily traffic
- Built time-series content analysis system
- Built ETL pipeline using AWS and Databricks
- Built API for GitLab commits analyse
- Build ETL pipeline for real-time data analysis of more than 30GB of data
- Created ETL pipeline for web scrapping more than 300k of pharmacies products
- Leading team of data engineers
A highly skilled Data Engineer with focus on Python, Spark, SQL offering a combination of Big Data related skills with more than 6 years of experience in Data manipulation and  modeling. My expertise includes creation of ETL architecture, research of DWH improvement based on cost and performance analysis, software solution suggestion for business goals of customers and building data pipelines from scratch to distributed BI systems.
"
data engineer,"I was developing big data marts, pipelines, ETL processes, high availability architecture(druid with Kafka + monitoring and visualization(Metabase, newRelic, Sumologic). I have experience with AWS, Spark, NiFi custom processors.
Apache Spark Hadoop Hive Linux NiFi Apache Kafka Python Pandas SQL Tableau AWS Git
"
data engineer,"I have been able to work with a team and have done a great deal of some couple of  works together.
As a data engineer I have been able to make data more easily accessible to the organization.
I used the open source ELT . 
Am a co-worker in the team.
I would want to improve my skills and technology level if am employed by your organization.
To work with individual who are advance than me so I can also improve my skills along the line."
data engineer,"
As Data Engineer
(October 2021- present)

IBM Integration Bus: developing REST API, Integration services and apps.
Tools: IIB Toolkit, ESQL, DB2

AWS RDS for Postgres DB – Salesforce integration: designing and developing a solution for DB –
Salesforce integration.
Tools: AWS AppFlow, AWS Lambda, Python

Migration of enterprise reports from SAP Crystal Reports to MS SSRS:
- developing reports
(creating configs, layouts, data formatting, testing, deploying).
Tools: SAP Crystal Reports, MS SSRS, MS Report Builder, Oracle

Informatica Data Integration: 
-building data flows with transformations and mappings according to business requirements.
Tools: Informatica Intelligent Cloud Services

Oracle DB maintaining, PLSQL development: developing and testing PLSQL functions and
procedures.
Tools: ORACLE RDBMS 19, PLSQL Developer, SQL Developer

Commercial experience as  Python Developer: (September 2020 - August 2021)
I was engaged in working with:
- AWS serverless application development with API, Lambda, Step Function, DynamoDB, Rekognition, Athena, Glue;
- ETL pipeline implementation;
- OpenAPI (Swagger), YAML.
I`m open for new opportunities due to the end of current project. I would like to consider the company where I can closely work with highly qualified professionals on inspiring technological  projects. If the potential employer is looking for long-time cooperation, I would love to be part of that company."
data engineer,"
As part of the team responsible for the design and implementation of the Quality Engineering datalake, I worked on the extract, transform, and load (ETL) pipeline to ingest data from various Quality Engineering data sources using Apache NiFi for transformation and storing the data in an AWS S3 bucket. I also configured Glue crawlers and used AWS Athena to create views for data visualization on the AWS QuickSight dashboard. These efforts supported the Quality Engineering team in making data-driven decisions related to quality, enabling informed and strategic decision-making.

 As a member of a machine learning project focused on fraud detection, I was responsible for maintaining the quality and integrity of the data used for training and inferencing. To accomplish this, I utilized Apache NiFi to streamline the cleaning and streaming process of mobile money transaction data to AWS Kinesis data streams, and then loaded the data into an S3 bucket for storage. In addition, I monitored the quality and distribution of the data fed to the ML model to ensure its proper training and accurate inferences. Through my efforts, I contributed to the development of a solution that helps detect and prevent online fraud.

 As a key member of the development team, I played a critical role in the creation and documentation of a hybrid automation suite using Robot Framework. This solution enabled the team to use a single tool for the execution of web, mobile, and API tests, improving efficiency and effectiveness through the establishment of clear structure, coding standards, and clean git work. I am proud to have contributed to this significant achievement for the team.
"
data engineer,"
Creating hyperparametrized ELT pipelines
Producing prototypes to demonstrate concepts and ideas
Development of OLAP databases
Introducing new data management tools & technologies into the existing system to make it more efficient
Development of a data extraction system with 170+ connectors (like Meltano)
Development and implementation of attribution resume_classifier and ontological resume_classifier to automate report generation Development of systems for building reports
Built and designed data lakes and data warehouses on GCP Big Query
Re-designing infrastructure for greater scalability
Automation of data exchange between different systems and Google BigQuery
"
data engineer,"
Data Engineer | Kapital Bank | Baku, Azerbaijan | Jul 2022 - Present 
- Redesigned and migrated more than 15 ETL packages from former projects to current projects.
- Designed and implemented 30+ ETL/EtLT pipelines using SQL, SAP BODS and various SQL databases. 
- Administered nearly hundred ETL jobs, fixed them when it is broken, monitored and implemented relevant changes to prevent 
possible future errors and performance issues.
- Modelling data and building data marts to minimize execution time of analytic queries.
- Tuning poorly performing SQL queries and typing ad-hoc report scripts.
- Rewriting report queries to migrate that operated on prior data warehouse.
- Searching for alternatives to ETL, data cataloging tools and testing their compatibility and harmony with existing architecture.
Business Intelligence Specialist | Veyseloglu Group of Companies| Baku, Azerbaijan | Sep 2021 – Jul 2022
- Analyzed, designed and developed new data pipelines based on business requirements, using SSIS, Azure blob storage, 
Synapse, Azure DevOps and other tools. 
- Monitored performance of data pipelines and made changes to optimize the overall performance.
- Managed and supported 200+ ETL /ELT packages and 10+ SSAS Tabular resume_classifier.
- Developed ETL packages that automated manual tasks and saved 30 minutes of BI specialists daily.
- Analyzed storage and compute costs and reduced them where it is possible.
- Implemented security guidelines by using user filters and row-level security, minimizing private data exposure.
- Writing SQL scripts for ad-hoc reporting, optimizing slowly performing report scripts that runs on ERP applications.
Data analyst | Freelance | New Jersey/ New York, USA| July 2019 – September 2021
- Built out the data and reporting infrastructure from ground up using Power BI, Tableau, Quicksight, SQL and Python to provide 
near to real-time insights into the products and business KPIs. 
- Redesigned and developed ETL processes to optimize data ingestion and check validation rules in various steps of pipelines 
ensuring data accuracy.
- Created dashboards and reports to monitor data quality and performance.
- Analyzed business processes and generated data, suggested solutions which reduced the waste.
"
data engineer,"
Data engineer with over three years of experience in ETL. The main technologies are Python2/Python3, Airflow, Scala/Spark, AWS, Docker. 
Worked with Scrum methodology, have experience in SDLC (working with requirements, design, coding, testing, debugging, deploying, publishing).
Interest in data processing, cloud architecture, pipeline design.
Responsible, good team player, good communication skills, result-oriented, fast-learned.
"
data engineer,"I always try to do my job as well as can
Design architecture of DWH and data lakes, development data pipelines and flows, improvement performance.
Many different tasks from development to architecture. 
Expiriens with PowerBI, Oracle BI EE,  Snowflake database, PySpark, Scoop, Kafka, NiFi, SSIS, SSRS, SSAS
"
data engineer,"Designed and developed in RDBMS Oracle a number of interfaces for CRM channels at Kyivstar.
Development of ETL processes for flat reports delivery via  mail server.
I started my career in IT as a information system administrator. My main responsibility was to maintain the system and conduct its configuration on demand. With time the scope of duties was broadened to writing new feature blocks on PL/SQL as well as SQL tuning. When realized that there is no further professional growth 
decided to move to Business Intelligence (BI) area. On a position of BI Software Engineer was responsible for designing of flat reports, new ETL processes development, SQL tuning, data quality control, setting new and maintaining of existing OLAP cubes. Not long ago made my mind to challenge myself and start on a new job position as a BA.
I'm interested in returning to software engineering tasks that will let me get hands-on experience of using Python and cloud technologies (AWS, Azure, etc)."
data engineer,"
I've been involved in both ad-hoc development projects, as well as projects that are designed to transfer and modify functionality from one repository to another.
Currently I'm designing data marts and building pipelines using Apache Airflow.
Main languages: SQL, Pyspark(scala spark)
Most of the time I work with hadoop and greenplum.
В первую очередь меня интересует современный стек технологий и разработка нового функционала."
data engineer,"Stress resistance, desire to learn new, responsibility
MSSQL 2012,MSSQL 2016,ETL, SSIS, DW, BI, Yellowbrick, Tableau, Power BI, Snowflake
part time job"
data engineer,"
November 2021 — Present       	 Prime Source LLP, Almaty, Kazakhstan
•	Position: 
    Software developer 
•	Responsibility: 
      Software support from the frontend(JavaScript React) and database using VS Code and ************************************************************
May 2021 — October 2021
6 months	
	Eurasian bank JCS, Almaty, Kazakhstan
•	Position: 
    Data warehouse developer 
•	Responsibility: 
    Developing of reports on Oracle BI, creating of packages, procedures on PL/SQL developer
****************************************************************
Dec 2019 — Apr 2021
1 year 5 months	
	Home Credit Bank, Almaty, Kazakhstan
•	Position: 
    Data analyst of portfolio quality management and late collection
•	Responsibility: 
    Generation of reports in Excel with connection to a database, comparative analyzes, data visualization in Excel, work with PL / SQL database (formation and optimization of scripts using hints, indexes, logging, etc.; procedures, setting up jobs), formation of XML packages for generating SOAP requests to the state database with further entering the received data into the database; parsing web pages and entering data into the database, automating work using Python and Jupyter Notebook (selenium.webdriver, beautiful soap, pandas)
"
data engineer,"Since the previous year switched to data engineering: took participation on creating ETL pipelines and infrastructure for storing data. 
 
Before I participated in financial and devops projects, where I had the next responsibilities: creating from the scratch  and providing stable automation process, the development and supporting of the automation framework, creating the environment for the integration testing, backend testing, automation of processes of creating test documentation,  building CI process for configuration of different softwares: Jboss, IIS, DB2, WebSphere, Jenkins, Bamboo, Twistlock, Jfrog, Artifactory etc.
The last project was dedicated to data reconciliation, data solution was specific ETL which pulls data from several sources, compares data and returns the difference to the user. The solution is based on python, pandas, spark, aws s3, docker. 

Previous projects related to CI/CD infrastructure and integration with third-party services.
Providing enough time and technical resources and a friendly  management"
data engineer,"
10+ years experience in Data Modelling, Data Warehousing and Data Engineering.
I have experience with
        BI tools: Oracle BI, Microsoft Power BI, Tableau, Microstrategy, Tibco Spotfire
	ETL/ELT tools: ODI, MS SSIS, SAP BODS, Apache Airflow
	Databases: Oracle, MS SQL Server, SAP IQ, Postgres, Mysql
	Programming languages: Python
	
Good knowledge in Kimball approach, Immon Approach, Data Vault modelling.

Projects:
	Participating in developing Data Warehouse and BI for Central Bank of Azerbaijan Republic
	Participating in developing Data Warehouse and BI for Ministry of Taxes of Kyrgyz Republic
	Participating in developing Data Warehouse and BI for ""Kapital Bank"" (Azerbaijan)
	Leading of creating and developing process of Data Warehouse and BI infrastructure for Ministry of Education of Azerbaijan Republic
	Leading of optimizing and developing process of Data Warehouse and Self-Service BI infrastructure for ""Yelo Bank"" (Azerbaijan)
	Leading of process creating DWH and Self-Service BI infrastructure from scratch for ""Rabita Bank"" (Azerbaijan).
"
data engineer,"At my current place of work, I developed the on-premise to cloud (Oracle to RDS PostgreSQL) terabyte-scale data migration and post-migration and validation(within a very tight time frame) solutions using the following stack of languages, technologies, and products: AWS Services (DMS, S3, EC2, etc.); Python and Perl scripts; Grafana, Prometheus for monitoring; Ansible, make and CloudFormation for deployment.
10+ years of experience as a DB Developer
5+ years of experience as ETL, BI Developer
2+ year of experience as a Data Engineer
2+ year of experience with AWS (Certified Cloud Practitioner)
Contributions to open source repositories: orafce, MySQL sys schema
I aim to work as a data engineer using relevant modern technologies, products, and tools."
data engineer,"- OLTP, Data Warehouse design and implementation
- Design, implement and optimize ETL processes 
- Development and implementation of reports (KPI's dashboards, drill through reports)
- Certificates: Microsoft Exam 761; Google Cloud Professional Data Engineer certificate
•	10+ years of professional experience in the Information Technologies (IT) industry;
•	Expertise:
-  DB architecture design;
-  Solid knowledge of SQL;
-  DWH;
-  ETL;
-  Reporting (SSRS, Crystal Reports);
-  Cloud Computing Experience (GCP, Azure);
I am looking for Data Engineer job"
data engineer,"Being involved in more than 25 projects that get production. On different roles, mostly related to DB and data flow engineering.
Latest:
- The infrastructure of entity-based continuous data archiving in the mid-loaded system.
- Zero downtime cross DC client migration (11DBs ~1,5 TB, PostgreSQL 10)
12 years of data engineering, development of data structures and stored logic.
Data pipelines.
Optimization, troubleshooting, maintenance;
Common ETL solutions;
Infrastructure stuff, db changes delivery, CI/CD e.t.c.
Up to 1,5 Pb storages, up to 5000 TPS.
Want to create something useful."
data engineer,"
13 years experience in Software Engineering with: 
OOP/OOD
Agile
.Net (C#)
Scala,
Python,
Spark,
Apache Beam,
Java,
Hadoop, 
Hive,
MS SQL,
Asp.Net
Asp.Net MVC, Web Api
HTML, CSS
JavaScript (several top frameworks)
Remote position as a Data Engineer"
data engineer,"I speak at conferences and meetups (recent is ScalaUA), organize meetups and visit conferences in Europe(Open Source Summit Europe, DockerCon Europe,  etc.).
1.5 years as a Data Engineer at Ciklum. 
Before that, I was a freelance Java developer (10 months)
Professional growth and challenging tasks, flexible schedule"
data engineer,"Deployed to production:
Automation of financial monitoring of customers activity in commercial bank as government regulator requires (ETL, Informatica)
Integration of dictionary with third system by means of stored procedure (PL/SQL, Siebel)
Implementation of mechanism of remote signing of loan agreement by means of one-time password in SMS (Siebel, Web-Services)
Developing non-vanilla mechanism of auto assigning (Siebel)
Improvement of automation of lead management (PL/SQL, Siebel)
Design and maintaine Dev-Test and Test-Prod deployment (Siebel)
15+ years of overall experience developing enterprise software (like ERP, CRM);
9 years experience as Siebel CRM developer;
2-year experience in ETL-processes development;

Data Engineer:
Data warehousing support and development (commercial bank, insurance company).
ETL processes design, development, and maintenance.
Analysis of business requirements and process implementation.
Process failure analysis and correction.

Projects:
* Developed automation of financial monitoring of customer activity as a demand government regulator.
* Migration of DWH and BI from Teradata to Oracle DB.

BI: Oracle BI Dashboards, Oracle BI Publisher
ETL: Informatica Power Center, Oracle Data Integrator
RDBM: Oracle Database, MS SQL Server, Teradata, MySQL (DML, DDL, PL/SQL, T-SQL)

Siebel CRM:
Automation by means of Oracle Siebel CRM for commercial banks, telecom operators, etc. (Ukrainian and foreign companies).
High proficiency in various Siebel tools and technologies: eScript, Workflow, Open UI, Runtime Events, Workflow Policies, Data Validation, ADM, etc.
Experienced in supporting and modifying legacy functionality, developing new ones, and integrating with third systems using various technology (Siebel/non-Siebel). 
Applications: Siebel Sales, Siebel Marketing, Siebel Field Service, Siebel Financial Services.
It would be greate to expand and deep my expertize in Data engineering area with a team of professionals. In Siebel area I can work with same effectiveness like a single developer or as a team-player."
data engineer,"0. Successfully completed full specialization of ""Машинное обучение и анализ данных"". Coursera certificate № 6DXHD243CRWF.
1. Create data warehouse for business intelligence.
2. Built resume_classifier for classifying and clustering customer behavior in the field of consumer lending.
3. Created reports in the field of business analytics based on requests to Sybase IQ servers.
4. As part of the graduation project at the institute, a designer of financial reports for customs authorities was developed. Development language - Turbo Pascal (IDE Delphi 5), DBMS - FoxPro and Paradox.
1. Created predictive resume_classifier (machine learning) of customer behavior. In tech view these projects are task of classification and clustering using SQL, Pandas, NumPy, Sklearn, XGBoost.

2. Create data warehouse for business intelligence on Sybase infrastructure using Watcom SQL and Java.
An opportunity to develop to a professional level in machine learning and programming."
data engineer,"Working with helthcare products as a machine learning engineer
creating multiple recommendation engines based on patient data
Using NLP to recommend best content to help patients with different mental health issues
Developed tools fot deployment and monitoring of the machine learning resume_classifier

Working as Data engineer for the E-marketing platfrorm 
Developing new and managing existing data pipelines
Taking part in building data-lake on s3 and AWS GLUE 
Developing multiple ETL pipelines using GLUE Jobs and AIRFLOW
1) Data Engineer
Working as Data engineer for the E-marketing platfrorm 
Developing new and managing existing data pipelines
Taking part in building data-lake on s3 and AWS GLUE 
Developing multiple ETL pipelines using GLUE Jobs and AIRFLOW

Working with helthcare products as a machine learning engineer
creating multiple recommendation engines based on patient data
Using NLP to recommend best content to help patients with different mental health issues
Developed tools fot deployment and monitoring of the machine learning resume_classifier
Developed CLOUD DWH system to store data

2) Data Scientist/Engineer
- Extracting, Transforming and loading data from various sources
- Developing analytical products using telegram-bots and machine learning
- Developing data visualisation solution for Data engineers for creating quick visualisations
- Creating automatic pipelines for data collection and validation
- End to end development of a machine learning solution for churn classification
- Creating Dashboards on Tableau
- Advanced usage of SQL and SQL-optimisation for working with BIG-DATA


3)  Data Analyst
- End to end development of a machine learning solution for credit default detection
- Integrating analytical solutions into CRM
- Report automation
- ETL

4) Data Analyst
- Creating weekly/monthly analytical reports
- Sales analytics
- Evaluating the work of the sales department and setting sales plans based on statistical modeling
I would like to strenghen my skills in deep and machine learning, so 
I would like to work with a project that involves these areas"
data engineer,"
1)	Epam, development for a banking company(September – March 2020/21)
 Position: Junior Database engineer
Role:
•	Writing new database logic
•	Improving sql queries
•	Working with Airflow
2)	 Epam, development for a cosmetics company(April – November 2021)
Position:  Data Quality engineer
Role:
•	Checking data on different layers
•	Testing data migration
•	Working with relational database in Azure
3)	Epam, development for a Healthcare Company(January 2022 – May 2023)
Position: Data engineer
Role: 
•	Creating infrastructure in GCP
•	Creating data migration 
•	Testing data migration
•	Writing end-to-end testing
"
data engineer,"
1) Experience in SAP ecosystem. Developing ETL from 1C (MS SQL) to SAP. Monitoring and troubleshooting. 
2) Experience in ETL with the Informatica DEl tool,
building showcases from oracle to hive, oracle to
oracle, hive to hive;
3) Experience in ETL with the Informatica PC tool, uploading from different sources and uploading to the staging layer ORACLE;
4) Creating datamarts for data analysis team. Communication with business leaders.
"
data engineer,"
1. Trainee Python Developer
- Supported a drop shipping platform by developing and maintaining robust applications
 - Automated business processes using RPA frameworks to enhance efficiency and reduce manual efforts
 - Developed a REST API using the Django framework that allowed seamless communication between multiple services

2. Trainee Big Data Engineer
- Implemented ETL logic for data from various sources to ensure data quality and consistency
- Developed a cluster environment using Docker containers for efficient data processing and analysis
- Worked with data processing using batch and real-time processing frameworks to handle large volumes of data
- Created analytics solutions using Superset for effective data visualization and reporting
- Developed a small ETL process using Databricks to extract, transform, and load data from various sources
- ETL Process Migration to Oracle Cloud using Terraform
"
data engineer,"
1. Worked on a recommender engine based on user analytics data / media items metadata, that provides personal / related recommendations to end users. Familiar with widely used algorithms for processing events sequences (Word2Vec, LSTM).
2. Worked on improvement / maintenance of auto-ML pipelines for resume_classifier training and deployment in Kubernetes / Amazon Cloud, batch processing of views history, syncing inference results with the application’s back-end.
3. Worked on optimization / reconfiguration of a multicomponent high-load  real-time analytics system based on Apache products, supporting several million concurrently active users.
"
data engineer,"
1+ year of Java experience; 3+ years of experience in SAP projects, 1+ year in Change/Release Management. My skills: Java SE, Java EE, Hibernate, Spring, SQL, MySQL, JUnit, Maven, Git, HTML, CSS, ETL, DWH, SSAS, SSIS
More back-end and less front-end."
data engineer,"Highlights: flattened Data Vault tables into one master table with automated feature generation powered by DBT macroses; migrated to Data Vault 10 pipelines including changes for 30 table, implemented comparison tests for old vs new approaches; written Python adapter for Hive metastore, etc.
•	2x AWS certified
•	Python, SQL, Snowflake, AWS, Azure, Airflow, dbt, MonteCarlo, Git, Docker
•	AWS Cloud Practitioner Certificate
•	AWS Data Analytics Certificate

RELATED
•	English: Advanced, IELTS 8.0 - C1
Remote work only"
data engineer,"NLP pet-project. I am working with the Ukrainian language corpus and improving the accuracy of the model released to Hugging Face. Speech-to-Text API, exposed only to the Ukrainian language.

Master's degree in computer science. The master's work was in NLP. The main tasks made a summarization of short texts, by using extractive summarization methods. Used libraries/approaches - spacy, fasttext / LDA, word2vec, tf-idf.

I won a hackathon with a team of 3 members. Developed a website providing a 3D tour, that helps you to find a proper space for business, and rent it. I was responsible for the front-end part.

Responsible, calm, communicative, team player, and strong-willed. Interested in learning new technologies and obtaining unique experiences.
3.5 years of experience as a data engineer. I was engaged in complex analytical projects. 

/1 Created a pipeline that found chronic issues in IoT devices in 60% of cases. Helped detect problems in the early stage, and prevent bad customer experiences.
/2 Building and maintaining a number of software solutions for customers, related to data ingestion, processing, and building visual representation (dashboards) that enable data-driven business decisions and operational monitoring.
/3 Writing, execution, and maintenance of data transformation jobs, ETL/ELT using Spark and its API. Mostly Pyspark.

/4 Spam/fraud attack prevention. After accomplishing research, I found a new type of malicious website – that helped to detect fraud in the early stages.
/5 Data scraping and collection for further analysis. Used technologies: Scrapy + Python (requests, selenium), PostgreSQL, and AWS.
/4 Experience with NLP by extracting useful information from the Text and working with ASR and Text-to-speech processing.



- Languages:	Python, SQL, JavaScript
- Technologies: 	Azure(ADF/Synapse Analytics); Databricks, Postgres, BigQuery, ClickHouse, Git, ElasticSearch, Linux.
- Big data:	Hadoop, Spark, Airflow.
- Other: Docker; ML/NLP; OWASP; Sisense, Superset, Grafana; VS code, Postman.

Certification: Azure Data Engineer (DP-203)
Interesting and complex tasks. Professional growth in a team of professionals. Stable working conditions."
data engineer,"
- 3 years of commercial experience designing full-stack CRM & data processing applications with JavaScript/TypeScript/Node.js & relational databases. 

- 0.5 years of commercial experience building data-intensive analysis applications with Python (Pandas, Plotly dash, numpy).
"
data engineer,"Successfully delivered complete IoT solutions that included data analysis,
multi-cloud infrastructure and embedded devices. 
Designed the architecture and implemented ETL data platform from scratch acting as Key Developer.
Successfully passed ""AWS Certified Data Analytics - Specialty"" and ""Databricks Certified Associate Developer for Apache Spark"" certifications
- 3 years of commercial  experience in Python/Cloud projects
- Experienced in all three major cloud providers: AWS, Azure and GCP
- Certified AWS Big Data Engineer
- Certified Spark Developer
- Experienced in major Big Data tools(Spark, Kafka, Hadoop)
- Bachelor's degree in computer science
- Fluent English writing and speaking skills

Interested in Big Data projects, preferably using AWS, Python and/or Scala.
Primarily interested in Big Data projects, preferably using AWS, Python and/or Scala.
NOT interested in Java, Web development, DBA, Android."
data engineer,"
4+ years of professional experience in the Data / Machine Learning industry;
 Experience in development/modification of ML algorithms using Python programming language,
including on ML/DL frameworks;
 Experience in Python focused on Data Science and ML (Pandas, NumPy, Sklearn); 
 Experience with data visualization using Python-based libraries (Matplotlib, Seaborn, Plotly, 
Dash) and visualization tools such as PowerBI, Metabase
 Experienced with deep learning frameworks such as PyTorch, Tensorflow/Keras;
 Strong knowledge of SQL;
 Experience with AWS Data Analytics and Machine Learning stacks;
 MLOPS in AWS Sagemaker.
 Solid knowledge of statistics;
Solid knowledge of Snowflake
AWS Data Analytics stack
"
data engineer,"Sped up Spark (AWS EMR) data pipelines by 30-70% with code optimization. 
Built dozens of integrations with different data platforms (APIs, databases, SFTP, S3), both for data ingestion and upload.
Designed and implemented data processing applications from scratch.
5 years of experience in building and maintaining ETL pipelines (partly freelance).
Advanced skills in Python, SQL, PySpark.
Extensive experience in debugging and performance profiling.
Result-oriented, attentive to details, good team player.

Stack:
Python, Airflow, PySpark, Flask, Superset
SQL (PostgreSQL, MySQL), Redis
AWS (EMR, Lambda, S3, CloudFormation, DynamoDB)
Linux, bash, Docker, git
Scrum, Jira, TDD
"
data engineer,"
5+ years of professional experience in data processing and integration;
Experience in database/ETL architecture development;
The main domain of experience: e-commerce, b2c online services, online media;
Experience highlights:
-Solid SQL data manipulation and processing expertise;
-Database integration and ETL development;
-Deploying and maintaining ETL GCP services;
-Report Design (Tableau, Google Data Studio, PowerBI);
-Developing custom tools and approaches for data orchestration;
-Working with Linux environment;
-Managing data access for the DWH in GCP;
-Managing code-free ETL data integrations;
-Defining the strategy for DWH cost optimization;
-Feature engineering and model performance evaluation;
-Strong focus on automation and data-driven approach.
"
data engineer,"
6 years of working experience in the IT industry as a Software Engineer, 3 of them as a Data Engineer. 2.5 years experience with Spark+Java+AWS, about a year experience with Spark+Scala+Hive. Solid knowledge of Java, Scala and Python are additional languages. Hands-on experience with SQL (PostgreSQL, MySQL) and NoSQL (HBase, Redis, DynamoDB).
Looking for challenging projects for professional growth, extending my experience to tech stack Spark+Scala/Python. Working in a team to be able to gain new experience and share the existing one."
data engineer,"
7 months of experience working as esports analyst for League of Legends teams, was responsible for gathering and analyzing data from the game, recognizing patterns, evaluating choices and presenting it to the team. 
2 months of experience as product analyst, used R as programming language, analyzed if customers use new functionality with events tracking data, made an interactive web dashboard with R shiny with refunds rate, orders and its categories.
Have hand-on experience with Looker and BigQuery via google cloud skills educational program and basics of google cloud infrastructue.
DataTalksClub DE zoomcamp with project required to get pass, hand-on experience with Mage, GCP, Azure, Terraform, docker, ETL, workflow orchestration.
Down to work with GCP and Azure, no experience with AWS."
data engineer,"I am passionate about leveraging my expertise in big data technologies to drive business growth and streamline data-driven decision-making processes. With a proven track record of delivering high-quality data solutions on-time and within budget, I am confident in my ability to make a significant contribution to any organization
8 years work experience 
Proficient in Apache Spark Scala for distributed data processing and analysis
Experience with Apache Airflow for workflow management and scheduling
Knowledge of PostgreSQL for secure data access and management
Proficient Apache Hive and Oracle Databases for data storage and management
Strong skills in advanced data analytics techniques for distributed system
Led the design and development of a real-time data processing platform using Spark Streaming, resulting in a 50% reduction in data processing time.
Built and maintained data pipelines using Airflow, ensuring timely and accurate data delivery to downstream applications.
conducted extensive data analysis using advanced statistical techniques, resulting in actionable insights that drove business growth.
Passion and Interest:  driven by my  passion for a in data engineering field or industry. I actively seek opportunities that align with my interests and allow me to work in areas they are genuinely passionate about."
data engineer,"
Accenture Latvia, AVS – M5 Command Center(ITA)
Data engineer(06/2017 – 05/2018)
 Support the Operation area through analysis and reporting on the operational data of the networking equipment,
the measurement of network performance (KPI) and the quality of the services provided by the IPTV product.
 Operate with critical identification objectives providing indications for continuous improvement of results.
 Support the customer by creating summary dashboards and system alarms derived from data analysis.
 Applications of the Splunk, Python, Shell, Postman products
 Universal Forwarder installation and configuration in Linux and Windows environments
 Indexer and Deployment Server installation
 Сonfiguration of new apps for monitoring data on machines
 Сonfiguration of monitoring rules for Text and script files .sh, .py
 Integration of Postman, Python with the Splunk product for the monitoring of APIs exposed by the Accenture
Video Solution product

Accenture Latvia, SDWAN (US)
Data engineer (04/2017 – 06/2017)
 Creating dataset mockups and visualizations of SDWAN work.
 Using Python and Pandas library (Python) for preparing data
 Tableau for analyzing and visualization of SDWAN work.

Accenture Latvia, Intelligent Bench Analysis (LV)
Data engineer (02/2017 – 04/2017)
 Creating dashboard with information about projects, where candidates auto match for every project by Skills, Skill
level and etc. and for every employee matches information about projects where he can work with his skills.
 Developing application for auto updating Riga Project Dashboards in Splunk and exporting data from this
dashboards for analyzing. Using Bash Scripts, Python for developing and Splunk for analyzing and visualisation
data

Accenture Latvia, Flights (LV)
Data engineer (12/2016 – 02/2017)
 Create a service which can collect information about flights to different destinations and for different duration stay.
 Creating summary dashboard and system alarms derived from data analysis.
 Using Python for collect data from sites like Google flights. Flume for inserting data to Hadoop. Spark for
transforming and data analysis. Hive for data analysis. Qlikview for visualization results of data analysis. Splunk
for system monitoring and build alerts based on analyzed data
"
data engineer,"- Developed and optimized ETL pipelines using Azure Data Factory to efficiently ingest and transform data from multiple sources, improving overall processing speed and accuracy. 
- Implemented Slowly Changing Dimensions (SCD) approach for key dimensions, ensuring data consistency and historical tracking. 
- Enhanced data warehouse architecture by refining data resume_classifier, adding new dimensions and fact tables, and streamlining existing tables to improve reporting and analytics capabilities.
- Collaborated with cross-functional teams to identify and resolve data-related issues, providing support and guidance on data engineering best practices. 
- Designed and implemented data validation and error handling mechanisms, including logging and metadata tracking, to ensure data quality and maintainability. 
- Conducted performance analysis and optimization of stored procedures and ETL processes to improve query execution times and overall system performance. 
- Implemented and maintained Power BI dataset refresh processes, ensuring timely and accurate availability of data for reporting and analytics purposes. 
- Assisted in the migration of historical data from Excel to the data warehouse, developing data pipelines and table structures to accommodate the new data sources. 
- Contributed to the creation of onboarding and technical documentation, facilitating knowledge transfer and team collaboration. 
- Onboarded a Data Engineer from the client's team into our development process, responsible for reviewing her first onboarding tasks.
A highly skilled and experienced Data Engineer with a strong background in MS SQL, Azure, DWH, and ETL, seeking a challenging position to leverage my expertise in data engineering, data warehousing, and 
cloud technologies to deliver innovative solutions and contribute to the growth and success of a forward thinking organization.

Skills
Databases: MS SQL Server, Azure SQL Database
ETL Tools: Azure Data Factory, SSIS
Data Warehousing: Star Schema, Snowflake Schema, SCD, Kimbal approach
Cloud Services: Azure DevOps, Azure Blob Storage, Azure Key Vault, Snowflake
Languages: SQL, T-SQL, Python
Data Modeling: ER Diagrams, Dimentional Modeling
Reporting & Visualization: Power BI
Tools: Git, Idea, Visual Studio, Jira
Methodologies: Agile, Scrum, Kanban

My strong skills are responsibility, reliability, tolerance, sociability and patience. I am definitely a team player.
"
data engineer,"
Almost 5 years of my experience as a Data Engineer include:
* Designing and building ETL pipelines.
* Processing large-scale data with Apache Spark.
* Building Data Lakes.
* Tracing and eradicating inconsistencies and errors in pipelines.
* Agile development.

I have been responsible for the data engineering part of projects in domains such as advertisement, behavioral analytics and anti-money laundering.

My main technology stack includes but is
not limited to:
* Apache Spark (PySpark)
* Apache Airflow
* Oracle PL/SQL
* Apache Nifi
* Apache Kafka
* MinIO

Utilized programming languages are Python, Java and C++.
"
data engineer,"Accomplishments:

AM-BITS LLC:

    Real-time Data Processing Leadership: Spearheaded the development of a real-time data integration system, leading to a 35% increase in data processing speed and achieving 99.5% accuracy in data integrations.

    Database Efficiency: Successfully migrated over 3TB of data from Oracle to PostgreSQL, resulting in a 25% improvement in query performance and a 15% reduction in infrastructure costs.

    Cloud Scalability: Migrated over 5TB of crucial datasets to cloud platforms (AWS S3 & Snowflake), realizing a 20% cost saving in storage expenses.

    Client Satisfaction: Guided a telecom client in their transition from Oracle to HDP, leading to a 30% cost reduction and a 90% satisfaction rate from the client.

MDM-Group:

    ETL Performance: Revamped and optimized ETL workflows with Apache Airflow, resulting in a 20% decrease in data latency and ensuring 24/7 data availability for analytics.

    Stakeholder Engagement: Integrated Apache Superset with the company's HDP stack, giving stakeholders real-time business insights which boosted data-driven decisions by 40%.

Kyiv Polytechnic Institute StartUp:

    QA Excellence: Identified and resolved over 150 platform discrepancies, ensuring optimal user experience for academic platforms.

    Automation Mastery: Implemented automated testing procedures with JMeter, reducing manual testing time by 45% and expediting the QA cycle by 30%.
AM-BITS LLC - Big Data Engineer (Apr 2021 - Present)

Real-Time Data Integration:

    Role: Lead Data Engineer.
    Tech: PySpark, Kafka, HBase.
    Details: Developed a real-time data system using PySpark & Kafka; enabled scalable storage with HBase.

Database Migration:

    Role: Data Engineer.
    Tech: PostgreSQL, Spark, Kafka, MongoDB.
    Details: Migrated data from Oracle to PostgreSQL using PySpark, Kafka, and MongoDB.

Cloud Migration:

    Role: Data Engineer.
    Tech: HDP, Apache Nifi, Hive, AWS S3, Snowflake.
    Details: Transitioned data to AWS S3 and Snowflake; aided a telecom client's transition from Oracle to HDP.

MDM-Group - Big Data Engineer (Nov 2019 - Jan 2021)

ETL Enhancement:

    Role: Data Engineer.
    Tech: Airflow, HDFS.
    Details: Optimized ETL workflows using Apache Airflow.

BI & Visualization:

    Role: BI Specialist.
    Tech: Superset, HDP.
    Details: Integrated Superset with HDP for real-time insights.

Kyiv Polytechnic Institute StartUp - QA Engineer (Aug 2018 - Feb 2019)

Academic Platforms QA:

    Role: QA Specialist.
    Tech: JMeter, Python.
    Details: Quality assurance for academic platforms using manual and automated testing.

Current Learning:
Enhancing skills in Tableau for better data visualization. Diving into Kubernetes for efficient model deployment.
"
data engineer,"
Analytic and monitoring system of production lines:
• Applied Python frameworks (Pandas, NumPy, SQLalchemy) for Data manipulating
• Optimized the generation of weekly reports using SQL and Python, reducing time costs by 80%
• Created a concept of data visualization with Tableau which included critical KPIs and production metrics
• Implemented Airflow for scheduling data pipelines

Pet-project
Developed system realized with aiogram and Airflow that send daily messages to the user with information about new opened jobs, and weekly reports about the market dynamic based on user’s needs. System collects data about open jobs using API, extracts information from collected JSON files, transforms data to Data Frame and loads to PostgreSQL Database, then extracts new job positions and sends it to users.
"
data engineer,"
An experienced bank manager with 10-year experience in bank/finance domain. Last 2 years on a position of head of a bank branch. Practical skills in analysis, planning, process optimization, problem-solving, negotiating, sales, communication with customers, handling meetings with customers. 
In 2018 attended courses of IT Business Analysis. Basic knowledge of different software development methodologies. Basic knowledge of modeling notations (UML, BPMN) and BA methodology and tools (functional/non-functional requirements, use cases, user stories, traceability). Basic knowledge in Python programming language. Experienced user of Microsoft Office.  

Eager to learn new things. Despite a lack of particular experience as a BA, hope to make a great contribution to the future team. My developed soft skills will help me to dive into technical details easily and within a short period of time.
"
data engineer,"
Apr-2022 – Now(2023) - Big Data Software Engineer, SoftServe, 
Project: Implementing Data Warehouse in BigQuery
Team Size: Dev team - 5 DQ team - 1 DevOps - 2 Support - 2 PM - 1
Project Role: Developer
Tasks performed:
•	Support data pipelines running on production environment
•	Implement new data pipelines from various sources (SFTP/API/RDS)
•	Migrate old legacy pipelines to be more flexible/extendable and runnable on GCP ecosystem
•	Resolve incidents/change requests; communicate with requestors and users on daily basis
Environment:
•	GCP BigQuery
•	GCP Composer
•	GCR, GCS, GSM, Cloud Function

Nov-2021 - Mar-2022 - Big Data Software Engineer , EPAM Systems

Customer: EV - Other
Project: Augment client's team to build data lake in Redshift
Team Size: Dev team - 3 DQ team - 3 DevOps - 2 PM - 1
Project Role: Develop
Tasks performed:
•	Develop new ETL jobs.
•	Supporting old jobs, enhancing them with new requirements.
•	Communicating requirements with clients.
•	Refactoring solutions and proposing improvements, such as code, libraries, tools, etc.
Environment:
•	Redshift
•	AWS, Python, Serverless, Jenkins
•	AWS Lambda, EC2, S3, Glue, Athena, Redshift, AppSync

Feb-2020 - Oct-2021 - Big Data Software Engineer, EPAM Systems
Customer: EV - Other
Project: Reimplement Cloudera-based Big Data system in AWS
Team Size: Dev team - 3 DQ team - 3 Automations - 1 DevOps - 2 Support - 2 PM - 1
Project Role: Developer
Tasks performed:
•	Take an unstable on-premises PoC system and create a scalable production-ready platform within a cloud;
•	Delivery of API extensions to support numerous Customer’s Data Analytics applications;
•	Exponential system loads at the harvest time;
•	Use of variety of sources for genotype & phenotype data (varying from 10 rows to bills of recs);
•	Conduct technology assessment, select and deploy cloud-based toolkit for data scientist to extract high-volume data sets and run analytic & machine learning workloads;
•	Provide system stability during peak workloads (harvest seasons);
•	Contain operational costs;
Environment:
•	AWS - Aurora, Redshift, Glue Postgres for metadata and Airflow
•	AWS, Python, Airflow, Serverless, Jenkins
•	AWS Lambda, EMR, EC2, S3, Glue, Step Functions, Athena, Redshift, AppSync, Event Bridge Airflow, Postgres.
"
data engineer,"
• Architected end-to-end AWS data pipeline and data warehouse
using AWS components such as Glue, Athena, Spark, S3, Lambda
functions, and EMR clusters.
• Engineered data warehouse using Hive, created and managed Hive
tables.
• Engineered spark code using python for faster testing and
processing of data.
• Modeled, aggregated, and triangulated multiple Log data and
populated to RDS data
• Devised high-performing data warehouse systems, optimizing
business intelligence and analytics.
• Managed life cycle elements of ETL development, from robust
testing to final deployment.
• Computed Machine Learning resume_classifier (using Deep Learning,
Bayesian Networks, SVM), applied to balance and deploy
resume_classifier.
• Extrapolated high dimensional health data.
• Developed briefs and curriculum for the Ministry of Health of Ethiopia
"
data engineer,"
A results-driven and experienced data engineer with 3 years of experience in the field, adept at designing, developing, and implementing robust and scalable data pipeline solutions. Possessing a strong technical background in programming languages such as Python and SQL, as well as hands-on experience with big data technologies such as Hadoop and Spark. A deep understanding of data warehousing principles and methodologies, and a proven ability to work with both structured and unstructured data.
"
data engineer,"
Armenian based data engineer with more than 10 years of experience in DWH/BI projecs using MS BI, Oracle and cloud-native stack
"
data engineer,"- Successfully built and maintained highly available data pipelines for a large tourism company.

- Developed and implemented ETL processes that resulted in a 20% reduction in data processing time.

- Received recognition from senior management for exceptional teamwork, problem-solving skills, and dedication to delivering high-quality results.

- Led the development of an innovative software application for a business which provides driving exams, resulting in increased efficiency and improved student outcomes.

- Built and deployed complex data pipelines for multiple clients, resulting in more streamlined data processing and analysis.
As a Data Engineer at Vention, I have completed a variety of projects and tasks utilizing a range of technologies to build and maintain highly available data pipelines. In one project, I collaborated with the data science team to build a real-time data processing pipeline. This involved building and optimizing ETL processes using Apache Spark and Apache Kafka, as well as developing automated data quality checks using Python and SQL.

In another project, I led the implementation of a cloud-based data warehousing solution using Snowflake for a retail client. This involved designing the schema and developing ETL processes to move data from multiple sources into the data warehouse. I also implemented a scheduling and monitoring solution using Apache Airflow and Grafana.

My skills in data engineering include proficiency in SQL and Python, experience with Apache Spark and Kafka, and expertise in cloud-based data warehousing solutions such as Amazon Redshift and Snowflake.

Moving forward, I am eager to further develop my data analytical skills. I plan to take courses in statistics and machine learning to better understand the data being processed and to help optimize the pipelines for better performance. I am also interested in exploring new technologies such as Apache Flink and Apache Nifi to improve the scalability and efficiency of the data pipelines.
I want to work in a Project where they use cloud technologies especially AWS. I am looking forward to work in a very communicative and supportive team. I would be happy to work in a Big Data field."
data engineer,"Master degree in Information Technology in Azerbaijan.
Oracle Certified Associate (1z0-071).
Oracle Cloud Infrastructure Foundations Associate Certificate.
PCEP – Certified Entry-Level Python Programmer
As a Data Engineer, I bring extensive experience in SQL, Python, and ETL, with a track record of successfully managing and executing complex data projects. My expertise in data modeling and database design has helped me design efficient and scalable data architectures that have contributed to the growth and success of multiple projects.

Currently, I am expanding my skill set by learning Cloud, Kubernetes, and Big-Data technologies, demonstrating a commitment to continuous learning and development. My passion for working with data and solving complex problems motivates me to stay up-to-date with the latest tools and techniques in the industry.

I am a proactive team player with excellent communication skills and the ability to work collaboratively with cross-functional teams. I thrive in dynamic and fast-paced environments, and I am always looking for opportunities to innovate and optimize data workflows to deliver high-quality results.
Expecting flexible working hours."
data engineer,"
As a Data Engineer, I have completed various projects and tasks that involved working with Python, SQL, Apache Spark, Apache Airflow, and other technologies. Some of the projects that I am most proud of are:

•  Migrating data from an in-house environment to GCP cloud (or GCP to Azure Cloud) leveraging DataFlow, PubSub, BigQuery (DataBricks, Data Factory, Logic Apps)
•  Implementing a data cataloging, templating and governance system ground up before DBT was a thing.
•  Automating data pipelines using Apache Airflow as a successor to Apache Oozie. As of now, the Apache Airflow setup not only has dynamic tasks configured just from YAML files, but also auto-resolving dependency tree parsed from SQL.

My current role in the team is to lead the design and development of the data engineering solutions, mentor and guide the junior data engineers, collaborate and communicate with Product people to encourage data-driven decisions.
"
data engineer,"
As a Data Engineer, I have successfully connected and prepared data from multiple sources to ensure seamless integration and analysis. I have created and managed data storage solutions in various databases, including both relational and NoSQL systems. Additionally, I have designed and implemented a robust data warehouse to facilitate efficient data analysis.

In order to streamline the flow of data within our organization, I have developed ETL pipelines with workflow orchestration using both Airflow and Prefect. Furthermore, I have created custom Python libraries for internal use to improve our team’s productivity.

I also have experience in developing and deploying cloud solutions using both GCP & AWS platforms. This has allowed us to leverage the power of cloud computing to enhance our data processing capabilities. In addition to this, I have created sophisticated Docker solutions to improve the portability and scalability of our applications.
"
data engineer,"Led the creation of comprehensive Data Platforms from scratch, effectively handling all aspects from data ingestion to Data Warehouse, Data modeling, and building both forward and reverse ETL pipelines.

Successfully deployed complex data-centric systems to production environments.

Handled and processed large-scale data sets, working with volumes in the range of hundreds of terabytes.

Successfully established and grew effective data engineering teams.

For one of the clients, I reduced Snowflake costs by up to 35% and AWS costs by up to 30%.

Initiated and implemented the use of DBT within the organization, enabling analysts to write their own data transformation pipelines.
As a Data Engineer, I have worked on projects in the Ad-Tech industry, handling high query volumes and managing terabytes of data for both SSP and DSP. I possess expertise in programmatic ads.
Tech: Scala, Python, Apache Spark, AWS, GCP. 
Both batch and stream data processing

Building Analytical Data Platforms from scratch for eCommerce companies. Model of a Data Warehouse.
Snowflake, DBT, Python, Go

Worked in Telecom as a Data Engineer. Building of Data platform to analyze the mobile network telemetry and predict the possible failures.

I have experience in migrating on-premise Data Warehouse and ETL into Cloud.

Having predominantly worked with startups, I deeply understand the importance of ownership, delivering fast results, and maintaining a problem-solving attitude. 

In addition to my data engineering expertise, I possess strong backend development skills in Python, Scala, and Go. I am also a beginner in Rust, continuously expanding my technical toolkit to adapt to evolving technologies and industry demands.

Overall, my diverse experience showcases my ability to deliver effective data engineering solutions using various technologies.

I'm looking for a job as a Senior/Staff Data Engineer or Backend engineer.
"
data engineer,"
As a data engineer with 3 years of experience, I've worked extensively
with big data, including organizing data warehouses, optimizing
procedures, and utilizing tools such as SSIS packages. I'm also familiar
with the Azure Cloud platform. My proactive and confident approach to
problem-solving has helped me excel in my field, and I'm excited to
continue growing and exploring new opportunities.
"
data engineer,"
As a Data Engineer with an M.Sc. in Computer Science, I possess expertise in SQL, Python, data preprocessing, transformation, visualization, analytics, feature engineering, and modeling. I have experience building fault-tolerant, distributed, and scalable end-to-end data pipelines, and am committed to delivering effective solutions that help organizations leverage their data to drive meaningful insights and achieve their strategic goals.
"
data engineer,"
As a highly skilled Data Engineer with a solid background in Data Analysis and Python Development, I possess a unique combination of technical expertise and industry experience. My proficiency in Python and SQL, in addition to hands-on experience with various tools and technologies such as Spark, AWS, Azure, Hive, MongoDB, Databricks, Snowflake, and PowerBI, enables me to effectively design, develop and implement data pipelines and business intelligence solutions.

I have a proven track record of success in building efficient and reliable ETL pipelines using Azure Data Factory, AWS and other tools, as well as in implementing robust business intelligence solutions using Power BI, SSRS, and dbt. I have completed the Google Professional Data Analytics course and am currently preparing to obtain Cloud certifications in AWS and GCP, further demonstrating my commitment to staying current with the latest industry trends and technologies.

I am an ambitious, results-driven professional who is passionate about leveraging data to drive business growth and success. I am confident that my expertise and experience make me an ideal candidate for any data engineering role and I am excited to contribute my skills to help organizations achieve their data-driven goals.
"
data engineer,"Certified Azure Data Engineer
Certified GCP Data Engineer
As a Senior Data Engineer at GlobusAI, I have completed several projects and tasks that have contributed to the success of the company. My main responsibilities include designing and implementing analytics platforms, automating data preparation and deployment, developing ETL pipelines, and ensuring smooth client onboarding and data migration.

One of my notable achievements was designing and implementing an analytics platform from scratch, which led to a 10% increase in client retention. To accomplish this, I utilized a range of technologies such as Airflow, Azure Log Analytics, Azure Blob Storage, MongoDB, PostgreSQL, DBT, Superset, Python, and SQL. This platform enabled seamless integrations with third-party client services through the development of scheduled and event-driven data pipelines using Azure Functions, Airflow, Logic Apps, Azure Queue Storage, Sharepoint, and Python.

I also optimized the data preparation process for Superset dashboards by implementing dimensional modeling in the data warehouse, resulting in a significant 5-fold improvement in speed. By implementing Superset dashboards with DBT and Great Expectations, I proactively monitored and enhanced the data quality of the central data warehouse, leading to a 30% reduction in report errors.

As the Senior Data Engineer, I am responsible for leading and collaborating with the product and business teams to ensure successful client onboarding and data migration. Additionally, I have automated the deployment of Power BI dashboards using Azure DevOps, Power BI REST API, and Python, resulting in a 2-fold reduction in client onboarding duration.

In terms of future improvement, I am keen on further enhancing the efficiency and scalability of the ETL pipelines by implementing CI/CD pipelines for all ETL processes using Azure DevOps. I strive to stay updated with the latest industry trends and technologies and continuously improve my skills in cloud platforms such as Google Cloud and Azure. Additionally, I aim to develop expertise in advanced data analytics techniques and machine learning to drive more impactful insights and value from the data.

Overall, I am proud of the projects I have completed and the positive impact they have had on client retention, data quality, and automation. I am excited to continue my role as a Senior Data Engineer, driving innovation, and contributing to the success of the organization.
Good team, interesting projects, moderate payment"
data engineer,"
As a Senior Data Engineer, I have: 

1. Designed and built pipelines to stream data from ERP Systems (SAP) using Apache NiFi to be consumed by a Last Mile Delivery application used to manage deliveries of the company. 

2. Ingested data from disparate sources such as SAP, Postgres, MongoDB, and MySQL into a data warehouse (Redshift) integrated with dbt to power transformations. 

3. Designed and implemented a real-time data pipeline to process semi-structured data to extract analytical points on game characters and players by integrating million raw records from the Riot Games API for League of Legends using Python, Spark, Databricks, and Azure Data Factory.

4. Collaborated with data scientists to extract and present analysis and recommendations using Power B.I. on 2 established and prospective customers, competitors, marketing channels, and resources to respond to rival marketing campaigns and improve unique selling points by 55%.

5. Developed a microservice with Microsoft REST APIs to organize and authenticate users and embedded Power B.I analytics into an application using the Django framework.

6. Managed the validation, documentation, and profiling of data to maintain quality and improve communication between teams.
"
data engineer,"Motivated and interested in learning. Experienced PC user. I learn new programs quickly and easily adapt to assigned tasks. I love to work with typesetting and formatting Text. Also, I study programming languages HTML, CSS, and JavaScript. I want to realize myself in the field of IT.
As a Сomputer science teacher I effectively utilized Google Suite (Docs, Forms, slides, sheets, and calendars) and other computer software to create meaningful lesson plans.
Was responsible for the software of class computers, updated, and adjusted it as needed.
Managed the school website page and was responsible for its content.
Created systems for effective communication with parents and students.
Organized and conducted distance learning using Google Class, Learning apps, Vchy.ua, Kahoot, and Zoom platforms.
Held tournaments and quests in computer science.
"
data engineer,"
As Data Engineer, I can able to data wrangling and cleaning (SQL & NOSQ),
make Machine Learning Models for specific areas using Python (libraries: Pandas,
Numpy, Scikitlearn, Matplotlib, etc), and data visualization interactive web apps
straight from Shiny, Panel, Tableu, such as:
1. Developing, implementing and maintaining data pipelines to support ETL
processes (using Talend, Jupyter) from multiple data sources like SQL
Server, PostgreSQL, Mysql, AWS S3, etc.
2. Experience with SQL, NoSQL (MongoDB), relational database design,
methods for efficiently retrieving data, as well as data
preparation/wrangling both on demand.
3. Collect and process raw data at scale (including writing scripts, web
scraping, calling APIs, Dockerfile, write SQL queries, etc.)
4. Be an active part in the development of innovation and strategy of a brand
or product.
5. Cloud Experience (AWS Glue and Sagemaker), Azure Data Factory, Azure
Data Lake.
"
data engineer,"Completed Courses:
1) Data Science Fundamentals by DataRoot Labs (October 2021)
2) Learn Python 3 Course by Codecademy (February 2021)
3) Analyzing Data with Python by edX (December 2020)
4) Python Basics for Data Science by edX (November 2020)
5) SQL for Data Science by edX (November 2020)
6) Practical Git: for Absolute Beginners(October 2021)
At this time helping as volunteer coordinator for students on Data Science Fundamentals course by DataRoot Labs.
While studying on Data Science Fundamentals, got basic knowledge/experience in the following:

1) Math Fundamentals for Data Science: Linear Algebra, Statistics&Probability
2) Practising basic Python algorithms & data structures, OOP  and Python libraries(numPy, matplotlib, pandas, sklearn) in Jupyter Notebook.
3) Supervised & Unsupervised Learning that included: labs on regression, classification, dimensionality reduction PCA for image, etc implemented in Jupyter Notebook.
4) Implemented project in PyCharm using template with a ready made structure and instructions. Created an app with DB using Postgres, DB and app interactions were done with the help of SQLAlchemy.  Created API methods and routes for app, used Flask for testing and Docker to deploy it.
5) The last project has been also implemented in PyCharm and Jupyter Notebook given a full working code example using any Kaggle data according to the preference. 
As data was initially clean, features were only label encoded
Trained the model using DecisionTreeClassifier(), RandomForestClassifier(), AdaBoostClassifier(), GradientBoostingClassifier().
"
data engineer,"Data Archiving Strategy:
Accomplishment: Developed a data archiving strategy using Python and SQL to move historical data from the primary database to a low-cost storage solution. This optimized the primary database's performance and reduced storage costs, resulting in 20% savings in infrastructure expenses.
Automated Data Testing Suite:
Accomplishment: Designed and implemented an automated data testing suite using Python's testing frameworks. The suite included unit tests and integration tests for data pipelines, ensuring the accuracy and integrity of data transformations. As a result, data-related issues were detected early in the development process, reducing the number of production incidents by 20%.
Automation and Scripting: Python is frequently used for automating repetitive tasks and writing scripts for system administration.
Data Analysis and Data Science: Python's libraries such as NumPy, Pandas, and Matplotlib facilitate data manipulation, analysis, and visualization.
System Administration: Linux administrators manage servers, configure networks, and ensure system security and stability.
"
data engineer,"Azure Data Engineer DP-203 certification
PCEP certification
PCAP certification
AWS, Azure cloud platforms.
Design and implement complex data workflows.
Experienced building data pipelines and applications at scale.
Experienced with Spark (PySpark).
DWH, Storage modeling and maintaining.
Excellent Python skills.
Team lead experience.

Software engineering and project management - Bachelor`s degree
Data Engineering project.

Please, do not suggest any web collecting/scraping project."
data engineer,"- PhD in Life Science;
- Certifications:
        AWS: Data Analytics Speciality, Developer Associate;
        Coursera: Deep Learning Specialization;
AWS Certified Data Engineer with 6+ years of experience in Data Engineering. Strong knowledge of data architecture, including data ingestion pipeline design, data modelling, machine learning and advanced data processing.

Technologies: Python, SQL, PySpark, AWS services (Redshift, Glue, Lambda, S3, etc), Azure (Data Factory, Synapse), Databricks, Kafka, Pandas, Sk-Learn, PyTorch, etc.

Certifications:
        AWS: Data Analytics Speciality, Developer Associate
        Coursera: Deep Learning Specialization 

Recent projects:
Project 1:
        Data from different sources were collected in Data Lake with real-time and batch data ingestion jobs, with following transformation and loading into Data Warehouse.
    • Optimized 3 modules in company's platform library that decreased data processing time for the R&D teams;
    • Decreased data processing time and reduced expenditures (from 10% to 120%) by splitting logic between Batch and Streaming mode;
    • Designed and automated QA dashboard that allowed to catch data issues on the earliest steps.

Project 2:
        Data Lake migration: The goal of the project is to build a Data Lake that allows to monitor and control product logistic and distributions. Our team developed and installed a Monitoring Solution to automatically detect potential issues with the product.
    • Designed and developed new advertise module that clusterizes incoming events into similar group and integrates with other services;
    • Deployed and supported architecture to detect product frauds that is now being actively used in more than 15 countries; 
    • Increased data reprocessing and fault tolerance by adding silver (intermediate) Data Lake in pipelines;
    • Provided leadership in a sub-team (4 engineers) for 6+ months;
    • Optimized ML resume_classifier with accuracy increase up to 12% by implementing assembled resume_classifier.
"
data engineer,"
AWS-certified Data Solutions Developer with 4+ years of experience in DA and BI. Committed to helping companies resolve business challenges, design and implement data analytics solutions.
DWH: Redshift, PostgreSQL, MySQL
Data Lake: S3, AWS Lake Formation, Snowflake
Data integration: AWS Glue, Lambda, AirFlow, Talend
Programming: SQL, PySpark, Python
Data Visualization: Tableau, Power BI, Data Studio, AWS QuickSight
"
data engineer,"
Beter Nov 2021 - ...
DB\BI Engineer 
-Developing etl pipelines for data loading from various sources (such as operational DB, S3
storage, APIs, Excel sheets). Technologies: MSSQL Server, Python, AWS, SSIS, Airflow
-Building a database architecture for OLAP cubes
-Aggregation of information to define metrics inside the data warehouse
-Development of own projects based on Docker for collecting information from websites and
automation of routine processes (Selenium)
-Processing of large data sets in cloud services (PySpark, AWS Athena, S3)
-Using elementary machine learning methods for processing time series data
- I will be able to develop my skill
- Complex and interesting projects where I will be useful"
data engineer,"Implemented  Data Quality practice from scratch.
Implemented TDD in DataMart development.
Betting and Healthcare domain
Ingest data sets from multiple sources(web scrapping, API).
Collect metadata.
Data Quality Check.
Automate data pipeline (airflow).
Develop ETL and ELT Pipelines (Spark, SQL).
Data Lineage.
"
data engineer,"
BI: design reports and all related stuff from creating ETL and data resume_classifier to building visuals. Qliksens, PowerBI, SAP, ms sql, pl SQL.
Data Engineer: creating data pipelines using Python under orchestration of mwaa, design dwh's in Redshift, supporting ongoing flows. Python, AWS services, SQL.
Ready to consider offers with less Salary, but with more tech stack, including Docker maintenance, Unix usage and etc."
data engineer,"
Bi Developer
VisiQuate
2020 Aug - Till present

Solve business data requests via Microstrategy, SnowFlake, AWS, Azzure and Microsoft Tech stack.

Bi Developer
MedeAnalytics
2017 Dec - 2020 Aug

Participate as BI in cross functional team Productivity Analytics project (2019 till now)

The goal is to build an automated, role based measurement module into Patient Access Information system to measure 100% of role-required employee activity (in real-time) and critical points of employee revenue cycle impact (over the longer term)

• Make different POCs by using elk-stack, kafka+vertica to obtain the right approach for the flexible and scalable product solution 
• Identify data sources are required for different measures (WEB - User input data, 835 - Claims/Denials/Adjustments, HL7 - Patient registration information e.t.c.)
• Solve integration tasks between different systems to align data formats to match records.
• Design data patterns to export from external system and inject data into data warehouse (Spark- HDFS - Vertica).
• Build data flow for real-time data reports and set up ETL for long-term(historical) data points.
• Optimize queries to get the best performance for real-time reports.
• Create customer faced data reports (Mondrian + DVT)

Play BI role in Cost of Operations project (2017 till now)

• Develop 4 products aimed at efficient care delivery: Service Line Analytics, Labor Productivity, Supply Chain and Throughput (Emergency Department & Surgical Service) Claims/Denials/Adjustments, HL7 - Patient registration information e.t.c.)
• Design and build integrated parts to meet criteria of “Enterprise Analytics” strategy. Used Vertica,Sql Server and Mondrian as primary stack.
• Craft different type of reports in SSRS and in Web Enterprise Platform
• Build data flow for real-time data reports and set up ETL for long-term(historical) data points.
• Help to implement products for the clients and support them
• Full filled Scrum Master during her maternity leave. Handled multiple meetings, backlog grooming, inspected velocity and burndown line, added stories and checked their progress during sprints, helped to get fast answers for blocked stories.


e.t.c.
"
data engineer,"
BI Engineer / Data Engineer
Jul 2022 - Present

Responsibilities:
- Aggregating data from the different data sources
- Ensuring correctness of data in the data warehouse
- Creating dashboards in Logi Analytics
- Developing an additional application 
- Writing documentation

Achievements:
- Developed tools for easier and faster reporting deployment
- Reduced inconsistencies in the database
- Build a reporting tool that provides brief information about data matching
- Wrote production-ready CI/CD scripts


Data analyst / Data Engineer
Oct 2020 - Jul 2022

Responsibilities:
- Aggregating data from the different data sources
- Developing reliable data source based on BigQuery
- Creating dashboards in Tableau
- Ad hoc reporting
- Developing an analytical framework for using within a team
- Writing documentation

Achievements:
- Wrote framework for connecting to multiple sources
- Initiated and ensured the transition from cron to another automation system that allows you to build pipelines, parallel processes, dependencies, etc
- Initiated the transition to BigQuery as the only data warehouse with testing of tables and data integrity. Now we use only it when building reports
- Redesigned many dashboards, both visually and conceptually, and held clarifying meetings with customers.
- Initiated and implemented the transition from periodical reports in Excel to dashboards in Tableau
- Unified the naming of metrics, and documented them in the knowledge base

Junior data analyst
Dec 2019 – Sep 2021

Achievements:
- Modified existing SQL queries to optimize efficiency and decreased execution time by up to 50%
- Created database structure for the client
- Wrote different macros on VBA for weekly reports


------
Skills
Databases
- Familiar with: MySQL, PostgreSQL, ClickHouse, MongoDB, MS SQL, BigQuery
- Experience in optimization and performance tuning

Programming
- Applied experience in Python
- Experience in designing, and developing data pipelines
- Experience operating with distributed systems
- Experience with data modeling, data warehousing
- Knowledge of REST/GraphQL APIs
- Experience with Git(GitHub, GitLab), CI/CD, Docker, dbt
- Orchestration tools: Airflow, Argo Workflow

Analytics
- Developed analytics for different business needs
- Automate periodical reporting
- BI: Tableau, DataStudio
"
data engineer,"
Big Data Engineer
Developing Python applications for integrating, transforming, and consolidating data from various
structured and unstructured data systems into a structure that is suitable for building analytics solutions.
Researching new methods of obtaining valuable data and improving its quality.
Technologies & Tools
Python, Spark, Spark Streaming, Databricks, Delta Lake, Delta Live Tables, Kafka, Azure, SQL, Debezium,
Docker, Git, Linux, Scrum, Jira.

Back End Developer 
Developing back-end of web applications
Technologies & Tools
Python, Django, Django REST framework, React, JavaScript, HTML, CSS, Git, Docker
"
data engineer,"
Big Data Software Engineer
Responsibilities:
· creating views, developing ETL scripts in Teradata;
· developing and supporting NRT DAGs using Apache Airflow & Python;
· collaborating with Business Analysts and stakeholders on projects requirements;
· consuming data from Kafka topics into Teradata using Apache Spark;
· creating pipelines that output data into AWS S3 buckets for Business Intelligence Engineers;
· implementing scripts for backfilling data into production fact tables;
· on-call activities, documenting solutions for arising issues.

Big Data Software Engineer
Responsibilities:
· creating views and stored procedures in BigQuery and Snowflake;
· working on sharding, partitioning, clustering optimization in BigQuery;
· supporting Data Analysts team (reviewing PRs, creating new sources);
· integrating Salesforce sources with Snowflake DW using Fivetran and Segment;
· creating integrations from AWS S3 to Snowflake using AWS SNS topics and Snowpipes;
· executing performance scripts, parsing raw data, documenting implemented solutions;
· developing and supporting ETL pipelines using Apache Airflow & Python;
· configuring data unload from MongoDB, PostgreSQL, MySQL to GCS and AWS S3.

Data Warehouse & Business Intelligence Engineer
Responsibilities:
· creating views and stored procedures in Oracle;
· developing and supporting Qlik Sense dashboards;
· modelling section access structure for a different level application security;
· developing and supporting NPrinting 17+ reports (PixelPerfect, Excel, HTML);
· collecting and analysis requirements for the projects from customers and end-users;
· creating a three-level architecture for projects in Qlik Sense;
· migrating dashboards from QlikView to Qlik Sense and NPrinting reports from 16 to 17+;
· developing and supporting IBM DataStage ETL processes (from files, databases to Oracle);
· developing and supporting SSIS ETL processes (from Salesforce, Google Analytics to Oracle).

Data Warehouse & Business Intelligence Engineer
Responsibilities:
· creating views and stored procedures in MS SQL Server;
· developing and supporting Qlik Sense dashboards;
· uploading customer sales on a daily or weekly basis;
· administrating a system for training and testing (edX);
· creating advanced SQL queries, producing reports using Qlik Sense;
· generating reports by mail from dbms on a weekly or monthly basis;
· unloading user testing results using MySQL, MongoDB and MS SQL Server.
"
data engineer,"
Bike Rental Service 
Implemented data pipelines such as:
1) Spark streaming from MinIO into Kafka topic
2) Spark batch pre-processing data for further computation
3) Spark batch processing two datasets from MinIO and Cassandra with further saving to Elastic and visualization in Kibana
4) Kafka alert producer via Kafka Streams API
Additionally, I was responsible for: Spark Stateful Streaming operations; building uber-jars and creating common Utils module using Maven; Airflow with Spark containerization and deployment using custom image.
As a team member, I was involved in writing docker-compose files and entrypoint scripts for setting up the environment, configurating spark to execute multiple jobs in FAIR mode, creating DAG for batch jobs orchestration via airflow and bug-fixing on every development stage as well.
"
data engineer,"
Building ETL pipelines using AWS Glue (Spark) in Scala. 
Implementing AWS API Gateway + AWS Lambda based API interface.
Working with AWS Lambda to execute a variety of other tasks, for example Email sending, schema validation, workflow configuration backup.
Interested in a switch to Python/PySpark.
"
data engineer,"Built data warehouse on Hadoop+Hive from scratch and set up ETL from various datasources
I have done various research projects to improve company performance (Fraud detection, Tracking issues, etc)
- building, tuning and maintaining Data Warehouse
- design, setup and launch ETL processes
- providing complex custom reports for Product Analysts and Product Managers for different projects
- monitoring of data quality
- monitoring business metrics, issues investigation and researching
- experience in processing both structured and unstructured data
Would like to work in a data-driven company with technical challenges that will allow enhancing my professional skills"
data engineer,"Graduated with distinction in ML Engineering, Data Engineering, and Web3 intensive training/internship from 10 Academy.
Built an ELT Data Warehouse pipeline for traffic data collected through swarm UAVs.
Helped build an ETL data collection pipeline using Airflow, Spark, and Kafka.
Helped to build an end-to-end MLOps for an A/B ad campaign performance hypothesis test using Machine Learning.
Helped to build a scalable backtesting framework for crypto-trading strategies resulting in 70% profit.
Built a causal model for a client that provides delivery services in Africa.
"
data engineer,"
Business Intelligence Professional with 4+ years of experience
- Oracle Certified Specialist (OCA-071)
- Oracle Database PL/SQL Developer Certified Professional
- 4 years working within a financial environment
- Expertise in building templates, dashboards and analyzing data using BI Tools (Power BI, Oracle BI, Tableau)
- Expert knowledge of Oracle and Postgre SQL (3+ years) 
- Experience with PL SQL/T-SQL languages
- Statistics and modelling expertise
- English – Upper-intermediate
"
data engineer,"I am Senior Level Data Engineer with about 6 years of experience in the field Business Intelligence. I have huge experience in SQL Database development using mostly TSQL , PL SQL, MySQL and many others; and  ETL work using SSIS and Azure Data Factories mostly;  and Datawarehouse Design , Report writing using different tools like PowerBI and SSRS mostly.
T-SQL is one of my strongest skills. I created several complex and efficient stored procedures, user defined functions, triggers, views, indexes and almost all other database objects. Also, I am strong at SQL performance tuning and troubleshooting.
For Data Engineering and ETL work, I have done development, configuration, and deployment of many SSIS pkgs and Azure Data Factory pipelines in both OLAP and OLTP environment. 
I have also great experience in data modeling for creating ERD resume_classifier for OLTP systems and Dimensional Models for Datawarehouse.
CLIENT:         DataSite Technology
  ROLE:             SQL BI SSAS, SSRS, SSIS Developer 
  DURATION: June-2017 – May-2019
 
 CLIENT:         Shell
  ROLE:             ETL/MSBI Lead, Architect 
  DURATION: August-2019 – Present
"
data engineer,"
• Cloud: Azure: Data Factory, Databricks, SQL DB, Synapse, Storage account
• Data analysis: Python (expirienced), R (basic level)
• MS SQL Server (experienced)
• Reporting and dashboards: MS Reporting Services, MS Power BI
• ETL: MS Integration services, Control-M
• Team management: building up and management of a software development team
• Technical design: developing concepts for the implementation of new requirements, defining user stories, prioritizing requirements and maintaining the product backlog, planning of release and responsibility for functionality, usability, performance and quality

Summary

• 8 years of working experience in database development area, more than 10 years in software development area.
• Persistence in achieving goals, team player, good communication skills, high level of adaptability, leadership skills, strong mathematical and analytical skills, high level of responsibility.
"
data engineer,"
Collected and cleansed large datasets, applying data wrangling techniques to ensure data quality and consistency. Implemented data validation and cleansing processes to identify and rectify anomalies and discrepancies in the data.
Utilized Spark to implement distributed data processing, enabling faster and more efficient analysis of big data. Leveraged Spark's capabilities for parallel processing and in-memory computing to handle large-scale data sets and improve data processing speed.
Developed robust ETL pipelines using Airflow, orchestrating the end-to-end data flow from various sources to the target database. Created and maintained DAGs (Directed Acyclic Graphs) to schedule and monitor data extraction, transformation, and loading tasks.
Collaborated closely with project managers and analysts to understand their data requirements and deliver optimized data pipelines that supported their KPI tracking needs. Translated business requirements into technical specifications, ensuring the data pipelines met the desired outcomes.
Actively participated in project documentation design and maintenance, ensuring that all processes and procedures were well-documented and easily accessible to team members. Created comprehensive documentation for data pipelines, including data flow diagrams, data mappings, and transformation rules.
Worked with a range of tools and technologies including Python, ETL, PySpark, Postgres, Redshift, Alembic, Airflow, AWS Services, Jenkins, and BitBucket. Leveraged these technologies to design, develop, and deploy data solutions that met business needs.
"
data engineer,"Commercial experience is absent, but I've completed the DataRoot Labs Data Science course, and several other courses about Machine Learning (by Michigan university on Coursera) and Deep Learning (by Andrew Ng on Coursera). 
Currently working as a Junior Game Tester
Competed in several Kaggle Competitions, created Exploratory Analysis Jupyter Notebooks, where I looked at apartments dataset, which I webscrapped myself, and a restaurants of Kyiv according to TripAdvisor data set, which I also scrapped myself.
"
data engineer,"
Completed several SAP HANA DWH BI projects. 

Responsibilities: Data modeling using Dimension Views, Star Joins, Calculation, and Custom/Scripted Views. Modeling security resume_classifier in HANA specific to schemas, packages, and analytical privileges
Developing Tables, Procedures, Table Functions, XSJS/XS OData API services Performance Tuning and Query Optimization
Developing Data Extraction Transfer Loading (ETL) processes using SAP Data Services and Python.

The reporting and Business Intelligence system of Telecommunication Management that includes Ad-Hoc Analytics, On-Demand Data Exports, Interactive Dashboards.

Responsibilities: Designing, developing, and maintaining a reporting environment. Developing PL/SQL Stored Procedures, Functions, Packages to implement Business Rules, preparing ad-hoc SQL reports. Designing, developing Universes in Business Objects Designer for generating the marketing reports
Optimizing the universe and SQL tuning to increase the query performance.
Highly skilled data engineer with over 9 years of experience in business intelligence, data management, and data engineering. Proficient in Python and experience in developing ETL pipelines, data modeling, and building data systems using Hadoop Stack and various databases such as Vertica, MySQL, and SingleStore. Strong background in SAP HANA Financial Reporting and Oracle database. Looking to further develop skills in data system architecture"
data engineer,"
Configured Amazon Kinesis Data Firehose to extract streaming data from the source
and then upload it to S3. Created delivery stream in which configured partition of
extracted data.
Created and deployed a CloudFormation template for an S3 bucket in which
configured the Name of the bucket and access rights.
Scripting: Written python script for reading the parquet data from the corresponding
S3 Location using the boto3 client for S3. Extracted partition attribute value
separately from the respective partition prefix. Created class that corresponds to the
events structure and mapped the events to the class instance. Collected instances in
the list and output to stdout.
Created and configured Glue Job with the Spark script editor, which uses the
concatenate and aggregate functions, added a new column to the created dataframe,
which is calculated ecpm according to a formula in which contained data from other
columns. Uploaded data to S3 storage.
Created dashboard in Grafana in which added 4 panels in which displayed
information by device for some metrics. Performed query using the Athena query
string to query data in AWS Glue data table created before.
"
data engineer,"
Create and build data warehouse platform from zero for a banking company. which includes optimizing and creating ETL processes using Python, Pyspark. migrate, modify a data with appropiate requirements and store them on HDFS platform for further usage.
as well as developing an application(angular/django) which orchestrates these processes and makes it automated.
"
data engineer,"
Created a fully functional search engine capable of answering user queries. Developed a highly-scalable distributed system that uses Elasticsearch as a full Text search engine and asynchronous Python backend
 Built a pipeline of automated integration and cloud deployment of Docker images with Ansible scripts
 Developed parallel web crawler that maximizes the use of machine's resources for better efficiency
 Applied language detection model on parsed web pages based on N-gram analysis
 Implemented own power method-based fast PageRank algorithm for advanced search results ranking
Performed algorithm benchmarking, ranking quality tests, and written a research on this topic
 Configured load balancing using nginx, performed load and stress testing
 Gained experience with system security hardening and working in Linux environment
"
data engineer,"We were doing data migration from Hadoop to Snowflake 
I tasted pipeline. Created script and generate a report for Business 
Generated instructions for support team. I used to ""Confluence"" and ""Jira""
I spook with Develop team and I had talked about issue in the pipeline.
•	Create logical, create mapping and SQL code for Developer team.
•	Testing new logical, new table
•	Data quality
•	Communicate with business and find ways for resolving issue 
•	We use: SQL Oracle, Oracle data integration, Git, Jira, Confluence, Excel(VLOOKUP), VMware
•	Creating views, tables, procedure , function, testing procedure
•	  Slack,  Monday, Webex.
•	Calculation events. 
•	Calculation Cybersport events.
•	Creating  DB, sql Server, Snowflake.
•	Design DB.
•	Creating views and stored procedure.
"
data engineer,"
Creating data pipelines extracting from API data or other databases like Oracle. Data sources: FB, FBAds, Google analytics, Instagram and couple of internal databases regarding some technical data. Currently implementing Unity Catalog in Databricks and evaluating new features like liquid clustering.
"
data engineer,"Creating own variation of Airflow(tool for orchestration of tasks with workers and metadata). Creating hyperparametrized ELT pipelines, which takes data from 1000 of different data sources and calculating marketing and product metrics for analytics. Creating a DWH with kimball metodology.
Creating realtime synchronization system of SQL(Postgres) and NoSQL(ArangoDB,ElasticSearch) DB's with Kafka,Debezium and Python.Developing Big Data Pipelines with Talend,Airflow and Python. Deploying projects with Docker and Kuberneties.
Having good experience with new technologies."
data engineer,"Complete integration between Google Big Query and CRM
+ Creation and maintaining data-lake for all departments in company
+ Transfer data from different sources to Data Warehouse
+ Writing ETL scripts
+ Routine automation
+ Data exchange between CRM and Data Warehouse with hand-written scripts and Google Cloud Platform tools
+ Preparing tools for data collection on the website
+ Creating ad-hoc and regular reports
+ Resources monitoring
+ Preparing regular notifications and alerts
Office at Dnipro city"
data engineer,"
Currently, I am involved in an internal project focused on developing a resources accounting platform. Within this project, I hold the positions of Data Engineer and Data Analyst. My responsibilities primarily involve integrating monthly data into MS SQL and creating Power BI reports. This includes extracting raw data from our OLTP system, performing data cleaning, creating data resume_classifier, and finally visualizing the data. To accomplish these tasks, I utilize various technologies such as Power BI Dataflow, M query, DAX, and T-SQL.
"
data engineer,"
Currently I work as Data Entry Specialist at GeeksForLess Company.

I studied an online course ""QA Manual"" at Beetroot Academy(Sep 2022-Nov 2022). Also, I passed an online QA marathon on GoIT course in November.

Previously, I worked as Customer Support Representative(USA) and English teacher.
Interesting projects and communication with the team."
data engineer,"
Currently I work at the International Information Technology University as senior lecturer. Besides, for the last 3 years I worked in the company ALLDATA as a data engineer. Our team tasks were collecting a data and preparing it to front-end and back-end teams with the help of ETL tools, writing Python scripts and SQL queries, automatizations on Apache Airflow with Celery executor, administrating a Data Warehouse(Postgres). Also, I have a little experience in the field Cloud Technologies
"
data engineer,"
Currently working as a Data Engineer in the Enterprise Data Reporting and Business Intelligence Team of Ecobank Nigeria.

My job role involves:

•	Adhoc report/data generation using oracle and sql scripting languages. 
•	Utilize business intelligence software to assist end users in the creation and development of reports and dashboards.
•	Rendition of Regulatory Reports 
•	Translating business needs into analytics/reporting requirements.
•	Automation of Business Processes
•	Translating business needs into analytics/reporting requirements. 
•	Interfacing with other units/users to ensure successful delivery of reports. 
•	Delivery of business intelligence solutions to end users to facilitate more formed business decisions.  
•	Developing, packaging and delivering new service offerings related to business intelligence with strict adherence to the laid down procedures.  
•	Use of excel vba(macros) to develop report dashboards and to automate business processes. 
•	Managing the reporting and analytics platform(s) to ensure that scheduled jobs are successfully run and analytics data aligns to timeliness/completeness requirements.  
•	Working with database administrators to ensure that databases feeding the reporting platforms are tuned for optimal performance.  
•	Collaborating with end users to identify needs and opportunities for improved data management.
"
data engineer,"
Currently, work in Kaspi.kz as a Data Engineer.
Kaspi.kz is a payment, marketplace and fintech ecosystem in Kazakhstan that is used by more than 13 billion active users
Developed ETL processes for new products from scratch
Optimized process that helped to reduce execution time by 90%
Prepared datasets for machine learning resume_classifier
Created monitoring processes for analysts and business customers, which led to a timely response
to possible errors
"
data engineer,"
Current role: Data Engineer
Preferred career direction: BigData, Cloud services

- API Integration with Python Scripts: Devised Python scripts for seamless interaction with APIs from various data vendors and CRM systems.

- n8n Workflow Automation: Utilized the n8n tool to construct complex workflows, thereby enhancing overall data flow and operational efficiency.

- Data Automation with KNIME: Leveraged the KNIME analytics platform to automate data workflows, which included data cleaning, transformation, and analysis tasks.

- OpenAI API Integration: Authored, debugged, and tested code for interacting with the OpenAI API, which involved crafting prompts and evaluating the results.

- Generating Reports & Analyzing Data: Employed tools such as Python, Pandas, SQL, Google Spreadsheets, and KNIME to perform data analysis and generate comprehensive reports.

- Streamlit Web App: Developed a web application using the Streamlit library, n8n, Python, and Pandas, resulting in improved data visualization and user interaction.

- Reporting in HubSpot CRM: Created and implemented intricate reports and dashboards within HubSpot CRM, providing useful metrics to support strategic business decisions.

- Business Rule Development: Developed and articulated business rules for the company's procedures, thereby enhancing adherence to processes and compliance.
"
data engineer,"
Data Analyst / Data Engineer - Thedigital (March 2021 - present)
Engineering:
- development of data collection and data processing strategy from the ground up
- deployment and maintenance of a Data Warehouse (Bigquery), managing DW budget
- deployment of Airflow + Python ETL scripts, building data pipelines
- prototyping of a Golang client-server telemetry network for scheduled internet speed measurement
Analytics:
- creation of automated financial reports (Tableau, Data Studio), time series analysis, regression analysis
- geo-spatial analysis: creation of an algorithm constructing isolated chains of shortest paths for thousands of points (Python, OpenStreetMa deployed as a local Docker instance, QGIS)
- point-in-polygon analysis of client coordinates (shapely, geojson)
-plotting of complex geospatial data

Data analyst - Uploadcare, Canada (1 year, remote)
- conducting product-, marketing-, finances-related analysis using Python (jupyter, pandas, pyspark, multiprocessing, numpy, plotly, seaborn, regexp), BigQuery, Chartio, PopSQL, Amplitude, Heap Analytics and Tableau (complex queries in Standard SQL):
- calculating and visualizing SaaS-related business metrics - MRR/ARR/churn ratio/customers' debt/conversion rates etc.
- parsing gigabytes of raw json logs for data warehouse backfilling, historical analysis based on raw data. Building basic data pipelines with Apache Airflow.
- cohort- and segment-based financial analysis, ad hoc drilldowns, detecting outliers and anomalies within the working datasets
- setting up front-end tracking events using Heap Analytics, Google Tag Manager, Segment (event routing)

Marketing-Manager (Israel, US market).
Responsibilities:
- Competitive analysis: (US, French market)
- Optimization of internal workflows: pitching of automation ideas, close cooperation with Dev department, design of use cases and user stories
- Design of A/B tests, analysis of metrics, reporting (Google Optimize, Google Analytics, SEMrush)
- Client relations, translation of product specifications, promotional and advertising materials

Pet projects:
- Web-app with as Interactive Visualization of covid-19 cases in Ukraine, fatality and recovery ratios, testing rate (requests, MySQL Server, Pandas, Plotly/Dash on Flask)
- Telegram bot for Kiev Municipal Blood Center (requests, threading, SQLAlchemy, MySQL Server, PythonTelegramBotApi (JSON))
- Jupyter scripts for historic data analysis of raw data, visualizations using Plotly
Searching for an enthusiastic team, eager to learn new stuff."
data engineer,"On the data-analyst position I get used to making auto-tests that help to find and solve a lot of problems in a short period
Also I made an AI model for forecasting deposits depending on bonus parameters
Data Analyst OxTech - 1,8 years - now
Data Analyst AddTech - 4 months - now

I have been working on Data Analyst positions for 2 years now. And for now I'm working with auto-test full-time and Data collecting + Data Engineering on part-time.

I used to work with Python: selenium, pandas, numpy, skikit-learn, api`s
SQL, Clickhouse, PowerBI, Graphana
I want to work on products that socially responsible, work with new technologies and grow as a professional with dedicated people. 
Beside of regular analytics tasks, I used to write AI resume_classifier like polynomial, logistic regression resume_classifier.Also, i write python scripts for health checks with SQL usage and alert sending.

I have little experience with pytorch and I want to grow in ML and Data engineering."
data engineer,"
Data and Balance Sheet Engineer
Mykolaivoblenergo
Achievements/Tasks:
- work with documents and acts;
- work with alternative sources of electricity;
- electricity flow calculations;
- tabulation;
- data validation and aggregation.
"
data engineer,"
- Database design and development
- Writing SQL scripts, stored procedures, functions
- Building and managing ETL processes
- Cloud database servers (IBM Cloud, SQL Azure)
- Working with data on Python: cleaning data & visualization
- Basic knowledge of network technologies
- Experience of working with Git
- Java Core, JDBC & Hibernate
- Working with UNIX systems \ bash
- Experience of using C\C++
"
data engineer,"I have experience of databases design, maintenance and in query optimization and database performance tuning and implementation.
Database developer with 6 years of experience.
RDBMS: MS SQL Server 2016, 2014, 2012
Technologies: SQL, T-SQL, ETL, SSIS
Tools: MS SQL Server Management Studio, Microsoft Visual Studio, Git
Full-time job 
New experience"
data engineer,"Currently located in Czechia with work permit (+ private entrepreneurship (aka FOP) )
Database Development (MS SQL Server, T-SQL) - Senior
Python for data analysis - Middle
Machine Learning - Middle
Algorithms & Data Structures - Senior
Git, AWS, Jira, Mathematics
I would be happy to jump into a project related to health care."
data engineer,"Creating  and configuring workspace for data scientists on jupyterhub allowing them to choose their own environment, organizing  all the connection and access to minio, hdfs, and spark cluster.
Creating first version of chatbot using Rasa and telegram bot api. 
Installation and configuration of Apache Airflow on kubernetes. Creating first dags in a company for ETL processes. 
Collecting all money transfer reports from different transfer services and creating unique board for automating it. 
Streamlit app with the functionality of implementing model on datasets, visualization of inputs and results, and automatic plot builder.
Data cleaning using pandas and apache spark. Creating data pipelines using Nifi. Orchestration of ETL processes using Apache Airflow. Creating gitlab ci pipelines for kubernetes deployments. Writing simple data visualization pages on Streamlit. Installation and configuration of services on kubernetes cluster using helm and kustomization (Apache Airflow, Jupyterhub, Grafana + Prometheus, Keycloak). Installation on local cloud using Juju.
"
data engineer,"Led transition of team from paper-based questionnaires to digitized tools like KoboCollect, improving data collection accuracy and efficiency
Data Collection and Analysis Coordinator
Program Support Services
2019 - 2021

- Led transition of team from paper-based questionnaires to digitized tools like KoboCollect, improving data collection accuracy and efficiency
- Developed and implemented new data collection tools, trained team on effective use, and collaborated with technology partners to ensure tools met specific requirements
- Led data collection and analysis efforts on various projects, using Pandas and Excel to transform and visualize survey data
"
data engineer,"
Data Collection and Quality Analysis Intern - Casafari 
I did data exploration and preparation, using special development tools.
I parsed data from html-pages using css selectors, regular expression, json and  jmespath. Also I integrated data into the system and monitored its quality.
Research trainee - Waterloo AI 
I made a research connected to social media analysis.
"
data engineer,"
DATA ENGINEER 10.2022-Present
N-iX (Software Development Company) 
- Performing data integration and migration processes in Snowflake 
- Performing data transformation processes using dbt
- Creating new pipelines and adding features to existing
- Providing documentation for created processes
- Dealing with high-load processes and sql stored procedures
Technologies: Python, AWS(DynamoDB, Lambda, ECS, ECR, S3), SQL, PostgreSQL, dbt, Snowflake, terraform

BEST::HACKath0n
 27-28th, May, 2023
- Led a team on hackathon project and was a speaker introducing it
- In 24 hours developed a volunteer assistance website aimed at enhancing the volunteering experience.
- Implementing backend REST API using Golang
- Set up CI/CD and Web Server using Docker, docker-compose, AWS RDS, AWS EC2

RPA ENGINEER 12.2021-10.2022
Future Proof Technology (Software Development Company) 
- Developed and tested digital robots for automation purposes
- Developed internal reusable components (libraries) for working with Microsoft API (OneDrive), SFTP
Technologies: Python, RPA framework, MS API(+Azure), Google API, selenium, Pandas

PYTHON ENGINEER 09.2021-03.2022
Naturalex Sárl (Food and Beverage Services)
- Worked solo on automation processes
- Performed web-scrapping, web-crawling, data mining
Technologies: Scrapy, Selenium, Requests, BS4, Pandas
"
data engineer,"Data Engineer achievements:
[1] 2x AWS Certified:
    - AWS Certified Cloud Practitioner
    - AWS Certified Solutions Architect – Associate
[2] Public speaker at International Conferences among English and Russian communities
[3] Have my personal blog
[4] Open-source contributor on a regular basis
[5] Was responsible for trainee's onboarding
[6]. Developed standalone tool, that allows:
    1) Parse data by passing list of companies into raw dataframe
    2) Clean raw data frame
    3) Convert data frame into suitable for analysis format

Data Science achievements:
[1]. Won AI Spring Hackathon 2018 in computer vision track.
[2]. Took part in kaggle competition: the result is top 22% out of 3800+ teams.
[3]. Object detection project has been done. (tensorflow+keras, deep learning)
[4]. Participant of Ukraine data science club: 
- created image classification neural network (keras+tensorflow, deep learning)
- worked with time series
[5]. Passed a lot of courses in a variety of platforms, including Machine Learning and Data Analysis from Yandex and Deep Learning from Andrew Ng. 
[6]. SMART Academy (Data Science 17.10.2017 – 22.11.2017).
Data Engineer
5 years of commercial experience in IT: Android Developer => Data Scientist => ML Engineer => Data Engineer

Aiming to work with AWS or GCP in the future 

Computer Science bachelor's degree.

Skills:
- Python | Scala | Java
- AWS: DynamoDB, DMS, EC2, CloudWatch, S3, IAM, SFPT, Lambda, EMR, Lake Formation, Glue, Athena, Kinesis Data Firehose, Amazon ElasticSearch, CloudFormation (IAC), Code Commit, Code Pipeline, Code Build, ECS, ECR
- Spark
- Airflow
- Docker
- SQL
- ElasticSearch
- Kafka
- Jenkins
- Redis, MongoDB: fundamentals
I have 5 years of software development experience and am looking for a Data Engineer position with AWS cloud provider."
data engineer,"Junior Data Engineer for IT Product and Outsource companies.

Experienced with technologies/services such as AWS, GCP, Python, PostgreSQL/MySQL, Databricks, Snowflake, Git, Apache Airflow, Docker, and Kubernetes.
Data Engineer
Amazinum [ 6 Jan 2023 – 30 Jun 2023 ]
City: Ternopil
Country: Ukraine
- setting AWS EC2 for hosting the website, processing data
- managing GCP VertexAI for training resume_classifier
- using Kubernetes for managing containerized apps
- planning and monitoring workflows using Airflows
- setup a CI/CD pipeline with GitHub Actions and AWS
"
data engineer,"
Data engineer/analyst with almost 2 years of experience in IT. Bachelor of applied maths in Kyiv-Mohyla Academy. My hard skills: 
Power BI (also sertified by Microsoft), Google Data Studio
Python - standart IDEs, Databrciks
R
SQL - SSMS, PostgreSQL, DBeaver
Azure (Data Factory as well), Google Cloud Platfrom - configuring and administration
In addition: Postman, Git, MS PP Excel Word, Jira
"
data engineer,"
Data Engineer at Parimatch Tech (GR8 Tech)

Developed and maintained our core Data Lake service, including transformation strategies, data governance, backfill, cross database synchronization pipelines.
Developed database sql migration processes with alembic for smoother workflow and maintenance. 
Designed and integrated native row level security based on lakeformation data cells filters, enabling regional principals to have granular access. This eliminated the need to support separate tables for regions.
Designed and implemented tableau reporting framework optimization, resulted in 10-12% 30d/90d SLA improvements and 3x reports refreshment time reduction. 60 extracts moved to hyper api.
Provided teams with third party api data integration, security, governance by developing event based lambda services, airflow pipelines, lakeformation permissions.
Mentored interns, contributing to the growth and development of the team.
Skills: Gitlab CI/CD, Apache Airflow, AWS (S3, Athena, Glue, Lambda, SNS/SQS, IAM, Lakeformation, RDS, SSM, ECR), Tableau, PostgreSQL, IaaC Terraform, Spark.
Do not expect on duty calls"
data engineer,"Honors and Awards:
• Start-up Initiatives 2020 – Winner with “Breast cancer detection using AI” project
• Open Data Chellenge 2019 – 2nd place in Public Sector with “Online booking application”
• UzAuto Market Start-up platform – 2nd place with “Breast cancer detection using AI” project
DATA ENGINEER at Richard Fleischman and Associates Inc (RFA) / Applied Labs LLC

• Worked on building and designing ELT/ETL with various data sources, third party APIs, databases and data warehouses
• Built and maintained 20+ data connectors (extractor / loader) using Singer.io python framework
• Deployed data integration pipelines on Meltano, Singerly.co and Pipelinewise
• Maintained Snowflake Cloud Data Warehouse (SQL, databases, schemas, tables, views, stages, snowpipe, data sharing, warehouse, roles and organization)
• Automated serverless tasks using Azure Function and AWS Lambda Function
• Created data modelling and data transformations on DBT
• Scheduled data pipelines through Airflow
• Created web scraper and automation with Selenium
• Built Docker image, workflow using Terraform and used AWS Glue for batch processing
• Worked on big data components using Kinesis Analytics, Kafka, and Spark for streaming data
Tech stack: Python, Snowflake, SQL, Singer.io framework, Meltano, Pipelinewise, Singerly.co, Airflow, AWS (S3, Lambda, Kinesis, Glue), Azure (blob storage, function, container, devops), PostgreSQL, MSSQL, MySQL, SQLite, DBT, Docker, Spark, Kafka, GitHub with CI/CD action, Postman, Power BI, PowerShell, Linux, Windows, Teams, Zoom, JIRA


SOFTWARE ENGINEER at Turin Polytechnic University in Tashkent

• Involved on project “Breast cancer detection using Artificial Intelligence”
• Involved on project “Face recognition, identification and verification system”
Tech stack: Python (Django framework), Django REST, SQLite, OpenCV, Keras with Tensorflow backend, Deep Learning, U-Net, dlib, CUDA, Linux
I am considering only remote work or with relocation option."
data engineer,"
Data Engineer; Company: Unibank; January 2023-Present
- Developed scalable databases capable of ETL, ELT and EL processes using PL/SQL and Apache Spark, S3, Python.
- Estimated the workflow and increased the efficiency of data pipelines that process over 20 TB of data daily.
- Used Apache Nifi, Kafka and created DAGs on Airflow to build ETL solutions.
- Developed the SQL server databases system in order to maximize performance benefits for clients.
- Assisted senior-level Data Scientists in the design of ETL processes, in SSIS packages.
- Utilized Postgre and Oracle to create SQL databases that harvests data from a variety of sources.
- Developed, deployed, and maintained data services.
- Transformed data from XML and JSON to structured data.
- Worked closely with stakeholders across departments to design, build and deploy various initiatives within the data platform

BI Engineer; Company: Unibank; January 2021-January 2023
- In-depth understanding of fundamentals of object-oriented design, data structures, algorithm design, and problem solving
- Experience in creating reports, dashboards using IBM Cognos, Power BI, Tableau, SQL Server Reporting Services (SSRS) and Excel (VBA)
- Advanced experience working with big and complex data sets within large organizations including languages SQL, PL/SQL
- Applying best practices and modern techniques to improve existing schemas for performance and efficiency
- Experience in daily, monthly, and yearly regulative reports
- Exceeded goals on 3+ projects by modifying extensive data into actionable business insights via data visualization and analysis
- Building and maintaining data warehouses, data marts and data lakes that support reporting, analytics, and other business intelligence needs.
- Working with the data governance teams to develop and implement data policies, standards, and procedures to endure data quality, security, and privacy.

Financial Specialist; Company: Unibank; January 2018-January 2021
- Developed and managed budgets, created financial forecasts.
Analyzed financial data, prepared financial reports, and presented findings to management.
- Created Excel automatic reports and dashboards for payroll.
- Performed a comprehensive analysis of the effects of financial issues like company debt and other financial structures.
- Worked in SQL and Excel (VBA).
- Planned strategies of departments and calculated bonuses for employees annual, monthly, and quarterly.
"
data engineer,"Finalist of INT20H Data Science hackathon (as a team lead) - 4th place
Data Engineer / Data Analyst
April 2023 - Present
• Developing pipelines to extract data from various APIs, websites, and CRM systems.
• Designing interactive dashboards that offer clear and concise visualizations of data.
• Working with AWS infrastructure, including VPC, EC2, RDS, S3 and Lambda.
• Hosted technical interviews for new candidates.

Trainee DataOps Engineer
October 2022 -April 2023
• Part of the SRE stream (FinOps team)
• Сreated a complex script to transfer data between various Amazon services, including Athena, S3, and RDS.
• Designed a sophisticated Data Warehouse utilizing Python, FastAPI, PostgreSQL, and Kafka.
• Launched APIs using AWS services such as RDS, S3, and EC2.
• Was one of the developers of a complex Data Platform connected to Airflow and various AWS services, which runs efficiently in ECS.
"
data engineer,"- I am a certified AWS solution architect. 
- I have experience with getting data from primary and secondary sources, converting them to usable format, analysis and build dashboards. 
- I have have experience with deploying and maintaining infrastructure using AWS, terraform and Kubernetes. 
- I have worked on analyzing data and create an API
- I have experience with setting up CI/CD, github Actions
Data Engineer, Data Analyst, Devops Engineer 

- Certified AWS solution Architect (Verification number: LSR768SJPBRE1LC1)
- Design, implement and maintain infrastructure for the team using AWS services, Kubernetes and terraform
- Design, implement and deploy API’s
- Design and integrate consumer data analytics visuals onto projects.
- Mining data from primary and secondary sources and transforming the data into a usable format
- Visualizing data for faster and better management decisions. Preparing dashboard and API’s
- Work on packaging the product
- Design, develop and maintaining ETL solutions
- Setting up data pipelines to collect and move data, designing and implementing data storage solutions, and setting up and maintaining data processing tools
- Setting up CI/CD
- Setting up monitoring using Prometheus and Grafana
- Optimizing SQL functions and stored procedures
- Worked with Agile methodologies
- Worked on AWS cloud provider
- Developing algorithms to process data
I hope to utilize my skills, gain new experiences, and contribute to the company's goals while also growing professionally."
data engineer,"
Data Engineer/Data Analyst with over 5 years of experience from working at the multinational automative corporation and multinational retail group in Germany.  

Multinational automative corporation. Stuttgart, Germany
- Development of backend (Databases, Tabular resume_classifier)
- Optimization of data streams and ETL process from multiple sources to central data warehouse
- Coordination of the new requirements with the local markets
- Cloud migration of on-premise data products

Multinational retail group. Neckarsulm, Germany
- Implementation of ETL solutions in Data Migration, Data Integration and Consolidation projects
- Provide technical solutions, preparation of technical specifications and mapping based on functional requirements from Business stakeholders
- Sub-project lead Data Migration project for Master Data Management program

Multinational automative corporation. Stuttgart, Germany
- Data extraction, manipulation and transformation for further analyses and processes
- Build reports, visualizations and create dashboards
- Analyze and identify business requirements for further development of global forecasting application

Software & Programming
SQL; MS Office;
R; Power BI;
GitHub; SAP DS;
ADM (ETL Tool); Jira/Confluence;
Python; Tableau;
Azure; Databricks.

Languages
Azerbaijani: Native
English: Fluent
German: Fluent
Turkish: Fluent
"
data engineer,"Handling large amount of streaming data
Data Engineer -  Data orchestration using Apache Airflow , ETL development (SAP BODS, Pentaho DI, SAP BODS), Data processing using Apache Spark (Scala) | Hive | Impala, Linux | Docker | Git
Data Warehouse Architect - DWH data resume_classifier and OLAP cubes
Database Developer - SQL | PL/SQL | T-SQL programming on Oracle | SAP IQ
BI Developer - Semantic layers and dashboards using OBIEE | Tibco Spotfire | MicroStrategy | SAP BO
Enhancing proficiency in cloud environments and possible transitioning to DataOps"
data engineer,"
Data Engineer/Data Scientist with 3+ years of experience. Worked
with IBM during work in IBA. Developed ETL jobs for ingestion, transformation,
integration data from from different types of sources like Amazon
S3, Cloud Object Storage, IBM DB2, IBM Box, Rest
API, etc using PySpark, Python. Developed Airflow DAGs. Prepared data for ML and developed ML resume_classifier using NumPy, Pandas, Scikit-Learn, TensorFlow, Keras, NLP.
"
data engineer,"
Data engineer/ DB developer / Data Warehouse with production experience in cloud migrations/development and CI/CD. Skilled in database development/administration, data integration, data visualization (Tableau) and Big Data workloads. 
I am always highly enthused about my work and tasks ahead, committed to learning and self-development.
"
data engineer,"- Professional Data Engineer Certification from GCP
- Interviewing/skills assessment
- Requirements gathering, Participating in architecture building
Data Engineer:
• Designing and building a new delta lake. Pyspark, Athena, Glue, Airflow
• Research on the possibility to accelerate Spark computations on GPU. Spark, Rapids
• Designing, and supervising batch/stream data pipeline building. REST, BigQuery, Airflow, Dataflow/Beam.
• Designing, and building batch/stream data pipelines
to support carbon-dioxide emissions reporting. Air-flow, BigQuery, Dataflow/Beam.

Machine Learning Engineer:
• Search intent finding. BERT, DBPedia, Spark.
• Designing a private cookieless data acquisition strategy, model building, and querying. Clustering algorithms. Differential privacy theory. Federated learn-
ing theory.
• Designing and developing large-scale data scoring and optimizer from scratch. Scala, Spark, Tensorflow, Singularity, API.
• Designing and developing of an AB-platform. Bayesian and frequent statistics, python, pySpark, Kubernetes, Singularity.
• Designing and developing of an automatic bidder for advertising trading from scratch. Python, pySpark, statistics, Tensorflow, Kubernetes.
"
data engineer,"-- 4 years of hands-on experience in developing, implementing and maintaining data pipelines, DWHs and databases, and backend architecture, preferably in Azure.
-- Working in Norwegian oil industry.
-- Dr. rer-nat. in Particle Physics from Heidelberg University.
-- Member of the ATLAS collaboration at CERN
-- Working with data at the Large Hadron Collider.
-- Publications in top peer-reviewed physics journals.
Data Engineer
Dr. rer. nat. in Physics

10 years of experience in science and 4+ years in IT as DE/DS 

-- Ciklum (Dec 2020 - present)
Develop and implement pipelines to ingest, process, validate and visualise data about estimated product deliveries and online sales for large brewery company

- Data pipelines using Azure Data Factory, Databricks and Synapse
- Ingestion of raw data from external sources on a daily basis with environmental tracking and failure recovery
- Data validation using Great Expectations Python-based tool
- Data stored in Delta tables and Synapse views
Tools and technologies: Azure, Python, Azure Databricks, DevOps

Project for audio book selling company
 - Azure data processing platform for a company selling audio books.
 - ETL pipelines to load sales data from different sources into Azure environment. Data processing, model features calculation and data ingestion into Azure ML predictive model.
 - Backend architecture for the client’s web and CRM applications
- Tracking and notification system to monitor all data workflows 
Tools and technologies: Python, SQL, Azure, Hubspot, Git

-- Infopulse/Cognite AS (Jan 2020 - Dec 2020)
Modelling project for the oil and gas reservoirs in North Sea for Norwegian oil company:
- Pipelines to process mining data from offshore oil fields (oil/gas/water fractions, pressure, sensor measurements, etc.). Generation of historical time series.
- Data processing from oil reservoir simulations. Generation of simulation time series.
- Dashboard visualisation of time series in Power BI for petroleum engineers.
Tools and technologies: Python, Spark, Databricks, Python SDK, Cognite Data Fusion, Docker, Power BI, REST API, Git

-- The Ad Masters, 11.2019-12.2019 
Python based A/B testing framework for the online selling platform

-- Ph.D. at Heidelberg University/ATLAS experiment, CERN, 05.2014 - 03.2019
- Statistical data analysis to identify potential dark matter candidates from particle collision data collected with the ATLAS detector
- Interpretation of the final search results. Studies of different resume_classifier of dark matter production based on observed data.
 - Implementation of parallel processing of large collision data sets on Linux based clusters
- Publications in peer-reviewed physics journals such as Physical Review Letters and Journal of High Energy Physics

Technologies & Tools:
-- Statistical data analysis, Bayesian methods, Python, C++, Bash, Linux, Git, SVN
-- Cutting edge technologies: machine learning, applied sectors (physics, biology, biophysics, healthcare, education), finance, energy industry, etc.
-- Close collaboration with international companies
-- Remote work and flexible working hours"
data engineer,"
Data Engineer | DW/BI Engineer
English - Upper Intermediate (B2).

Data Engineer with more than 4 years of commercial experience in the IT within different domains.
Professional expertise in database development, ETL processes, data warehousing, BI, data engineering and cloud.
Certified Azure Data Engineer Associate.
Certified Oracle Associate.
Hard-working and reliable team player.

- SQL, PL/SQL, T-SQL, MS SQL Server, Oracle
- Microsoft Azure Data Engineer Stack (Data Factory, Databricks, Synapse Analytics, Logic App, Data Lake, etc)
- Oracle Data Integrator, SAP Data Services, Dollar Universe
- Power BI
- ETL / ELT
- Batch and Streaming Processing
- Data analysis, Database modelling, Data Warehouse architecture
- CI/CD, Azure DevOps Pipelines, Liquibase
- Git, TFS, Azure DevOps Repos
- Agile, Scrum, Kanban, Azure DevOps Boards
"
data engineer,"
Data Engineer:
Epam, September 2021 - Present
Performed data model synchronization between systems SimCorp and AWS Glue/Athena combo.
Performed ETL using Snowflake and ActiveBatch.
Performed PowerBI data model synchronization.
Performed PowerBI Dashboard update/fix.


Database engineer: 
Innohub, February 2021- September 2021
 Architecting, implementing and operating stable, scalable and
highly performant procedures and functions
 Diagnose and resolve bugs and performance issues 
 Analyzing and optimizing queries
 Create/alter/drop schema’s object: tables, views, indexes and
others
 Preparing and validating data for testing, training ml-resume_classifier
 Build ETL-process with SQLalchemy (Python library)
Technologies were used:
PostgreSQL, pgAdmin, DataGrip, Jira/Confluence, Docker, PLpgSQL, SQLalchemy 

Oracle PL/SQL Developer: 
UNITY-BARS, November 2019- January 2021
- Architecting, implementing and operating stable, scalable and
highly performant packages of procedures and functions
- Direct work with Oschadbank( the customer), discussion of
the formation of requirements.
- Analysis of the architecture of the current project for the
system analysts department and further evaluation of the work
- Diagnose and resolve bugs and performance issues 
- Analyzing and optimizing queries.
- Creating reports in FastReport.
- Writing reentrant scripts
- Writting technical documentation.
- Create/alter/drop schema’s object: tables, views, indexes and
others
- Preparing and validating data for testing

I've worked on project such as: financial monitoring, implementation of IBAN, integration with SingleWindow (international money transfer systems such as Western Union, MoneyGram), analytics on deposits and loans.
Technologies were used:
Jira/Confluence, PL/SQL, Oracle Database 11g, FastReport, TOAD, PL/SQL Developer.
"
data engineer,"
Data Engineer (ETL) - DataMola Ukraine/Poland (March 2022 - Apr 2023):
-- develop new, maintain & improve existing ETL pipelines from different sources (Python, Prefect, IBM DB2, AWS S3, 3rd party API, SharePoint, SFTP, SSIS);
-- participate in build unify data flows (data pipelines) for all company projects (Prefect Dataflow Automation, Python, AWS S3,


Senior Pricing Process Optimization Specialist/Reporting and Process Automatization Specialist - Metro Cash & Carry Ukraine (June 2019 -  Aug 2022):
-- create, maintain and improve data pipelines (ELT)/data processing for reporting and working tools in department (MS Office, Teradata, Python, VBA Excel, VBScript, Corporate Storage and Systems);
-- develop promo planning tool (work templates (MS Office), data processing (VBA Excel,VBScript, Python, Teradata, Corporate Systems);
-- pricing process automatization (develop tools for price analysis & price setting (MS Office, Teradata, VBA Excel, VBScript, Corporate Systems).
I am looking for a job where I can deepen my technical and professional skills in field of Data Engineering."
data engineer,"
Data engineer ( Februrary 2023 - Present)
GR8 tech ( ex Parimatchtech) | Ukraine (remote)
B2B iGaming company that provides stable, resilient, adaptive, and highly scalable business solutions for iGaming companies.
- Implementation of async connector to OpenSearch for rapid data retrieval
- Implementation of batch & stream metrics consumers
- Increase test coverage of legacy code base
- Adding pre-commit hooks

Data engineer / ML engineer (July 2020 - Present)
PTE LTD “Waste Labs” | Singapore
IT startup building B2B logistics optimization & lead generation services for waste logistics companies to make collection more profitable & sustainable
- Building event-drivern analytical data pipelines in AWS and setting up GIS dashboards on Grafana with Apache Athena
- Implementation infrastructure code using Terraform on AWS cloud provider
- Configuration of CI/CD for projects
- Development & Deployment REST api, data pipelines and their mutual integration
- Repository configurations: semantic versioning, pre-commit hooks, linters, formatters, tests and etc.
- Development internal tools & libraries for geospatial analysis
- Investigation of deployment practices for companies use cases
- Building reusable machine learning modules for integration to products
- Packing mathematical resume_classifier & their deployment to production

 Data scientist (October 2019 - November 2020)
“Fraudhunter” LLC | Kazakhstan
Company providing B2B services in telecommunication industry to reduce profitability loss & improve quality of services
- Development & deployment anomaly detection in telecom base station logs by profile generation
- Development & deployment call planning module for test call generation system
- Building data ETL/ELT data pipelines


 Junior Data scientist (March 2019 - October 2019)
“DAR tech” LLC | Kazakhstan
Company providing outsource services where i did:
- Research time series analysis for sales forecasting using (ARIMA, FBprophet)
- Research credit scoring for microfinancial organization using classical score cards & logistic regression on python
"
data engineer,"• Worked for MNCs like Veritas, Seagate, godrej, and Schaeffler.
• Been part of the United nation flood awareness project.
Data Engineer having 5 years of experience in AWS and Azure Cloud, Open-Source Big Data Tools, Python, PySpark, SQL, and AWS Cloud to develop innovative big data solutions for clients to reduce their efforts and give a boost to their workflow.
----------------------------------------------------------

Data Engineer
Schaeffler · Full-time
May 2022 - Present 
Frankfurt, Hesse, GermanyFrankfurt, Hesse, Germany

• Responsible for data migration, creating pipelines, maintaining the deployment, and automating the manual process to reduce time and increase the efficiency of the system.

----------------------------------------------------------

Data Engineer
i2e Consulting 
Aug 2020 - Apr 2022 

• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.

----------------------------------------------------------

Associate Data Engineer
Mactores 
Jul 2019 - Jun 2020 
• Build Data Lake for the analytics team which includes extraction of data from a wide range of sources, the transformation of data on AWS Cloud, and loading data in S3 for analytics on Tableau.
• Responsible for an 80% reduction in the Data Processing and Re-design of the Dashboard which reduced the loading time from 25 min to 6 Sec.
Looking for better opportunities."
data engineer,"
Data Engineering and Business Intelligence specialist with
expertise in designing, optimizing and supporting Data
Warehouses, Data Marts and ETL processes.
Posseses good communication skills and feel comfortable with intra-team interactions.
"
data engineer,"
Data Engineer interested in distributed data processing, and functional programming with a passion for Spark, have sound theoretical knowledge of main Big Data concepts (MapReduce paradigm). Worked in such domains as AdTech, FinTech, and Retail & Distribution. Have good experience building data pipelines, validation, visualization, and systems performance optimization. Responsible, organized, and business-oriented. Quick learner, able to work autonomously.
"
data engineer,"
Data Engineer / MLOps engineer
Fractal Analytics | Dec 2021 - present
- Building Vertex AI Pipelines. The project's goal is to migrate from Airflow to Vertex. I build a pipeline using the KubeFlow SDK and work with GCP and BigQuery SQL. 
- Data migration project. We migrate data from old storage to GCP.

Data Analyst
City Development Solutions | Sep 2020 - Feb 2021
Responsibilities:
- Data collection, cleaning and processing 
- Analysis of real estate market objects 
- Plotting graphs, creating reports
Achievements:
I've automated and sped up the process of collecting information using Python
"
data engineer,"
DATA ENGINEER | NIX SOLUTIONS | Python · PySpark · Pandas · SAS · NiFi · IBM Cloud · Azure | Aug 2021 – Sep 2022
    Ingestion and transformation pipelines for medical data
     • Improved testing of the existing system, raised coverage to 80%
     • Extended, improved, and modified functionality of pipelines
     • Solved performance bug that improved processing time x6

    Large-scale processing of medical data
     • Refactored existing Spark model to new OOP architecture

    Data enhancement of medical data
     • Improved existing CI/CD processes
     • Refactored SAS resume_classifier to new Pandas architecture

TRAINEE | NIX SOLUTIONS | Python · Spark · Airflow · Flask · Git · Linux · Docker · Postgres · Algorithms · Data Structures | Apr 2021 – Jul 2021
     • Successfully finished DE project using PySpark, Airflow
     • Successfully finished REST project using Flask
"
data engineer,"IBM Data Science Professional Certificate, 2022
MLOps Fundamentals by DataCamp, 2023
Database Design by DataCamp, 2023
Data Engineer 
Project: RexSys, platform that makes it easier to understand and navigate through the regulations in different countries, and to compare multiple regulations

• Extracted data from various sources and transformed it into a standardized format for analysis by domain experts
• Used Beautiful Soup and Selenium to scrape multiple websites, collecting a dataset of approximately 50,000 documents
• Developed a model, with an 81% accuracy rate, that offers document recommendations based on input document
• Implemented data cleaning and preprocessing, incorporating tokenization, lemmatization, and stemming techniques, using NLTK, spaCy, and gensim
• Automated complex tasks for domain specialists using Python scripts, resulting in significant time savings and improved efficiency

Project: Android application for Smart POS devices 

• Participated in the integration of additional payment methods and alternative payment systems, including Visa, MasterCard, Pure and American Express into the application
• Developed a user-friendly interface for the Android application, ensuring easy navigation and a seamless user experience
• Contributed to the overall project success by writing critical transactions in Java for various payment systems
• Demonstrated expertise in data encryption and decryption algorithms, resolving one of the major project issue
• Gained valuable experience in SQL, successfully applying it to extract and manipulate data

Looking for:
Complex and interesting tasks
Professional and personal growth
"
data engineer,"
Data Engineer. September 2022 - Recent
QZhub, Nur-Sultan. Full-time
• Worked with Business Analytics team in different projects.
• Worked with Odoo business management software, mainly - creating, editing modules, connecting modules to PostgreSQL database, troubleshooting on Python.
• Parsed multiple websites using Python and libraries like scrapy, selenium, bs4. 

Risk Advisor. December 2021 – February 2022.
Deloitte LLP, Nur-Sultan. Full-time
• Analyzed accounting entries to identify transactions that are not typical for business and signs of dishonest actions on the part of the organization’s management.
• Analysed system data to identify and designate areas requiring comprehensive expert research using ACL Analytics tool.
• Assigned with more than 7 projects, which involves leading companies in primary, manufacturing and service sectors from Russia, Kazakhstan and Kyrgyzstan.
• Worked closely with an audit team and a client side.

Data Mining Intern. June 2021 – September 2021.
Kazakhtelecom JSC, Almaty. Part-time
• PROTELECOM Internship Program is dedicated for the 3rd and 4th year students pursuing bachelors degree in Kazakhstan.
• Worked in Data Mining and Big Data transfer team under the supervision of 3 mentors.
• The main tasks were: retrieving records of subscribers using SQL scripts, parsing and presenting statistics of the clients with charts in python, using libraries like pandas, numpy and matplotlib.
"
data engineer,"
Data Engineer | SoftServe Dec 2021 - Present (1 year 8 months)
Integrated new data sources and prepared data pipeline and infrastructure.
Prepared data for business users and optimized legacy SQL code.
Technologies Used: Python, DBT, Bigquery, Google Cloud Storage, Apache Beam, Apache Airflow, Git

Data Engineer | DAXX Aug 2021 - Nov 2021 (4 months)
Integrated new data sources and prepared data pipeline and infrastructure.
Prepared data for business users.
Technologies Used: Python, Bash, Azure Function App, Azure Storage, Git

Data Engineer & ETL Developer | EPAM Dec 2020 - Aug 2021 (9 months)
Developed data marts and gathered business requirements.
Prepared data pipelines and data for business users.
Technologies Used: Python, Bigquery, Google Cloud Products (Container registry, Compute Engine, Cloud Build, Cloud functions), Google Composer (Apache Airflow), Docker, Git

Data Engineer | Letyshops Jan 2020 - Nov 2020 (11 months)
Developed ETL processes, collected and compiled data from different sources.
Prepared data pipelines and data for the analytical department.
Developed corporate BI tools and worked closely with the analytics team.
Technologies Used: R-script, Python, Bigquery, Google Cloud Products (Container registry, Compute Engine, Cloud Build, Cloud functions), Google Composer (Apache airflow), Google Dataflow (Apache beam), Google data studio, Docker, Git

Product Analyst & BI Developer | Letyshops Aug 2018 - Dec 2019 (1 year 3 months)
Developed analytics systems, ETL processes, and marketing campaign plans.
Created reports (ad-hoc, bi-dashboards) and A/B tests.
Technologies Used: R-script, Shiny (R-script framework for dashboard development), Python, Bigquery, Google Cloud Products (Container registry, Compute Engine, Cloud build), Google data studio, Docker, Git

Head of Data Mining Department | 4Service Aug 2017 - Aug 2018 (1 year)
Developed analytics systems, discovery reports, and ad-hoc reports.
Optimized and automated operational reports.
Technologies Used: R-script, Shiny (Rscript framework for dashboard development), MS SQL, Git

Lead Analyst | Доброго дня Аптека Aug 2014 - Aug 2017 (3 years)
Developed analytics systems, marketing campaign plans, and ad-hoc reports.
Optimized and automated operational reports.
Technologies Used: R-script, MS SQL, Excel (Power pivot, Power query), MySQL, Qlikview
Complex and interesting tasks;
Emproovment Skeels;
Green field projects"
data engineer,"Oracle Certified Professional - Oracle 10g Database Administrator;
Learning Hadoop, Issuing authority: LinkedIn.
Python programming, Issuing authority: Stepik, Issued Aug 2021
Apache Spark Essential Training: Big Data Engineering, Issuing authority: LinkedIn.
Scala Essential Training, Issuing authority: LinkedIn.
Data Engineer, Tallinn, Estonia
In this role, I collaborate with an international team, navigating through multiple time zones and remote work environments to design and implement data projects for large-scale U.S. businesses. My core responsibilities and achievements include:
• I've spearheaded the development of an app leveraging Large Language Models and Neural Search frameworks, such as Haystack and Langchain, for advanced question-answering systems. I've also utilized OpenSearch for efficient data management, and actively stayed updated with shifts in the NLP and LLM ecosystem, ensuring our applications always remain at the forefront of technological innovation.
• I've created and maintained data solutions for both internal requirements and customer-facing production systems, striking a balance between organizational needs and client satisfaction.
• Assessing the potential of Apache Airflow and Airbyte, I've been instrumental in deploying these tools in a Kubernetes cluster. By integrating data from diverse source systems into Snowflake, our cloud data warehouse, I've facilitated the creation of data views crucial for our BI tools.
• My contributions extend to developing a system that collects and processes data using REST API endpoints and Apache Spark. I've published this data using Kafka, and ensured its successful push into Snowflake, further improving our data processing pipeline.

Data Engineer, Kazakhstan
In this role, I worked as a Big Data Engineer helping the customer in building a cloud-based corporate data lake to decrease time-to-market for new data products and also enable costs savings due to data pipelines automation and usage of cloud computing resources. 
• I used Python, SQL, and Spark to collaborate with data engineers to create a cloud-first ETL pipelines, ingesting data across disparate sources using Apache Airflow and AWS Elastic MapReduce. I also worked with AWS Athena, AWS S3 and AWS Glue for data stores. 
• I analyzed different aspects of data, fixed the issues and worked on improving the performance of data fetch jobs.

Java Developer, Kyrgyzstan
In this role, I developed production fixes and application enhancements and provided production support to CRM and microservices platform.

Senior DBA, Data Engineer, Kyrgyzstan
In this role, I used oracdc & Kafka Connector's for near real-time data integration and replication;
### I am considering offers with a Location only in Estonia! ###

As a highly experienced Data Engineer with over 15 years in the IT industry, I have honed my skills in fintech and telecommunications services companies. I am passionate about tackling challenges and am eager to continue building my career and expanding my expertise in the fields of Data Engineering and DataOps."
data engineer,"1. Created parameterized SQL scripts to collect analytics and business metrics, such as applied recommendations, calculating the impact of our decision on client revenue and sales. It was widely used by customer facing and customer success teams.

2. Created and supported automatic data pipeline tests, for Entry (raw client data) and our processed data. Significantly simplified and accelerated QA process.

3. Adapted dataset and whole pipeline for RNN (recurrent neural network).

5. Working with complex billing and invoicing systems for mobile operators.

6. Supported and improved handling of different types of events, such as different balances spendings, regular payments, late landings; aggregating them into daily and monthly aggregates, forming invoices based on the results of the month.

7. PostgreSQL & MongoDB databases profiling and
optimization.
Data Engineer with 1.5 years of experience in two projects, with Google Big Query, Oracle and Postgres.
To gain more experience in interesting projects with a good team; improve my existing skills and get new ones. Interested in trying myself in Product Analyst/Data Analyst positions as well."
data engineer,"
Data engineer with 1+ years of experience. 

Data engineer(January 2023 - present):
- migrated data and logic from PostgreSQL to Snowflake using dbt;
- created dbt macros;
- Created tables using Data Vault approach 
- created dags in Airflow;

Data engineer(June 2022 - January 2023):
- created tables, views and functions in Postgresql and MsSql databases;
- created python scripts in Azure Synapse(Pyspark);
- integrated Azure Synapse with Azure key vault and Application insights;
- created pipelines in Azure Synapse;

EPAM DWBI Course (November 2021-May 2022):
- created tables, views, triggers, stored procedures with sql;
- got acquainted with clouds (AWS, GCP, Azure)
- worked with Python(Pyspark, pandas, numpy)
- got basic knowledge of data visualization tools (Tableau, Power BI)

Certifications - Azure DP-203, DP-900
"
data engineer,"
Data Engineer with 3,5 years experience in the field. Have strong analytical and communication skills. Keen to effectively solve complex tasks, bringing the maximum business value and constantly grow professionally, as well as, personally. Interested in Big Data, Cloud Computing, ETL and ELT systems, Machine Learning

Experience:
- AWS Certified Solution Architect - Associate
- Development of ELT processes for near real-time data analysis(Spark Streaming)
- Automation of orchestration & deployment for
AWS EMR, EC2, Lambda, Spark jobs
- Solid kowledge of Algorithms & Data Structures
"
data engineer,"
Data Engineer with 3 years of experience 

main skills:
Python
MapReduce stack (mr2, spark, hive, hadoop)
SQL
ETL/ELT data pipelines
cleaning data
maintaining DWH (CDP, Google Cloud)
importing data from various sources
data migration
writing API (Flask, FastAPI)
Tableau/Looker (only data ingestion and solving problems with reports, can`t write reports without design information)
deploying data resume_classifier in production
"
data engineer,"
Data Engineer with 6 years of experience with a key focus on Apache Spark, Apache Kafka, Hadoop, cloud (AWS), and container (Docker, Kubernetes) technologies. Experienced in building and designing data lakes and data warehouses.
"
data engineer,"
Data Engineer with 9+ years of commercial experience.
Worked in outstaff, outsource and product companies.

Details of experience:
DB: Amazon Redshift, MS SQL Server, Aurora SQL, Oracle, MySQL, Dynamo DB
Technologies: SQL, PL/SQL, T-SQL, AWS services, Python, DBT, Airflow, Databriks, Fivetran, LookML, DAX, ETL, SSIS, Talend
Reporting: Looker, Power BI, SSRS, Apex
Tracking: Asana, Jira, Confluence, Redmine, SVN, TFS
Stable and long-term project"
data engineer,"
Data Engineer with experience in analyzing, designing and transforming data. Have a strong level in T-SQL, SQL and also have experience working with PowerBI, Azure, SSIS, C#, Python, AWS. A team player and a highly motivated individual willing to learn new technologies and methods related to data analysis. Able to adapt to different work culture and always ready to accept new challenges.
Develop stored procedures, views, tables.
Troubleshooting.
Bugs fixing in C#/VB code.
Data analysis.
Maintained DB.
Improve performance.
Learning AWS.
Learning Python (novice).
For me would be great to work and learn something new or develop my current skills but I also believe i am able to work at any task."
data engineer,"Support and refactoring of legacy projects. Created big data projects from scratch.
Data engineer with leading experience
Years of experience: 6+
Domain: analytics systems for the banks, fintech companies.
Key technologies: Python, AWS
Databases: PostgreSQL, MySQL
Queues: RabbitMQ, Apache Kafka, Amazon SQS

Additional skills: PHP, Javascript, Node.js, Lua, Google BigQuery, Amazon DynamoDB, Redis, MongoDB.
Additional: Have 6 years of experience in software engineering.  I have an experience with a large and complex legacy architecture and with the microservices architecture. I built end-to-end analytics, reports for a business, ad-hocks. I worked as a team leader of data engineers, software engineers and qa engineers. 

Graduation: Specialist's Degree in System software.
Analytics development"
data engineer,"
Data Engineer with proven practical experience in design and implementation data pipelines from various sources into the targeted data warehouses and data lakes.
Proficient in the assortment of ETL and orchestration technologies, including Airflow, SSIS, IBM Data Stage. Experienced in development of data-ingestion processes from REST and SOAP API.
Proven expertise in database development for Postgres, MS SQL Server, AWS Redshift. Experience in applying AWS technologies to build cloud data storages.
Hands on experience with creating and running Docker images for various applications. Experience in creating Tableau dashboards.
Good understanding of software development methodologies, including Agile (Scrum).
Able to effectively self-manage during independent projects, as well as collaborate in a team setting.
Looking for a job as a Senior Data Engineer.
Interested in opportunities for professional development and learning new technologies."
data engineer,"Set up Apache Airflow server on Celery Executor and 3 workers on remote VMs.

I facilitated the implementation of Apache Kafka for Change Data Capture (CDC). Before my involvement, data was extracted from databases using batch processing.

I successfully carried out the integration of a data lake with a data warehouse (DWH), a task that had not been undertaken prior to my involvement.

Set up Grafana for Airflow & Kafka monitoring.

Developed telegram bot for pipelines and services monitoring.

I presented at a meetup with the topic 'DWH Architecture'.
Data Engineer:

Worked on Oracle based DWH using Airflow, PL/SQL, Kafka.

Implemented Kafka (Debezium, Confluent Connector) for CDC. Before me, data from PostgreSQL databases were fetched in batches. 

Set up Apache Airflow server on Celery Executor and 3 workers on remote VMs.

Developed telegram bot for pipelines and services monitoring.

I presented at a meetup with the topic 'DWH Architecture'.

Junior Data Engineer:

I've transitioned the legacy pipelines from Informatica to Airflow.

I've also developed a predictive model for the Monthly Active Users (MAU) of the application.

Junior BI analytic:

Designed dashboards for senior management highlighting key performance indicators.

I also tested the new Data Warehouse (DWH).
"
data engineer,"
Data Enginner
DWH Project
From April 2022 – now
Key responsibilities: 
•	Create SQL scripts;
•	Create mappings, sessions, workflows, connection in  Informatica;
•	Write shell and python scripts;
•	Support 

Data QA
DWH Project
From July 2020 – April 2022
Key responsibilities: 
•	Prepare and execute SQL scripts for testing functionality;
•	Work with PL/SQL  developer;
•	Prepare test protocols;
•	Testing installation with ETL Informatica;
•	API testing (REST);
•	Registered found bugs into Jira bug tracking systems.
"
data engineer,"
Data enthusiast with experience in Data Engineering , Machine Learning and Software Engineering. I have been building scalable Data Analytics solutions for the companies around the world. My strongest side is problem solving.
"
data engineer,"
Data Ingestion: Pulling data from various platforms and databases 

Data Cleaning and Triangulation: Data validation and reconciliation to ensure accuracy. 

Data analysis and visualization: creating interactive dashboards for internal and external stakeholders 

Designing database queries: Prepare advanced SQL scripts to extract Cardano blockchain data for various purpose
Pull data from various online and offline data sources into staging database and setup automatic incremental refreshing scheme. 
Design data cleaning protocol and apply in data cleaning packages.

Design and implement ETL (Extract Transform and Load) packages for data cleaning, flagging, and transforming on recommended tools including SQL Server Integration Services (SSIS) or Pentaho Data Integration (PDI).  
Analyze project data and design complex queries to extract data per project reporting demand. 

Design reports and dashboards to support management decision making and adaptive learnings. 

Lead data reconciliation meetings with project stakeholders to ensure all data is on expected level of quality and quantity.  
Design and implement innovative ICT4D solutions including web application  

Support capacity building efforts in utilization of ICT4D tools and reports.
"
data engineer,"
- Data manipulation and querying. 
- Data integration. Building and Maintaining SSIS packages. 
- Data Warehousing (SQL, Snowflake) 
- Creation of initial SQL stored procedures to export/import data.
- Maintenance of existing SQL stored procedures according to the changed client’s requirements.
- Communication with BAs and QAs daily.
- Generation of reports from the Salesforce.
- Code review
- Arrangment and driving Sprint planning meetings
 
I'm highly motivated, disciplined, responsible and well-organized. At the present moment, I'm learning Python. 
 

Tools&Technologies: T-SQL, ETL, Data Warehousing, Snowflake, MS SQL Server 2019, SQL Server Management Studio 2018, MS Visual Studio 2019 (SSIS), Jenkins, Scrum, Azure DevOps Server, Salesforce, Jira
"
data engineer,"Engaged in self-directed learning, active enhancement of expertise in data science is 
the primary focus. Proficiency in statistical analysis, predictive modeling, and 
programming defines the skill set. The immediate goal is to apply and refine this 
proficiency in the data science field, with an aspiration to secure a data scientist role 
within a forward-thinking organization valuing existing skills and the commitment to 
continuous learning
Data Science Intern Aug 2023 – Present
Standard Bank (Remote)
 Integrated business understanding, data preparation, and advanced modeling 
techniques to execute a machine learning project during Data Science Internship 
at Standard Bank.
 Evinced keen grasp of data understanding and evaluation, resulting in strategic 
development and execution of machine learning project that contributed to 
company's digital transformation goals.
 Pioneered rigorous approach encompassing data preparation, modeling, and 
evaluation to achieve tangible outcomes in machine learning project, enhancing 
organization's analytical capabilities and data-driven decision-making processes.
Data Science Intern Jul 2023 – Present
British Airways (Remote)
 Overhauled efficient web scraping and analysis, leveraging data-driven insights 
to inform strategic decision-making as part of machine learning project during 
British Airways Data Science Internship.
 Trailblazed exploratory data analysis and predictive modeling efforts, producing 
actionable insights that guided decision-making and contributed to completion 
of machine learning projects at British Airways.
 Innovated a natural language processing (NLP) project, demonstrating 
advanced skills in data manipulation, analysis, and project management during 
British Airways Data Science Internship.
Data Science Intern Feb 2023 – Jul 2023
Data Glacier (Remote)
 Demonstrated expertise in business understanding, data understanding, and 
data preparation, culminating in execution of comprehensive data manipulation 
and analysis during Data Glacier internship.
 Bolstered modeling, evaluation, and model selection, contributing to 
development of effective machine-learning projects that harnessed advanced 
techniques for actionable insights.
 Ameliorated version control (Git) and executed model deployment with Heroku, 
ensuring seamless collaboration and efficient deployment of machine learning 
resume_classifier during Data Glacier internship
"
data engineer,"
Data Scientist - Trementum Analytics
May 2022 - Current
Implementation of similarity search.
- Set up Elasticsearch on Kubernetes cluster on the cloud, further maintenance;
Pipelines to transfer data between databases, and further transformations (ETL). Data cleaning, working with OpenAI embedding model, using Elasticsearch as a Vector database for a similarity search engine. All scripting on Python;
- Provide results on dashboards - Kibana or Grafana for simple and Plotly Dash when more complex functionality is needed; 
- Results - developed a new effective method for analysis: efficient Text search and powerful cluster algorithm.

ETLs -  extraction from different sources (like APIs) to Postgres database.
-nControl data completeness and quality (with test scripts and dashboards);
Assistance with implementation, testing, and maintaining pipelines on Jenkins (CI/CD)
- Find and fix problems in pipelines when occurs. Provide detailed reports for developers for problems with data extraction methods;
- Results - creating and development of the company's major product, which includes multiple automated data pipelines.

Involvement in implementing new technologies and building teams:
- Research of new services and technologies for the need of the company, comparison, and building demo projects to understand the possible value from them;
- Taking part in candidates' overviews and interviews. Results: successful hiring of two new analysts, who helped our team to advance our productivity and build new products.


Financial analyst/manager, sales department
Petrovets Apartments · Full-time
Mar 2019 - Dec 2021

• A real estate project at the sea resort. In 2,5 years, our department managed to sell more than 200 apartments and to find clients all over Ukraine
• Creating and controlling reports and dashboards using MS Excel, SQL
• Cooperating with the marketing team to improve our performance indicators
• Forecast future earnings and expenditures based on previously analyzed financial data
"
data engineer,"I'm working as a software engineer in a tech company, mostly with databases, with some involvement in web development.  Creating reports, creating scripts for data migration, supporting system, writing specifications, solving client problem requests.  Creating new databases, stored procedures, optimizing current sql-scripts, functions, triggers, views and queries etc. 
Control, monitoring, and Safety Systems.
Technical data support for other departments.
Also Dynamic  CRM
Work with limited direction, usually within a complex environment, to drive delivery of solutions and meet service levels.
Participated in modifying and changing the architecture of the original employee Retention application. Develop plug-ins to synchronize/update payment details to the CRM system.
Worked closely with customers
Team-player, deadline orientated and having the ability to learn fast. Independent, self-motivated team player that is meticulous and methodical in creating solutions.
DB servers maintenance;  
Experience with relational database, data modeling, business intelligence/reporting;
Data transactions updating, fixing, validating, data queries development;  
Experience building and optimizing data pipelines and data sets;
Creating projects app from scratch, including end-to-end processes and workflow;
Experience as a Business developer, delivering software projects with deep understanding of core functional capabilities
Refined problem-solving aptitude and analytical skills;
TECHNOLOGIES: 
JavaScript, PHP, Interbase, MongoDB, MySQL, 
T-SQL, Node.js, jQuery,  JSON, Bootstrap 3, Angular.js, React.JS, Redux, Webpack,  REST API, Websockets
TOOLS: 
WebStorm, PhpStorm, IBExpert, MS DB Management Studio, SublimeText , XAMPP, Denver,  PHPMyAdmin,  Adobe Photoshop, Avocode, K2 blackpearl
OS: Linux, Windows, Mac OS
I'm looking for an interesting job where I can use my skills and experience and looking for a chance to work in an environment where I can enrich my technical knowledge. I would like to join a friendly team where I will be able to grow
I'm interested in long-term projects, professional growth, and development/education."
data engineer,"
* December 2018 – February 2019: SDE Intern at Samsung R&D Institute Ukraine.
	* Developed and modified cryptography algorithms.
	* Developed applications for steganography and steganalysis.
	* Evaluated performances of different approaches and algorithms.
* December 2019 – March 2020: Junior Security developer at Samsung R&D Institute Ukraine.
	* Developed key-logger applications for PC and smartphone.
	* Created resume_classifier for user authentication/verification with data collected from those apps.
	* Evaluated performances of different resume_classifier.
* June 2020 – February 2021: ML Engineer/Backend Developer at Adoriasoft Inc. 
	* Developed ML algorithms for image and video watermarking.
	* Implemented and tested multiple cutting-edge approaches and resume_classifier.
	* Evaluated costs of product in production and optimized overall performance of product.
	* Deployed resume_classifier to production and created APIs for serving deployed resume_classifier.
* September 2020 – December 2020: part-time Data Scientist at Artellence.
	* Collected needed user-data from social networks.
	* Used collected data in e-commerce and credit-scoring resume_classifier.
	* Used user clustering and segmentation for internet-marketing resume_classifier.
	* Worked with big amounts(100+ million users) of client data.
* February 2021 – Now: ML/Data Engineer at Genesis.
	* Led team of 2 data engineers/data analysts.
	* Developed complex ML API’s and infrastructure using Python and AWS/GCP.
	* Developed ML resume_classifier for photo/video editing and personalisation that were core features of our app → app grew from 0$ to 200k$ MRR in just 2 month after first build.
	* Introduced Airflow as core of any data pipelines on project, optimized SQL scripts and tables → reduced infrastructure costs from 10k$/month to 3k$/month.
	* Created and led AB-test evaluation automation → saved our team 20 hours/week.
	* Integrated off-site LTV calculation and marketing automation service → saved 10k$/month in marketing costs.

* Tech Stack:
	* Languages: Python, JS.
	* Cloud Computing Providers: AWS, GCP.
	* VSC skills: Gitlab, Github, CI/CD.
	* AWS Stack: ECS, EC2, ECR, Lambda, S3, Redshift.
	* GCP Stack: GCS, GCR, GKE, Cloud Functions, Big Query, Cloud Composer, Compute Engine, Dataproc.
	* Databases: BigQuery, Postgres, Mongo, Redshift.
	* Analytics and Data Packages: Airflow, Scipy, Numpy, Pandas, Pyspark, GCP and AWS libs for python.
	* Analytics Software: Appsflyer, RevenueCat, Amplitude, Firebase(Google Analytics), Algolift.
"
data engineer,"Own business automatization platform with dinamic GUI of DB
Delphi Developer remoute
Moja Marketplace, USA
Mobile Market Place for Android and iOSMobile Market Place for Android and iOS
(October 2022 - April 2023)
•	Create marketplace in Android (and iOS)
•	Create forms
•	Connection with AWS 
•	Testing
Tools and Technologies: Embarcadero Delphi Studio, FireMonkey, Android, Amazon Web Services (AWS).

DB Developer / Delphi Developer / Project Manager 
Freelance PerfectSoft, Plato Kharkiv, Ukraine
(April 2009 – December 2011, August 2013 – Present) 
•	Created and modernization business automatization platform 
•	Database design and development 
•	Core design and development
•	Business analysis 
•	Support and Installation
 Tools and Technologies: Delphi, Firebird, SQL, Builder C++, CMS WordPress, MySQL.

Director / Manager Projects (not IT)
Rope Park S-Park, Kharkiv, Ukraine		Rope Park Korsar Kharkiv, Ukraine
(June 2009 – December 2011, July 2013 – February 2022)	(October 2013 – March 2019)
•	Business analysis 
•	Result orientation and command work 
•	Marketing and Internet Marketing
•	Make documentation and open LTD
•	Accounting registration. Budget management
•	Opening a branch, opening new directions
 Tools and Technologies: MS Project, Management, Marketing, Sale, Business Analysis.

DB Developer 
Intetics ltd, Chicago, IL / Ukraine, Kharkiv, Development Office 
(June 2012 – June 2013)  
•	Database design and development
•	Portable scripts to Big Data system
•	Download / upload data
•	Validation and investigation root cause of discrepancies
Tools and Technologies: SQL, NoSQL, ETL, Netezza, Oracle.

DB Developer/ Delphi Developer
Trading company OLKO, Kharkiv, Ukraine
(January 2012 – June 2012)
•	Core design and development
•	Database design and development
•	Requirements gathering
•	Estimations
•	Deployments
 Tools and Technologies: MS SQL, Delphi, Fast Reports.

DB Developer / Delphi Developer
Software development company CS ltd, Kharkiv, Ukraine
 (January 2007 – March 2009)
Bank automating system  
•	Design and implement graphical user interface for existing application suites
•	Database design and development
•	Code review
•	Business Analyst
 Tools and Technologies: Oracle PL/SQL, Object Pascal, Oracle database design, stored procedures, triggers, analytical functions etc.
SQL-developer, BI, DataScience, Oracle developer, MS SQL developer, Big Data, Data Engineer"
data engineer,"Create data model and develop and deploy algorithm to calculate monthly search volume based on Google Trends;
Develop data validation for all ETL pipelines;
Develop CLI based service for automated tableau reports publishing;
Optimize SQL queries;
Develop forecasting and planning processes that required minimum human interactions;
Create a fully automated project's performance reporting system;
Based on statistics methodology develop fraud detection algorithms and reports;
Developed pipeline with acquiring third-party data,  uploading to the data lake, and further delivering to ML systems.
Design and develop ETL pipelines;
Design and develop reporting platform with over 1000+ client reports;
Integrate third party data;
Design and develop data lake;
Looking for challenging and interesting tasks where I can gain data mining, machine learning skills. I like python, and looking for an opportunity to improve my programming skills. Friendly colleagues.

Product company only."
data engineer,"
Design, development and testing of project deliverables:
- Relational and multidimensional metadata resume_classifier,
- ETL process,
- OLAP structures,
- Technical documentation,
- End-user training,
- BI platform administration (installation and deployment, content administration, URM, system monitoring and
security),
Technical requirements analysis and solution design.
Involved in system architecture and DWH database model design.
Maintenance of DWH system: analysis of reported incidents and bug fixing.
Performing testing and preparing UAT documentation and operation guides.
"
data engineer,"
Designed, developed, and tested the ETL strategy to populate data from multiple source systems. Designed and developed Power BI graphical and visualization solutions with business requirement documents and plans for creating interactive dashboards. 
• Leads the activities of data warehouse project teams in the design, development and implementation of data warehouses; and the configuration and auditing of data warehouses to ensure quality control of data.
• Oversees the development, operations and maintenance of data warehouse environment, including organizational balance and system coherency between front-end desktop/client environment and data warehouse back-end processing functions.
• Provides guidance regarding the use of data warehouse systems, capability of systems to deliver information, and subject matter expertise regarding current systems and emerging technology.
• Lead team of Data Integration Engineers both onshore and offshore resources.
• Be a thought leader and driving force behind data initiatives
• Learn our client’s users’ needs both from a historical/warehouse perspective and an operational/transactional perspective
• Create standards and conventions for data warehouse, analytics, and ETL systems. Lead governance and enforcement of standards.
"
data engineer,"
•	Designing and implementing Extract, Transform, Load (ETL) processes to efficiently move and transform raw data from various sources into usable formats for analysis.
•	Utilizing cloud-based platforms (such as AWS, GCP) to deploy and manage data engineering solutions, leveraging their scalability and flexibility.
•	Integrating data from disparate sources, both internal and external, to create a unified and comprehensive view of organizational data.
•	Developing and maintaining data resume_classifier that define how data is structured and organized in databases or Data Warehouses to optimize querying and analysis.
•	Designing, constructing, and managing Data Warehouses to ensure high-performance storage, efficient data retrieval, and scalability.
•	Maintaining version control for code and documentation of data engineering processes, facilitating collaboration and knowledge sharing.
"
data engineer,"
Designing, developing, optimizing, testing, monitoring traditional classic data warehouses and ETL/ELT processes

Communication with businesses, Data scientists etc

2+ years of engineering experience
1 year of BI developer experience
"
data engineer,"
Develop data architecture development roadmap in a fast paced industry.
 Give technical support regarding data readiness and availability.
 Develop, design, and schedule data pipeline for new service.
 Monitor and maintain running data pipelines cloud server for services.
 Support and maintain data warehouses
 Develop and design BI reports including executive reports and detail reports.
 Improve data pipeline performance run time by: - migrating old data pipeline into modern data pipeline,
- optimize pipeline schedule, - optimize query.
Tech stack: SQL, Python, Apache Airflow, pandas, dbt, AWS RDS, AWS redshift, AWS S3, AWS Quicksight, Google Cloud Platform, Google Cloud Storage, Google Data Studio, Microsoft powerBI
"
data engineer,"3 years of Data Engineering/Data Analytics/BI Engineering experience. Worked with financial, marketing, user data. Have freelance experience. 
Certificates: AWS CCP, SnowPro Core.
Currently studying for Masters degree in Data Science in Hungary ELTE/Sweden KTH.
Bachelor's degree in Data Science, Ukraine KPI.
First place in 2017 and 2018 at the all-Ukrainian competition of research works among high school students in the field of Math.
Developed Data Audit and Reconciliation framework for BlackRock Enterprise Data Platform. Tech stack: Python, Airflow, Snowflake, dbt, Great Expectations, S3. Implemented a diverse range of data quality checks, worked on financial data processing orchestration.
Developed BI applications for website and mobile app marketing and user flow analysis. Tech stack: MySQL, Postgresql, BigQuery, Python, Tableau, Google Sheets. Analyzed A/B-test results using bayesian statistics - implemented internal platform to track experiments' flows using Plotly Dash. Integration with third-party APIs such as Google Ads, Facebook, Pinterest etc.
Designed and implemented a BI application for YouTube channel performance analysis. Chosen data pipeline tech stack: BigQuery, Google Cloud Storage, Google Cloud Functions, Data Studio, Python, YouTube Data and Reporting API.
Developed BI applications for mobile game marketing and user flow analysis using BigQuery, R (tidyverse + ggplot2), Google Sheets API, Google Data Studio. Integration with third-party APIs such as Facebook, IronSource, AppsFlyer.
Interested in the Data Engineering position in first place. AWS, Python stack is a priority. Eager to work with new technologies and tools., such as Spark and Databricks
Highly interested in working flexible or late hours due to Master's studies."
data engineer,"Latest: personally implemented Airflow + Azure Kubernetes(docker+helm) + Github Actions as a standard for basic data engineering
Developed databases from scratch based on different data resume_classifier
Performance optimization and tuning
ETL/ELT in db, based on Python services or using tools like Airflow, Data factory and dbt
Deployment via Github Actions + Terraform
Kubernetes - maintaining Airflow instances
Some team lead/manager activities in last 4 years - talking to different departments, discussing solutions, providing new tools and platform for data engineering and analytics. Team of 4 + 1 data engineers.
I would like to step back from devops activities and focus more on development and management"
data engineer,"
Developed reports for controlling and analyzing
company workflow (SQL scripts + PL/SQL
procedures and functions + Excel automated
via VBA scripts)
Designed ETL processes for collecting and
summarizing data from different sources
Developed cross-validation processes for
international data reports
Develop processes and criteria for analyzing and summarizing data
Interpret data, analyze results using statistical techniques and provide ongoing 
reports
Filter and “clean” data by reviewing computer reports, printouts, and performance 
indicators to locate and correct code problems
"
data engineer,"- developed several complex ETL pipelines, ETL plans, real-time processing
- monitoring and supporting up to 80 developed ETL processes  
- SQL optimization
- preparation and development of BI dashboards
- implementation of fixes in the existing dashboard
- data preparation
- quickly learn/use new technologies
- etc
Developing and supporting ETL processes (real-time and scheduled processes).
MSSQL/PostgreSQL/Azure SQL/Presto/Spark querying (DDL, DML, stored procedures, etc), data preparation.
Currently working with salesforce, extracting/updating objects. Creating pipelines to transfer data from/to hive, Snowflake, Postgres, etc.
Interested in new opportunities (full-time/part-time)."
data engineer,"
- developing PL/SQL procedures, DWH, tuning SQL;
- providing information for solving marketing and technical cases;
- analyzing and converting business problems into technical requirements;
- creating  reporting system as DB and PL/SQL developer;
- data visualization (Power BI)
- creating tools with alert option for monitoring and tracking data integrity in DB (PL/SQL)
"
data engineer,"
Development and automation of business
processes;
Creating of push messages;
Analysis of CRM campaigns;
Creating and updating procedure, view;
Automation of reporting;
MS SQL, Oracle
Python(Basic level)
Excel
Json
Jira
Git
Airflow (jobs)
"
data engineer,"
Diverse project contributions, both small and large scale, along with continuous skill development through personal projects.

Most recent project was the development of a reporting automation app for my company. The app is fully automated and built on top of the Google Cloud Platform, leveraging its serverless capabilities to ensure scalability and meet the demands of a growing number of accounts. I was a key member of a small development team, collaborating closely with two other individuals to bring this project to fruition.

In addition to my previous experience, I am currently involved in a cutting-edge project that involves creating a suite of technology solutions powered by a proprietary Marketing Intelligence Engine. This project is designed to enhance decision-making processes within key areas of digital marketing strategy, bringing greater efficiency and effectiveness to marketing efforts.
"
data engineer,"Implemented a version of StyleGAN encoder
, built a Data Studio connector
, competed at Kaggle
, go crazy about dev productivity - have 51 Github gists (and counting), version-controlled VS Code snippets, half a dosen self-built utilities and paid subscriptions to multiple AI-based dev tools
, been developing physics engines - state-of-the-art one at work, and an ordinary one for fun, as a hobby.

Didn't save a forest, didn't sell a company, but well, that's something too.
Doing data engineering in one form or another since 2016. Mostly with Python and AWS, but also worth noting that at different times with - DataHub, Gruntwork, Apache Superset, Thrift, Quantopian.

Work best if trusted with team-level objectives.
Looking for an opportunity to spearhead a data initiative for a company of ""Series B size"" or bigger"
data engineer,"•	SSIS
•	Talend ETL
•	SAP BODS 
•	Oracle ODI / OBI	•	SAP BO   
•	Power BI
•	Qlik View/Sense
•	SSRS
DWH & BI Unit Manager – PayCore Payment Systems - Jun 2021 –  Working
-	Designed, DWH architecture of Payment Management Systems, Mastercard systems and many related systems.
-	Uses SSIS, Talend Open Studio, SAP BO/BI, Power BI
-	Managed an agile team.
Senior Solution Architect – MDS ap Turkey(Midis Group Inc.) - Feb 2019 – May 2021
-	Adaptation of ETL and Business Intelligence projects.
-	ETL/BI project in Türkiye Insurance Inc. 
-	Consolidation project for TWF(Turkish Wealth Fund). 
-	ETL Development project for Rotana Dubai.

Founder - Fox Analytics – Jun 2019 
-	Developed ETL processes and BI Reports any tools
-	Served consultancy solutions and project management

Enterprise Applications Manager – NETAS Telecommunication Inc. - May 2015 –  Jan 2019
-	DWH: Project management for the creation of company DWH in SAP BODS and Oracle.
-	Responsible for SAP BI, Power BI: Created structures in accordance with DB and DWH, developed BI dashboard. 
-	Managed SAP R/3 and Oracle E-BS HR: Team management supporting Logistics, Finance and HR units, managment of consulting firms.
-	Led a team in Salesforce CRM: Management of the team that develops and supports internal and external applications.

Senior Software Developer - Yataş Inc. - Agu 2011 – May 2015
-	Java ADF Developer; developed Store applications with Java within the scope of Mobile Field Sales Applications project
-	Oracle E-BS Developer; developed Oracle Forms, OAF, PL / SQL, Workflow, Discoverer, XML Publisher.
-	After the R12 transition was completed, developed warehouse and shop handheld terminal applications. Made improvements within the scope of Production, Sales, Purchasing, Shipping, Warehouse process improvement projects.
-	As part of Oracle R12 Project, responsible for additional enhancements, personalized and report enhancements in the production and logistics modules.  Re-coded the applications used in the current E-BS 11.5.9 in R12.

IT Project Manager – Tırsan Logistics Inc. - Nov 2009 - Agu 2011.
-	Developed software used by the Shipping, Warehouse, Accounting, Sales and HR unit, project management, process analysis and accounting unit.
-	Executed improvements in logistics operations and oversaw management of projects that were developed according to customer demand. 
-	Headed the team that develops software with .Net, Java and Oracle
-	Met new customer / project automation / integration needs in shipping and warehouse units, coordinated the project at the stage of developing B2B applications
"
data engineer,"
DWH developer:
- Python (ETL) / Pandas / Streamlit / SQLalchemy
- Dagster
- FastAPI
- PostgreSQL 
- MSSQL 
- Pentaho 
- GIT / Linux / PGAdmin

Integration engineer (health care):
- PostgreSQL
- HL7, PDF, CSV integrations 
- REST API
- CSV, JSON
- Scrum
- Docker / Elastic / Jira
- Monitor issues and troubleshooting

Technical support (fintech):
- PostgreSQL
- Redmine 
- ElasticSearch
- Logs analysis, troubleshooting

Claims agent (aviation):
Started from customer specialist, was promoted to team manager after almost a year of work. Had an own team in the subordination, so as well was responsible for compliance with the following duties:
- cooperation with foreign colleagues from other countries (calls, emails, Skype meetings, Slack communication);
- increase productivity and quality results of my team;
- productivity reporting for management;
- creating tasks for each day.
Interesting in professional growth, learn new technologies."
data engineer,"
- Enhanced the team's performance by 85% by accelerating data processing speed and generating trends from diverse sources using Snowflake and Databricks.
- Created executive and operational dashboards using Tableau, Databricks and Snowflake.
- Identifying customer requirements, discussing details, and creating a step-by-step plan for problem-solving
- Created TensorFlow-based Machine Learning resume_classifier (Linear Regression, Random Forest) for the prediction of data fluctuations, encompassing drops and spikes.
- Performed data analysis by using Pandas and NumPy libraries in Python, provide strategic insights based on the existing data metrics.
- Skilled at crafting intricate SQL queries and procedures to retrieve, manipulate, and update extensive datasets proficiently.
- Supervised data quality audits on processed data. Provided mentorship and guidance to junior data engineers.
- Created and updated numerous technical documents
"
data engineer,"
EPAM 
2022 June - now
Maintain ETL processes written mainly in Python. Monitor airflow dags. Develop data pipelines in Databricks ingesting from different sources into S3.   

Ru-Center Group 
2021 Oct - 2022 May
ETL development in Python, Spark, Airflow. Maintaining datalake in Hadoop.

BI Innovations
2021 June - 2021 Sep
Work in the customer analytics department. Developed ETL processes using Talend, Python, SQL Server. Responsible for data cleansing.

Geometry
2020 Feb - 2021 May
Collection and processing of data:
    - site parsing (Beautiful Soap, Selenium, UiPath)
    - extracting data from REST Api
    - work with databases (MSSQL, PostgreSQL, MongoDB)

    Skills:
    - Python (pandas, numpy, multiprocessing)
    - SQL (stored procedures, transactions, triggers)
    - Apache Airflow for orchestrating and running tasks daily
    - Power BI
    - RPA (UiPath)
"
data engineer,"Always work efficiently and carefully, responsible, good team player, quick learner.

Achievements:
Migrated 100+ ETLs from variety of legacy platforms;
Implemented automated file comparison service;
Helped with QA of new platform features;
Implemented configuration of CCPA compliant privacy policy and alerting service.
ETL/ELT processes development (SQL, PySpark);
Work with Databricks, Snowflake, Teradata, SAP DS, GoAnywhere, Talend, Streamsets platfoms;
Work with Gitlab, Rundeck, Airflow, Kafka tools;
Delta Lake data processing;
ETL processes optimization;
Big data tools application.
"
data engineer,"I became a team lead in small but productive team
Participated in the Django Girls as a lecturer
Contribute to Apache Airflow
Expereance with async server side tecnologes like Tornado,  familiar with ClickHouse + SQLalchemy 
Strong knowledge of Python3, expereance with FastAPI, flask, Aiohttp, Tornado
Experience with building ETL pipelines with schedulers like Airflow.
Unix system administration skills, 
Cloud platforms: AWS, GCP
familar with Vagrant, Docker, Terraform, Ansible
 Redis, Celery, 

Basic knowledge of Java/Scala
Working on interesting, LONG-TERM and FOREIGN project in friendly remote team with new technologies , resolving difficult tasks.

I do not consider fullstack positions!"
data engineer,"
EXPERIENCE

10+ years of outsource experience in software development (including Telecommunication, Regulatory Reporting Projects, Reference Data Services)
15+ years experience in the IT industry
Maintenance and implementation of ETL processes (data extraction, transformation, loading, cleaning, and validation) for data migration and loading large data sets within Transactional Data Warehouse.
Technical Lead / Developer with experience and knowledge in all stages of project life cycle.
Technical Leadership of delivery group (up to 12 FTE). 
Strong communication skills, experience working with USA, UK and Asian customers and multi-vendors interactions.

TECHNICAL SKILLS
RDBMS: Oracle 10g/11g/12g/19c/Exadata, SAP Sybase, MS SQL;
ETL Tools: Informatica (PowerCenter 8.6.1/9.1.0/10.1.0);
Data Modeling: ErWin, Rational rose, BpWin;
Languages/Utilities: SQL, PL/SQL, Java SE, Shell script, Python, JavaScript, C++, C#, Pascal , Batch, XML/XSLT/XSD/Xpath;
Issues tracking: Jira, ALM;
Version control system: SVN, GIT, Rational Clear Case Explorer;
Continuous integration: TeamCity/Jenkins;
Database Tools :  PL/SQL Developer, SQL*Plus, SQL*Loader, SQL Developer;
BI Tool: Oracle Business Intelligence Discoverer
Other Tools: Control-M, Hadoop, dBusJMS, IBM MQ, Altova XMLSpy, Confluence, Informatica Data Validation Option (DVO).

PERSONAL QUALITIES

Responsible, detail oriented and results driven, good organizational skills, open to new ideas/opportunities, capable of focusing and working under pressure,  critical thinking skills, creative, honest, with a good sense of humor, positive team player.
"
data engineer,"Made fully-automated ETL process for small company.
Have certificate ""Python"" from coursera.org
Have certificate ""Data Engineer"" from robot-dreams.cc
Experience (4.5 years):

- ""Amanat Insurance"" (2021 feb - 2022 feb)(1 year) :

Data Enginner(1 year): 
Build DWH (PostgreSQL) from the beginning and automated ETL processess (Talend). Also, I had work with GITLab for the first time. All CI/CD and ETL/ELT orchestrated by Airflow. In the end, made views on SQL for DA team.
Instruments: Python, PostgreSQL, Talend,  Airflow, Git
Team: 2 members

- ""DAR ecosystem"" (2017 oct - 2021 feb)(3.5 years):
Data Engineer (2 years): 
Made ETL/ELT processess on Pentaho for structured instances. And used Python for non-structured databases. Had great experience in integration with GA, YM, FA by rest API. Worked in different projects and different spheres 
Instruments: Python, PostgreSQL, Pentaho, Postman
Team: 4 members
 
Data Analytics (1.5 years):
In the first part of my career, I made visualisations in Power BI and wrote SQL quieries for table aggregation. I was part of reporting team.
Instruments: Power BI, PostgreSQ, Pentaho, Excel
Team: 5 members

Now, I want to work as Data Engineer
Today,progress in IT sphere is rapidly developing, So I want to learn new technologies (Docker, Cloud Databases, Apache Spark) and gain experience from foreign companies to be strong competetive person."
data engineer,"
Experienced Data Engineer(5+ years)
Have a solid understanding of classic DWH and Data Lake architectures, ETL&ELT solution design. Communicative team player, with strong analytical skills. Still young and proactive. 
Upper-intermediate+ English.

Experience:
- Scala, Python.
- Implementation of classic ETL workflows (SSIS, DataStage, Talend, etc.).
- Building real-time high-load Big Data pipelines (Spark Streaming)
- Collecting and parsing data from different sources (DB, Parquet, JSON, CSV, EDI X12, API endpoints).
- Processing of large amounts of data, and performance optimizations.
- Building a cloud data platform from scratch (AWS, Azure).
- Working with a wide type of data storages (Hive, Data Lake, Relational database, DWH, Kafka, Elastic). 
- DataOps stuff. 
- Data validation and tests.
Not interested in big old projects with thousands of unnecessary meetings. The product should be interesting and useful for humanity. I don't want to be a part of a project where the main idea is to get money from customers.
No russian co-workers, projects, or targeting russian markets."
data engineer,"
Experienced Data Engineer. AWS Certifications, Python/Scala and SQL/NoSQL databases experience.

Located in Ukraine.
"
data engineer,"
Experienced Data Engineer with a demonstrated history of working in the computer software industry. Skilled in SQL, Python, data transformation cycle (ETL/ELT), and data warehousing. Self-paced, proactive, and result-oriented person who can work independently with a high sense of ownership.
Below please find a list of my experience in key technologies:


Programming Languages:

- Python - 5 years. Object-Oriented, functional programming, developing ETL/ELT solutions;
- SQL - 5 years. Working with transactional (MySQL, PostgreSQL) and analytical (BigQuery, Redshift) databases. Skilled in query performance optimisation (both on time and memory);


Clouds:

- GCP - 3 years. Working mostly with BigQuery, Cloud Functions, Cloud Compute, Cloud Composer, Cloud Scheduler, Cloud Storage. Have solid experience working with GCP API, optimising data warehouse / storage, as well as IAM management;
- AWS - 2 years. Worked with Redshift, Aurora, EC2, Athena, Lambda. Experienced in dwh modelling and optimization;


Orchestration:

- Apache Airflow - 3 years. Experienced in building the tool from scratch, as well as in complete management of executors, dags and other key features;


DE Fundamentals

- Data Warehousing: Experienced in Design, Implementation,
Optimisation of differently structured warehouses (Normalized/Denormalized, using Inman/Kimball approaches).
- Data Quality Assurance: Implementing processes and checks to ensure the accuracy, completeness, and reliability of the data. This can include data validation, outlier detection, duplicate removal, and data profiling techniques.
- Data Governance: Establishing policies and procedures for data management, including data privacy, security, and compliance.
- Documentation and Collaboration: Documenting the data engineering processes, workflows, and system architectures to facilitate collaboration among data engineering teams and other stakeholders.

List of recent job experiences:

- Senior Data Engineer in FinOps Startup
- Senior Data Engineer in hardware Startup
- Data Engineer in big e-commerce corporation
Flexible schedule, possibility to work from home and passionate team."
data engineer,"- Improved company’s evaluation significantly by building a client facing reporting dashboard that process more than 2GB of log data daily that helps the company standout from competitors as data driven adtech business.
- Prevent a historical data loss from a third-part platform by ingesting an API call on a daily basis which then partition and load the data to an S3 data lake in parquet format.
Experienced in building and managing high-performance big data pipelines to enrich data for analytics, ML and AI, delivering analytics solutions using BI tools such as Power BI, Tableau and superset, designing and implementing efficient data infrastructure to organize and refine data.

I am currently working as a data engineer. My role involves building data pipelines, developing campaign performance reports that meets client’s requirement, onboarding and coordinating junior data engineers.
- Improved company’s evaluation significantly by building a client facing reporting dashboard that process more than 2GB of log data daily that helps the company standout from competitors as data driven adtech business - providing real time, mission critical, performance boosting, and quality controlling insights and intelligence to clients. It has three microservices, among which two are containerized in docker: React app with ApexCharts, Flask API and ETL pipeline.
- Prevent a historical data loss from a third-part platform by ingesting an API call on a daily basis which then partition and load the data to an S3 data lake in parquet format.
- Diminish the time it take to get a report for campaign managers to almost zero by orchestrating a google sheet report generator python script with AWS Athena and Glue.
- Reduced development time by 10 hrs to develop flask authentication module for all data team by building authenticator package that can be easily used as a decorator.
- Avert incorrect reporting flow that would drive away clients by exploring the data and identifying the cause of a discrepancy.

I am keen to utilize my skills to create data solutions with a heavy emphasis on great user experience and looking to further improve my skills in big data engineering.
"
data engineer,"SoftServe IT Academy 

Responsibilities: Relational, Staging, DWH design;
Creating stored procedures, views, functions;

Tools & Technologies: T-SQL, MS SQL Server, Jira, Confluence.
Experienced in:
Creating stored procedures, views, and functions;
Visual Studio SQL project
"
data engineer,"AWS Certified Solutions Architect
Experienced python engineer with expertise in developing data pipelines, ETL processes, and data warehousing solutions. Skilled in Python, SQL, and cloud-based technologies such as AWS. Passionate about turning complex data into valuable insights that drive business growth. Looking forward for interesting collaborations.
"
data engineer,"
Experienced, result-oriented, resourceful and problem-solving Data engineer with business management skills.
Adapt and met challenges of release dates. Over 3 years of diverse experience in Data engineering fields,includes development and implementation of various applications in data processing environments.
Willing to provide systematic solutions to complex challenges in competitive environments. Ability to create automated data management systems for recursive data processing operations.
Working with:
- New technologies - especially Cloud Based technologies.
- Large datasets
- International team 
- Develop ETLs from Flat files, RDBs, online spreadsheets, online applications, API endpoinds (POS systems), cloud storage systems. 
- Interested in IOT systems, real time data processing"
data engineer,"
Experienced Senior Engineer with a demonstrated history of working in the broadcast media and financial industry. Skilled in Automation, NoSQL Technologies and Cloud Platforms.

* Full support for NoSQL databases (Cassandra, Mongo, Couchbase) and Analytics platform (Apache Spark) inside the department:
** Creating scripts for automating installation and support:
*** Ansible 
*** Puppet 
*** VMware 
*** AWS 
*** OpenStack 
*** GCE 
*** GKE 
** Creating guidelines for implementing monitoring for this platforms: 
*** Graphite 
*** Prometheus 
*** ELK 
** Creating support documents 
** Creating docker images and documentation for its use with orchestration technologies: 
*** Kubernetes 
*** Swarm 
*** Mesosphere 
** 3rd level support for this platforms 
** Data modelling and tuning

Technologies: Cassandra, Couchbase, Mongo, Apache SPark, ELK, Ansible, Puppet, Python, Bash, AWS, VMware, OpenStack, GCE, GKE, Kubernetes, Swarm, Mesosphere, Graphite, Prometheus, Nagios)
"
data engineer,"
Experienced Software Data Engineer with machine learning skills. Skilled in development, building and maintaining ETL pipelines using Python, designing data architecture, and SQL performance tuning. Also, I am engaged in developing pet projects which can be found on my GitHub, as well as studying new technologies and ML for NLP tasks.

Experiences:
– Development of data acquisition pipelines for retrieving data from source systems and development of data transformation pipelines using Databricks with defined Lakehouse architecture
– Report performance optimization: query optimization, data migration, incremental update configuration 
– R&D: Trend analyzing system, Interactive map of the full-scale invasion 
– Development of data infrastructure (data migration to Synapse, defining data model) and ETL pipelines for the retail clients
– Data analysis and demand forecasting functionality for retail clients 
– Development of a custom instrument for monitoring Azure resources using a microservices architecture with Python Azure Function for custom monitoring of Azure resources

Skills: 
Azure (Databricks, ADLS, Data Factory, Functions, Log Analytics, Key Vault, Synapse Analytics, ML Service)
Python, Apache Spark (PySpark, Scala)
SQL (creating and performance tuning of SQL queries, views, stored procedures, and functions) 
Machine learning (developing time series forecasting resume_classifier, NLP)
"
data engineer,"
Experienced Software Engineer with a demonstrated history of working in the computer software industry. Skilled in Python, Data Engineering, Web-technologies and Databases. Got a Master of computer science degree at the Computer science field.
"
data engineer,"Programming Languages/Technologies
SQL/T-SQL

RDBMS 
MS SQL Server

Development Tools 
SQL Server Management Studio
MS Visual Studio
PowerBI
Tableau

Methodologies
Agile, Scrum
Certified Student of Management School

Language 
English C1

Frameworks/Libraries/Tools
SQL Server Data Tools for Visual Studio
SSIS

Version Control
Git/Github

Tracking
Jira
Confluence

Operating Systems
Microsoft Windows
Experienced Tableau Report Developer and highly qualified Data Engineer with 1+ year of experience. I am thrilled about data science and making it more visual and interesting. 
I participated in designing and maintaining OLTP Database as well as OLAP Database using MS SQL Server, SSIS and data visualization tools such as Tableau and PowerBI. I have mainly worked on writing SQL queries, store procedures and creating SSIS packages. 
I have experience in Tableau and PowerBI Report Developing.
I would like to concentrate on data reporting but I am also interested in learning new technologies and evolving."
data engineer,"- Build event-driven pipeline for data collection from the social media platform
- Build ML pipeline for ranging videos for posting
- Developed ETL pipeline from scratch (using Airflow and Docker) so it got much better in terms of monitoring, configuration, and execution time 
- Refactored dashboards from static to more interactive and informative.
- Built a notification system about changes in business processes, so it became possible to do process analytics by analyzing logs from this system (eg. reporting based on log files)
- Detected fraud (which wasn't detected by ML) based on basic descriptive analytics and saved the company's money
Experience in:
- Building dashboards (Tableau/Data Studio)
- Building ETL pipelines (Airflow\Docker)
- Data modeling/DWH
- A/B tests analysis
- Using statistical and ML techniques to solve business problems
- Project management

Interested in further developing in:
- Programming
- Statistics/Data Mining/ML
"
data engineer,"Completed courses: Functional Program Design in Scala, Functional Programming Principles in Scala, Parallel Programming in Scala, Big Data Analysis with Scala and Spark, Apache Airflow: The Hands-on Guide.
Experience in creating Big Data applications and ETL pipelines.
CI/CD software.
Experience in working with Data Warehouses, like Snowflake.
Experience in working with Cloud Providers, like AWS.
Familiarity with Data Lakes technologies, like Delta Lake.
Experience in Python - 1.5 years.
Looking for interesting job in a friendly team, where I can combine and apply my skills, energy and knowledge in order to be productive and useful for a company and also grow as a professional."
data engineer,"
Experience in database development, data engineering. Throughout my career, I have successfully executed the complete Data Warehouse Lifecycle, encompassing essential aspects like data modelling, performance tuning, ETL architecture. Over the past five years, I have expanded my knowledge and proficiency in software engineering and database services in a cloud platform, further enhancing my ability to contribute to diverse projects.
"
data engineer,"
• Experience in migration to new DataPlatform (S3, MinIO, Apache Hive, Apache Spark)
• Development of ETL\ELT: Airflow, dbt (Data Build Tool), MS SSIS;
• Work with databases: MS SQL, PostgreSQL, ClickHouse, Hive, Trino;
• Some experience with Apache Kafka;
• Experience developing on Python;
• Development and support web service for internal clients (Django Framework);
• Experience using bash scripting;
• CI/CD tools: TeamCity, GitLab CI/CD
• Experience with Kubernetes, Docker, Grafana, Greylog, OkMeter, Apache Superset;
• Experience with Git, GitLab, TFS;
• Troubleshooting, problem solving.
Need to further develop as a Data Engineer with Python used in a modern team with advanced skills and technologies"
data engineer,"Hi,
I'm working currently at a big IT Company as a data analyst more than one year in Kyiv and I'm interesting in developing myself in Data Science. Unfortunately, there are not projects in my Company where I can improve my skills as a Data Scientist. So, I'm interesting in your suggestions.
Experience in processing and analyzing large amounts of data with Python;
Awareness of Agile;
Upper English language skills;
BS in Computer Science;
Experience more than 1 year in IT Company as a data analyst (data engineer) and 7 years in analytics, risk-management analytics at big industrial companies
I would like to improve my skills in Data Science"
data engineer,"Successful projects and leadership roles in both large international customers and product companies;
Driving data architecture and performance optimization;
Driving complex data ingestion and streaming pipelines;
Successful ML resume_classifier training, delivery and serving in production;
Significant cloud costs optimization;
Expert Data & Machine Learning Engineer

My overall experience can be described but not limited to:
* 5+ years data engineering (ETL, DWH, Big Data)
* 5+ years data science and machine learning
* 4+ years with cloud computing and Big Data technologies
"
data engineer,"
Focused Data Engineer dedicated to staying apprised of all new engineering tools and techniques. Committed to applying emerging technologies to streamline product development and business operations. Adheres to ascribed business and engineering goals by driving constant analytical skills, process innovation and iterative improvements.

Current position:
Data Engineer at Epam Systems, RMG Event Base Billing Project
Participation in development of  big data integration application based on GCP and Airflow for business users and benefit is the ability to surcharge their customers manually and automatically for infractions.
"
data engineer,"
For the last three years I have been working with BigData. This is a huge high-load project on Google BigQuery. The work takes place both in BigQuery UI itself and in two other related projects in Python and NodeJS. Before that, there were 6 years of development in PHP. Before this, 4 years of developing software applications in C++ and C#. I am also involved in scientific activities and have published on arxiv.org.

At current place of work:
 - working with a Google BigQuery columnar database - creating and optimization of queries, views, functions; creation of ML resume_classifier; architecture optimization;
 - working with a Python project interacting with BiqQuery;
 - working with a NodeJS project interacting with BigQuery;
 - working with the Adverity data intake platform;
 - working with Google Apps Scripts;
 - data analysis in Google Data Studio and JupyterLab.
"
data engineer,"
For the last year I have:
- Created an ETL process for Marketing Data from different source systems;
- Created Grafana Dashboards for further Data Analysis;
- Created Airflow pipelines;
- Created SQL scripts for cleaning and transforming data for PostgreSQL and Amazon Redshift;
- Created pages for table data using Paper Dashboard;
- Analyzed and visualized data;
- Worked with Google Cloud Platform for ETL process and app deployment;
From the new work, I expect, first of all, the opportunity to develop in technology, an interesting project, and a friendly, experienced team."
data engineer,"
Freelance 2018-08-01 - 2021-08-30
Python and Web-developer
On freelancing, I took orders for front-end development (HTML/CSS/JS), the development of telegram bots (Python/PyTelegramBotAPI) and web-sites (Flask/Django).

Alnicko 2021-08-30 - 2022-05-21
Python Developer
I am writing an interface (PyQt5) and a backend part(Python, Mosquito, InfluxDB, SQLite, Flask API, etc..) for electronic devices, deploying them on boards (Raspberry PI, Docker, Bash). I also write unit tests for all of this.

Epam 2022-05-21 - now
Data Engineer (Python)
As a data engineer I am working on developing pipeline (ADF) and notebooks (Databricks/Spark), performance optimization and investigating new development approaches.

Mate Academy 2023-01-27 - now
Mentor (Python)
As a mentor I am helping students with questions regarding tasks on Python, checking homeworks and conducting teck-check (test interviews) for them.
I would like to get into a company with a good, friendly and responsive team, which gives you the opportunity to ""grow"" and develop in your favorite business."
data engineer,"
From my perspective, I have had a valuable experience in the field of big data and data analysis. Throughout my career, I have had the opportunity to work on a variety of projects that have honed my skills in data modelling, data visualization, and data cleansing. I have also gained experience with several programming languages such as Python, SQL, and Java.

However, my primary interest lies in the field of machine learning, which I am eager to explore further. I have had the opportunity to work with machine learning algorithms, particularly in natural language processing. I have applied machine learning techniques to practical problems such as sentiment analysis and topic modelling.
As I am finishing my Bachelor's degree in the Artificial Intelligence department, I am excited to continue developing my skills in machine learning. I believe that this field has tremendous potential to make a positive impact on society, and I am passionate about contributing to it.

Overall, my experience in big data and data analysis has provided me with a strong foundation for pursuing a career in machine learning. I am enthusiastic about exploring the possibilities of this field and applying my skills and knowledge to solving real-world problems."
data engineer,"Databricks Lakehouse Fundamentals Accreditation (February 2023);
Neo4j Certified Professional (January 2022);
dbt Learn badge: dbt Fundamentals (January 2022);
Confluent Fundamentals Accreditation for Apache Kafka (October 2021);
Astronomer certifications for Apache Airflow: Fundamentals, DAG Authoring (August 2021);
Snowflake badges: Data Warehouse, Data sharing, Data applications (August 2021);
Redis Certified Developer (July 2021);

Co-founder and lecturer of the NGO “GenomicsUA”. 
Contributor to many open-source projects such as Apache Airflow;
Ukrainian translator for Scala and Python documentation;
Lecturer at Kyiv Data Science Club;
Full-time: python developer / data engineer in GlobalLogic: Jul 2022 - Apr 2023;
Project: Division of a cybersecurity company focused on machine learning, large-scale scientific computing architecture, and information visualization. 
Tasks: Internal framework development for Big Data transformations, data ingestion, query optimization, and data pipelines management;
Stack: AWS (S3, Lambda, MWAA, EMR), Snowflake, Python, GitHub Actions;

Full-time: data analyst / data engineer in TEAM International Services: Aug 2021 - Jul 2022;
Project: Account-based marketing, advertising, sales intelligence, and data company.
Tasks: Web crawling, stream processing,  query optimization, optimization of legacy data pipelines;
Stack: GCP (BigQuery, DataStudio, Cloud Storage, Composer, Dataproc, PubSub), AWS (S3, EMR, RDS), Astronomer, Apache Spark (Scala), GitLab CI/CD, Grafana;

Full-time: data analyst in National Health Service of Ukraine: Nov 2019 - Jul 2021;
Project: NHSU is the central executive body that implements the state policy regarding state financial guarantees for healthcare servicing.
Tasks: medical data analysis and visualization, report generation and sending, disease spread modeling, automatic monitoring, fraud detection;
Stack: CentOS, Python, PostgreSQL, Microsoft PowerBI, TeamCity, Gitlab;

Full-time: family doctor: Aug 2015 - Nov 2019;
Interested in healthcare data analysis, bioinformatics and data engineering."
data engineer,"Achieved:
* built various data services/integrations from scratch and reached desired reliable performance rate;
* improved various existing solutions with cost, resource consumption optimization in mind. In several cases reached 5 figures worth of optimization effect;
* set up several knowledge sharing team activities, which lead to positive impact on team performance, new tech/approaches adoption rate;
* successfully launched 4 Python- and data- related courses. Reached 75-95% student participation rate and high GPA according to university-standardized measurement tool.
Gained experience:
- built and maintained various data services/integrations using python 3 stack, dagster orchestrator, dbt and postgres/redshift DBs;
- maintained code quality with unittests (pytest), loadtests (via locust), black, ruff and related.
- set up and maintained data quality with great expectations / dbt tests;
- wrote and maintained easy understandable documentation;
- created data acquiring utilities for non-api sources using Scrapy, Selenium;
- created data visualizations using scikit, matplotlib-related, Altair-related, Jupyter Notebooks/Colaboratory tools.
Looking for a Data engineer position in a data-driven company. Preferable to work with dagster+dbt based ETL/ELT stack.

Expect a collaborative / friendly environment with a ""growth mindset in the air"".

Expect possibilities to form / impact on decisions about approaches, tooling, tech stack."
data engineer,"
- Gathered the business requirements from the stakeholders to design the data-sourcing project outcome.

- Integrated data sources below by creating a Databricks Python operator to run on the Databricks cluster, processed the data using Apache Spark, wrote the data into AWS S3 buckets, and automated the tasks using Airflow DAGs.

- Created an operator to push the Braze events into Mixpanel, and saved 30K USD / year by replacing the paid connector provided by Braze with my operator.

- Sourced the Leads, Accounts, and Opportunities data from Salesforce using the Bulk API.

- Sourced the data from Tickets, Users, Organizations, Groups, and Ticket Management endpoints using the Zendesk API via the cursor-based pagination method.

- Sourced the Campaign and Traffic Source data from Hubspot using the Analytics API.

- Sourced the App Performance data of our Apps and Competitor's Apps from the AppAnnie using the data.ai App Performance V2.0 API.

- Sourced the Amenity, Shop, Tourism, Leisure, and Landuse GIS data for 27 cities from OpenStreetMap (OSM) using the Overpass API.

- Sourced the required financial data using the Microsoft Business Central API.
"
data engineer,"
GR8 Tech, Data Engineer
My responsibilities at this position:
- Maintain existing Data pipelines. Configure extracting data from different sources (Kafka topics, another API), filter/transform and load to the database (Postges)
- Build and maintain REST API for CRUD operations with database
- Build new data pipelines. Schedule extracting data from external API and load do database
- Set up monitoring and alerting (Grafana) 
- Write tests for new functionality
"
data engineer,"
Graduated from a paid internship program offered by the German-Ukrainian project DigiJED. Completed a six-month practical training and received a certificate. Currently enhancing my technical skills in analytics and machine learning, as well as working on a project related to machine learning and information security.
Used:
Python programming language,
Tensorflow,
Pandas,
Numpy,
Matplotlib,
scikit-learn.
"
data engineer,"
Green professional service P.L.C
My duty was analyze the dataset for missing values, duplicates, and outliers and deal with them accordingly. Make a new variable based on the multiplication of two columns. Create three Data Marts and establish relationships between them using foreign keys and primary keys. Load selected columns into the data warehouse for future use and visualize the three Data Marts.
The technology I used for the project is PostgreSQL, Jupiter Notebook and power bi.
"
data engineer,"
Had an opportunity to perform a Business Analysis of Boeing work processes in Spares Engineering project, System Analysis of available software and systems on workstations and by using Excel Visual Basic for Applications, as the only accessible development environment, built a team efficiency and quality monitoring system, which collects and shows all data in some Data Analysis reports and charts. It led our team come to 96+% of quality and producibility. The same as at Spares Engineering project after making Business Analysis of workflow stages at Product Development project and System Analysis of available software used by our team, created new metrics system, using the only available Visual Basic for Applications in Microsoft Excel, to rise up our efficiency and time planning. Also as a freelance project has been analyzing credit banks database.
I dream in some days become an experienced Data Scientist. Hate inefficient work processes, and always try to create the improvements on them."
data engineer,"
• Handle the development cycle end-to-end: from learning and understanding the requirements, translating them into code, and deploying to a production environment.
• Collaborate on ETL (Extract, Transform, Load) tasks on Informatica, maintaining data integrity and verifying pipeline stability.
• Extract data directly from relational databases, data warehouses or other data sources using SQL to solve business problems.
• Assess business needs, compile necessary data and develop dashboards/reports on Cognos to provide strategic insights for stakeholders.
"
data engineer,"
- Hands-on experience on Python using Pandas, NumPy, scikit-learn + Plotly & Matplotlib
- Strong SQL skills + ETL operations + Spatial querying ninja (PostGIS, pgRaster, pgRouting)
- ArcPy, PyQGIS, GDAL/OGR
- Building pipelines with Apache Spark & Apache Solr, scheduled and monitored with Apache Airflow
- Google BigQuery + Elasticsearch + ClickHouse + Greenplum
- Google Cloud Functions and Azure Functions Deployment
- Web scraping using Scrapy & automation using Selenium
- Django & Flask Web Development
- Broad experience in ESRI Suite (ArcGIS for Desktop Advanced, ArcGIS Pro, ArcObjects C#.NET)
- Solving problems with scientific methods
"
data engineer,"
- Have attended several projects in different industries like Healthcare, Insurance, Banking, Finance, and so on. 
- Collaborated with experts to draft source-target mapping documents for migration.
- Designed data warehouse process resume_classifier, encompassing source data sourcing, loading, transformation, and extraction.
- Leveraged SSIS for ETL processes, enhancing both existing and new data loading operations.
- Orchestrated and deployed SSIS packages, configuration files, and automated schedules for data generation in CSV.
- Implemented Event Handlers, Error Handling in SSIS packages, and communicated results to stakeholders.
- Demonstrated proficiency in creating parameterized, user-friendly reports using Power BI, inclusive of charts, graphs, and drill-down functionalities.
- Engaged in data migrations from on-premise SQL Server to Azure Data Lake Store using Azure Data Factory (ADF V2).
- Configured Azure components like data pipelines, blob storage, and data lakes. Automated data flows via Azure Data Factory.
"
data engineer,"
Have strong skills in designing scaleble software for different purpose. During my web-experience created strict RESTful APIs using FastAPI, Flask frameworks, Postgre, MongoDB, Redis databases, JWT authorization and other modern tools to create software. Also opened myself as a Data Engineer designing data lakes, data processing and validation modules using pandas and airflow. Always trying to automate testing, staging and deployment pipeline as much as possible as well as to write understandable code for every one who will read it in the future.
Big data processing and analysis with great team, where I can be useful and improve my skills. Also do like to write APIs to access this data from UI (as an example)."
data engineer,"- Successful migration of Looker Explores based on Snowflake DWH to GCP BigQuery
- Team growth from 2 to 6 engineers when leading
- Creation of data processing pipelines and DWH (based on GCP) for multiple customers' plants
- Creation of the PoC product for detecting credit fraudsters that started to bring the company revenue (data science project)
Have worked in both startup and enterprise environments both in local and distributed across the globe teams. My experience is focused mostly on Data Engineering and Clouds, I posses  knowledge and practice in Data Science projects as well though. From technological perspective right now I am working mostly with Python, SQL, dbt and various GCP services (certified) and devops stack (GitHub Actions, Terraform). Led teams on 2 projects.

Projects:

- Snowflake DWH migration. 
   Responsibilities:
   - Spinning up the GCP environment 
   - Consulting the customer's team
   - Migrating data processing from Snowflake to BigQuery, Composer, dbt, Python

- Clinical Data Lake assessment
    Responsibilities:
    - Assessment of the Data Lake implementation state (based on GCP services)
    - Preparation of the corrective actions report based on the assessment results (included data ingestion, working with sensitive data, orchestration)

- Application for mining engineers
   Responsibilities:
    - Leading the data engineering team
    - Elicitation of requirements
    - Data pipelines design and implementation (BigQuery, Dataflow, Composer, Storage)
    - Trainee/junior engineers mentorship

- BigQuery Omni Alpha evaluation
   Responsibilities:
   - Implementing PoC with BigQuery Omni private alpha service
   - Documenting results as an article on C2C global

Those are some of the projects that I was working on. We can go more in depth into my experience during the interview.
"
data engineer,"
Having 10+ years working on data migration projects across the globe. Experienced in data discovery, profiling, quality, cleansing and data modeling.
 - Skilled at data migration ETL, bulk data manipulation.
 - Working knowledge in Telecom - Products and Services domains.
 - Experienced in SQL and SQL performance tuning.
 - 10+ years’ experience in Oracle SQL, PL/SQL.
 - Oracle APEX.
"
data engineer,"
Hello :)

I aim to be involved in projects on automation, Big Data, infrastructure engineer and cloud computing

My skills are described as:


- Big Data: Development and administration on top of Hadoop ecosystem. Ingestion Data Flows, Data Driven Development with Spark;

- Operating Systems: More than fifteen years of experience on Linux and *BSD systems.

- Programming: Python, Scala, bash as well as experience on build tools and SCM (Maven, GNU Make, Git ). Knowledge in collaboration and continuous integration tools ( Bamboo, Jenkins, JIRA, Confluence ).

- Networking and security: TCP/IP stack, firewall and network services configuration.

- Processes: ITIL v3 process management and through Lean, Scrum
and Continuous Delivery

- Infrastructure as Code: Design, coding, and tests of infrastructure components with Chef.

- Cloud Computing: Provisioning and orchestration on OpenStack, AWS and Azure providers

- Virtualization: Research and knowledge on Unikernels, Docker Containers, and KVM.

- Application Servers and Java: Virtual machine tuning, administration, and provisioning of WebLogic application servers.
"
data engineer,"Here are some results for the last 3-5 yerars
1) migration 200+ jobs from ControlM to Stonebranch enterprise scheduler;
2) developed a solution extracting data from pages. I used here a few technologies such as database development, scraping, restapi. Hence, such programming languages as sql, bash script, python were actively used during the implementation;
3) helped to migrate both data and stored functions/procedures from Oracle DB  to PostgreSQL. Besides that I developed a detailed guide how to do data migration in Windows environment,
4) was able to build up a big data environment in Docker Swarm taking into account a fact that I had never worked with Apache Livy, Apache Knox, Docker swarm tools before that moment
Hello there. 
Am an experienced data engineer. Started my career as a DBA/System Administrator in 2007. Here are some notes regarding noticeable activities:
1) DB2 migration 8.2 - 9
2) Informix migration 9.4 - 11.50
3) DB2 replication (from z/OS to AIX)
4) Oracle -> MS SQL data migration
5) building data pipelines in Hadoop with leveraging Hive/bash
6) migration sourcecode of stored procedures from Oracle to PostgreSQL
7) scraping data from a few platforms, and storing all the records in a DB
8) implementation of a delta lake and buliding up a dockerized big data environment (hadoop, spark, knox, livy) from scratch
I:
1) avoid monkeyjob, 
2) don't like working on boring tasks
3) am against of working in a huge office spaces.
My dream is a small development team (up to 10 employees)."
data engineer,"
Hey!
I`m data engineer with almost 4 years experience.
Currently looking for opportunities for senior position
"
data engineer,"
Highly-motivated person, that is always eager to learn and improve
Experience:

Junior Data Engineer:
Data processing, modeling, and maintaining pipelines
Analysis of diverse data sets
Visualization dashboards

Internship at the company:
Main Tasks:
build DWH using open data, learn AWS Services such as S3, CloudFormation, Amazon EMR, RDS, PySpark
"
data engineer,"
Highly skilled Freelancer, with 10+ years experience in Database Development/Administration & Business Intelligence Developer.

Certifications earned
Microsoft Certified Solutions Expert: Data Management and Analytics — Certified 2019
Microsoft Certified Solutions Associate: SQL 2016 Database Development - Certified 2019

Known for non standard thinking with strong development abilities, building and maintaining the solutions based on SQL Server family products, working in stressful situations, i deliver the most of my capabilities in desired result.



As part of my Database Administrator position, i got big experience in:
 Backing up the databases and ensuring that they meets the businesses Recovery Point Objective;
 Configure SQL Server monitoring;
 Install and configure new SQL Servers;
 Expertise of current or upcoming needs for the hardware, that will assure excellent working of SQL Server environment;
 Elaborate documentation for organisation database environment;
 And much more :)

As a Database Developer, i got experience in: 
 Development of solutions using T-SQL scripts, such as: stored procedures, functions, functions, views, triggers, in-line functions and so on;
 Ensure that developed code meets standards for performance, readability and reliability;
 Design of indexes for existing applications;
 Dealing with query performance issues and help developers to resolve it;
 Table design with picking proper datatypes;
 Using traces to find the most the frequently running queries.

Another good stuff that i am good at: 
 SQL Server Integration Services (3+ years)
 SQL Server Reporting Services (5+ years)
 SQL Server Analysis Services (1+ years)
 Microsoft Power BI (2+ years)
 PostgreSQL Development/Administration (2+ years)
 MySQL Development/Administration (2+ years)
 Oracle SQL Developer (1+ years)
 AWS Redshift (1+ years)

 Python development (8+ months)
 C# development (2+ years)
 Powershell (1+ years)

 Windows Server 2012 R2 Administration (2+ years)
 Git (2+ years)
"
data engineer,"Azure and Power BI certifications
Highly-skilled professional with a background in designing, building and maintaining large databases and data warehouses in a team environment, as well as playing an impactful role on the BI & data visualization front. My main data skills are around SQL and Power BI (Microsoft Fabric). Experienced in Python for data cleaning, processing and statistical analysis. Passionate about the combining expertise in Finance, Operations & Procurement with engineering to drive comprehensible business reporting and consistent decision-making.

During my work experience, I have used different tools and technologies to deliver business value, such as R in R Studio, DAX & M language in Power BI, Python in VS Code, T-SQL in SSMS, Spark in Databricks and MS Fabric.
"
data engineer,"I have experience in:
 - Development of realtime data pipelines from scratch
 - Building automated reporting system
 - Improvement and optimizing existing solutions to save costs and speed up processes
 - Development of a successful open source projects
Hi! I am a Data Engineer with strong software development background. I like to build tools that enables data-driven business decisions. I have extensive experience working in distributed teams as well as with foreign colleagues.
I am ready for new offers where I can grow as a professional and I can go up a notch. I give preference to product-oriented teams and new projects."
data engineer,"
Hi,

I have 10+ years experience of in development in all major database systems including Oracle, PostgreSQL, SQL server, Snowflake etc. .
Let's connect and start our journey. 
Hope to hear you soon.



Regards, 
Aram
"
data engineer,"•Databricks Certified Associate Developer for Apache Spark 3.0
•Academy Accreditation - Databricks Lakehouse Fundamentals
Hi, I'm a Data engineer. I thought I would just write my experience here, but if you are interested, I can send you my CV.
	Had experience with Azure, AWS, Python, PySpark,  SQL, Airflow, Jenkins, and Scala.
	I'm interested in working with AWS or Streaming Data. I am not against new technologies. I will gladly master them at 150% learning speed. Trying to always learn and develop myself in the Big Data domain.
	So, if your company appreciates employees who are always eager to learn something and, in addition, learn quickly and immediately use their knowledge in practice. We might have a perfect match :D
"
data engineer,"I tend to be owner of the project regardless of what it takes to deliver. When setting up new data pipeline - it's not just technical steps for me. For me it's talking to all stakeholders to understand requirements, making sure that data is being generated in expected way (especially with manual input and automations), making sure data being collected in the most efficient way from data source, making sure that ELT transforms data per stakeholders needs, making sure visualisation of data clear and user-friendly.

I was mostly working with marketing data, some key results:
- Defined process flow and build data pipeline for automatic channel attribution in Salesforce
- Set up data pipelines for measuring full circle of Inbound Leads processing by Marketing team: incoming, qualified leads, tracking of the time lead was in work and communication activities
- Defined and set up process flow for automatic leads qualification in Salesforce
- Set up process flow and automatic leads attribution by Markets
Hi there! I have 7 years of experience (3 years as BI Analyst + 4 years as Software/Analytics/Data Engineer), currently working as Data Warehouse Engineer in one of the unicorn companies. I love my current job, but have some spare time for interesting projects on a part-time basis. Ideal projects scope right now: help setting up analytics framework from the scratch, consultations, ad-hoc analytics, mentoring analytics team.
Overall I worked with such data stack: AWS (s3, Athena, Glue, Redshift), Snowflake, Python, custom data ingests from various APIs, Airflow, Stitch, _dbt, Matillion, Hightouch.
I can't work full time at the moment."
data engineer,"AWS Certified Developer – Associate
I am a Big Data Engineer with 2.5+ years of experience. I have a master's degree with a data science specialty.
I have experience with a wide technology stack, so it is not a problem for me to teach or investigate something new. I am open to new experiences and changes.
February 2021- until now
Third Party Data Team: works on the delivery of a variety of data sources to Data Lake, then configures Data Lake to process it.
Developing a custom framework in Scala to process (unarchive, change structure, rename, etc.) different types of data into EMR
Developing the Python API to query data from PostgreSQL and Elasticsearch
Developing lambdas and step-function workflows to process (ingest, read, etc.) data through our frameworks
Working with glue jobs: writing custom jobs on Scala to transform, ingest, and populate data
Supporting and extending the Spring Boot application.
Developing custom Bash and bat scripts. 
Developing SQL procedures and schemas
I also have experience with DynamoDB and other AWS services.
October 2020-January 2021
The project was focused on data migration from on-premise Oracle DB to cloud GCP Big Table.
My main responsibilities were:
Ingestion Tables from Oracle DB to GCP
Write complex sql queries for BigQuery
Development and configuration Airflow Dags
Write share knowledge documentation about project
Set Up and configuration clusters on DataProc
Prefer work with Java/Scala, AWS."
data engineer,"
I am a Big Data Engineer with experience in designing and implementing large-scale big data solutions using cutting-edge technologies such as Hadoop Ecosystem tools (such as Hive, Spark, and Pig) and NoSQL databases (such as Cassandra and MongoDB), and programming languages such as Python and Scala. I have utilized k8s, Docker, and Docker-compose to deploy and manage scalable and fault-tolerant big data applications, and integrated them into a continuous integration and delivery (CI/CD) pipeline using tools such as Jenkins and GitLab.

I have also created and maintained efficient data pipelines for seamless data processing and storage using tools such as Apache Nifi and Kafka, and automated the deployment and monitoring of these pipelines using tools such as Ansible and Terraform. I have analyzed vast amounts of data using data processing frameworks (such as Spark and Flink) and machine learning libraries (such as TensorFlow and PyTorch) in Python and Scala to uncover insights and support data-driven decision-making.

In addition, I have experience in SQL scripting, ETL processes, and data analysis and reporting using tools such as Tibco Spotfire, BI Publisher, and SAP Data Services. I have worked with various databases such as Sybase IQ, MS SQL Server, Oracle, and PostgreSQL. I also have advanced proficiency in English and fluency in Azerbaijani, as well as a range of digital skills including programming languages, databases, cloud computing, BI reporting, and data warehousing systems.

With strong communication and teamwork skills, I collaborate well with cross-functional teams and communicate technical information to both technical and non-technical stakeholders. I am always seeking to stay up-to-date with the latest advancements in the field and apply new technologies to improve processes. I provide technical support and troubleshoot issues to ensure the smooth operation of systems, and I am committed to continuously improving my skills and knowledge.
"
data engineer,"
I am a Data engineer at Space International. I am Creating, optimizing and maintaining data sources for analysis using MS SQL, Python, SSIS, SSRS; Developing reporting database; Taking part in creation of data analysis resume_classifier and charts with data scientists and data analysts
using Python and Power BI.
I have 5 years experience in SQL, 2 years experience in python, 2 years experience in working on ETL using SSIS and creating reports in SSRS as well.
"
data engineer,"500+ problems solved on Codesignal
Actively solving problems on Leetcode, Kaggle
I am a data engineer with 8 years of professional experience in ETL processes and analysis. My practical knowledge of the most popular database management systems and languages, including Python and SQL, allowed me to participate in exciting projects where I managed to extend my experience with requirements gathering, and customer communication.
I have a solid background in Database Design and Architecture, and some data science skills. also have good knowledge and working experience with Agile Scrum methodology.
I am a good team player with solid self-motivation and communication skills. 
Passionate about learning new technologies and solving complex problems. Good knowledge of algorithms and data structures, ability complete projects efficiently, find non-trivial solutions and satisfy customers with qualitative results.
"
data engineer,"- AWS Cloud Practitioner certified
- Azure Foundation AZ-900 certified
I am a data engineer with almost two years of commercial experience in data engineering. I have solid data engineering expertise in different domains. I worked on training and commercial projects during my career.

Training project - E-commerce domain
Technologies: Python, Python, Spark, Airflow, AWS (glue, lambda, emr, dynamoDb, Kinesis, Firehose)
Team: 1 lead engineer, one senior engineer, and one junior engineer.
Project responsibilities: Implementation of big data solutions to optimize the work of the high-load service for online banking. 
I have implemented scalable and high-performance pipelines for extracting, transforming and loading data.
Participated in each stage of project development and worked with all these technologies.

E-commerce domain
Technologies: AWS (S3, Glue, EMR, DynamoDB), Python, Spark, Airflow, Snowflake, DBT
Team: 5 Data Engineers, DQA Engineer, BI Engineer
Project responsibilities:
- Building Data Lake
- Migrate tables/columns from on-prem database into cloud
- Creating and orchestrating ETL pipelines
- Data testing
- Improving existed CI/CD pipelines
- Providing Snowflake ingestion
"
data engineer,"
I am a Data Engineer with excellent knowledge in the following areas:
dimensional data modeling, ETL development and Data Warehousing. Proficiency with 
RDBS; NoSQL; Clouds; ERP. Passion to automate everything. Have an experience of 
working on several projects in parallel.
"
data engineer,"
I am a data engineer with In-depth data analytics and engineering skills and experience working on Big data. I am very good with descriptive, predictive, and inferential statistics. I have worked on data projects requiring big data migration using pipelines.
"
data engineer,"- Migrated hundreds of ETL pipelines from Teradata to Snowflake using Airflow and AWS S3
- Maintained legacy ETL pipelines using Teradata and WhereScape for internal direction
- Performed bug fixing and troubleshooting of the migrated ETL pipelines
- Communicated directly with the stakeholders
- Mentored newcomers
I am a Data Engineer with two years of experience in data migration and enterprise data warehouse maintenance. I am looking for a team that develops a high-end data-driven project that matters and where I can constantly implement best practices.
- No russia or belarus
- Remote mode or on-site in Lisbon
- Meaningful project
- Professional growth"
data engineer,"
I am a data engineer with vast experience in building and optimising pipelines. I use tools and technologies such as SQL, Python, Kubernetes, Kubeflow, Docker, Big Query, Amazon Redshift, Postgresql, Numpy, Pandas, Web Scraping and several others.

Built ETL data and machine learning pipelines involving data scraping and collection from a variety of sources for analysis, model training and evaluation.
Built and designed data lakes and data warehouses on GCP Big Query and AWS S3
Set up Apache Airflow for pipeline orchestration and monitoring.
Led a team that implemented a climate change project from data collection to analysis and model training.
"
data engineer,"
I am a data/ETL engineer with 6 years of experience of data engineering in leading Ukrainian and European companies. As a data engineer I have been involved in several gamedev and one telecom project, have been working with requirements, design, integration, testing and automation of data pipelines. I have experience working with Unix OS, BASH scripting. As a data/ETL engineer I am working with Python, orchestration tools - Apache
Airflow, Agile/Scrum, Cloud services – GCP, AWS, containers - Docker.
"
data engineer,"AWS Certified Data Analytics – Specialty
I am an experienced software engineer with a background in automation engineering and back-end development. Over the past year, I have been working as a data engineer on a data platform project. My responsibilities include 
 Develop, extend, and maintain data ingestion pipelines from various data sources.
 Utilize AWS resources such as Glue (with PySpark), Airflow, DynamoDB, S3, Redshift, Athena, and Appflow for data integration.
 Maintain and optimize the data warehouse to ensure its smooth operation.
 Develop infrastructure for dbt resume_classifier to facilitate efficient data
transformation and modeling.
 Deliver data to QuickSight for visualization and reporting purposes.
 Develop and maintain project infrastructure on AWS using Terraform.
 Support other teams by assisting with data access and troubleshooting
infrastructure-related issues.

And the list of technologies I'm working with: 
 AWS Glue
 Airflow
 DynamoDB
 Amazon S3
 Amazon Redshift  Amazon Athena  Amazon Appflow  Dbt
 Amazon QuickSight
 Terraform (for AWS infrastructure provisioning and configuration)
"
data engineer,"My personal quality 
Flexible, Adaptive, Team player, Innovation, Integrity, etc.
Training and certifications:
CCNA - Cisco Certified Network Associate 
CCNP - Cisco Certified Network Professional R&S 
Aviatrix Certified Engineer- Multi-Cloud Network Associate
ICSI/CNSS - Certified Network Security Specialist
PMP- Project Management Professional (PMP) 
Actatek Access Control Security Installer
ASIGRA Asigra Cloud Backup (ACB)
TTC Mobile-Field Support Engineering
TTC Mobile-Networking Engineering.

Below are key projects executed in my Job roles.
Design Implementation of CBN WAN connectivity over VDT network using L3 MPLS VPN 
Implementing CBN core network using cisco IWAN 
Implementing DMVPN, QOS, PFR for CBN
Design and Implementation of Winner Church Ota (Living faith church Ota) link the first high-capacity link for VDT Networks
Design and Implementation JUSTRITE SUPERSTORE for VDT Networks
Design and Implementation JENDOL SUPERSTORES for VDT Networks
Deployed Master and Slave DNS servers
Design and implementation Managed and delivered over 725 projects for VDT Data services 
Proper monitoring of Trunks/Lastmiles/POPs/Base.
Design and Implementation of VDT access/edge network
Design and Implementation of IPV6 on VDT Networks
Service research, design, implementation, and release of VDT SME and APN products
Design Implementation of CBN WAN connectivity over VDT network using L3 MPLS VPN
Design and Implementation of VDT IP/MPLS networks.
Merging of Bitflux IP/MPLS network with VDT 
Design and Implementation of VDT access/edge network 
Service research, design, implementation, and release of VDT SME and APN products
I am a solution-driven professional and a highly ambitious Network Engineer Professional with a track record in networking planning and deployment. building scalable Core and Access networks that support IP services, WAN, LAN, Microwave, configuration, WIFI, Security, Cloud, and RF. Advanced knowledge of intelligent network design concepts and principles to enhance proper network implementation. Ability to discuss design, and develop advanced network addressing and routing for proper network planning, ability to analyze network requirements to meet customer needs.
Efficient, detail-oriented, and proactive with strong communication and analytical abilities.
Specialist in the design, implementation, troubleshooting, monitoring, and maintenance of networks of various sizes.
In-depth understanding of BGP, OSPF, EIGRP, RSTP, MST, VTP, Private VLANs, HSRP, VRRP, IPSec, and so on.
Detail-oriented, proactive, and innovative with a unique combination of analytical skills and proven ability to meet functional deadlines.
I am a solution-driven professional and a highly ambitious Network Engineer Professional with a track record in networking planning and deployment. building scalable Core and Access networks that support IP services, WAN, LAN, Microwave, configuration, WIFI, Security, Cloud, and RF. Advanced knowledge of intelligent network design concepts and principles to enhance proper network implementation. Ability to discuss design, and develop advanced network addressing and routing for proper network planning, ability to analyze network requirements to meet customer needs.
Efficient, detail-oriented, and proactive with strong communication and analytical abilities.
Specialist in the design, implementation, troubleshooting, monitoring, and maintenance of networks of various sizes.
In-depth understanding of BGP, OSPF, EIGRP, RSTP, MST, VTP, Private VLANs, HSRP, VRRP, IPSec, and so on.
Detail-oriented, proactive, and innovative with a unique combination of analytical skills and proven ability to meet functional deadlines."
data engineer,"I completed the English language course at the intermediate level.
I am a student of applied mathematics, I performed laboratory works on SQL, Windows Forms, and also used Matlab even for my bachelor's work. In my graduate work, I described the functions of Matlab on Python.
I have a great desire to work as a Data Engineer, work in team and with team."
data engineer,"
I am a versatile Data Engineer with a wealth of commercial experience in building and supporting large-scale data and backend systems, delivering impactful outcomes. Throughout my career, I have successfully modernized data organization structures, developed CI/CD processes, created cloud-based solutions, and built robust RESTful APIs.

My expertise spans a broad range of competencies and technologies, with a strong focus on:

* Data Engineering: Proficient in utilizing dbt, Redshift, and Fivetran to create efficient data pipelines and structures.
* Backend Development: Skilled in Python, enabling the creation of reliable and scalable backend systems.
* DevOps: Experienced in leveraging Docker, Terraform, and AWS to establish efficient and automated deployment workflows.

Currently, I am working as a Data Engineer in a successful Road Tolling startup. As a vital member of the Product team, I played a key role in building a Data Warehousing solution and creating an Analytics system that empowers informed business decisions. I now support and enhance the existing stack, developing new data-driven applications, adding new partner integrations, and providing valuable data insights to the team.

I have a deep passion for Big Data Engineering and Distributed Systems and a strong interest in Cybersecurity.

I am a great team player, and my passion for learning allows me to quickly align with team requirements and swiftly learn new technologies to deliver the best solutions possible.

I am confident that my skills and experience make me a valuable addition to any development team, capable of contributing to the creation of outstanding solutions.

Best regards.
"
data engineer,"I have completed courses:
SQL for data science in courera
CS50 completly in 2019
Epam basic course for .Net
I am finishing my bachelor's degree as AIS developer.
I am currently creating my first project, using the Python tool. This project includes big data analysis and classification. All this is implemented with the help of machine learning algorithms, such as: decision tree, catbust, logistic regression, knn.
Also, I'm familiar with java/scala, and work with c# and .Net platform on begginer level.
Have knowlege about using git.
I'm looking for a place where I can study as an analyst and engineer working with data. Also, I have no desire to work in gamedev or such as."
data engineer,"Designed scalable data processing solutions capable of accommodating exponential data growth. Collected TBs of data from different sources. In my current projects, I have processed substantial amounts of data, reaching hundreds of gigabytes. This data includes diverse formats, such as videos and images.
I am currently working as a SW & Data Engineer, responsible for creating ETL pipelines to process and collect data (video, image, audio). Overall, my role requires me to have a strong understanding of both software engineering and data analysis, allowing me to design and implement efficient and scalable solutions for our team. In my current projects, I have processed substantial amounts of data, reaching hundreds of gigabytes. 

English: Upper-Intermediate level
Programming language: Python
Software development tools: Jira, Confluence
VCS: Git/GitHub, BitBucket
Web Scraping tools: Selenium WebDriver, Python + BeautifulSoup, proxies
NoSQL Databases: MongoDB, ElasticSearch, DynamoDB
SQL Databases: PostgreSQL, MySQL
Cloud Storage: Amazon S3
Key-value db: redis
Virtualization: Docker, Docker Compose, Kubernetes
Messaging systems: SQS
Message Queuing and Task Processing: celery+redis(rabbitMQ)
Working with clouds: AWS (lambda, SQS, MWAA etc.)
API: Flask framework, FastAPI
Work with the integration of different data tools into your project, and improve the quality of your data. Design and Develop ETL Pipelines: Create efficient and scalable ETL pipelines to process diverse Text, image, video, and audio data, ensuring seamless data flow from source to destination.
Implement Best Practices: Utilize industry best practices to optimize data extraction, transformation, and loading processes, ensuring data quality, accuracy, and reliability.
Incorporate Newest Technologies: Stay up-to-date with the latest advancements in data engineering technologies and tools, integrating cutting-edge solutions to enhance pipeline performance and maintain competitiveness."
data engineer,"
I am data engineer at the fintech company in Baku/Azerbaijan. Tools we are using regularly are Python for ETL's, GCP, particularly BigQuery for ELT, Firestore for the storing unstructured data.
Took big part in migrating and streaming Intersystems Cache's SQL data to the GCP.
Apart above tools, I used PowerBI for the viz, S3 as object storage and got hands on experience with the AWS Glue. 
For the future, I want have more experience in Airflow (Data Orchestration), PySpark for the streaming data and if possible investing time to the learning second programming language like Scala or Rust as. I like OOP programming but it would be cool sometimes to switch to the Functional programming or to the low-level language like Rust.
"
data engineer,"
I am Data Engineer with more than 5 years of experience in the field Business Intelligence. I have huge experience in SQL Database development using mostly T-SQL, PL SQL, MySQL and many others; and ETL work using SSIS and Azure Data Factories mostly, I have worked with Azure Data Lake, Synapse Analytics, Azure SQL;I have been working with different services (AWS) such as S3 for our main data lake, Redshift for the primary data warehouse. Also used Aws Glue for ETL solutions and Athena for analysis and querying the data. Also worked with DynamoDB where data was collected using single table design. Created and maintained different Lambda functions using pyspark. Report writing using different tools like Power BI and SSRS mostly, created paginated reports with using Power BI report builder. Also I have experience with working Airflow, PySpark, Python (Pandas, NumPy, Matplotlib, seaborn, scikit-learn)
"
data engineer,"
I am junior data engineer 

I have experience ONE year
I worked for a big IT company 
We built DWH  and Data Lake.


We used to Snowflake and SQLserver 
My responsibilities - Dev testing new pipeline
"
data engineer,"I participated in lots of different activities (they all are listed in my CV). To be short, I participated (and won) in different hackathons, tried using Reinforcement Learning techniques to train game agents which can complete levels, participated in the google cloud skillboost program, where I worked with different GCP services, and listened to and participated in lots of different events and courses connected with Data Science and especially Machine Learning.
I am working as a Software Engineer on a data modelling application. My experience includes implementing git support for our application, Oracle plugin development, refactoring and adjustment of conflict resolution, and lots of others improvements. My huge project impact was recognized by both the customer and the team leader. 

Now, I am looking for a career change to ML Engineer, as my current position does not fulfil my technology curiosity.

I have an experience with python, especially in creating different APIs, bots, scripts, and notebooks for modelling and data processing purposes. In addition, I have a deep understanding of production machine learning along with cloud environment (AWS, GCP) experience. This all is supported by gained certifications. Also, I participated in the University of Toronto research program, where worked on disease prediction from raw sensor data from the smartwatch, and many others activities related to that field.

I will be happy to give you more details.
You can expect my proactivity and big desire to grow as fast as possible. If you can't guarantee such possibilities, I won't be interested."
data engineer,"
I developed Data Warehouse from scratch. Developed and automated data pipelines using Apache Airflow. 
Now I write comprehensive python scripts, integrate the data by Talend Data Integration.
I want to try Cloud Technologies, because too many companies have started use of Cloud Technologies, but, unfortunately, I never had a chance to work with them.
"
data engineer,"
I develop in the field of Data Engineering. I like to study this area, data, I am ready to learn and try new things.According to the Gallup test, my top 5 are Input, Intellection, Learner. I can study something new day and night, take courses, etc. I do not claim that I know everything, I can quickly adapt and learn what I do not know.
Preparation of data for decision-making systems, client analytics, preparing data for analysts to implement data resume_classifier on recommender systems;
Optimization of the data mart calculation process;Development and support of ETL processes on Oracle Exadata, using an internal ETL tool; SQL query optimization; Working with SAS, developing communication nodes from one database to another or transferring data to web services;
Tools: Oracle, PL/SQL, Python, SAS, Kafka
"
data engineer,"Created several projects for Governmental organizations of Armenia, including the HRMIS system and improvement of Taxpayer Management System modules,
Improved and worked on ETL tool for Praemium International Company (Data Aggregation Platform Software).
Working on development of AI/ML modules for Humanize.security Salience solution
I has held several positions in various industries throughout their career. They worked as a Principal ML Engineer at Humanize.security from June 2021 to the present, where they developed AI/ML resume_classifier and reports for cybersecurity systems using Python and C#. Prior to that, I worked as a Wordpress Developer at Mininginfo.am from February 2021 to April 2021, developing a Wordpress CMS-based Armenia Mining Information Portal using Wordpress and PHP.

I also held positions in PRAEMIUM RA, where they worked as a Developer/Team Lead of DAP project from April 2017 to August 2022. In this role, they developed Import/Export Tool and additional modules for existing software, software maintenance and support for Client, Server, CRM Core and Database modules, led the PRAEMIUM DAP project for Data exchange tool between Data providers and Praemium Wealthcraft CRM, and led the PRAEMIUM SGA project for a SQL Server performance analyze tool using C# and SQL Server.

Before that, I worked as an ICT Expert at USAID Tax Reform Project from July 2013 to December 2016, managing IT projects for State Revenue Committee and developing project plans, work breakdown structure, technical specifications, workflows, and other required documentation. They also developed IT security policy documents and a project website using CMS Joomla/PHP/MySQL.

Additionally, I worked as a Technical Consultant at Future Payments System LLC from October 2009 to June 2013, where they were responsible for the internationalization of bank terminals software and checks, and worked together with banks, ARCA processing center, and Ingenico company to provide compatibility of payment module with ARCA processing center and Armenian banks standards.

Another my experience is  working as a Software Projects Specialist at Yerevan Brandy Company Pernaud Ricard Group from February 2009 to June 2013, where they worked on several IT-related projects including internal IT audit, preparing hardware/software purchase contracts, and participated in the development of Grape Purchase Management System. They also maintained 1S/Accounting software for YBC Accounting and Finance Departments and developed several applications for different YBC departments, including Excise Marks Accounting system, IT Stock management software, and Graper's queue automation system using C# Windows Forms, Visual C++, MS Access, MS SQL Server, ASP.Net, and MS SharePoint Services.
Finally, I worked in The USAID Armenian Tax Improvement Project
Not interested in
- full-time work
- work on holidays, weekends, late or night hours
- employee tracking soft (time trackers, bossware)"
data engineer,"BSc in Computer Science at Ukrainian Catholic University Lviv, Ukraine.
Average rate - 87
Also, for my thesis, I developed a political and social product that showed countries that voted like Russia in the UN with the help of data visualizations
I have 2 years of experience as a Data Engineer at GlobalLogic. During this time, I went from a Trainee to a Middle Data Rngineer.
During this time, I worked on a project where the main goal was to rewrite the old database using new technologies such as Python and AWS cloud. The client himself works with pharmaceutical data.
Responsibilities on the project
-Building data pipelines using Apache Airflow
- Design DWH on Snowflake and Postgres
-Building ETLs using SQL, Spark(PySpark)
-Cover code base with unit tests using Pytest
I wish to continue working with data, creating and designing databases and etc.
Also worked on various projects at the university. These include: creating a module to do polymorphic code, writing my Rick and Morty Universe google in React.js, AI related projects like writing an Atari boxing bot using RL and a fake news analyzer. Development of a system for working with the WIKI API using Django, Kafka, Spark, etc.
"
data engineer,"Django for Everybody Specialization on Coursera; EF SET English Certificate (C2 Proficient)
I have 2 years of experience with Python. I have extensively used it for Data Science: building ML pipelines from scratch, automating analysis.
I would be happy to join your team as a Python Developer or as a Django Developer. I’m looking for a big project with complex architecture and a challenging technology stack."
data engineer,"- Several cloud application integrations.
- Integrations with Salesforce, Salesforce Marketing Cloud, Box, Phlex. 
- Work with Oracle Databases.
- Optimization of ETL flows.
- Knowledge transfer.
- Debugging and maintaining data flows.
- Implement integration tools into the Informatica environment.
I have 2+ years of software development in the game industry and 1+ years as a Data Engineer. I work with Informatica Intelligence Cloud Service (IICS) CAI on an integration project.
I am expecting good teamwork with great data engineers.
Stable career growth and ability to improve and learn new skills."
data engineer,"
I have 4 years of experience in data science and data engineering. I worked in Playrix as Data Engineer and in KazDream Technology as NLP & Data Engineer. In NLP sphere I improved products and build bilingual speech recognition system by using NeMo, Kaldi, Wav2Letter and DeepSpeech. As data engineer my duties included data mining from various web resources, data cleaning, controlling the flow to databases and other ETL tasks

NLP project was developed by our team from the scratch and further it was developed to AI startup - SpeechLab. We controlled all stages of machine learning resume_classifier deployment, starting from data collection and ending model’s performance optimization in order to achieve fast inference and state of the art accuracy in kazak/russian ASR sphere. During the development of this system there were some branched projects such as Speaker Identification/Verification and Gender/Age recognition.

Data Engineering experience was mainly focused on ETL pipelines, analysis of social networks and terabytes of data. For building reliable pipelines with easy debugging I worked with Luigi, CloudWatch and Redshift. Moreover, I used multiprocessing in order to speed up the data collection process and RabbitMQ for synchronizing those processes. For data visualization we used PowerBI that connected to periodically updating tables.
"
data engineer,"
I have a 2-year of job experience as a Data Engineer and Machine Learning Engineer. As a Data Engineer, I have worked with AirFlow, Kibana, and ElasticSearch. I brought these tools to the company where I have been working as Data Engineer. Apart from that I configured AirFlow for 300 independent tasks that diminished the server cost. Elastic Search and Kibana were used for monitoring purposes of tasks where we analyzed logs and used Kibana as a dashboard. Moreover, I am an NLP research assistant in the university where I am doing my master's degree. Throughout as a research assistant I have been involved in research projects where I built resume_classifier for Part Of Speech Tagging, Truecasing, and Punctuation Restoration.  I am doing my master's degree currently in Computer Science and Data Analytics field in Goerge Washington University and ADA University (double degree program)(GPA: 3.56/4).
"
data engineer,"
I have a background in accounting and finance with over 4 years of experience in that field. While I enjoyed my work, I always had a passion for data science, and after taking some courses to learn more about the field, I decided to make the switch.

Now, I am thrilled to be working in the exciting world of data science, where I can apply my analytical skills and love of data to help organizations make data-driven decisions. My experience in accounting and finance has given me a solid foundation in business operations, and my new skills in data science allow me to approach challenges in a new way.
"
data engineer,"Successfully developed a clinic data extraction parser that collects clinic data from various sources, parses their websites, and generates comprehensive profiles. The model effectively utilized Google APIs, PyTorch, NLP tools, MongoDB, and Docker.
I have a broad experience building RESTful APIs using Flask and Django REST Framework, deploying to Gitlab and Docker. I've extensively designed and maintained MySQL and MongoDB databases and have a complete hand in developing ETL pipelines on AWS and personal architecture. I've created Telegram bots, and maintained various Python applications. Recently, I developed a comprehensive data extraction parser using a blend of Google APIs, PyTorch, NLP tools, OpenAI ChatGPT API, Redis, MongoDB, and Docker.
I thrive in roles where I can leverage my expertise in data management, ETL, AI, and NLP."
data engineer,"Fast learner - when joined the last project I knew very little about Microsoft Azure but managed in a short time to learn all the necessary things and develop solid solution.
I have almost 4 years of experience in the Data Engineering field. Solid knowledge in T-SQL and Python. I have worked with such clouds as Amazon AWS (Redshift, Glue, S3, EC2, IAM), Microsoft Azure (Data Factory, Data Lake, Synapse Analytics, Azure SQL, Purview), and Google Cloud Platform (Big Query). Familiar with Snowflake, MS SQL, PostgreSQL and MySQL databases. Created complex ETL pipelines (both batch and streaming) in Microsoft SSIS, Azure Data Factory and Matillion ETL tool, also with Python and PySpark using Databricks and Delta Live Tables. For orchestration, I used Airflow and Rundeck. Created dashboards both from scratch and templates in Power BI and Tableau. For CI/CD used Git and Azure DevOps. I have experience in modeling and creating DWH. Mentored a small group of data engineer trainees.
Opportunity to study new technologies and improve my tech skills, friendly team, flexible schedule"
data engineer,"Architected and built a data lake solution utilizing AWS S3 as a storage layer, and Athena and Python as a powerful query and transformation engine, which enabled you to store any data as it is, eliminating the need for costly and time-consuming data transformations.

Designed and implemented a custom ingestion engine that facilitated a 40% increase in data processing efficiency while enabling seamless data ingestion from various production databases into the data lake. That allowed for more streamlined data processing and captured historical data.

Developed a custom Python package that utilizes AWS services and a public Python package to efficiently serve, clean, and transform data from the data lake, resulting in a 60% reduction in data processing time for various teams.

Implemented data documentation using OpenMetadata and hosted it on AWS Lambda to improve data governance and discoverability. As a result, the number of data-related questions to the data engineering team has decreased by 80%.
I have architected and built a data lake solution using AWS S3 as a storage layer, Athena and Python as a powerful query and transformation engine. I have also designed and implemented a custom ingestion engine, developed a custom Python package, implemented data documentation using OpenMetadata, and deployed a credit scoring API using AWS Lambda and microservices architecture. My current role as a Data Engineer at Kifiya Financial Technologies is to ensure that the data stored in the data lake is fit for purpose and meets the diverse needs of data consumers.

In terms of improvement, I am focusing on developing my skills in Big Data Analytics. Additionally, I am working on improving my communication and collaboration skills, which are essential for effective teamwork and stakeholder engagement.
As a Data Engineer, my ideal expectation is to work on challenging data projects that allow me to apply my technical skills and problem-solving abilities. I hope to collaborate with cross-functional teams to design and implement scalable and reliable data solutions that drive business success. Additionally, I would like to contribute to data governance and quality control measures to ensure the trustworthiness and usability of the data. I aspire to become a subject matter expert in the field of data engineering, stay up-to-date with emerging technologies and industry trends, and provide mentorship and guidance to junior data engineers."
data engineer,"Certified Python Developer (PCAP-31-03)
I have around 4 years of work experience in  Banking, Telecom, Financial Trading, and Investment companies as a Data Engineer/Analyst. 

The main roles:
• Building Data Quality check pipelines and dashboards
• Design and Implementation of data pipelines
• Development and maintenance of database
• Design, development, and maintenance of ETL workflows
• Big data analysis and insight-driven visualization


The main Tech Stacks:
• Python
• Rest API
• Spark
• Tableau
• SQL
• AWS

The main working methodology: 
• Agile (scrum)

Education:
• MSc in Computer Science with Data Analytics from The George Washington University

Languages:
• English (Fluent)
• Russian (Intermediate)
"
data engineer,"
I have been doing data science projects for banking industry.
Projects include client segmentation, transaction analysis, churn prediction, evaluation of bank products (loans, deposits, etc.), report sending and automation tasks
"
data engineer,"
I have been employed at Alfa-Bank (Sense Bank) since December 2020 and continue to work there presently.

Responsibilities in the company:
Development of decision-making systems for banking systems using Spark (pyspark), Kafka, and Hadoop technology.
Responsible for ETL processes (MSSQL, Pyspark, BigQuery) and construction of pipeline stack technologies (Airflow, Google Cloud Dataflow).
Development of dashboards (Django) and other types of data visualization (Libraries Seaborn, Matplotlib, and others...).
Administration of Windows servers with big data and Linux servers.
       Big data optimization. For example, optimizing a table with a size  of     6 billion records with a physical size of more than 1 TB.
"
data engineer,"I have created a scrapy crawler that has successfully collected Amazon Mobile Application data.
Also I have ported complex Azure Data Factory pipeline to spark code, that helped to significantly decrease costs and improved overall processing time.
I have been working accross different projects that include data scraping, ETL, streaming and batch processing within AWS/Azure infrastrcture using python and spark under EMR/Databricks environments.
Competitive Salary, nice team and interesting project to work on."
data engineer,"
I have been working as a data engineer for past year. I have worked with Big data which was in Petabytes of data in total. I have developed KPIs for the customers in telecom industry to show how their network is performing compared to other customers. I have developed pipelines and scheduled them in airflow. I have been working on machine learning projects in innovation days in our company. I have also done my master thesis on the topic of 'predicting next critical alarm in radio networks using NLP'. I have used spark, airflow, python, sql and bash so far in daily work day.
"
data engineer,"
I have been working for 1.5 years as Data Engineer and Data Analyst. Azure tools has been massively by my side and especially Azure Data Factory is a part of my daily routine. I can confidently say that I am fluent in T-SQL. For me Power BI is a key to giant Business Problems and DAX is a piece of magic. Analysis Service is able to response as fast as Bugatti.
"
data engineer,"- Migration whole data systems to Azure cloud. Many years company used old technologies such as Kalido, SSIS jobs and etc. Company top 1 taxpayer in Kazakhstan.

- 3D visualization of oil fileds using Unity3D, Revit and C#. We made a pilot project for company with revenue more than billion$.
I have been working with data processing and analysis for 4 years. Worked in several areas of IT. Found myself in the position of Data Engineer. Approximately 2 years experience in this position. I was engaged in data migration from local database (Kalido) to cloud (Azure). Gathered data from multiple sources and produced reports for clients. Performed data analysis and reports. Worked with Azure Cloud.
"
data engineer,"Finalist in hackathon ""Hucuthon"", master of sports
I have been working with Data Science Platform in "" ELEKS"" for more than 1 year with such tasks as:
- Optimised the platform.
- Wrote unit tests and CI/CD jobs.
- Added logging Tools and technologies used: Python, SQL, Kubernetes, Docker, Azure, ML
I think all new knowledge and experience is exiting. So I'm open for various variants and especially interested in Data Science and Data Engineering."
data engineer,"Successfully designed and implemented a real-time data streaming platform using Kafka, which enabled the efficient processing and storage of large volumes of data. This platform could have helped the organization to obtain real-time insights into business operations and improve decision-making.

Improved the performance and scalability of existing data processing pipelines by optimizing the code and enhancing the infrastructure. This could have resulted in significant cost savings and improved the efficiency of the organization's data processing capabilities.

Developed a custom data visualization tool that helped the business to visualize complex data sets in a more intuitive and accessible way. This could have enabled the business to quickly identify trends and patterns in the data and make more informed decisions.
I have completed various projects and tasks using a range of technologies, including Kafka, Microsoft Azure, Python, MySQL, and Docker. My current role in the team involves creating and managing data pipelines that process large volumes of data to extract meaningful insights for business decision-making.

Some of the notable projects that I have worked on include building a real-time data streaming platform using Kafka and integrating it with Microsoft Azure cloud services to enable efficient data processing and storage. I have also developed data processing pipelines using Python and SQL to extract data from various sources, transform it into a structured format, and load it into data warehouses for analysis.
"
data engineer,"
I have developed backend applications using FastApi and PostgreSQL. Developed large scale web scraping applications. Improved performance of existing ETL pipelines and developed new ones. I have deployed and maintained services on Kubernetes cluster using Helm software. I usually do data modeling and data ingestion.
"
data engineer,"
I have excelled as a data engineer for over a year, leveraging my expertise to build cutting-edge automatic data pipelines and streamline legacy systems through the power of Apache Airflow and Apache Nifi. My role also encompasses efficiently managing data warehouse structures, ensuring seamless integration and optimal performance. Before gaining professional experience, I was a self-motivated developer, dedicatedly acquiring knowledge in ML and Data Science.
In my pursuit of a fulfilling career as a data engineer, I am driven by a profound passion for innovation and cutting-edge technologies such as Big Data and Cloud. My expectation is to be part of an organization that embraces these advancements, enabling me to leverage the latest tools and platforms to push boundaries and drive meaningful impact.Furthermore, I place great value on mentorship and learning opportunities. I believe that continuous growth is crucial in a fast-evolving field like data engineering. Therefore, I am excited about the prospect of being part of a team that fosters a culture of mentorship, where experienced professionals guide and inspire me to reach new heights. I am eager to learn from their expertise and gain insights that will further enrich my skill set."
data engineer,"Implementing qlikview and nprinting , modeling to one the big insurance company ;
Architect crm system, develop and implement
I have experience as database administrator,  and as .net developer.
In ms sql used complex query, all type of function, procedure, trigger, linked object etc. And also worked longtime with BI tools like Qlikview for reporting and nprinting for distributing.
For the last three years, I worked on ASP.NET MVC, with some additions like .NET Core, ASP.NET Web Api and Angular. Also for the time, I've started to take a part xamarin form mobile developing . I am also expert in Blazor. 
Almost all projects, where I took part, included close communication with the client regarding feature analyzing, possible solutions, and difficulties that can occur. I am currently team lead of one insurance company with consist tree part. Analying business wants, developing and testing and then supporting that module.
NET, C#, Entity Framework, ASP.NET, .NET Core, MSSQL, REST API, MS SQL Server, LINQ, Git, OOP, SQL, ASP.NET Web API. Blazor
Not interested in Gaming and mobile development.
Mostly interested in USA company. Have SSN(social security Number)- work permit in USA"
data engineer,"
I have experience:
• deep understanding of business processes in a wide range of industries (Oil and Gas, Pharmaceutical market, Retail, Logistics, Human Resources, Tourism);
• creating data pipelines as base for solving problems with right data;
• outstanding problem solving, analytical and self-management skills, ability to work in a team as well as independently;
• strong understanding of Agile methodologies (Scrum and Kanban).

Python 3:
- Creating ETL pipelines with Prefect workflow management system;
- Pandas, Dask: dataframes manipulating, load data from/to csv, json, sql; pivot and  unpivot tables;
- Matplotlib and Plotly: analyzing data with different types of charts.
- SQLAlchemy (for PostgreSQL), SQLite3: select, update, create tables and views;
- RegEx: applying regex for find patterns and clearing raw data with drugs names.
- Atoti: used multi-dimensional analysis with OLAP cubes and created dashboards.

GIT Bash: creating and maintaining git-repositories for all my ETL projects.

Jupyter Lab (Notebooks): writing draft versions of ETL pipelines. 

DBeaver: DDL, DML, DQL.

CI/CD: Jira, GitHub.

PowerBI: creating dashboards.

Microsoft SSMS 18, T-SQL:
- studied SQL Query, DDL/DML/DQL;
- creating and filling the tables;
- creating views;
- constructing triggers and creating store procedures;
- working with XML, JSON ans CSV data-types;
- got practice with SQL partitions in database.
Introduction to Business Intelligence and Data Warehouse:
- Created Date Dimension table (Python, SQL).

SSIS:
- studied ETL Processing with SSIS;
- created solution with few packages for ETL data from/to database, flat files, Excel file.
Studied basic course about Shell, Spark, Hadoop and Hive intro:
- developed few scripts for simple ETL pipeline;
- practiced using Databricks Platform.
Key facts about me:
• have solid math and computer science background;
• take motivational, performance driven attitude;
• focus on creating state-of-the-art ways to connect businesses with customers, easily and with outstanding results;
• keep up-to-date with the latest trends in computer science;
• constantly learn how to use advanced technologies at their best.

I want to work in the team, where my computer skill, analytical and presentation skills will be useful and that my knowledge and experience in the enterprise projects will bring a great benefit to the company."
data engineer,"
I have experience in fin-tech as Data Engineer in one of the biggest bank systems in the world. There i worked with such tools as Datastage also with Cloudera, Hive, PL/SQL; It was exciting to develop and troubleshoot ETL in IBM Datastage where i got experience about DWH Design. 
Also i worked more than 1,5 years for American product company in healthcare. It was large and complex analytics system for many clients.
There i took part in developing internal data platform, 
ETL processes and CI/CD pipelines for different purposes.
Those tasks allowed me to learn and practice with following technologies:
ETL orchestrators: AWS Glue, Apache Airflow
AWS Services: Athena, S3, EventBridge, Lambda, Secrets Manager
Snowflake
Jenkins
Groovy
Python 
Git
"
data engineer,"
I have got a hands-on experience in using Spark, Spark Streaming, Kafka, Hadoop, Airflow and Hive, ELK stack. I used GCP Dataproc for running services, mentioned above. Also I am experienced in using databases, developing relational database design, working with data warehouses, ETL and BI tools and data analytics. I have worked with GCP BigQuery to aggregate, transform and extract data, and also used Power BI to visualize needed information according to business demand.
I would like to gain more practice in Spark Structured Streaming, working with big amounts of streaming data, ETL/ELT processes, Data Warehouses and Lakes, Cloud Services, Python, and technologies in Data Engineering and Big Data stack. Also, I am keen on working as a part of a team, and seeking opportunities for continuous learning."
data engineer,"
I have more than 3 years of experience in the IT industry. Currently employed as a Data Engineer.

In my day-to-day operation I create and maintain data pipelines. I have the most experience with PostgreSQL and Neo4j databases, but also work with MongoDB and Elasticsearch (not a database, but still).

I am looking for a more demanding and interesting position, because there are no growth opportunities at my current company. I am particularly interested in positions involving streaming data (i.e. Apache Kafka) and/or distributed computing (i.e. Spark).
"
data engineer,"
I have more than 4 years of experience in big coportations on Data Engineering field. Solid knowledge in SQL and Python. I have worked on cloud infrastructures such as Amazon AWS (Redshift, Glue, S3, EC2, IAM, Matillion, Lambda), IBM Cloud (COS, DB2, IBM Infosphere). Working with great engineering teams in my previous workplaces like HBO MAX, IBM - I have experience with data processing and scheduling tools like Apache Spark, Apache Airflow, Kafka, Firehose, Cron Jobs. Familiar with database/storage solution like Snowflake, PostgreSQL, DynamoDb and MySQL. Created complex ETL pipelines (both batch and streaming) in above-mentioned tools, also with Python and PySpark using Databricks. For cloud infra, I used terraform, GitHub CI/CD, Kubernetes, Docker. Created dashboards both from scratch and templates in Power BI and Tableau. I have experience in modelling and creating DWH. Mentored a small group of data engineer trainees. I am also experienced on PII data management including GDPR regulations. Please refer to my resume for further details.
"
data engineer,"SnowPro Core Certification
I have more than 8 years of working with data and data solutions.
Deep understanding of building DWH, ETL/ELT, and data migration in general.
SQL Master, Python Enjoyer
Database - Oracle(Exadata); Postgresql, Snowflake
ETL - Informatica, Pentaho data integration, Matillion
Database Concepts - architecture, objects, statistics, parallel execution
Cloud - AWS 
Business Intelligence - Tableau, SAP BI (Web Intelligence, Universe designer); SAP Dashboards (Information Design Tool, Xcelsius)
Personal information - punctual, responsible, learning on the fly
"
data engineer,"I am interested about data Engineering,passoniate about data
I have one year and half experience in area of Artificial Intelligence and now shaping my skills in area of data Engineering ,Certifiying myself with IBM professional data Engineering,I am good at python,data visualization and data preprocessing,ETL and many more
I read articles related to thing i come across,google for it and try to do all my best ,if it doesn;t work ,I ask senior person in that areas"
data engineer,"
I have over 1.5 year of experience as data engineer. During all that time I participated in long-term project is for building data analytics platform. Lately I've been the main data engineer in the team and was taking on ownership of whole work streams.

Worked on development and maintenance of ETL pipelines to ingest data from various sources (such as structured tables or files) using Airflow, Python and SQL.

Worked on Data Warehouse design, its layers (bronze, silver, gold) and logical structure.

Developed data preparation pipelines for Data Science team and also had an experience of deploying ML resume_classifier.

Strongly involved in interaction with stakeholders (both technical and business ones).

Actively participated in discussions about domain-specific business challenges such as requesting new data sources, accepting and evaluating new metrics and formulas, investigating and offering solutions to achieve set goals and meet business needs.

Partly involved in migration to a new cloud provider and infrastructure upgrading using Spark and Databricks.

Worked mostly with GCP (Google Cloud Platform) cloud but willing to work with another ones as well.

I also constantly improve my skills by learning best practices and new technologies.
"
data engineer,"
I have over 4 years of experience working as a Python Developer / Data Engineer. During my time at a US based startup, I was involved in the development of data pipelines, which included designing, building, and maintaining systems to process and analyze large amounts of data.

I value honesty and transparency in my interactions with others and strive to build relationships based on trust. As such, I have developed strong communication and collaboration skills, which have enabled me to work effectively as part of Scrum teams and to mentor and interview new team members.
"
data engineer,"
I have over 7+ years of IT experience in Analysis, Design, Modeling, Development, Testing and Maintenance of Data Warehouse Business Applications. I have worked in different domains like Finance, Commerce, Legal and Retail. 
I have extensive experience in creating PL/SQL Packages, Procedures, Functions, Cursors, Event Handlers, and Database Triggers. I have excellent SQL skills, written complex queries and correlated sub queries. 
In all my projects, I have created database objects like Tables, Views, Sequences and Synonyms. I am proficient in logical and physical Data Modeling. In my recent project with Georgia Pacific I was involved in both physical and logical Data Modeling, Forward and Reverse Engineering. In all my projects, I was involved in Performance Tuning, Partitioning tables, Optimizing Indexes and Queries. I have worked on both UNIX and Windows environments. I am proficient in migrating data and worked with SQL Loader and DTS packages. I have experience in UNIX Shell scripting.  I have an excellent understanding of Iterative, waterfall and Agile Methodologies and was involved in all stages of Software Development Life cycles during my projects.
"
data engineer,"
I have solid experience as Data Engineer in building real-time data processing pipelines. I am skilled at solving complex tasks and I am interested in new challenges.

My main focus on current project is:
- Development of Java applications for real-time data processing and transformation
- Improvement of data quality and accuracy
- Creation and configuration of the necessary infrastructure
- Maintenance of platform
- Communication with stakeholders

I am experienced in full-cycle development - from discerning the customer's needs to delivering a final product.

Most used technologies and services:
- development: Java 11, Kafka Streams, Kafka, SQL
- integrations: Kafka Connect, custom self-developed applications
- deployment: Docker, Gitlab CI, Kubernetes
- monitoring: Grafana, Sentry
- cloud: AWS S3, AWS Athena, AWS MSK, AWS Secrets Manager
Interesting projects and professional growth"
data engineer,"- Redesigned flawed cross-cloud ETL using SOLID principals and software architecture patterns. Provided business intelligence and data awareness to client by designing informative dashboards.

- Designed and implemented multiple complex ETL systems that included data enrichment from several sources (DB, REST and scripted lookups), data cleaning and validation. I visualized data in Kibana in different ways: histograms, pie charts and time series.

- Researched several papers and implemented CV mathematical resume_classifier for image transformation.

- Participated at each stage of ML solution integration from data collection with scrapers to training and deployment.
- I have solid experience in python development and data pipeline creation. I worked on cloud and software architecture.

- I am also skilled in SQL and worked with some DBs like PostgreSQL and ElasticSearch. 

- I have experience in data visualization. 

- I am comfortable at working with data analysis tools like: dask, pandas, numpy, etc.
- I am looking for data-centric project that will have tasks in: BigData tools like PySpark, data modelling, SQL, NoSQL and DWH."
data engineer,"
I have started my professional work experience as a Data Engineering Intern at a local Bank. During my 6 monthes there I worked on banking data. My responsibilities were to gather this data from different branches of the bank, clean it, restructure it and save it in Spark to make processing easier for Machine Learning Algorithms.
After my intern period, I was hired by a US-based company on a contract to carry out web scraping and automation projects related to financial data. I set up ETL pipelines with AWS lambda and Overwatch to set the process to be carried out automatically. 
My final experience was at Robert Bosch Kft. as an academic researcher. I was tasked with Computer Vision project related to autonomous driving. I used PyTorch to accomplish task.
"
data engineer,"- created a desktop application from scratch with a variety of automation scenarios for non-tech users to reduce everyday manual work from a few hours to 10 minutes.
- built an ETL solution with 10+ various data sources, which runs on an everyday basis. Further BI solution is built on top of that.
I have various experience in the data engineering field:

- Implementation of ETL workflows

- Collecting and parsing data from different sources (DB, .csv and .excel files, API endpoints)

- Data Quality testing and Data preprocessing

- Automation of reporting and data processing routine

- Creating a desktop application from scratch with a variety of automation scenarios for non-tech users to reduce everyday manual work.

Besides tech skills, I have quite good communication skills. So I'm absolutely comfortable if a clarification call with a client is needed. I always try to understand what is the background of a problem I'm solving. It helps to find a variety of possible solutions and choose an optimal one. I like meetings with teammates for some research and thoughts sharing purposes.
Would be nice to work with:
- AWS
- Snowflake
- Building Data platform from early stages (development ETL, building Data lakes/warehouses, etc.) to Data visualization or building ML resume_classifier on top of that in the future.

Definitely don't want to work with:
- gambling
- dating
- trading
- focus on the post-soviet market (any russian companies, partners, or collaboration on work purposes)"
data engineer,"Studied in an international school in Hong Kong(Ontario curriculum)
Studied in the U.S.
Opened a consulting business as a hobby, serving clients with interest in tourism, relocation, e-sports.
I have worked as a Data Engineering Intern at Ignatica, a Hong Kong based insurtech startup. I was responsible for data engineeringing tasks such as ETL in Spark on top of Azure, creating SQL views, writing cronjobs and various bash code to supplement Spark streaming with internal batch jobs. 

Moreover, I was tasked with setting up Hyperledger Sawtooth blockchain infrastructure and achieving node replication on Azure cloud. Automated spin-up with Terraform, visualised with Grafana.

Technologies Used: SQL/Mongo/Hive, PySpark/Scala, Airflow/Databricks, Azure, Terraform, Grafana, Blockchain

During my time at a university in the U.S. I was a part of LIGO Scientific Collaboration as a Quantitative Student Researcher, working on gravitational wave glitches and categorising them to help scientists find unique signals that correspond to important cosmic events, such as black hole mergers. The technology, and job requirements varied during my 2 years as a student researcher, but it was mostly Data Science and Data Engineering

All of my skills were learned independently, or during the job I was assigned to do, which gives me confidence to quickly pick up the skills unique to your project.
As a first, entry-level job, my expectations are to learn on top of what I already know. I am confident that I can get up to speed with what your company is already doing in a short amount of time and swiftly pick-up new tech skills if the need arises."
data engineer,"
I have worked with relational databases and with Big Data technologies on Amazon Web Services. I prepared data for future analysis, developed management decision reports, coordinated tasks with customers, looked for ways to improve the process of calculating indicators.

Professional Experience

Project Description:	The company is one of the biggest players in transforming lives through innovative medical solutions that improve the health of patients around the world.
Role:	Data Engineer
Duration:	Nov 2020 – Apr 2022

Responsibilities:	
- implementing business logic and calculating indicators in AWS Athena using Presto SQL
- creating the ETL processes, data cleaning using the AWS Glue Spark jobs
- moving changes to other environments using AWS Cloud Formation
- developing dashboards and visualizations in Tableau
- conducting Data blending, data preparation using SQL for Tableau consumption, and publishing data sources to Tableau server
- developing and implementing Row-level security in Tableau
- communicating with the customer on a daily basis to clarify priorities and technical requirements.


Project Description:	Development of an application for creating a telecast schedule on TV-channels depending on historical data, views, and prediction of views based on machine learning. Implementation of processes of loading historical and current data on a daily basis.
Role:	DWH engineer
Duration:	Mar 2020 - Oct 2020


Project Description:	Managing and supporting an information and reporting systems of the Bank: preparing the data for future analysis, creating the processes for cleaning data, grouping data due to business rules, the formation of internal reporting.
Role:	Data administration and analysis specialist
Duration:	Sep 2017 - Mar 2020
Looking for an interesting project where I can continue growing as a Data engineer on Cloud services. It would be great to work with data processing and with data visualization as well."
data engineer,"
I have work in the 3shape company since 2018 year on the position Data Engineer / Business Intelligence Enginee.My responsibility  is create ETL pipeline and build  DWH model on the different platforms. I have good communication skills not only in my team and also with other departments. I like read a lot of articles about new technologies in the world. I like sport and my favorite one is football
"
data engineer,"
I know languages: Scala, Java and basic Python.
Have a deep knowledge of Spark(Core/Streaming).  In different projects I was involved in creating a big service for a Big Data project from the scratch, maintain existing ones, I've done a lot of investigation related to technologies in order to choose the best for requirements for a project. Was involved in many discussions regarding a project. My responsibilities were: implementation of program code, creating pipelines, analysis of data, creating documentations, unit-tests, configuration docker, finding and fixing bugs in a code, working with some databases, team supporting.
I would like to be involved in the interesting Big Data projects and participating in the different kind of tasks. :)"
data engineer,"- Creation of data pipelines/infrastructure that powered web and mobile reports across the departments from scratch.
- ETL, report automation in Python/SQL in the cloud from scratch. The clients included a multimillion-user e-commerce B2C web app, B2B wholesale platform, fintech companies, food tech.
- Creation of Inbound Marketing from scratch with a stable income of ~10 B2B leads / month.
I like moving data from one place to another. Use all the cool tools while doing it too.
Lots of experience with batch processing and 3rd party tools. Worked across the clouds, so I'm familiar with most of the popular tools. Learn new things really fast; all I need is a bit of trust in me taking responsibility.
- Looking for streaming/Spark, or at least an opportunity for one of those. Okay with working alone, managing architecture and writing code.
- Being a one-man army, in terms of managing projects and also coding, is not ok =/."
data engineer,"
I'm a Database Developer with more than 4 years of MS SQL 
Server(Azure SQL), have strong experience in T-SQL, solid analytical skills based on Math background. I like being a team member, a good team inspires me
Work with databases
I'm not an Administrator (DBA) or Power BI developer
Coding time more than 50% of work time
Good team
Complex tasks"
data engineer,"
I’m a Data Engineer with 4.5 years of production experience in data engineering and machine learning. I also have experience teaching Python programming and Data Science in a non‐profit academy. My career goal is to build expertise in the whole process of data‐intensive applications development.
"
data engineer,"
I'm a Data Engineer with a total of 6 years of
experience in software development in the banking domain and 
2 years of experience in Data Engineering.
Technical stack: 
- Apache Spark, Pyspark, Apache Nifi, Airflow, Kafka
- Python (Flask), Scala (Akka), Java
- Kubernetes, Docker, Hadoop
- MongoDB, PostgreSQL, BigQuery, Minio

Last workplace responsibilities: 
- As part of a team of 4 data engineers, worked to provide data to 2 data scientists.
- Developed ETL pipelines using Apache Nifi, and Apache Spark, and orchestrated them using Apache Airflow. Thanks to this, data scientists quickly received data for further analytics and model building, reducing the time to provide results to the business
- Automated the deployment of ETL pipelines using GitLab CI/CD, which made it possible to quickly fix errors and deploy new ETL pipelines.
- Built a GPU model training environment for a chatbot development project using Gitlab CI/CD and Docker and then Kubernetes. This allowed data scientists to quickly deploy changes to the chatbot model.
- Integrated the chatbot with internal banking systems, and developed plug-ins for it in Python. Also engaged in debugging various issues, working with colleagues from other departments. Thus, I identified critical errors in other bank systems that affected the work of the chatbot.
I am looking for a Data Engineer or a Backend Engineer position, I want to work with Apache Spark, Python, Scala, or Java, both on-premise and clouds. I don't want to work with reporting."
data engineer,"• Databricks certified data engineer associate;
• Microsoft certified data engineer associate;
• English - C1;
• Exceptionally great feedback from the past projects;
I'm a Microsoft and Databricks certified data engineer passionate about delivering efficient and innovative data solutions. I have hands-on production experience working with Python, SQL, Spark, and Docker, and proficient in utilizing Cloud Platform services, such as Databricks, Google BigQuery, Google Composer, Google Dataproc, Azure DevOps, etc. I enjoy designing and building data pipelines and systems that deliver valuable insights. As a dedicated problem solver, I thrive in collaborative environments and am skilled in working with teams to develop effective data-driven solutions. In terms of my interpersonal skills, I would consider teamwork, discipline, willingness to help, and stress resistance my main strengths.

The current projects domain - Retail & Distribution, implies me following responsibilities:
• Contribute to the project by collaborating with a team using the Scrum framework and Jira dashboard;
• Gather requirements from customers;
• Support the architecture development and configuration-driven generic ELT/ETL framework. Develop Delta extension features;
• Model data sources and layers for efficient storage and retrieval;
• Develop an orchestration system for sources with inner and outer dependencies using Google Composer;
• Cover code with thorough unit and integration tests to ensure high quality;
• Document knowledge on the Confluence.

From the most challenging tasks that I have faced, I can point out 3 of the most interesting:
• Development of release reprocessing dag, that uses graph structures to set complex dependencies between multiple sources;
• Creation of data layer logic in multihop architecture;
• Development of an advanced multisource filtering engine that outputs data to SCD2.

As a data engineer, my desirable project would be one that challenges me to leverage my technical skills and creativity to develop robust and scalable data infrastructure solutions. Ultimately, my ideal project would allow me to work collaboratively with a team of skilled professionals and make a meaningful impact on the organization's data-driven initiatives.
"
data engineer,"Optimization of data processing
Improving the performance of the ETL process
Creating dashboards to track performance
Optimization of SQL queries
I'm an ambitious Data Engineer with experience indeveloping and optimizing data processing processes. Istrive to create effective systems for collecting, storing andanalyzing large amounts of data in order to providebusiness with valuable information for decision making
It is important for me to avoid such jobs where there is no opportunity to use advanced technologies and develop new solutions to optimize data processing. My goal is to develop and apply best practices in the field of data processing and analysis, and I am looking for opportunities where this goal will be achieved."
data engineer,"
I'm an experienced Data Engineer skilled in data architecture, database management, and ETL processes. My expertise includes designing data pipelines, warehouses, and resume_classifier. Over my career, I've proven adept at tackling data challenges and maintaining data integrity.

Currently, I'm pursuing a Data Engineer role to leverage my technical skills, build strong data solutions, and grow in the field. With a reputation for precision, a strong work ethic, and reliability, I'm confident in consistently exceeding expectations.
"
data engineer,"Data Engineer
I’m a team member with experience in building and implementing data platform architectures for the biggest companies in the world. I worked on implementation of ETL pipelines, modeling of data warehouses and used to DevOps techniques closely.
I’d like to work on project where is modern data stack and there is option to build architecture from scratch."
data engineer,"
I`m currently a data engineer with 3+ year experience, as following:
Designed and built Data Lake and Delta Lake on AWS S3. 
Migrated data from MSSQL Server to Amazon S3 using AWS DMS service. 
Designed and created batch and stream processing data pipelines using various AWS services like Lambda, Glue/Glue Streaming, DMS, Kinesis Data Stream, S3, Redshift, SQS and Step Function.

Skills: SQL, Python, AWS, Airflow, Spark, ETL, Kinesis
"
data engineer,"
I'm data analyst with strong engineering background in data. I'd like to growth like data engineer.

On the last position I've been building analytics in company from scratch:
- data infrastructure set up (remote server with PostgreSQL RDBMS; GCP services like Compute Engine VM's, Cloud Storage, BigQuery; Apache AirFlow; GitLab)
- System analysis and data modeling
- Building ETL pipelines with python and orchestrator like Airflow
- writing test and data quality implementing 
- Developing dashboards on Tableau 
- Ad-hoc reports
- Documentation

Also, I took data engineering course from Karpov Courses where I got more theory and hands-on experience in this field, and some of them I applied in company
"
data engineer,"
I'm Data Enginner with 3 years of experience in bulding and maintaining ETL pipelines.
My key resposibilities are:
- Ingesting data sources in different file formats (JSON, Parquet, CSV, Delta) and perform data -- - Inspection and analyzing using Athena, Zeppelin notebook
- Engineering and developing storing data in Data Lake using S3, Delta, Athena.
- Developing and maintaining data pipelines using Spark and Scala. 
- Automating ETL processing on AWS Glue platform with Step Functions, Lambda
"
data engineer,"
I'm experienced Middle Data Engineer in Teamwork Commerce. My job is building and extending data base architecture using:

- Apache Airflow runned on Google Cloud Platform's Composer for building ETL pipelines.

- DBT to keep and deploy all architecture in one place.

- Google Cloud Platform's BigQuery as a warehouse for the data. 

- Looker to manage and visualize the data.
"
data engineer,"
Implementation of ETL processes and DWH from scratch in a big logistics company.
Core bank system implementation and migration in a few large banks in Ukraine and Azerbaijan as a technical business analyst.
Development and implementation of a new B2C application for an insurance company in OAE as a system analyst.
"
data engineer,"the analyst team of my company faced the problem of verification and analysis of large Text files data. I implemented a solution that saved $61,000 in annual costs in analyst team work-hours
in general, my work as ETL Developer consists in moving data from the source to the destination, which can be C3, FTP, SFTP, DB. During this process, the input data is validated and, if necessary, transformed.
I am comfortable working on monotonous jobs, on multi-tasking and diverse tasks, individually or in close cooperation with other team members. However, I would not like to be a pioneer in finding and implementing a completely new development environment."
data engineer,"
In my previous role as a Data Engineer, I was responsible for designing and implementing data pipelines and data warehousing solutions. I have experience working with a variety of data technologies, including SQL, Kafka, and Spark. Additionally, I am skilled in programming languages such as Python and Java, and have experience using cloud-based data platforms such as AWS, some GCP. I am passionate about using data to drive insights and improve business performance. I enjoy working in a fast-paced environment, and am comfortable managing large datasets and complex data structures.
"
data engineer,"Optimizing ETL Processes: I led an initiative at Pasha Insurance to optimize ETL processes, resulting in a 30% reduction in data processing time. By fine-tuning SQL queries and introducing parallel processing techniques, I significantly enhanced the efficiency of data ingestion and transformation.

Data Pipeline Scalability: At Agile Solutions, I played a pivotal role in designing a scalable data pipeline architecture for a high-velocity data stream. This architecture allowed us to handle a 50% increase in data volume while maintaining performance, ensuring uninterrupted data flow for critical business operations.

Cross-Functional Collaboration: As part of the IFRS 17 project, I collaborated closely with actuaries and financial analysts to ensure the accurate representation of insurance contracts in our data systems. This collaboration resulted in compliance with regulatory standards and provided valuable insights for financial reporting.

Technical Mentorship: I've actively mentored junior team members, sharing best practices in data engineering and ETL development. Several team members have progressed in their careers and contributed effectively to our projects, a testament to the impact of mentorship.
In my role as an Data Engineer at Pasha Insurance, I have gained extensive experience in data management and ETL (Extract, Transform, Load) processes. My responsibilities encompass:

Writing complex T-SQL queries, including recursive CTEs, temp tables, dynamic queries, subqueries, and intricate joins for creating Stored Procedures, User-defined Functions, Views, and Cursors.
Implementing robust error and event handling mechanisms, such as precedence constraints, breakpoints, checkpoints, and logging.
Importing/exporting data from various sources (Python files, flat files, Excel) using SSIS/DTS utility.
Utilizing C# within the SSIS script component.
Developing dynamic SSIS packages.
Defining data warehouse structures, including star and snowflake schemas, fact tables, cubes, and dimensions.
Resolving technical and data-related challenges in the IFRS 17 project.
I am an integral part of the team, contributing to the technical aspects of the project and collaborating with colleagues to ensure its success. I continually strive to enhance my skills and aim to improve my expertise in ETL processes, data warehousing, and database management.
Seeking a challenging data engineering role with a focus on complex data transformations, ETL development, and database design, including cloud-based solutions. I value opportunities that allow leadership in designing data warehouses and thrive in dynamic, learning-oriented environments. Prefer roles that minimize administrative tasks to focus on technical contributions."
data engineer,"
Inter Cars / Link Group, Data Engineer, present - 2022-07 (~1 year)

Technology: Scala, Spark, Kafka, Hive, Airflow, Kubernetes, GitLab, Podman

Data Engineer – B2B Contractor.
• Building an event sourcing platform which is the main information hub in the company (on-prem).
• Supporting new team members in the on-boarding process and creating new documentation.
• Basic usage of Kubernetes: Pod, Job, CronJob, Deploymnet, ConfigMap and Secret.
• Setting up new Team and Project.
• Setting up GitLab CI/CD pipelines, PagerDuty, Service Accounts for the Team.
• Rewriting a legacy Python project to Scala Spark.
• Creating Containerfiles and using Podman to improve developer experience in the Team.
• Presenting a talk on Improving Developer Experience at an internal Big Data Community event.
• Working with Business Stakeholders on new features and business requirements.

AXA, Data Engineer, 2022-05 - 2021-09 (~9 months)

Technology: Scala, Spark, Azure, Databricks.

Data Engineer - B2B Contractor.
• Migrating on-prem big data platform and databases using Spark and Scala.
• Working on skill matrix for the team and proposing internal training topics.
• Providing training to the team to fill the skill gaps. 
• Working with business stakeholders to tailor the solution to their needs.

JLL, Software Engineer, 2021-08 - 2019-11 (~2 years)

Technology: Scala, Azure, Docker, Python.

Software Engineer - B2B Contractor.
• Development of web app for JLL LightHouse Data Quality.
• Preparing JLL LightHouse for internal architecture review including SDLC.
• Setup and management of Azure DevOps for the project.
• Documenting architecture decisions.

Volunteering for JLL Building Pride Network (Part of Diversity & Inclusion)

EPAM / UBS, Scala Engineer, 2019-11 - 2019-02 (~10 months)

Technology: Scala.

Scala Engineer - Contractor for UBS.
• Development of NGA (Next Generation Archiving) which provides global capturing, document processing, archiving and retrieval solution to all business divisions.
• Working on setting up monitoring and logging for NGA project using Splunk, Netcool and Miro+
• Backup for Scrum Master.

Volunteering as Lead of UBS Wroclaw Pride Network (Part of Diversity & Inclusion)
"
data engineer,"Took part in development of antifraud system for detecting gsm terminators. Had important role in integration with government services. Rewrited balance and payment parts of payment aggregator to a different technology for better performance under high load.
I participated in development of electronic document management using cuba framework. Took part in development of anti fraud system, CRM, payment system and other internal projects. Languages and technologies used: Java 8, typescript, golang, java ee, JSF, spring boot, spring mvc, graphql, thymeleaf, rxjava and angular framework.
Interesting project with room for growth. I've been working as a Big Data Developer writing ETLs for last 9 months, but I'd like to get back to the web development using mainstream frameworks and technologies."
data engineer,"
I switched to Data Engineering from .NET.
I have experience developing Data Warehouse on Azure SQL database and different ETL processes with Data Vault pattern. Experienced in Azure Services like Functions, Logic Apps and especially Data Factory. 
Was involved in different kind of Data Integrations tasks and sometimes in data analysis dealing with Power BI and report generating
Looking for a place where I can develop and sharpen my skills on real interesting projects"
data engineer,"
I usually work on creating and maintaining ETL pipelines. I work with Python and use AWS as a cloud plus other supportive technologies like docker, sql, spark etc. Currently, I learn scala as well to be more flexible and improve my AWS knowledge.
"
data engineer,"
I've been actively studying data engineering/analysis for more than six months, completing various hands-on projects, and will do more.

I've gained theoretical and practical experience in DevOps, DataOps, Analysis, ETL, Machine Learning, Computing, Networks, Visualization, and Architecture on the Google platform.
Certified Google Cloud Professional Cloud Architect and Data Engineer => which helped to structure my knowledge.   

2 years in an American company. I was responsible for vendor and customer communication, as well as the integration of Oracle Netsuite API.
"
data engineer,"
I've been engaged in ETL projects holding responsibilities of extracting and delivering data from various sources for further processing using Spark, building automated data pipelines with Airflow, and analyzing data using Kibana;

Implemented monitoring solution using Elasticsearch and Kibana;

Wrote/Read data using Kafka;

Implemented DAG to run pipeline on AirFlow;

Converted data to different formats with Spark;

Implemented docker-compose file to create environment on Docker;

Stored and managed data in HDFS.
Can be discussed"
data engineer,"In both my academic and extracurricular endeavors, I have made significant progress.

One of my greatest achievements was winning several hackathons where I used my data science and machine learning expertise to solve actual issues. In addition to polishing my technical abilities, these experiences helped me develop vital teamwork, communication, and problem-solving skills that are essential in the tech industry.

I have taken various trainings and courses in AI and data science, including hands-on projects that allowed me to obtain particular expertise in these subjects, in addition to hackathons. These encounters have given me a thorough understanding of the algorithms and tools that underlie AI and data science, as well as how to apply them to tackle challenging issues.
I've completed a number of projects involving data modeling and analysis. As an example, I created a face verification application with computer vision techniques and a home prediction model that required web data to be scraped and cleaned. I've also won numerous data science and machine learning competitions at regional universities.

As the president of the Data Science club at my university, my current role in the team involves organizing events and workshops related to data science and machine learning. I also mentor team members on data science projects and facilitate discussions on current developments and trends in the industry.
I have a lot of experience with Python, as well as frameworks and libraries like TensorFlow, PyTorch, Scikit-learn, and Pandas, among other technologies. For machine learning projects, I have also employed cloud infrastructure like Google Cloud Platform and Amazon Web Services.
Moving forward, I also have good comprehension of Tableau, a data visualization tool, and  MS SQL for querying the data for the projects. Also, I want to keep learning about the most recent developments in deep learning and machine learning and explore new tools and technologies that can improve my data science skills.
"
data engineer,"
I've worked on a AI Semantic Search project using Pinecone, openAI API, and python.
Also worked with a team to build a categorization web app using AWS resources lie AWS Cognito, Lambda, S3, Code build, cloud front.
Presently developing my data engineering skill learning tools like spark, kubernetes terraform in other to be able to develop a robust and orchestrate scalable, reliable and agile systems and products.
"
data engineer,"
I was working as data engineer in product company that scrape and analyze data all across the web. My main responsibilities are:
•	Create and maintain various production microservices with Python, asyncio, multiprocessing, grpc, FastApi.
•	Update and maintain different internal libraries and tools.
•	Provide solutions for parsing data from various web sources.
•	Create ETL pipelines for data processing using Python and Airflow.
•	Configure monitoring and alerting systems using Prometheus and Grafana.
•	Write and optimize some Java code. 
•	Integrate DS algorithms into production environment.
•	Work closely with DS team to create optimal database structure and ETL pipelines.

I am looking for work in product company with data oriented team. I want to grow as engineer, get new experience, work with highly motivated and dedicated colleagues and provide SOTA sollution for real world challenges. I would be really enthusiastic to work on products that produce significant social value.
"
data engineer,"I have been learning Python and Data Science in 2022 at the ITEA courses.
I worked at the Government Service untill 2021 on the position ""Head of a structural unit"". There, I acquired soft skills such as: personnel and resource management, coordination of people's work to achieve results, organization and control over the execution of tasks, document circulation management, crysis managment, analysis.
I am looking for a job. I want to improve my knowledge in Python and develop in the field of Data Science."
data engineer,"Took part in successful re-designin and re-implementing cloud-based Datalake into alternative service stack: AWS EMR/Kinesis + Redshift DWH into AWS Glue/Lambda/StepFunction + Snowflake DWH
I worked on:
- Implementing ETL pipelines, using Spark and AWS stack

- Optimizing Spark jobs for lower memory, cpu and time footprints 

- Migrating data-processing stack from on-premise to completely cloud-based(Java + Sybase into Scala + AWS)

- Writing general use libraries for specific data transformation/data migration purposes that were used among multiple teams inside company
"
data engineer,"
I work in FinTech project as a Middle Big Data Engineer at one of the biggest IT companies in Kazakhstan. My journey started in May 2021 when i graduated from Big Data School by Alfa Bank Kazakhstan. I use Airflow, Git, Python, SQL, Hadoop, Spark, Mesos everyday.
"
data engineer,"
I work on designing, building, and maintaining the systems that manage data within an organization. This includes creating and optimizing databases, developing ETL processes to extract, transform, and load data, and integrating data from various sources.

In order to accomplish this, I use my knowledge of programming languages such as SQL and Python, as well as experience with database management systems like PostgreSQL, Snowflake or BigQuery. Additionally, I am familiar with cloud-based technologies like GCP, which I use to set up and maintain scalable, fault-tolerant systems.
"
data engineer,"I am certified AWS Certified Solution Architect Assotiate and Data Vault modeller .

I have 3+ experience with AWS and 2+ experience with Data Vault and Dimentional Modelling
Jan-2017 - Jun-2017 - Student, EPAM Systems, 
Customer: EPAM
Project: BI Lab 2017
Team Size: Students: 8 members
Project Role: Student

Jul-2017 - Mar-2020 - Software Engineer, EPAM Systems, 
Customer: EV - Telecommunication
Project: This project covers all NDC related work for Telefonica O2 (Germany).
Team Size: Dev Team: 30 members
Project Role: DWH Developer, Oracle package development for DWH

Mar-2020 - Mar-2021- Software Engineer, Godel Technologies 
Customer: Insurance
Project: UK Insurance company Compare The Market (CTM)
Team Size: Dev Team: 5members
Project Role: DWH Developer, Redshift, AWS, DWH development

Mar-2021 - Now - Senior Software Engineer, Soname Solutions
Customer: Insurance
Project: Allianz Deutchland
Team Size: Dev Team: 10 members
Project Role: Data Modeller, Data Vault 2.0, Business Vault, Erwin, Erwin Fet Templates, Synapse Azure, Python, Kubernetes
I would love to continue working in AWS Cloud with Data Vault modelling pattern"
data engineer,"
Java - 8 years
Spring - 6 years
Hadoop - 5 years
Kafka - 3 years

Worked with Microservices and container-oriented technologies on bare hardware and cloud(Docker, Kubernetes, EKS)

Was working on data-driven projects including BigData technologies and streaming.  

Was team leader on two projects(from start to support phase).
Have experience with MapReduce and Spark.

Last two years working with micro-service architecture from scratch based on Spring Boot/REST/Kafka and Kubernetes
Looking for BigData project with competitive tasks in Lviv. 
Would like to get leading role(Team/Tech leader) to develop better soft and technical skills.

Would love to get my hands on big amounts of data and writing analytics system to solve business problems."
data engineer,"Honorary Decree for the highest GPA score from Azerbaijan Technical University (97/100)
Google Data Analytics Professional Certificate
IBM Data Science Professional Certificate
IBM Data Analyst Professional Certificate
Data Analysis and Visualization Foundations Specialization
Data Engineering Foundations Specialization
Hands-on Introduction to Linux Commands and Shell Scripting
Introduction to Containers w/ Docker, Kubernetes & OpenShift
Fundamentals of Kubernetes Deployment (LearnQuest - Coursera)
HackerRank SQL (Gold Badge)
HackerRank Python (Gold Badge)
Job-related skills	Tools and Components of Data Architecture.
Create an interactive reports via Excel (Pivot Table), Tableau and Apache Superset
Data extraction from different (excel, RDB, flat files, Google sheets) sources, transformation, preparation
Manipulating, cleansing & processing data using python, Excel, SSIS and SQL Analyse large datasets to improve process flow and quality of data on it
In-Depth Knowledge of SQL and Other Database Solutions. ETL or ELT processing via SSIS.
Data Warehousing and ETL Tools.
Data Analysis via Tableau and Python Reporting (SQL, Tableau)
New experiences, working with cutting-edge data technologies and satisfied Salary."
data engineer,"IT Team Management program
Knowledge of Software Testing Life Cycle;

Design and development of technical documentation (test cases, checklists);

Processing tasks in Jira;

Analysis of the internal clients’ requirements to the desire software tool;

Adapting and conducting the unit testing through creating SQL queries;

Development of the validation rules and requirements to the incoming data.
."
data engineer,"
Laba Group -  Data Engineer 
Lviv| December 2022 - March 2023| Remote

As a Data Engineer, I leveraged my expertise in Python to develop various scripts that interacted with external APIs. These scripts automated several routine tasks, such as data extraction, data transformation, and data loading. As a result, it significantly improved the overall data processing speed, accuracy, and efficiency.

I also utilized my proficiency in SQL to create reports for Sales and Marketing departments. I leveraged data from various sources to generate insightful reports that helped the business make data-driven decisions. These reports helped identify trends, patterns, and opportunities to optimize sales and marketing strategies.

I worked with GCP, BigQuery, and Google Spreadsheets extensively. I was responsible for managing data workflows, ensuring data quality and consistency, and automating data pipelines.  

Global CRM Solution - Strong Junior Data Engineer
Lviv| May 2022 - October 2022| Remote

As a Strong Junior Data Engineer, I utilized my expertise in Python, SQL, and Pandas to collaborate with a team of data engineers. Together, we created a cloud-first data ingestion system that significantly improved the processing speed of data. 

I also utilized Airflow to develop ETL solutions that effectively enhanced conversion rates. Furthermore, I contributed to the improvement of the DWH structure, and ingested data from different sources using SQL to build data views for BI tools like Tableau. 

Additionally, I communicated with investors to better understand their needs and translated their feedback into actionable dashboards in Metabase.

Freelance - Data Engineer
Lviv| April 2021 - April 2022| Remote

As a Data Engineer on Freelance for about a year, I gained valuable experience in various aspects of data engineering. I was responsible for parsing data from multiple web services using Python and SQL and then storing it in the database. Additionally, I worked on designing and testing databases to identify and address any issues or inefficiencies.

I was also responsible for locating, extracting, manipulating, and organizing data from various operational sources to support the development of analytic tools. Communication with clients was a crucial part of my role, and I regularly interacted with them to understand their needs. Based on their feedback, I developed actionable reports and dashboards using Tableau, which helped clients make informed business decisions.
I am looking forward to working closely with cross-functional teams, including data analysts, data scientists, and software engineers, to design and develop solutions that meet business requirements.

I also expect that I may need to collaborate with other stakeholders in the organization, such as project managers, business analysts, and data governance teams, to ensure that data engineering solutions are aligned with business objectives.

Overall, I am enthusiastic about the challenging and rewarding projects that come with this role. I believe that my strong technical skillset and my ability to collaborate with cross-functional teams will enable me to thrive in this role as a Data Engineer."
data engineer,"
Last job description
My job was to develop and maintain the whole data engineering part of the analytics department.
The data lake model was used where data came in to Redshift dwh or S3 (+Spectrum) from backend sources like MySQL and Mongodb replicas and RabbitMQ. It then got transformed with Lambda jobs and Airflow tasks. The final set of data was used in Tableau reports, in alerting, web API Dreamfactory.
The infrastructure heavily relied on AWS.

Technologies used:
- Python, some JS, some Java, some Scala
- Airflow for scheduling tasks. Written many custom plugin Operators
- Redshift + Spectrum. Querying and maintenance
- RabbitMQ, binlog MySQL and oplog Mongo streaming. NiFi
- Timescale + Grafana. Influx for small legacy use case
- AWS. EC2, Kinesis, Lambda, S3, Glue, LEX, Cloudformation
I want to get a challenging Data Engineering position solving business needs with cutting-edge on-premises and cloud technologies."
data engineer,"
Machine learning algorithms design and implementation.
Data engineering"
data engineer,"12 years Oracle database;
 7 years Data Warehouse and Business Intelligence;
 1-year  Big Data.
 participated in 10 projects (insurance, telecommunication, banks, goverment)
Machine learning, Data mining, Data Science,
Hadoop, Hive, Flume, Spark, Pig, noSQL, 
SQL, PL/SQL,
Java, Python, R, 
OOP, Database, Data Warehouse, Business Intelligence
Participate in Big Data, Machine Learning or Data Science projects.
Relocation opportunities.
Remote job"
data engineer,"Administrative professional with technical background who backfilled the VP of EPAM. Delivered services to the one of the biggest hedge fond of the world (financial/trading domain).
Macroeconomic data analysis
extract macroeconomic data (gdp, cpi, ppi, flow of funds) releases from provider web site
convert to custom format using python scripts
check for mistakes, anomalies in data on test environment, launch appropriate reports,
load data to platform 

Market (financial) data analysis:

analyze structure, types of data,
ETL/ELT practice,
relations, define metadata/time series fields and measures of market datasets (export/import commodity data, ownership, equity, financial statements),
used Python libraries to extract and transform data: selenium, pandas, numpy(depends on source of data),
discuss with client results of analysis and provide it to business analysts.
"
data engineer,"
Main responsibilities:
- build new data pipelines and support existing ETL pipelines
- optimize existing bottlenecks (query performance etc.)
- data validation
- interact with internal data consumers to understand data needs
"
data engineer,"
Mar-2021 - Till now Tasks performed: • Developed ETL batch Spark job, covered with unit tests • Gained experience with AWS services(Glue, SNS, CloudWatch, Cloudformation, SecretsManager, IAM, S3). Environment: • AWS services, Github, Maven, Git, Jenkins, Jira • Spark, Scala, Java, AWS Services Oct-2020 - Mar-2021 Project: Aim of the project is to gain experience in production like tasks that included simulation of ETL process that included implementing and orchestrating multiple Spark jobs. Multiple batch and streaming jobs were implemented to clean, transform and load raw data into datawarehouse. As datawarehouse solution Hive was used. Hortonwork Data Platform was used as run all needed software. Gained experience with AWS services(EC2, EMR, S3) Tasks performed: • Developed ETL batch and streaming Spark job, covered with unit tests • Working with Hive managed and external tables • Implemneted solution for tracking already processed files • Gained experience working with Kafka API and managing offsets • Gained experience with AWS services(EMR, EC2, IAM, S3). Environment: • Hive, MongoDB • Intellij IDEA, Virtualbox, HDP, Data analytics studio, Maven, • Apache Hive, Apache Hadoop, Kafka, Spark, Spark Streaming, Java 8, MongoDB May-2020 - Oct-2020 Project: Project to track RD-related activities and education costs in UA Office, to separate it from other GDOs and mid-term to replace huge and old EPM-FARM. Large amount of tables for store information about test, users, topics, groups of testers, liquibase information, test sessions for testers, permissions for test manager and so on. PostgeSQL is using for RD Portal, it contains information about students, locations, time slots of testing and so on Tasks performed: • I worked with DB: searching, retrieving, sorting, updating and etc. Wrote REST end-points, covered code with JUnit-test, bugfixing, change/add configurstions, work with Email- notifications, create/improve/change scheduled tasks, refactor code. • In case of difficulties with the work process, I contacted the team lead or project manager. • Worked in SCRUM team, completed tasks according to SCRUM sprint. Completed tasks was checked by Team Lead. Environment: • MySQL, PostgeSQL. MySQL is using for Test Portal. • Intellij IDEA, MYSQL Workbench, Git, JIRA
Looking for an interesting project with progressive technologies, infrastructure and professional team"
data engineer,"
• May 2021 - present. Data engineer. NDA.
Building/enhancement/development/maintaining of ETL pipelines, new data sources integration into existing DWH. Interacting with data analytics team for requirements gathering and defining. Initial data analysis using Spark/Python/SQL. Implementing automated data quality control. Main technologies: Spark/PySpark, AWS, Apache Airflow.

• February 2020 - April 2021. Data engineer. NDA. Building ETL pipelines, integrating new data sources(API/scraping), data validation and quality check automatization. Architecture planning, requirements defining. Legacy code refactoring. Main technologies: Spark, Airflow, Databricks, Python.

 • December 2018 - February 2020.
Python developer. NDA. Development of analytical system(data crawling, testing, servers managing). Integration of new data sources. Requirements defining, mentoring and interviewing. Implementation of distributed crawling system. Main technologies: Kafka, MySQL, Python3, Selenium.

 • July 2018 – December 2018. Python developer. NDA.
Development of procurement’s system backend(API endpoints, data validation, autotests, unit tests) using Python, Redis, pyramid, Coach DB.

 • September 2017 – July 2018. Python developer. NDA. Game stimulator development  (multitenant functional, registration procedure, permissions system, notifications system) using Python 3 and Django/DRF. Product deployment on client’s servers.
"
data engineer,"DWH Development 
Master Data Management Development
Middle Data Engineer more than 4 years of experience in Data Engineering, Database and Data Warehouse development and Master Data Management.

My experience briefly:
Experience with DWH development/ETL pipelines ,BI architecture and solid experience in the designing and implementing of database-based solutions.
Working with Database architecture and development specifically via PL/SQL.
Hands on experience in data integration.
Basic skills on database administration (Oracle, MS SQL).
Performance tuning  or SQL query optimization.
Solid experience in Master Data Management - Master Data Services.
Business Intelligence Architecture and Administration.
Report generation and analysis using various Business Intelligence tools, using them to visualize data to get much more valuable insights from data.
"
data engineer,"
Middle Data Engineer with 2.5 years of experience. Graduated with, Bachelor’s degree, in Computer Science, Artificial Intelligence Systems.  I have a good knowledge of T-SQL, PostgreSQL, Redshift, and MySQL. Also experience with Amazon Web Service(like S3, EC2, IAM, Glue, Lambda), Python, and PySpark. Used to create the CI/CD pipelines via Jenkins, create ETL jobs using Stored Procedures in SQL, SSIS packages, and via Rundeck. Have created a lot of DB objects for reporting services like PowerBI and Tableau. Worked using SCRUM and Kanban methodologies. Ready to study something new and improve my tech skills.
"
data engineer,"
Migration Project for Life Sciences & Healthcare company from NA (role - Data Modeller):
_______________________________________________________________________________________________
• Performed bulk code refactoring and compiled dashboard source views when migrating from Teradata to Postgres
• Wrote Python data quality tests for checking data correctness returned by new versions of source views
• Analyzed data discrepancies and documentated compatibility issues between Teradata and Postgres
• Conducting POC analysis aiming at evaluaing options for deploying DWH read replica of main instance (Redshift, Aurora DB, RDS Postgres)

Environment:
• Postgres (RDS), Teradata
• PyCharm, DBeaver, Teradata Express Studio
• Postgres plpgsql, Teradata SQL, Git, Python

Enterprise Development for Payment Processing company from UAE (role - DB Developer):
_______________________________________________________________________________________________
• prepared SQL scripts for reports in accordance with incoming requirements
• developed new functionality in way4 manager (menu items related to extracts, scheduler jobs)
• fixed prod defects connected with performance problems and report data discrepancies
• documented reports logic and structure in Confluence
Environment:
• Oracle 11g
• way4 Manager, SQL Developer, Git Bash, Bamboo Automation, Bitbucket
• Oracle SQL/PLSql, Git,WAY4
Migration Project for Life Sciences & Healthcare company from NA (role - Developer):
___________________________________________________________________________________________
• Preparing Data Migration scripts for the upcoming platform release.
Environment:
• PostgreSQL 9.6.5
• DBeaver, Docker, Git Bash, pgAdmin
"
data engineer,"
More than 10 years of software development, working experience with Financial, Travel, and Industry domains, familiar with Waterfall and Agile development resume_classifier, strong analytical and problem-solving skills. Have experience of database and data engineering concepts - from SQL, NoSQL to Hadoop and Cloud
Key technologies:
•	Teradata, MySQL, Oracle, Amazon Redshift
•	Big Data technology: Hadoop-Hive, Sqoop, Apache Spark (pyspark, SQL), Amazon S3, Qubole
•	Informatica
•	Bash, Python
•	Perforce, Stash, GitHub, Jenkins
"
data engineer,"
More than 6 years of experience in providing reliable and meaningful data solutions for companies from different sectors, including government agencies. It helped to reduce the time to get the expected product by development and automation. My experience spans IT transformation, data integration, distributed storage systems, DevOps practices, cloud solutions, and agile practices.
"
data engineer,"Create ML forecasting system in one of the biggest Ukraine retail company.
More then 10 years work with data in FMCG, Retail, Supply Chain. Begin as data analyst, data scientist and now look for Data/BI engineer position. Have skills in mathematical modeling and forecasting.
Good team and new technologies"
data engineer,"
More then two years I was working in the Big 4 (PwC) as an auditor. Mostly worked with big international bussiness in cooperation with collegues from CEE region. Vast majority of my time (65%) I was involved to the audit of an global IT company (was auditing set of the entities whole over the world). During my work I developed such skills as: project planning, time managmend, people managment, data analysis, data processing, financial analisys, accounting, auditing, presentation skills, teamwork. 

Ongoing work with data and participation in digitalisation projects of the auditors procedures (which involved close cooperation with data engineering team) made me think about changing the area of my work. 

So for the past two years, I have been working in the data team of an international company that collects, processes and sells data (Enverus). The company has its own product for which we provide tech support. During this period, I became a confident user of SQL, as well as git, grafana, nomad, Azure, c# and am currently taking courses to learn Python.
"
data engineer,"Just the ordinary Engineer, whose achievements are yet to come.
Motivated and efficiency-driven with over 5 months of experience in Testing of Web applications and Mobile applications. I have good knowledge of both Web and Mobile testing, Client/Server environment, Agile and Scrum test methodologies. I have experience in writing, executing and managing Test Cases, Bug Reporting and Tracking Defects using TestRail and JIRA. Also, I have basic knowledge of Python, SQL, JavaScript, HTML, CSS. Moreover, I worked with Linux OS and Unix Terminal and have experience in REST API testing (Postman) and automation testing (Cypress).

HYS-Enterprise, Junior QA Engineer, November 2021 - Present

QA, “Joyn.de” (Web/Mobile) test project, November 2021
- Working in a SCRUM team
- Defining the scope of manual coverage for release cycle
- Designing tests for new features including web UI and API interfaces
- Planning, creating and maintaining decomposition, test plan, test cases using test design techniques and RTM for smoke, integration, regression, usability, functionality and user acceptance tests, etc.
- Execution of manual testing (cross-platform testing on real devices to provide better quality)
- Finding, logging and reporting bugs into JIRA and tracking until closure

QA, “Conduit” (test project), October 2021
- Requirements reviewing with Product Owner (Story refinement)
- Developing Decomposition
- Developing and executing Test Cases in TestRail
- Reporting bugs into JIRA 

QA, “Petstore” (test project), September 2021
- User Story reviewing 
- Developing test cases using test design techniques and RTM
- Executing test cases in TestRail
- Reporting bugs into JIRA
Growth both personally and professionally."
data engineer,"
MS SQL Server developer in banking and finance
Business consultant 2nd and 3rd level of support
-Stored Procedures, views, user-defined functions. Create and optimize

-ETL processes

-Migrated data from one database to another

-Develop technical documentations

-Writing user manuals

-Involved in creating DWH structure

-Create, develop and maintain SQL processes for channel performance data warehousing

-Power Bi solutions development

-Reports for banks and MF companies using MS SQL and micro service architecture

-Creating, develop and optimize procedures for dashboards
"
data engineer,"- 3.77 of 4.00 GPA at Boston University (M.S. ADA)
- designed and deployed a recommendation system for a groceries delivery app (Ryadom, part of Choco Superapp)
- designed and engineered a fraud detection system for a QR payments platform (Rahmet, part of Choco Superapp)
M.S. student at Boston University studying Applied Data Analytics with a concentration in data engineering.

Before beginning graduate school, I worked for nearly 2 years as a data scientist for the largest IT company in Kazakhstan (Choco Superapp). As a result, I worked on various services including food/groceries delivery, online/QR payments, airline tickets, and medical appointments.

I have professional and academic experience with ETL (Python, SQL, and dbt) and DWH design. In addition, I have designed and deployed numerous ML resume_classifier that interacted with thousands of users every day (Python, scikit-learn, Apache Airflow, AWS S3).

I am currently looking for a data engineering internship for the summer, with a possibility of full-time work after graduation (January 2024).

My available dates are May 15th, 2023 to September 1st, 2023.
I have solid skills in all aspects of data work, including data analysis, data visualization, data engineering, and machine learning. Currently, I would prefer to work primarily with data engineering and ML."
data engineer,"
My current project stack is Spark Streams, pySpark, pytest, AWS (EKS, S3, Athena, Glue, other), Hudi, Argo CI, Kafka

Few years ago I started with Python and used it as a convenient way to manage different cloud and  bigdata APIs. 
Now I improved Python skills much, cause project requires not only write pySpark code, but also cover it with unit/integration tests on pytest for Spark jobs.

Had some experience with Databricks, Snowflake, Cloudera (Hive, Spark), Azure.
Spark processing on java/scala.

Had some experience with Snowflake and Redshift, got SnowPro certificate.

Overall, I spend >10 years as an Oracle DB developer. 
ERP for Retail chain, DWH for telecom company,
(PL/Sql, SQL optimisation).
Have some experience in Tableau
and much more hands-on experience with QlikView.
I'd like to hone such skills as Python, pySpark, AWS, Snowflake.

definitely no any Reporting, RDBMS.

I set Salary in net, for gross there should be ~6500$"
data engineer,"
My experience consists of skills in Hadoop stack, BI (SQL databases), DWH (SSAS, SSIS, SSRS, PowerBI), Cloud Engineering (AWS, GCP, Palantir Foundry, Azure), ETL using Spark (Python, Scala), DevOps Engineering (Cluster Management, IAM, UseCase reporting dashboards development, ELK stack). Additionally, I have knowledge of Java Core, and participated in Java Software Engineering.
I describe myself as self-organized engineer, who is capable of responsibility to manage technical processes in team, problem solving and strong time-management. I have an understanding of business relations, analytics and SCRUM process organizing. 
I managed entire development process starting from getting requirements, analyzing scope and estimating work to implementation, communicating status, testing and release management.
With no hesitation I’m open to new challenges and new opportunities.
"
data engineer,"The credit scoring system I built with a team just in 6 months the system has reached 50,000 businesses, disbursed more than 500 million Birr.
My experience includes building a credit scoring system for small companies with no bank transaction using machine learning, creating different end-to-end data pipelines for various machine learning resume_classifier, analyzing historical data to help banks make lending decisions, working with geospatial data to optimize a bus ticketing system, speech recognition resume_classifier for low resourced languages and others as well. These projects have required me to use a variety of technologies, such as python, SQL, docker, dbt, Airflow, and AWS (lambda, glue, Athena, redshift, cloudwatch and more). Currently, I work as a data engineer and am working on improving and experimenting my skills in spark and Kafka.
"
data engineer,"•	Transformed reporting system in according to new account matrix. Reviewed packages/procedures and processes ETL.
•	Optimization processes aggregation and preparing  core data to MIS Reporting 
•	Redesigned data’s for process budgeting and forecasting 
•	Fixed many bug’s of MIS reports based on Oracle Analytics Report (OBIEE)
•	More 50 reports was created in according to business during the last year
•	My major responsibilities are manage of processes software development, implementation systems at fintech/finance domain, data analisys, data science,  business intelligence and visualization;
•	Understanding of principals OLAP/OLTP systems.
• Strong understanding of DWH principals.
•	Strong knowledge  ETL processes and how to build ETL solutions.
•	8+ years of overall database development expertise in DBMS;
•	8+ years of overall Data warehouse & Data Mart modeling expertise;
•	Strong expertise in BI stack: Data Warehouse design, data analysis, automation and implement analytics methods, Data Marts design, BI tools (OBIEE, Qlik, Tableau, Power BI);
•	Strong expertise in business intelligence systems. Experiences in business requirements analyzing and a definition of business processes.
•	Skilled in technical interviewing and evaluation processes
•	Expertise in mentoring for DWH&BI theory and concepts.
•	Knowledge and understanding of relational database design and programming.
•	Knowledge and production experience with developing, tuning and deploying reports and BI solutions.
"
data engineer,"Spark Jobs with Python, I would like to work with AWS / Python
MySQL (SQL) - 3 years
Oracle (SQL, pl-sql) - 8 years
PostgreSQL (SQL, PL/pgSQL) 3 years
Teradata - 2 years
AWS Redshift, EMR, S3, RDS - 3 years 
Apache Airflow - 3 years 

- maintenance and creation database objects;
- development and design of database/DWH, optimisation database structures;
- build queries based upon analytical functions, creating hierarchical queries;
- using collection, dynamic SQL;
- queries optimisation;
- creating SQL scripts for ETL processes organised in the Apache Airflow; 
- creating and maintenance Airflow DAGs
- creating PL/pgSQL functions for parsing json/jsonb PostgreSQL data type with following data transformation and loading to DWH in compliance with required SCD types. 
- creating ETL processes in the Core banking platform based upon different sources (Text, xml, csv files; database tables; SQL queries) with using different approaches for data parsing (XQuery, Shell, Python scripting). Building reports based on loaded data;
- AWS: S3, EC2, RDS, Amazon Redshift, Lambda, IAM (basic level), QuickSight
- IBM InfoSphere DataStage (version 8.5)
- Python: pySpark, pandas, BeautifulSoup libraries, creating airflow DAGs
- creating Spark Jobs on Python for data aggregation in ORC, Parquet, AVRO, CSV formats.
- creating a pipelines/DAGs in Apache Airflow to process data from different sources like RDS, s3, kinesis with the using a pyspark jobs which run in EMR and upload final aggregated result to the Redshift tables;
- cost and performance optimisation EMR Clusters;
- cost optimisation of s3 resources.
interesting to work with the following tech stack: Data Engineer with Python, Spark, AWS Services."
data engineer,"- I'm analitical, self-confident and responsible person. 
- I've great communication and negotiation skills. 
- Graduate of EPAM Data Engineering Lab. 
- I'd the opportunity to work with SQL MS, MYSQL, ELT//SSIS, Windows Forms.
- I've experience with such programming languages  as C#, T-SQL.
- Basic knowledge of implementation of ETL solutions using on-premises software (SSIS) 
- I've experience cloud-based services (AWS and GCP) 
- Basic Python skills 
- One of my goals is to continually improve myself
Oct, 2021 – Jul, 2022
Job Position: Trainee Data Engineer
Project Roles: Trainee Data Engineer 
Worked with various tools and technologies:
- wrote SQL queries;
- wrote Python code;
- worked with Git and GitHub;
- worked with Amazon Web Services;
- created S3 bucket, Lambda function, DynamoDB table, SNS, AIM, EC2;
- worked with Google Cloud Platform;
- created Cloud Storage, BigQuery, Composer, Cloud SQL, Cloud Functions; - developed DAGs in Airflow;
- developed Dashboard in Tableau;
- developed Report in PowerBI;
- worked with Hadoop, Hive, Spark.
Database: SQL Server 
Tools: SQL Server, Pycharm, Jira , Visual Studio 2019, SQL Server Integration Services (SSIS), AWS, Power BI Desktop, Tableau.
Desktop Technologies: SQL, ETL with SSIS, AWS, GCP, Tableau, PowerBI, Airflow, Hadoop, Hive, Spark.

Jul, 2022 – Nov, 2022
Job Position: Data Engineer
Project Roles: Data Engineer
Project: Data is stored in CSV files. The CSV files uploaded into Google Cloud Storage named Buckets. From the Buckets CSV files are migrated into Cloud SQL (SQL Server or PostgreSQL or MySQL). Here we build relationships between tables and bring to the third normal form. Next step set up workflow with using Cloud Composer for orchestration. On the Cloud Composer we built on the Apache Airflow and operates using the Python programming language. By using Airflow and Python we are going to create Data Warehouse in the Cloud BigQuery with arranging all staging and aggregation steps on this database. The whole workflow should be built to maintain full load and incremental loads. On Cloud BigQuery in very last step we will create views with data prepared specifically to reports. For the reports can be used any of tools Tableau/Power BI/Looker.Participation- Create a Cloud Bucket and uploaded CSV files and JSON in there; - Create a Cloud Funcion that runs when a CSV file is uploaded to the bucket and uploads the data to Cloud SQL; - Migrated required data from Cloud SQL to BigQuery; - Local work with AirFlow and create pipelines - Analysed raw data in MySQL Workbench and selected those necessary for generating reports;Team Mentors team: 2 members, Dev team: 4 members Database MySQL, MS SQL Server, BigQuery Tools Cloud Storage, Cloud Function, Cloud SQL, BigQuery, MySQL Workbench, GitHub, Jira, AirFlow Technologies Google Cloud Platform.
"
data engineer,"Successfully implemented the integration of Apache Airflow and dbt, enhancing the reliability of report pipelines and data transformation processes.
     Implemented advanced partitioning practices that dramatically reduced the processing time of complex queries from 26 minutes to an impressive 4.5 minutes, significantly boosting data processing efficiency.
One of my notable achievements was a highly productive collaboration with data analysts during my tenure at my previous workplace. This collaboration not only enriched my understanding of data but also played a pivotal role in solving various complex challenges in research and development. I thrive on the opportunity to work collaboratively with cross-functional teams, leveraging data-driven insights to drive innovation and progress.
While I am deeply committed to my professional growth and the ever-evolving landscape of data engineering, I also place great importance on maintaining a healthy work-life balance. I believe that a balanced approach to work fosters creativity and productivity, ultimately benefiting both my career and personal life."
data engineer,"
One year at a large business project designed to streamline business consulting and auditing with a micro service architecture that makes various calculations, checks and generate reports. 

And half a year I worked on freelance before that. I did various small projects.
Good teamwork and working conditions"
data engineer,"
One year of experience as a PySpark Data engineer.
Work with modern technologies (e.g. cloud services, kubernetes, airflow etc.)."
data engineer,"
Oracle Data Integrator Engineer:
supported existing data flows (Oracle Data Integrator), optimized the performance,
integrated new sources of the data (includes a Knowledge Module developing),
developed a new functionality (e.g. transferring data between schemas),
conducted code review activities,
enhanced the deployment process,
took part in migration from ODI 11 to ODI 12
"
data engineer,"
Overall experience in IT and Data Engineering - 12+ years
- Leading Data engineering teams - 4+ years
- Data engineering competency development and conducting technical interviews - 5+ years
- Developing Data integration pipelines(ETL, ELT) and Data Lakes(Delta Lakes) - 7 years
- Data platforms development - 5+ years(using data lake, delta lake, data mesh approaches)
- Cloud providers - GCP, Azure, AWS
I would not like to work on some project where legacy coding needs to be maintained; My preference is to work with up-to-date and new/evolving technologies and approaches."
data engineer,"- started automation project from scratch 
- Read several books and finished several courses on DE topics
overall experience in software development/testing 5+ years
decided to switch to DE ;
worked in network, automotive, document stream generation areas
preffered small team/company"
data engineer,"
Over the past 3 years, I've excelled as a data engineer, crafting ML systems, generating data visualizations, and conducting insightful data analysis. My contributions span agile software development, where I've harnessed technologies like Python, GCP, Terraform, and SQL to achieve remarkable outcomes. Currently, I hold a pivotal role within the team, leveraging my experience to drive efficient and impactful outcomes. I'm eager to collaborate with motivated individuals, further enhancing my skills while making substantial contributions to companies' success. My goal is to continuously elevate my capabilities and deliver high-level value.
"
data engineer,"
Over the past five years, my professional journey has been primarily centered around working with data. Initially, I gained extensive experience in utilizing SQL Server and the K2 automation tool. Following that, I spent a year immersed in leveraging MicroStrategy for data analysis and reporting. For the past two years, my focus has shifted towards the realm of cloud computing and data engineering projects. Specifically, I have been actively engaged in tasks involving Azure Cloud, utilizing tools such as Azure Data Factory, Azure Synapse, Azure Databricks, and Python.
"
data engineer,"
Participated in daily meetings with the team, in grooming and planning meetings to organize work. Worked with Agile methodology (Scrum).
Created complex queries and store procedures with SQL.
Created resume_classifier for ETL work using dbt.
Created datawarehouses, users and databases for different environments in Snowflake using Terraform (Go programming language).
Created Airflow DAGs using Python and with created DAG builders, and set up parameters for the DAGs.
Done data migration between different cloud storages and raw sources with using ETL pipelines and manual scripts.
Worked on the collecting requirements with analyzing.
worked with the No-SQL databases, and created structured tables from them, the tables were updating(dynamic).
Created ETL pipelines(Azure Data Factory/AWS Glue/Airbyte), SSIS packages to deal with ETL jobs.
Created different reports, dashboards and other BI solutions with using Power BI/Tableau/Looker.
Created different paginated reports with using SSRS/ Power BI Report Builder
"
data engineer,"- Databricks Certified Data Engineer Associate certification (2023)
- DeepLearning.AI TensorFlow Developer (2023)
Passionate about DE with almost a year of production experience mainly with PySpark, Airflow, Databricks.

I have high level of personal morals and integrity. I am willing to 
work hard and have a great desire to learn.
Databricks; ability to gain experience with visualisation tools (Power BI, Tableau) would be a plus."
data engineer,"
Period: June 2022 - Present
Place: Bishkek, Kyrgyzstan
Company Name: Key DEV LLC
Position: Junior Back-End Developer
Description:
• Implementation of Python code with assistance from senior developers;
• Refactoring of old Python code to ensure it follows modern principles;
• Identifying and fixing code where bugs have been identified.
Period: May 2021 - January 2022 
Place: Bishkek, Kyrgyzstan
Company Name:
Representative Office of the American Center for International Trade Union Solidarity in the Kyrgyz Republic (Solidarity Center)
Position: Monitoring and Evaluation Specialist
Description:
• Participation in the development of the work plan for the supervised area (monitoring and evaluation);
• Planning and development of research design
and methodology;
• Compiling monitoring and evaluation
requirements for regional implementers of program activities, as well as partner organizations;
• Coordination of monitoring and evaluation activities of regional projects and partner organizations, provision of technical assistance; • Coordination of research, analysis of the obtained data;
• Participation in the preparation of reports for donors.
Period: February 2019 – May 2021 
Place: Bishkek, Kyrgyzstan
Company Name:
Research and consulting company “M-Vector”
Position:
Project Manager\Analyst at Social Research Department
Description:
• Management of international and local social research projects;
• Preparation of commercial proposals, estimation of budgets and preparation of work schedules;
• Development of research methodology and design,
 sample,M&E indicators, and tools
• Building and maintaining relationships with a range of key clients;
• Ensuring appropriate, accurate, timely and efficient collection of all M&E indicators data and analysis;
• Data processing and analysis, preparation
and formatting of spreadsheets, annexes, and graphs;
• Training and directly managing all field staff
and data collection department to ensure accuracy and relevance of both quantitative
and qualitative indicators;
• Development of analytic reports according to main findings of M&E, with main conclusions and recommendations;
• Provision of ongoing feedback to projects and programme.
"
data engineer,"
Philip Morris International July 2021 - Present
Position: Data engineer
My duties:
Development and support of solutions for business process automation (mulesoft applications, ssis packages, aws glue, lambda, c # interfaces, python interfaces)
Working with databases MS SQL, PostgreSQL, S3 bucket. Their support, writing procedures, creating architectures for projects
Participating in migration of our on premise servers to cloud (AWS, Terraform, S3, RDS, Glue, Lambda, IAM)

Highers.co December 2020 - March 2021
Position: Software developer
Technology stack: 
Web platform : NodeJS/TypeScript ,React,  MongoDB, Docker
Bot : Python, flask, pythelegram-bot-api, PostgreSQL
My duties:
Development of architecture and support of the bot telegram platform in Python
NodeJS-based server support
Development and implementation of new microservices using Docker and Docker-compose
Participate in project migration from NodeJS to TypeScript
"
data engineer,"
Philip Morris Ukraine
Date:			January 2023 – April 2023
Role: 			I was a Data engineer at a tobacco company
Responsibilities:
-	creating and maintaining data pipelines;
-	processes automating;
-	optimizing data delivery;
-	working with databases;
-	integrations with 3rd-party APIs;
-	integrations with different web-services
Technologies: 	Python, Snowflake, AWS (Redshift, S3, EC2, Lambda), Matillion, PostgreSQL, MS SQL, SQL Server, Confluence, Salesforce, Jira, SharePoint

Energy 365
Date:			December 2021 – October 2022
Role: 			I was a Data engineer at an energy trading company (gas and electricity)
Responsibilities:
-	creating and maintaining data pipelines;
-	processes automating;
-	optimizing data delivery;
-	working with databases;
-	integrations with 3rd-party APIs;
-	integrations with different web-services
Technologies: 	Python, HTML, JS, Django, PostgreSQL, Airflow, Pandas, NumPy, Beautiful Soup, Selenium, WSDL, SOAP, PowerBI, MS Office

Freelance
Date:			June 2020 - December 2021		
Role:			I was a Python developer with varying tasks.
Responsibilities:
-	Web-site creation;
-	Bot creation (telegram and trade);
-	Data collecting and representing through scraping and APIs;
-	Creating APIs;
-	Implementation of new features;
-	Development of new and maintenance of existing web application features;
-	Product support and documentation maintenance;
-	Deploying to AWS and Heroku
Technologies: 	Python, HTML, CSS, Beautiful Soup, Scrapy, Selenium, Pandas, NumPy, SQLite, MySQL, MongoDM, JavaScript, AWS, Heroku, Docker, K8s, GIT

Domlux LLC
Date:			June 2019 - May 2020		
Role: 			I was a Director at a real estate building company
Responsibilities:
-	search of tenders for participation;
-	collection of a package of documents and participation in the auction;
-	analysis and control of work performance;
-	negotiations with the client and solving various issues
Technologies: 	1C, AVK, MS Office, AutoCAD
"
data engineer,"Several projects developed start to end: 
- Brand/product integrity technologies; 
- Support of current systems by updates according to user requirements; 
- Development of analytical tools to track deviation from business objectives and means to achieve targets.
Playwing Ukraine – Power BI Team Lead
2022 – now
 Developers supervision 
 Participation in system architecture and capacity planning 
 Database: 
- Database architecture 
- Development of SP/Triggers
- Resource optimizatio (index, aggregations, etc). 
 Reporting: 
- Reviewing and refactoring datasets
- Development of report requirements with business parties 
- Reviewing and refactoring reports, optimizing designs
-------------------------------------
Playwing Ukraine – Power BI Engineer 
2021 – 2022
 Formulating requirements to SQL data structures used for reporting 
 Development of datasets/dataflows for Power BI reports 
 Requirements collection & preparation for report development
 Development of reports for end-user
-------------------------------------
Philip Morris Ukraine – Process Engineer
2018 – 2021
Manufacturing performance analysis; Performance improvement; Root-cause determination and analysis; Project management and implementation; Power BI Reporting development; Business processes & operation instructions development.  
-------------------------------------
Philip Morris Ukraine – Production analyst
2017 – 2018
Manufacturing data analysis; Excel reports development (Power Pivot based); Data verification and validation; Ensuring process adherence to standards. 
-------------------------------------
Philip Morris Ukraine – OPS planning analyst.
2017 – 2017
Cross-departmental analysis of KPI’s; Analysis and planning of
production equipment capacity; Statistical analysis for problem
areas identification; Acquiring data from primary or secondary
data sources and maintaining databases.
Participation in project teams, get deeper experience and understanding of how things done in IT industry, development of weak skills, using strengths to boost business results/build customer value."
data engineer,"
PL/SQL DEVELOPER
Allianz Technology Thailand
March 2021 - October 2022
support SICAR and Speeder application by using Oracle Database and SAP BO
Develop ETL project using Pentaho
Migrate ETL flow from Kettle 2.5.2 to Pentaho 9.2

ORACLE DATA INTEGRATOR
Accenture Thailand
December 2020 – February 2021
Experience with Oracle 12c databases
Analyzes the needs and requirements of users of existing and proposed database systems and develops technical, structural and organizational
Expertise in transform and loading (ETL) using various tools such as SQL server and MobaXterm
"
data engineer,"
- Preparing, deploying and destroying VPC, VM instances via IaC tools (Terraform)
- Work with IAM service accounts and Cloud Storage buckets 
- Permissions IAM management 
- Terraform infrastructure creation and optimization 
- Building a VPN between Google Cloud and AWS with Terraform
- Creating a Cloud SQL and Modular Load Balancer with Terraform (Regional Load Balancer)
- Creating CI/CD pipelines at the GCP platform
- Creating pipelines with Data Fusion and BigQuery
- Creating Prometheus alerts and Grafana dashboards visualization
Work with Cloud infrastructure - GCP"
data engineer,"
Professional Experience

Data Engineer & Mentor at Making Science • Sweeft, July 2021-Present

- Developed an analytical and visualization platform for automatically ingesting data to and from multiple marketing sources.
- Designed, developed, built, deployed and migrated existing numerous and reusable data pipelines using serverless and serverful technologies on Google Cloud Platform, resulting in a 50% reduction in data processing time and costs.
- Developed and maintained Data Lakes and Warehouses on Google Cloud Platform.
- Utilized best practices to support continuous process automation for data ingestion and data pipeline workflows.
- Developed reusable CI/CD workflows and IAC for data pipelines using Terraform and Cloud Build.
- Continually explored new technologies in an effort to always grow and uncover better ways to solve problems.
- Worked closely with stakeholders across departments to design, build and deploy various initiatives within the data platform
- Mentored interns aiming to become Data Engineers.

Technologies Used:
- Apache Spark(PySpark) on Dataproc
- Apache Beam on Dataflow
- Docker
- Cloud Build
- Git
- Python
- Bash
- BigQuery
- Firestore
- PostgreSQL
- Jupyter Notebooks
- pytest
- Terraform
- Airflow(Composer)
- FastAPI
- Cloud Pub/Sub
- Cloud Functions
- Cloud Run
- Cloud Workflows
- Cloud Storage


Personal Projects

ISS Satelite

- Built an on-premise ELT pipeline from a REST API providing real-time data about the ISS satellite.
- Designed PSA, Data Warehouse and Data Marts in PostgreSQL with read replicas using cross-database real-time writing.
- Used Apache Airflow for batch processes along with Redis for caching the latest data for the satellite.

Air Quality, Weather and Mortality Data Insights

-Collected continuously updated air quality, weather and mortality data from public OpenAQ, NCEI CDO and CDC sources
- Designed a Data Warehousing(Data Vault 2.0) solution and deployed the project on Google Cloud Platform entirely using Terraform and Cloud Build CI/CD.
- Developed ETL pipelines in Apache Spark(Dataproc) and Apache Beam(Dataflow), orchestrated by Airflow(Composer).
- Enriched the data and filled missing spots using various algorithms and interpolation techniques such as IDW and RBF.
- Built a presentation layer with visualizations and analysis.
"
data engineer,"
Proffiz, remote – Data Scientist/ML Engineer
January 2022 – February 2023
As a member of a cross-functional team, I contributed to the development of various solutions aimed at improving business outcomes, including strategies to enhance user retention rates and engagement.
Projects:
•	User behavior analysis
•	Life-time value analysis

I am looking for a suitable job. While working as a Data Scientist, I realized that I am more interested in what happens before data comes into my hands, rather than the process of training resume_classifier and building various metrics. That's why I decided to switch to Data Engineering. Right now, I am putting maximum effort into gaining as much new and high-quality knowledge as possible. Moreover, this job aligns quite well with my expectations in terms of the knowledge stack and tasks that await me.
"
data engineer,"
Project1 - CLIX-Imc learning management system. Here my responsibility was:
Tools: Oracle database 11g, CLIX

Installation and Configuration
Analyzing system performance
Clix database administration 
Tuning database performance

Project2 - Central Bank of Azerbaijan Analytical and Statistical Reporting System.
Tools: Oracle Database 11g/12c, Oracle Data Integrator 11g/12c, Oracle Data Integrator 11g/12c, Oracle Weblogic server 11g/12c, TimesTen 11 in memory database

Installation and Configuration Oracle Database
Installation and Configuration Oracle Business Intelligence 11g/12c
Installing Oracle Data Integrator
Integrating Data Sources
Data Transformation, Data Control and Data Quality
Creating ETL Packages, procedures and interfaces
Tuning SQL Queries 
Analyzing and Architecting Data Warehouse Database
Analyzing Source Data Models
Creating Fact and Dimension structure.
Star and Snowflake schema modelling 
Developing analysis and analytic reports in Oracle Business Intelligence
Creating Dashboards
Developing static BI Publisher reports
Applying formulas to reports
Tuning analysis and analytical reports
Defining Row-level security in Oracle Business Intelligence
Oracle Database Administration and Troubleshooting 
Partitioning and Indexing tables
Oracle Data Integrator Administration and Troubleshooting
Automation and Scheduling Packages, Procedures and Interfaces
Oracle Business Intelligence Administration and Troubleshooting
Creating Users, Groups and Application Roles in WebLogic server

Project3 -    Ministry of Finance Application reporting module
Tools: Oracle Database 11g/12c, Oracle Data Integrator 11g/12c, Oracle Data Integrator 11g/12c, Oracle Weblogic server 11g/12c

Same as in Project2
Integrating FARABI (app) with OBIEE 11g


Project4 - Kyrgyzstan Republic Ministry of Taxes Reporting Module
Tools: MS Sql Server, SSIS, Oracle Business Intelligence 11g/12c

Analyzing and Architecting Data Warehouse Database
Creating Fact and Dimension structure.
Star and Snowflake schema modelling
Creating Dashboards, Analytic and Static reports
"
data engineer,"
- Project #1
Translating and implementing commercial requirements into queries to database, collection and clear the necessary data.
Implementing the logic of transforming data and forming the final result that are published to a visualization tool.
Optimizing the existing queries.
Writing the logic widgets on Javascript for visualization a received information.
tech: PostgreSQL, Google Cloud SQL, Amazon RDS, Javascript

- Project #2
Migrating the databases and managing the database.
Designing, building, maintaining data pipelines to migrate and transform data.
Tracking pipelines stability and ETL testing.
Creating indexes for faster retrieval of the information and enhance the database performance.
Building database schemas, tables, procedures and functions.
tech: MS SQL, Oracle, Azure Data Factory, Azure Storage/Data Lake, Azure SQL, Azure Function App (Python), Bicep (ARM templates)

- Project #3 (domain: pharmacy management system)
Building Data Warehouse for gather data from across the organization.
Creating ETL/ELT code to populate the Data Warehouse.
Creating Spark jobs for data transformation and aggregation.
Creating Airflow DAG to drive ETL/ETL code.
Creating merge and purge functions.
Working on teams operating under an Agile Scrum delivery methodology.
tech: PySpark, Spark Streaming, Kafka, Airflow, PostgreSQL, ADLS, Docker, K8s, Grafana, Liquibase, Jaspersoft
Data Engineering tasks and tech. Focusing on the cloud products"
data engineer,"1) Being a BI Engineer, I used knowledge of Data Engineering to build a Data Lake storage in Azure to ingest, store and process historical weather data to provide deep information to mining specialists for making decisions.
2)  I helped AppDev team to build mobile application for controlling main business metrics in mine zone. The main responsibility in this project was to extract data from transactional database, transform it and provide necessary shape of data to build app's backend.
3) Create BI Dashboard to track logistic information about product delivery to railway terminals which become the main product in series of dashboards.
4) Having experience as Data Engineer, I built a service which send personal notifications about water outages in my city using OCR Engine, Apache Airflow for orchestration processes and API's for sending notifications in messenger.
5) Currently, I am gaining experience in Data Engineering products on  Google Cloud Platform. I built streaming solution to send simulated sensor data  to Pub/Sub topic, process it with Dataflow and send processed data to BigQuery.

Certifications:
1) DP-203 Azure Associate Data Engineer certificate
2) DP-900 Azure Data Fundamentals certificate
3) Python Basics - The University of Michigan

Badges:
1) Building Batch Data Pipelines on Google Cloud 
2) Building Resilient Streaming Analytics Systems on Google Cloud 
3) Google Cloud Big Data and Machine Learning Fundamentals 
4) Google Cloud Fundamentals: Core Infrastructure
5) Modernizing Data Lakes and Data Warehouses with Google Cloud
Project: Automated notification platform about water outages [Non-commercial project]
Involvement Duration: August 2022 – May 2023
A platform which sends personal notifications to users about water outages based on image and Text processing.
Project Role: Data/Software Engineer
Responsibilities:
1) Solution design.
2) Infrastructure configuring. 
3) Define Text processing flow.
4) Development.
Tools & Technologies: Python, Pandas, Apache Airflow, Tesseract OCR, PostgreSQL, Docker, Telethon, Git.

Project: Coal Processing project
Involvement Duration: June 2021 – March 2022
A project with set of Power BI reports focused on controlling coal processing on mine.
Customer: Canadian company
Project Role: Junior PowerBI Engineer
Responsibilities:
1) Maintaining existing BI solutions
2) Implementing new functionality
3) Creating new PowerBI reports
4) Data model refactoring
5) Integration of data resume_classifier into BI report.
Tools & Technologies: 
Microsoft Power BI, Postman ,Google BigQuery, Microsoft Azure

Project: Charity Dashboard [Volunteer project]
Involvement Duration: August 2022 
A near real-time dashboard which show users' donation activity
Project Role: Junior PowerBI Engineer
Responsibilities:
1) Implementing the visual design of report
2) Setup PaaS database in Azure Cloud
3) Design tables for database
4) Working with payment API to integrate it with report 
5) Deploying Azure Function
6) Creating PowerAutomate flows for handling required scenarios.
Tools & Technologies: PowerBI, PostgreSQL PaaS on Azure, SQL, Power Automate, Azure Functions

Project: Water resources in Ukraine [Non-commercial project]
A BI project which shows consuming of water by water users.
Involvement Duration: April 2021
Project Role: BI Engineer
Responsibilities:
1) Looking for a dataset.
2) Data cleaning.
3) Creating data model.
4) Implementing business logic. 
5) Building visualizations.
Tools & Technologies: Microsoft Excel, Microsoft Power BI

Superstore [Non-commercial project]
A BI retail project with processed sales data, visualizations, and performance indicators.
Involvement Duration: February 2021
Project Role: BI Engineer
Responsibilities:
1) Data cleaning.
2) Creating data model.
3) Implementing business logic. 
4) Building visualizations.
Tools & Technologies: Microsoft Excel, Microsoft Power BI
"
data engineer,"
Project:
Bacardi
Implementation of integration layer on Azure Stack for big retail client. The project was split into 2 parts: initial and incremental data loads: OLTP database -> SAP Gigya -> SFMC
Tech stack: TypeScript, Python, Azure Functions, EventHub, BlobStorage, SQL.

Project:
NDA
Development and maintaining multiple data-pipelines on GCP stack
Tech stack: Spark, Java, Dataproc, Pub/Sub, Cloud Storage.

Project:
NDA
Improving searching experience using ML/non-ML techniques for popular E-Commerce Solution. Working in close cooperation with Data Science team to support machine learning SDLC including feature engineering for ML resume_classifier.
Tech stack: Java, Kotlin, Spring Boot, Python, AWS, ElasticSearch, Kubernetes, Kafka, Presto, Apache Spark, Redis, PostgreSQL, gitlab-ci, NewRelic, Splunk, scikit-learn.

Project:
OBE, NDC
Worked on Online Booking Engine and adjoined services for Ailines & Tourist services providers. Implemented new microservices, integrated with 3-part systems, development of deployment scripts and infrastructure, bug fixes.
Tech stack: Java, Kotlin, Spring Boot, AWS, ELK stack, PostgresSQL, MongoDB.

Project:
Oioni
Worked in a small team, consisted only of Lead Engineer and CTO on new features implementation, bug-fixing, docker scripts preparation for project in a beauty services domain.
Tech stack: Java, Spring Boot, Spring Data, MySQL, Docker.
Interesting project
Modern technology stack
Good management
Grow professionally"
data engineer,"
* Project: On-premise migration to Cloud (using Azure Synapse Analytics)
Building Data lakes, Data warehouses architecture, and ETL solutions;
Used Slowly Changed Dimensions technique to build customer profile;
Experience in building ETL processes, understanding how to test them and check data integrity;
Experience with Azure Synapse Analytics, building ETL Pipelines, orchestration, DataMarts and Data Layers;
SQL query optimization;
Azure SQL DWH, Azure SQL managed Instance;

* Project: Teradata to AWS S3 migration
Building Data Pipelines with SQL, Python(PySpark);
Using DBT and Airflow to automate data quality testings;
Configuring Spark Cluster based on the needs;
Validation of data processing processes;
Analysis of the quality of data prepared for ETL and BI systems;
Development of complex SQL queries to various databases to check the quality of data and the results of their processing;
Databricks, Spark
5000"
data engineer,"
Projects I worked on were related to pricing. My task was to create a data conversion process and develop a calculation engine based on the company's product. Our team used the company's product, which allowed customers to easily generate prices, store their data and use an intuitive and user-friendly interface.

My role in the project was to create complex data transformations to ensure that customer data was properly processed and displayed on the platform. This involved extracting data from a variety of sources, processing the data to clean and standardize it, and transforming it to meet the requirements of the costing engine.

One of the key stages of my work was ensuring the accuracy and reliability of the calculation results, which affected the final product prices. Keeping sensitive customer data and ensuring its privacy and security was an important aspect.

I also actively collaborated with other team members, including developers and analysts, to understand the data requirements and functionality of the product. My work helped to ensure that all data was correctly integrated and displayed on the platform, which allowed users to work conveniently and efficiently with the company's product.
"
data engineer,"Certifications

- ITIL Foundation in IT Service Management
- Oracle SQL Expert
- Oracle PL/SQL Developer Certified Associate (OCA)
- Big Data Modeling and Management
- Systems by University of California, San Diego
- Oracle Advanced PL/SQL Developer Certified Professional (OCP)
- Introduction to Big Data by University of California, San Diego
- Azure Foundations
- Azure Data Foundations
- Azure Data Engineer
**PWC**
• Testing of IT General Controls (ITGCs) for information, billing and telecom
systems
in support of financial statements audit;
• Assessment of IT control effectiveness, application controls within the
systems in
variety industries including Banking, Oil & Gas, Telecommunication;
• Provision of recommendation on IT control effectiveness and optimization
• Pl/SQL code quality analysis
• Database analysis

**Bank Respublika**
• Responsible for the design, development and delivery of changes to the
banks core
applications using a variety of tools including net and Oracle, PL/SQL
Development.
• Provide technology and business expertise to end users and be heavily
involved in the
development strategy.
• Responsible for all phases of the development life-cycle - requirements
specification,
application and database design specification, program code, test plans and implementation scripts.
• Responsible for all Oracle/PL/SQL Development. Guide the development and
support
changes across the banks core applications which are built in a variety of tools
including; Oracle, PL/SQL, TOAD.
• Design reports to meet end user expectations
• Verify reports to ensure the reported data is realistic
• Build full-blown reporting cycle to support decision making
• Provide ongoing guidance, trouble-shooting support, and response to user
questions/problems related to MIS reporting.
• Analyze performance of MIS reports and propose steps to improve their
efficient
work.
• Experience working with banking system FlexCube

**AGBANK**
• Contribution to the entire software development life cycle from initial business
requirements to deployment and production support;
• Answering client support questions via phone and Email;
• Design and implement test scenarios and test cases;
• Preparing testing documentation;
• Involvement into the performance testing;
• Production support of banking software;
• Working with multiple database interfaces and familiarity with standard
database
architectures;
• Assistance in testing procedures;
• Testing and inspection results;
• Experience working with banking system TEMENOS T24;
producing high quality and accurate specification documents;
• Experience in the development of system requirements, testing, business
process
analysis and modeling solutions;
• Gathering initial requirements for system optimization
"
data engineer,"Computer Science courses:
Coursera:
•	Cryptography I Stanford University
•	TensorFlow in Practice Specialization
•	Deep Learning Specialization (deeplearning.ai)
MongoDB University:
•	M101P: MongoDB for developers.
Python: asyncio, aiohttp, Sanic, Flask, WebSockets, RabbitMQ, Pandas, Scrapy.
Databases: Postgres, MongoDB, Redis, ClickHouse.
AI/ML/NLP: Tensorflow.Keras, OpenCV, spaCy.
DevOps skills: Docker, docker-compose, AWS CloudFormation.
AWS: EC2, S3, Lambda, DynamoDB, Route53.
SCM/Tools: Git, Redmine, Jupyter Notebook, Postman.
Experience with third-part API: SalesForce, Zuora, Cloudflare, telegram, Google PlacesAPI.
OS: Windows/FreeBSD/Linux advanced user.
Software development: SDLC, CI/CD.
I am looking for a Data Engineer/Python Software Engineer job in a small company or startup related to AI/ML domain.

Not interested in either Django or Fullstack jobs."
data engineer,"
Python Developer and Data Engineer.
Have experience in different ETL projects:
- Apache Airflow;
- Scrapy;
- Playwright;
- Zyte;

Data Science and ML:
- Pandas;
- numpy;
- visualization(matplotlib, plotnine, pygal, plotly);
- sklearn;
- Time Series Analysis and Forecasting;
- Computer vision(object detection).

Clouds:
- AWS(Batch, Lambda, Cloudwatch, Secrets Manager, System Manager, DynamoDB, S3);
- GCP(Google Cloud Dataproc, Cloud Storage).

Databases:
- PostgreSQL;
- SQLite.
"
data engineer,"
Python Developer:
Improving Python ETL pipelines, working with S3, Databricks, and PostgreSQL.
Redesigned Cassandra's data model to speed up users’ requests and improved DialogFlow agents for the
chatbot.
Added FBProphet model as an addition to the existing one to highlight prices trend and predict future prices of
assets. 

 Software Engineer:
Responsibilities: Design, build, and maintain efficient and reliable C++ code of cross-platform desktop
applications. Changing engine pipeline, supporting new and existing modules, working with
multithreading.
Python, Data Engineering, Big Data Engineering, AWS, GCP, Airflow, Databricks, SQL."
data engineer,"Positive feedbacks from colleagues and mentors. 
Motivated to work towards results. 
Quick learner.
Python QA automation (4+ years).
Student of Master Degree Program (Software Engineer).

EPAM Big Data Master program graduation.
Study project on building ETL pipeline (DataBricks, PySpark, Spark SQL).
Study project on machine learning (Scikit-learn).
Want to switch to from QA Automation to development, DS, ML."
data engineer,"recommender system for a big startup, computer vision subsystem to improve user experience
python, recsys, celery, puppet, opencv, computer vision, Rstats, hadoop, mongodb, nosql, solr, data mining, tableau
more data, more smart people :)"
data engineer,"
Querying and formatting data using SQL Server, Big query or Athena. Formatting data using Python, SQL or spreadsheets/Excel.
Creating dashboards in Power Bi. 

I have a high level of analytical skills and logical thinking. Can easily master new technologies.
Highly motivated for study and hard working.
"
data engineer,"
Research And Development Engineer
- Installed 10 V-sat sites that provide internet connectivity for people in rural areas.
- Built python script that automates the supply chain of the cards and shortens the consumed time from 4-8 hours to 4 min per month.
- Installed more than 10 solar systems in the rural areas in order to provide stable and clean
energy for our site.
- Built BI reports for the site monitoring .
"
data engineer,"AWS Certified Solutions Architect - Associate
Responsibilities:
• Develop data pipelines with low-code platform (analogue of AWS 
Glue Databrew), PySpark
• ETL processes optimization
• Ingest data from various sources (flat files, s3, relational db)
• Build data catalog with AWS Glue Crawlers, Glue Data Catalog
• Processing data with AWS Glue
• Define infrastructure with CloudFormation templates
• Maintain Tableau dashboards
"
data engineer,"
Roles I've worked as: Data Engineer, Python Data Engineer, Power BI developer

Industries I've worked for: Retail, Media & Entertainment, Insurance, FinTech, Legal Tech

Languages: SQL, Python

Databases: Snowflake, PostgreSQL, Redshift, Azure SQL etc.

ETL Tools: Airflow, dbt, Talend, SSIS

Reporting tools: Power BI

Cloud: AWS, Azure, Google

Typical task could look like:
- Designing Data Warehouse, Data Lake, OTLP DB
- Designing, implementing, testing and deploying ETL jobs
- Validating data integrity and consistency
- Reporting implementation and automatization
Not interested in: DB administration

Consider only remote positions and B2B collaboration (as Portuguese PE). I don't have Ukrainian FOP (PE)."
data engineer,"Two-time winner of the All-Ukrainian Mathematical Olympiad.
Roles: ML Engineer, Big Data Engineer, Data Science Engineer.
Worked in a tight collaboration with data scientists and stakeholders, in cross-functional and global teams.

Industries: telecommunications, retail, fintech, transportation, automotive.

Languages: Python, Java, Kotlin. 

Key technologies: AWS, Vertex AI, Azure Databricks, Apache Spark, Apache Hadoop, Apache Kafka.

Familiar with ML approaches and algorithms.

Mentored a team of 2 students, Big Data course.
"
data engineer,"
R,  Python, SQL
Reporting systems (Spotfire, Tableau, Power BI)
ETL piplines creating and setting up
Ability to grow as professional
High rate of Salary"
data engineer,"
R-Style Softlab Kiev LLC July 2016 — March 2022
Chief Specialist of the Analytical Systems Department
Responsibilities:
implementation and maintenance of an information and analytical system for building a
centralized corporate data warehouse, in particular, the implementation of modifications
to customer requirements that are not included in the distribution kit, development of
management reporting, development of technical documentation, overseeing the support
group.
Projects:
HamkorBank (Uzbekistan) - building management reporting (Qlik Sense);
BelAgroPromBank (Belarus) - support of the storage facility in trial operation,
development of IFRS (PL / SQL) reporting.
BelAgroPromBank (Belarus) - development of procedures for checking the quality of loaded
data, setting up mappings for loading data (Informatica Power Center), building IFRS
reporting (including using vba excel macros).
"
data engineer,"* Process automation / monitoring
* Team building / merging
SAFe Scrum Master, Team/Tech Lead
* Customer communications (technical questions).
* Technical analysis / software design / DB design / detailed design description
* Code review / automated tests creation
* New features development / change requests implementing / bug fixing / optimization and performance enhancement
* Source code branch integration
* Team management / team merge
* Progress reporting / time estimation

10 years in NDS Navigation Maps continuous delivery.
* Geospatial ETL (GTL) process development and support.
* Geographic and map data analysis, visualization, transforming for GIS.
* R&D of various data sources for SD and HD Navigation purposes.
* Automation

Tools and Technologies:
• Java, Python, 
• SOLID, OOD, Agile methodologies.
• SQL - PostgreSQL, PostGIS, SQLite
• JTS, OpenJUMP, ArcGIS, GlobalMapper, KML, OSM
• CI (Jenkins)
"
data engineer,"
SAS LOGISTIC — WARSAW, POLAND. 2020-PERSENT DAY
SALES MANAGER :
Prospecting and interacting with potential and existing clients to secure sales of company
products and services
Conducting negotiations with clients to identify their needs and requirements
Developing and implementing sales strategies to increase sales volume and expand the
client base
Creating and maintaining a client database to ensure effective sales management and
reporting
Preparing reports on sales, market analysis, and competitor activity to aid decision-making
regarding sales strategy

JSC ""ZAMOZHNE MILK TRADE"" - ZHYTOMYR, UKRAINE. — 2015-2020
I worked at a dairy processing company where I headed the sales department and created
a complete inventory system for raw material procurement, usage, inventory, production
forecasting, procurement, and sales using Excel.
My main responsibilities included:
Developing and implementing a system for tracking raw material procurement, usage,
inventory, production forecasting, procurement, and sales.
Organizing and monitoring sales activities, ensuring high-quality and timely delivery.
Analyzing data on sales volume, expenses, margins, and other factors affecting production
and sales efficiency.
Planning and coordinating raw material procurement, ensuring quality and timely delivery
to the company.
I am proud of my achievements in this position and believe that my experience can be
beneficial to a Your company I wish to join
"
data engineer,"Built data processing pipeline for mobile app analytics ""from scratch"".
Self-driven Data Engineer qualified in building end-to-end data processing pipelines. Proficient with  ""from scratch"" implementation of small and medium-scale managed workflows and actively contributed to developing large-scale systems with multiple automated and manual input sources. Experienced in the optimisation of data processing & data warehousing techniques. 

My background include 2+ years on healthcare industry project. 
Main expertise:
- Build end-to-end managed workflows with Airflow;
- Create and maintain different scale pipelines for data processing on Python and Scala;
- Implement cloud-based solutions for automation of data collection and warehousing using AWS Lambda, AWS S3 and AWS Redshift;
- Adjust ETL & ELT processes according to the needs of analysts;
- Create and optimise Spark jobs for processing large amounts of data using AWS EMR;
- Collect and shape structured data with PostgreSQL and Alembic according to business needs.
Looking for a project to utilize my knowledge and desire to build and make things data-driven. 
Nice to have: possibility to work with ML."
data engineer,"
Senior Business Intelligence Consultant, Paladin Bilisim Hungary 2022-
POC implementation covers a complete assessment of the client’s data with the goal of understanding the primary business requirements
Ingesting data from different sources, design a data model and create awesome insights using PowerBI and Microstrategy

Senior Business Intelligence and Reporting Specialist,
 Assicurazioni Generali S.p.A Turkey 2020-2022
Understanding of business requirements
Designing Etl process with SSIS
Designing reports that display insurance KPI's on SSRS, PowerBI and TIBCO Spotfire

Doga Insurance 2019-2020
Understanding of user data requirements, analysis of source data and designing data resume_classifier to support analytic reporting.
Designing an ETL process with SSIS.
Designing reports and dashboards that display insurance KPIs such as Average cost per claim, Policy sales growth (%), Loss ratio on IBM Cognos

Obase  2014-17
Creating Microstrategy BI application reports and dashboards. And managed Microstrategy System Administration tasks many sectors such as telecommunications, textiles, insurance
Turkcell (IBM Cognos to Mstr Transfer Project) 

Data Science Earth"" is a volunteer community - Insurance research working group leader 2020-
The working group conducts data science studies in insurance with public data sets and works with Python using Numpy, Pandas, Seaborn, Matplotlib, ScikitLearn, Keras libraries.
Safe Driver Prediction: Aimed to detect if the driver is reliable based on certain attributes using the 'Safe Driver Prediction' dataset using classification algorithms. 

Eötvös Loránd University Data Science Msc projects are
Projects Credit Scoring Project : Created a Predictive model which can tell us approve a loan application or not using Classification algorithms (DecisionTree, RandomForest, Support Vector Machine, KNearestNeighbors), and clustering algorithms(KMeans, Hierarchical)
Anomaly Detection in Securing Water Treatment System: Detected anomalies in the SwAT water treatment dataset using SVM, KNN, DBSCAN. 
Emotion Recognition: Built and trained a multimodal deep neural network for emotion detection using tf.keras. (Vgg16, Resnet50)
"
data engineer,"Built terabyte scale data pipelines
Senior Software Engineer with 5+ years of experience in building trading applications and data pipelines, tackling challenging architectural and scalability problems in Finance. Recently helped COMTEX with Terabyte scale data pipelines.
- Collected 8 years of experience working as a software engineer
- Experienced in working with a variety of clients and deadlines
- Managed and led a team of skilled software engineers
- Maintained a remote work role for 3 years
I am interested in small team and startup projects."
data engineer,"
skilled data engineer with more than 10 years of commercial experience, including projects in automotive (maps assembly, Location insights choice for electric vehicles stations), marketing (mobile attribution, ads extraction + analysis, search analytics) and telecom domains.
"
data engineer,"
Skills:
- Languages: Bash, Python, HiveQL, PL/SQL, Scala (Spark)
- ETL-orchestrators: Jenkins, Airflow, Streamsets, Tidal Automation
- RDBMS: Postgres, MySQL, Oracle
- Hadoop ecosystem: Hive, Impala, Kudu, Sqoop, HDFS, YARN
- Snowflake
- Kafka, Zookeeper
- Spark
- a little bit of AWS: DynamoDB, S3, Athena
- git/svn
- Tableau

Work experience:
Data Integration Engineer
GameDev Company // Nov 2021 - present
- maintaining DWH & ETL processes (up to 2 petabytes of data daily)
- integrating data from Kafka, databases, and APIs to Hadoop/Snowflake
- query tuning & optimizing jobs (up to 10x faster)
- creating internal tools for metadata collection

Data Analyst
Startup - Mobile Apps // Dec 2020 - Nov 2021
- optimizing and maintaining internal BI system using python and Apache Airflow
- working with databases (PostgreSQL, DynamoDB)
- creating forecasting algorithms and improving the accuracy of the existing ones
- preparing analytical reports (Tableau)
- conducting researches

Education:
Master's Degree (currently pursuing) - University of Vienna, Faculty of Computer Science
Bachelor's Degree - Taras Shevchenko National University of Kyiv, Faculty of Computer Science and Cybernetics
- preferably startup/product
- diversity of data-tools
- flexible work schedule"
data engineer,"
SoftServe - Junior Data Engineer - January - October 2022 

Web and mobile appication for managing security system
Project Description: Project is a Web and mobile application for managing, tracking, and supporting of security systems, maintaining user roles, and permissions for large businesses. App is being assed by external AKQA team, and based on that we have a brand new development stage using GCP.
Client is a US nationwide company that offers home security monitoring, equipment and installation services. It is recognized as the nation’s premier full service security provider, offering security services to residential, business, national account, and integrated system customers including professional installation, a high level of ongoing customer service and company owned monitoring centers to ensure a rapid response. 
Customer: US company
Involvement Duration: 9 months
Project Role: Junior Data Engineer
Responsibilities: 
Bug fixing
Migration data from SQL Server to BigQuery
SSRS report deployment
Perfomance/data usage refactoring
Project Team Size: 14 team member
Tools & Technologies: MSSQL Server, Visual Studio, GCP, SSRS, Scrum , Sourcetree.


SoftServe IT Academy – Data Engineer Internship(LV-663 DB) – December 2021-January 2022 

    Internet shop database 
Project Description:    SQL Server Database for project. Development with more emphasis on database 
design. Development of features including transactions, stored procedure, functions, triggers 
for consistent product quantity, dynamic product rating based on reviews etc. 
Customer:    SoftServe IT Academy
Involvement Duration:    2 months
Project Role:    Data Engineer
Responsibilities:    Requirements analysis and clarification; 
Database design; 
Database development and deployment script 
Project Team Size:    5 team members
Tools & Technologies:    SQL Server, SQL Server Management Studio

SoftServe IT Academy – Service Engineer (LV-637 SE) September-December 2021
"
data engineer,"
Software Developer with more than 15 years of experience designing, implementing, and supporting software solutions.
 
• Design and development of OLAP, OLTP systems.
• Data migration from on-prem systems to the cloud
• Design/development ELT/ETL processes
• Enterprise datawarehouse design/development
• Data harmonization, normalization and integration

• Azure Data Factory
• Azure Sql Database 
• Azure Data Lake Storage
• Azure Synapse Analytics
• Databricks
• Delta Lake 
• Oracle
• PL/SQL
• MS SQL Server
• T-SQL
• SSRS
• SSIS
"
data engineer,"
Software developer working as data engineer implementing both streaming and batch data processing with Spark, Kafka, Airflow. Have experience with Elasticsearch as core search engine on big eCommerce project.
"
data engineer,"
Software Engineer, inspired by developing in IT industry, and coming with a strong background in industrial engineering, product design and project management.
Experienced in elaborating product strategies, developing project procedures for automation and testing, and team management, I also have a master’s degree in industrial management with an orientation to market research and user experience analysis, and a PhD in industrial engineering where I elaborated an own Product Design Methodology for a smart orthosis – both, hardware, and software parts. 
The ability to adapt and fast learn allowed me to access execution positions in different projects through which I acquired significant knowledge of product management, risk management, quality assurance, best practices for developing and implementing procedures for optimizing internal processes and achieving specific organizational objectives.
"
data engineer,"
Software engineer with 8+ years of experience in the IT and Big Data world. Participated in many projects with different technical stacks (Java, Scala, Big Data & ETL) and business domain areas (e-commerce, agriculture, telecommunication e.t). As an engineer, I am always trying to keep a balance between tools and business needs. Believe in Scrum and teamwork. Musician.
Looking for an opportunity in a product company that can move our world forward. Maybe something that relates to the health care domain with AI/ML. No gambling products. Wants to work in the office with people together."
data engineer,"Developed and delivered a system that notifies a marketing team about incorrectly created or modified user segments.

Created a dashboard that shows real-time ETL job statistics and saves time by not gathering that information manually.

Developed and delivered an automated ETL pipeline that pulls data from a third data source, transforms it according to the marketing team's requirements, and uploads it to BI.

Developed and delivered a report automation system that generates reports for specific requirements and sends them to the marketing team Email on a schedule.
Software Engineer with over 5 years of industry experience, including 4 years dedicated to data engineering. Proficient in developing and maintaining ETL pipelines, creating data migration tools for seamless infrastructure transitions, crafting Airflow DAGs, and populating different databases using third-party APIs and data sources while leveraging expertise in data engineering best practices.
I'm open to any suggestions, but the main thing for me is the opportunity to grow as an engineer."
data engineer,"Experience in working in the USA and Europe-based companies for a long term.
Created different ETL processes.
Working on SQL performance queries.
SQL query coding skills. 
Experiences in Qlikview and python.
Snowflake.
AWS Developer Certificated.
Experiences in data warehouse integration processes (ETL) and use various tools to perform a board range of data migration tasks. 
Able to do comprehensive reporting for a variety of data sources to meet various business needs.
Understanding of OLTP and OLAP.
Strong data analyzing, processing, validating and reporting skills. Proficient in developing, manipulating and testing data in the database according to the business requirements. 
Familiar with various businesses and financial related terms and concepts.
"
data engineer,"
Stack: Apache NiFi, IBM DataStage, SQL (PostgreSQL, Sybase IQ/ASE, Oracle, MSSQL), MinIO, AWS S3, Docker, Kafka, Linux, Trino (Presto), Apache Airflow, Python, PySpark, Hadoop, Hive, SSIS, Microsoft Office (Excel, PowerPoint, Access).

As Data Engineer (2+ y of exp):
Development of new ETL/ELT pipelines: uploading terabytes of data to DWH and Data Lake.
Development of data marts, procedures, functions.
Migration of existing ETL/ELT pipelines from to open source solution: code refactoring, performance improvement and optimization.
Development of core flows, which decreased the time to build similar ETL/ELT pipelines for uploading to Data Lake and DWH.
Performing PoCs and implementation of new tools & environments.

As Data Analyst (2+ y of exp):
Aggregation of the credit portfolio data from different systems.
Development and maintenance of credit portfolio data marts.
Analysis and dividing of credit portfolio by different segments.
Analysis of credit portfolio collateral property.
Calculating loans priority considering the loan segments, making of a distribution of credit portfolio among the collectors.
Building and automation regulatory and business reports, creating presentations.
Forming portfolios to provide Debt Sale deals, making portfolio applications for the Credit Committee.
Ensuring data quality.
I like complex and varied tasks and I'm sure that this is the most effective way to develop professionally and achieve success in getting new skills and learning new tools. I believe that I will be useful and able to use my existing and future knowledge in data engineering in projects that socially matter."
data engineer,"Built data lake ground up
Developed complex database solution in-house - cdc (change data capture) on cloud storage (s3)
Leaded transition from traditional data platform to data mesh
Staff Data Developer (Shopify - 09.2022 to 06.2023):
- Ensured data pipeline integrity and established best practices for data development.
- Mentored engineers and guided the data platform evolution process.
- Led the transition to a cloud-based infrastructure and migrated data resume_classifier.

Principal Data Engineer (SumUp - 05.2021 to 08.2022):
- Developed data platform roadmap and managed risk assessment and mitigation.
- Reviewed design documents and played a key role in shaping team performance.
- Transformed the data platform into a Data Mesh approach.

Software Developer - SRE (Google - 10.2019 to 04.2021):
- Monitored production services and optimized hardware resources.
- Resolved infrastructure issues and deployed code changes.
- Led the migration of monitoring services and implemented database-level isolation.

Big Data Engineer (OLX Group - 01.2018 to 10.2019):
- Built and maintained a data lake, ensuring GDPR compliance.
- Enabled data access democratization and implemented data governance.
- Optimized storage format and developed replication system for RDBMSs.

Data Warehouse Developer (Zalando - 02.2016 to 01.2018):
- Performed data modeling and developed ETL pipelines.
- Conducted performance tuning and mentored colleagues.
- Redesigned core dimension tables and ingested pricing data.

Data Warehouse Developer (Azercell - 01.2011 to 02.2016):
- Created and supported ETL jobs and developed data architecture strategy.
- Calculated data-mart values and ensured data quality.
- Provided online reporting support and facilitated real-time data streaming.
A job with complex challenges in a company which is doing meaningful business"
data engineer,"
Strong knowledge of software development processes In-depth knowledge of Oracle 10, 11 database, PostgreSQL, RDBMS concepts
DWH, OLTP
SQL, PL\SQL,PL\pgSQL Stored procedures, triggers, collections, partitioning, basic administration
Performance tuning, query optimization
ETL development
Python
AWS - Cloud Practitioner Certificate
C Builder, Delphi, C
XML, XSL – transformation.
CVS, SVN, GIT
JIRA, ALM
Intermediate level of English.
"
data engineer,"
• Strong knowledge of SQL, PL/SQL, PostgreSQL
• Experience in constructing DWH both 3NF and Star / Snowflake (cleansing and normalizing source data, transformation and loading with PL/SQL procedures)
• Experience in BI Tools: MS Power BI
• Development of multifunctional dashboards using various data sources, editing dashboards and issuing rights to view and edit using MS Power BI
• Familiarity with test documents creation (test cases, checklists, test strategy)
• Novice skills in test automation using Python
• Proficient in analyzing the efficiency and effectiveness of business processes.
• Participation in the discussion of business requirements and proposals for their improvement
• Skilled in assessing risks and control environments.
• Proven ability to provide recommendations for process improvement and risk management.
• High responsibility and reliability, good team relationships.
"
data engineer,"
Summary:

Data Engineer with 3+ years of experience. Extremely interested in building reliable data systems capable of delivering business value. Team player and respectful colleague, fast learner and attentive to details.

Professional Experience:

Data Engineer
Playrix - remotely from Georgia
Jan 2022 - Present
- Working with ETL based on Airflow and Luigi with notifications to Slack.
- Using data lake and data warehouse principles in AWS - infrastructure (s3 and Redshift).
- Using other AWS tools to optimize work of engineering: Athena+Glue for structuring raw data, Lambda functions, CloudWatch for logging, ParamStore for confidential data, DynamoDB and others.
- Set up a lot of APIs with different data formats.
- Developing full-stack systems for analysts to simplify work with various API using Django and Javascript (for example, to simplify the management of creatives, ad bidding rates, etc.)
- Using complex SQL queries, views and query plans.
- Using dbt as the main tool for working with tables and views.
- CI/CD in TeamCity and GitHub.
- Full-fledged work with Git (pull requests, code review, etc.).
- Kanban methodology in Asana.

Data Engineer
DominiGames - remotely from Georgia
Sep 2020 - Dec 2021
- Automated data flow from scratch, integrated main stores - Google Play, App Store, Amazon, Steam and Windows Store.
- Set up the data flow from the Adjust mobile attribution system. The data was exported to Google Cloud Storage and then uploaded to the company's server.
- Configuring API for advertising networks.
- Set up Clickhouse (at the time of employment, Postgresql was functioning in the company, which did not cope well with analytical data).
- Worked closely with Tableau to create dashboards and data sources for them.
- Interacted with Firebase and BigQuery.
- Trained junior data engineers.

Data Scientist
BELTEL
Feb 2019 - Sep 2020
Successfully completed projects:
- An optimization system based on a neural network that minimizes losses in the sugar production process. Bonus: got a master's degree from my university with this project.
- Recommendation system for retail to predict the demand of goods for the period of promotions.
- System for a food distributor for optimizing supplies by using product similarity.
- Analytical system for monitoring deviations from the set modes.
Tools used: PostgreSQL, Flask/Django, Python (pandas, numpy, matplotlib, seaborn), Machine Learning (lightGBM, sklearn), Deep Learning (keras), Dash/Plotly, Azure
"
data engineer,"
Support and develop ETL pipelines. 
Maintaining ETL process into DHW from client files;
Creating and developing data pipelines in Apache Airflow
Creation of reports for clients.
SQL query optimization.
Interesting and challenging projects, complex and non-standard tasks, professional growth/development of yourself, friendly team, coherent work process, adequate management, flexible hours"
data engineer,"
Tabular model developer:
- Writing and optimization of SQL queries (creating tables, views, functions, SELECT queries, Openquery, Linked Servers etc.) to load data into Tabular model
- Scheduled jobs administration, logs analysis 
- Using DAX to build calculation formula expressions for Tabular resume_classifier and PowerBI reports
- Implementation of complex calculation logic for business tasks
- Tabular model modification using VisualStudio, SSMS, TabularEditor
- Developing and maintaining TM structure using JSON
- Working with Azure DevOps boards
- Communication with foreign teams and POs
Professional growth, learning and using of new technologies, deep understanding of the company's product and specializations."
data engineer,"
Team lead | Kaspi Bank
Sep 2021 – Present
As a team leader, I assembled a team of 10 employees. Trained and adapted to the work environment. 3 of these employees passed the attestation of their skills in 2022 and got a job promotion.


Risk Specialist | Kaspi Bank
2018 - 2021 - Automobile Scoring system
Implemented developments in automobile scoring strategy.
- Provided thousands of automobile sale & purchase operation in Kz.
- Launched product changes, increased the conversion for 42% in 2021;

2019 - 2021 - Face verification system
  Developing and integraEng face verification system with face anti-spoofing service for mobile app, ATM and Cardomats.
- Decreased time spent to manufacture debit cards from 15 days to 1 min. 
- Prevented up to billions dollars of potential fraud attempts;
- Implemented online service for account opening due to COVID restrictions in 3 days. Thousands of people were able to get government support payments online in quarantine.
"
data engineer,"
Technical Support Agent (call-center, establishing remote connection with the customers in order to troubleshoot different problems on their computers - macOS, Windows).
Customer Experience specialist (anlyzing the technical part of the cases, where the customers requested their money back and left negative feedbacks due to the dissatisfaction with support team's work).
"
data engineer,"I established deployment process in Azure DevOps and taught team how to use it. 
I mentored junior and trainee teammates. I performed technical and code review of their tasks.
I found point of improvement in code structure and started refactoring process.
I configured Row-Level-Security for reports. I written complex DAX expressions and created data model using Power Query. I used Native Query to connect to databases and written complex SQL queries.
Tech Stack: SQL Server, Power BI, Python, Spark, Airflow, Git, dbt

Project 2: 
I created analytics for big Ukrainian IT comany. I wrote complex stored procedures to transform data (SQL Server). I created reports (Power BI), enhanced performance of existing reports. I established deployment process using Azure DevOps service and helped team with understanding git. I found points of improvement in code structure and began refactoring process.

Project 1: 
I worked on creating Self-Service BI solution. I communicated directly with customer. I created reports using several data sources. I offered my own designs and solutions to best answer business questions by reports.
Strong BI Engineer switching to Data Engineering. Currently I am studying Python, Spark and AWS to enhance my tech stack. Therefore I would prefer projects where I can practice and develop these skills."
data engineer,"
The development of new features as well as the introduction of changes to the existing functionality of software products, according to the terms of reference; 
solving problems associated with the use of previously developed functionality;

Technologies used:

T-SQL:
-Writing procedures for loading/unloading data using processing logic and validation;
-Creating different reports;
-Creating procedures for loading data into DWH from OLPT databases of the information system;
-Designing data warehouses, and also individual measures and measurements.

SSAS:
-Creation and modification of measures and measurements, including historical ones;
- Basic understanding of MDX.

SSIS:
-Create, Modification of ETL packages;
-Solving problems associated with the operation of ETL packages.

Auxiliry:
Git;
Jira.
"
data engineer,"
The latest project is ETL for processing data from gaming applications. We are building it using Airflow, Snowflake, Looker. GCP is used as a cloud provider.
Dataflow and Airflow are used for data processing. Dataflow jobs written in Python are responsible for streaming processing.
Airflow DAGs written in Python are used for batch processing.
All the BI dashboards are built with Looker.
The data are stored in Snowflake.
Also, we use FastAPI as starting point for some pipelines. There is New Relic integration. Terraform is used to manage GCP
infrastructure.
I lead a team of 9 people.
"
data engineer,"
The product is CIM System. Tune SQL scripts, design complex queries and user-defined functions using SQL, fix bugs, test and debug code.
"
data engineer,"Modeling and create DWH on MS SQL 2017. 
Data Warehouse architecture, modeling, creating. ETL, Merge, Slowly Changing Dimensions use.
Gained more experience for T-SQL.
T-SQL, PL/SQL
DDL, CTE, Windows functions, temporary tables, dynamic SQL, stored procedures and functions
Query plans. Query optimization. Indexes.
MS SQL Server 2017-2019
Microsoft SQL Server BI – Integration ETL, Reporting Services, Analysis Services
Data collected and processed, made Reports and OLAP Cubes  
Data Warehouse architecture, modeling, creating. ETL, Merge, Slowly Changing Dimensions use.
Some experience in  PHP, CSS, Java script
"
data engineer,"I am confident that achievements will be made in the near future:)))
Under NDA - Data Engineer
May 2022 - current
Develop pipeline which automates processes of data collection from multiple
data sources, cleaning, validation and analytics.

Vodafone Ukraine - Data Engineer
October 2021 - May 2022
Facilitated cross-functional team cooperation in business decisions making processes. Designed and implemented reporting & modeling the data to drive business solutions in the financial domain. Conducted business analysis and
research process. Created Quality Assurance tests.

LetyShops - Data Analyst Intern
January 2021 - March 2021
 Created dashboards in Google Data Studio about statistics on the incidence of Сovid-19 in Ukraine.

Self-employed - Mathematics Tutor
2019 - 2021

Goal:
Looking for a job of data engineer with an 
interesting stack: PySpark, AWS, Azure, etc and with opportunity in future will change Python to Scala.
Excel, Daily dashboards and reports."
data engineer,"
- updating database, creating stored procedures, views, complex queries, etc. (SQL, Python);
- data analysis (Python: pandas, numpy, plotly etc);
- ML algorithms (Python: scikit-learn, XGBoost, transformers, torch, nltk etc.);
- Creating and deploying microservices (Python, AWS, Docker, Airflow etc.).
"
data engineer,"In my first job, I had the opportunity to work with data from large well-known companies, which brings me closer to my dream job in the field of Big Data.
  Then I focused on the goal of becoming a Big Data Engineer, so I dove into the development of skills in this field (current job at DS company), practically gaining experience and preparing for Big Data.
VeraData - Data Engineer, 1 year (current)

- design and develop ETL pipelines
- data integration and cleansing
- implement stored procedures and functions for data transformations
- ETL processes performance optimization

For this jop I mostly used Python(Pandas/PySpark), Alteryx and SQL.

---------------------------------------------

PwC - Data Analyst, 6 mos

- parsing and further data processing
- building ETL workflows
- reporting the results of the analyzed data

For this jop I mostly used MSSQL, MS Excel, Alteryx and Qlik Sense.
"
data engineer,"Did some optimisation and found few flaws in the projects which saved big amount of money. 

Achievements:
- FLEX program. 1 of 200 people that were chosen for the exchange program and scholarship.
- Awarded scholarship for Ukrainian Catholic University by ELEKS.
- SpinOffHack Ukrainian Hackathon. 1st place out of 20 teams.
- M-Start startup competition. 2nd Place out of 30 startups.

Leadership:
- Organizing the charity half-marathon “Values Run” to raise money for kids with cancer from Lviv’s Chernobyl Hospital
- Leading university projects teams with 5-6 people (Problem Solving, Linear Algebra)
- Moved for a year to study in US, live with host family and work on projects for FLEX

Certificates:
Udemy: Big Data Analysis with Scala and Spark, 2021
Udemy: The Complete Programmatic Advertising Course, 2021
Udemy: Streaming Big Data with Spark Streaming and Scala, 2021
Coursera:  Neural Networks and Deep Learning, 2020
Was working as Python Developer on various projects for 4+ years.
Mostly with Flask, PostgreSQL, CouchDB and various analytical tools (Altair, Pandas, NumPy). Also worked for about two years maintaining data pipelines and consumers with Python and various official/unofficial social APIs, have a good knowledge of the adTech domain. Have a solid experience with Docker, Jenkins, AWS.  
For the last year have been working with Scala, Spark, Airflow as my main stack for the adTech domain for an Ad Exchange Platform.
Want to continue working as a Data Engineer interested in trying GCP instead of AWS."
data engineer,"
With a background in data analysis and visualization, I've developed into a highly skilled data engineer with experience designing and implementing complex data architectures to unlock the full value of data. My ability to think strategically about how data can drive business value, while also being detail-oriented and capable of handling complex technical tasks, makes me an effective data engineering leader.

My diverse skill set includes a range of data engineering tools and technologies, such as DBT (data build tool), Apache Airflow, and Google Cloud Platform. I've gained experience in machine learning while building customized product designs and forecasting resume_classifier, and I have a deep understanding of data modeling, data warehousing, and ETL processes.

Throughout my career, I've tackled a range of challenges from building robust data platform and conducting data migrations to mentoring colleagues and conducting code reviews. I'm passionate about building effective data-driven solutions to help businesses grow.
"
data engineer,"
With over 2 years of commercial experience, I have a strong background in working with the AWS stack. I have successfully completed several projects that involved creating ETL pipelines, scheduling them and developing services for notifications. My primary programming language is Python and I have also worked on tasks that required rewriting Java code with AWS services. I am well-versed in writing PySpark scripts for AWS Glue and EMR and have a solid understanding of SQL. With my technical expertise and strong attention to detail, I am confident in my ability to deliver high-quality solutions.
"
data engineer,"For over 5 years of business expertise, I proved my skill in SQL script-writing and whole database work understanding from the university minimum to the advanced database administrator, who can deal with a great diversity of problems and needs that Data Engineer faces. I am ready to learn new languages/techniques and improve my own knowledge for the new job.
With over 5 years of experience in technical roles such as Data Engineer I have a strong background in SQL, T-SQL, Microsoft SQL Server, and databases, SSIS, Azure Datafactory, Tabular, and have also worked with other technologies such as REST APIs, JSON, XML, XSLT, and Git. Experienced in developing, optimizing, and maintaining stored procedures, triggers, and views, as well as in preparing technical specifications and program and data flow diagrams. Skilled in using various BI and reporting tools, including Tableau and Microsoft Power BI, and have experience in Agile methodologies and working in cross-functional teams. Demonstrated ability to collaborate with stakeholders and global teams, and have successfully facilitated enterprise implementations and product releases.
I definitely want to continue working with Databases and SQL, it is my passion. But I also want to develop myself further as a Data Engineer/Scientist with Python knowledge. I would be glad to work with Power BI, Tableau or other data visualization tools, as well as with Data Factory and Tabular. I would like to be part of the communication function in my new position, like contact with clients and different business functions. I am looking forward to an interesting project which I can dive into."
data engineer,"Motivate students towards AI/ML applications
Identified a security problem in the organization and come up with internal solution
Contributed to increased revenue for the company through showcasing the work
Certificate on AWS cloud practitioner
Wolfram verification for Cardano on chain data
Setting up dashboard with metrics and scheduling for postgres-bigquery comparison analysis
Data migration using dbt/trino
Develop example queries and graphs in Data Studio for bigquery
CI/CD workflows setup for Pull request( PR) alerts on slack channel
Apply test driven development (TDD) and refactor codes accordingly
Data quality check on tables in database
Provide knowledge base on entities relationships and dependencies
Optimize queries that tend to take up too much resource
Set up scheduled cron jobs in Pgadmin and Kubernetes
Apply monitoring and alerting for various metrics , databases, bigquery, pgcron in Prometheus and Grafana


Design and implement desirability ranking, performance ranking, status of saturation,number of blocks created, ROS(Return on stake) prediction of stake pools building customized ML model on AWS Sagemaker
Integrate customized model result from Sagemaker into AWS Quicksight for better visualization
Perform anomaly detection using amazon Quicksight
Identify address and fund distribution of byron and shelley for cardano blockchain
Classify UTxO ages based on addresses to distinguish the number of old ADA holders from new ones using db_sync data source
Carry out behavioral analysis of client's users with google analytics
Extract cryptocurrency data from CoinGecko, CryptoCompare and CoinMarketCap using mixed analytics
Study patterns and movements of exchange accounts and their liquidity
Perform on-chain analysis on the Cardano platform
Extract data from db_sync source
Differentiate pool owner's withdrawal activity from that of delegators
Categorize stake pools based on their delegation size
Identify public and private rewards on epoch basis
Evaluate new delegation activity
Settlement desk validation/testing -> shift from Redshift to RDS
"
data engineer,"
Work as a Data Analyst more than 2 years in digital marketing and IT companies.
My job involves:
• Built ETL from different data sources (Google Ads, Display and Video 360, payments services, etc.) with Python and Jenkins
• Automatisation of processes in marketing and web departments with Python
• Create dashboards of financial and user data in Tableau and Data Studio
• Analyse data of the web users and create insights
More involve in Data Engineering tasks, with interesting stacks and tools (AWS, Airflow, Data Flow, etc.)"
data engineer,"•	Supported and monitored daily Data Processing
•	Support of existing Datalake (Adverity, Talend, GCP BigQuery) solution, issues troubleshooting
•	End-to-end data flows checks
•	Data issues investigation
•	Data transfer maintenance
•	Creating, fixing and improvement ETL's procedures based on Big Data solutions
•	Extension of ETL processes
•	Designing database schema according to data processing needs
•	Creating new Looker explores within LookML resume_classifier
•	Creating new explores, improving existing dashboards in Looker Studio
•	Improvement of existing ETL processes and data transfer
•	Loading data from different external systems into Data Warehouse (ETL) (Informatica, Netezza)
•	Data quality insurance
•	Development of Reports and Dashboards (MicroStrategy, Excel,  DB2, SSIS)
•	Providing Ad-hoc reports
•	Maintenance and Enhancement of existing standard reports
•	Working with external/internal requestors to develop and analyze BI needs
•	Data analysis and providing insights

Technologies:
•	GCP BigQuery
•	Adverity
•	Informatica
•	Netezza
•	DB2, AS/400
•	MicroStrategy
•	MS SQL Server 	
•	MS Excel
•	Git
•	MS Visual Studio (SSIS)
•	Data Warehouse, ETL
•	Jira, Confluence
Work Experience:

 - 01/2022 – 06/2023 - Data Engineer (Advertising)
- 10/2020 – 01/2022 - BI Developer (Finance)
- 08/2018 – 10/2020 - Data Analyst (Healthcare)

Experienced Data Engineer with a background in collecting and processing data from multiple sources into Data Warehouse as well as data analysis and visualization tools. Also worked as Data Analyst, contributing to data ingestion and reporting for notable clients in the Finance, Healthcare and ERP sectors. Adept at utilizing tools such as GCP BigQuery, Adverity, Netezza, Informatica, MicroStrategy, MS SQL, Excel. Demonstrated ability to deliver data-driven solutions and provide valuable insights to stakeholders.
Collaborative team player with good soft skills. Detail-oriented and committed to data integrity and accuracy with analytical, research and problem-solving skills.
Preferably looking for the long-term project."
data engineer,"
Work experience: 2.5 years
Samsung Electronics Ukraine R&D Center | Software Engineer | Mobile Commercialization for CIS customers

I am looking to make a career change and start working in the data science field.

In preparation for that, I finished many paid courses through which I worked with and obtained the following technologies/skills:

Programming: Python, R, SQL

Data Science Packages: pandas, NumPy, matplotlib, seaborn, sklearn, XGBoost, TensorFlow 2, Keras

Data Science Algorithms:
•    Classification (Logistic Regression, Naïve Bayes, KNN, SVM, Decision Tree, Random Forest, XGBoost)
•    Regression (Linear, Multiple Linear, Polynomial, SVR, Decision Tree, Random Forest, XGBoost)
•    Clustering (K-Means, Hierarchical)
•    Association Rule Learning (Apriori, Eclat)
•    Reinforcement Learning (UCB, Thompson Sampling)
•    Deep Learning (ANNs, CNNs)
•    Cross-validation (k-fold)
•    Hyperparameter tuning (GridSearch)

Tools:
•    PostgreSQL
•    MS SQL
•    SSIS
•    Jupyter Notebook
•    Tableau
I am very passionate about data science in general. My goal is to constantly learn and improve while applying my skills in practice. Currently, I am continuing studies in the field of ML/DL and aiming to grow fast."
data engineer,"
WORK EXPERIENCE

Data Engineer
Temabit | Dec 2021 - now
- Building DWH for analytical department of mobile bank project.
- Writing sql queries and procedures (MS SQL Server).
- Designing and developing data pipelines (Airflow, Python).
- Experience with MapReduce frameworks (PySpark).

Data Engineer
Avenga | Feb 2021 - Nov 2021
- Developing integration flows to connect different sources, mainly REST API, Salesforce, RDBMS, DWH, Cloud File Storage, etc.
- Migration of an existing integration solutions to the IICS.
- Experience with RDMBS (Oracle, SQL Server, Postgres, etc.)

JS/SQL Developer
FUIB| Sep 2020 - Feb 2021
- Risk strategy developing and optimization.
- Writting technical documentation.
- Building sql queries and procedures

Purchase Manager
Goodwine | Feb 2016 - Feb 2020
 - Analyzing sales and money turnover.
 - Keep track of retail industry trends and new products.
 - Negotiating contracts with third parties and suppliers.
 - Developing and structuring analytical reports.
 - Organization of the educational process of sales team

COURSES

 - Data Science & Engineering - DataRoot Labs 2018
 - C++ Basecamp - Global Logic 2018
 - Python Developing - EPAM 2019
 - Stepic / Coursera: Math, Statistics, Python / C++ Programming, NN
"
data engineer,"Have improved the internal organization of project – reorganized the process of sprint releases and backups for project essential things. Developed a new ETL process and implemented a new pattern for data cleaning. In addition, I implemented a pattern of interacting with the source in close to real-time - to improve the project in the future.
Working as a data engineer - ETL and DWH on Azure stack - (Azure data factory + MSSQL + Azure functions on Python).
Last year passed an internship in C# + Angular stack.
Have a couple of pet projects based on ASP.NET MVC + SQL and ASP.NET Core + SQL and Angular.
Looking for new knowledges, friendly team,  and field for growing as a specialist."
data engineer,"1) Formed Strong Team;
2) Developed 100% processes;
3) Deployed Airflow using with Git sync for Team;
4) The largest number of educational materials in Department;
5) Passed RHCSA, good sysadmin;
6*) Survived brain surgery.
Working experience: 6 years;
Great soft skills. 

2.5 years as Data Engineer: ETL, Informatica PC/DEI, Oracle/Postgre/Hive databases, Datamart Development, workflow orchestration, research and development. 
3.5 years as Red Hat certified System Administrator (teamlead, data center operations, DevOps)
Looking for new experience, projects and continuous development. True leader, reach the goals.
Professionalism is my top priority and advantage."
data engineer,"Certified Microsoft Azure and GCP Data Engineer.
Working with streaming data using PySpark, Kafka, Azure and SQL.
- SQL;
- PySpark;
- Azure;
- Python;
- Kafka;
- Microsoft Excel;
- Jira;
- R;
- Python;
- MySQL, PHPMyAdmin;
- Ubuntu (Linux);
- PHP, Wordpress, Symfony;
- HTML5, CSS3, jQuery.
"
data engineer,"
·	Writing Advanced SQL queries 
·	Finding errors from Stored Procedures and views
·	Creating new stored procedure or views for new part of business.
·	Optimizing queries
·	Analyzing advanced queries and rewrite them from zero
·	Creating new Reports from zero using SSRS, Report Builder, Quicksight, Tableau or PowerBI
·	Checking reports working correctly or not
·	Finding easy way to difficult solutions
·	Extract data using SSIS or ADF
·	Using SSIS multi tasks(DATA FLOW, Script, For Loop, For Each Loop, Execute SQL task, Execute Package Task, Execute Process Task, Sequence container)
·	Working with data using Python (Numpy, Pandas, Matplotlib and other libraries)
"
data engineer,"Second place in the republic at the logic olympiad
First place in the city at the Physics Olympiad
İn my first job ,İ worked as a SQL reporter at the bank. Our team was building a new Dwh and as a new employee,i prepared a report and analyzed the data using structured query language(Oracle SQL,Pl/SQL) .After a while, i learned Etl  and started receiving data from T24 core banking system and other bank applications .At that time i am working as a Data Engineer in one of the large banks of our country. There are a lot of big platforms and a lot of databases (Oracle  databases,Postgre,Mangodb,Core System,Cms etc). Data Warehouse project  is new and we are have already completed the project. All Data analytics and reporter already use our new project. Our tecnologies is  ETL (Sap Data Designer),Oracle Enterprise Edition and Oracle Bi. Now we support these project using Oracle sql ,Oracle Tuning and Etl . Our bank don’t allow to save their data on the cloud. That is why i am interested in Cloud technologies and i want Iearning Big Data deeply
"
data engineer,"
Oluchi is an experienced Data analyst / Data Engineer with over 8 years of history in the financial industry. She has successfully implemented and supported various Financial applications in several banks in Nigeria. Her core competencies are Database Development, Business Process Automation, Financial Reporting, ETL Development, Data Analysis and Visualization.

Projects: 
- Business Intelligence Solution – Implementation of On-premises Enterprise Datawarehouse
- Implementation of Azure Enterprise Data Warehouse 
- Operational data store migration from on-premise to Azure
- Monthly Performance Report

Role -  Data Engineer/Analyst
•	Design and build ETL pipelines to automate ingestion of data from various sources.
•	Maintain and optimize existing ETL workflows, data management and data query components
•	Maintained business intelligence resume_classifier to design, develop and generate both standard and ad-hoc reports.
•	Generated reports and dashboards for internal and external customers for business performance monitoring and business decision making.
•	Participated in project planning sessions with project managers, business analysts and team members to analyze business requirements and outline the proposed solution.
•	Worked with Project Managers to develop and execute project plans within assigned schedule and timeline.
•	Managed the development, maintenance, and integration of enterprise data warehouse and data reporting solutions.
•	Update business intelligence and data warehousing solutions to meet changing business needs.
•	Coordinated ETL processes to retrieve data from various data sources.

TOOLS & TECHNOLOGIES		
        Database:	MS SQL, Oracle, MySQL, Sybase
Software Tools:	MS Office Suite (Word, Excel, PowerPoint), 
BI Tools:	T-SQL, SSIS, SSRS, PowerBI, Azure Data Factory
Business Skills:	IT Projects, Requirement Gathering, Presentation and Training, Stakeholder Management, Business Plans, User Documentation

•	Microsoft Certified Azure Data Engineer Associate
•	Microsoft Certified Data Analyst Associate
"
data engineer,"
Data Engineer(Analyst) with 3+ years of experience in Data Integration.

- Interests: Data Analysis, Data Integration, Business Intelligence, Data

Engineering & Machine Learning. - Languages: Python(Pandas, Numpy, Matplotlib, Seaborn, Scikit-learn), PySpark,  Matlab

- Databases: MySQL, Postgres, , MS SQL Server

- Tools: SQL, AWS, GCP, Azure, Git, Athena, Redshift, Snowflake, MS Excel, Power BI, Tableau - Other: Data structures, algorithms, distributed systems, engineering best practices
"
data engineer,"I solved a data warehousing issue we had as a team when the data we got from excel was so disorganized, dirty and consist of numerous pages. I was able to solve the issue when I joined my current company after it has been an issue for a long time. The MD was so happy about it that he rewarded me handsomely.
Snapnet Limited 
Business Intelligence Developer/Data Engineer 
• Translated business needs to technical specifications. • Designed, built and deployed BI solutions as in SQL, SSAS, SSIS, Power BI for businesses 

• Maintained and supported data analytics platforms including Microsoft Power BI, Tableau. 

• Created data storage and data resume_classifier as in OLAP cubes. • Conducted appropriate data capture, data cleaning and data governance.
• Evaluated and improved existing BI systems. 

• Collaborated with teams to integrate systems. 

• Developed and executed database queries and conduct analyses. 

• Created visualizations and reports for requested projects. 

• Developed and updated technical documentation. 

SKILLS 
Programming: Proficient user of SQL (PostgreSQL), Azure synapse, data warehousing, SSIS, SSRS. SSAS, Sql server, Python (Pandas, Sci-kit learn, NumPy, etc.) and SPSS to leverage data treatment and analysis. 

Report writing: Able to transform conclusions on highly complex datasets into simple English for nontechnical audiences. Data 

Visualization: Advanced user of Microsoft Power BI, Python visual tools (Matplotlib, Seaborn) with the ability to create accurate and creative infographics and presentations. 

Statistical methods: Expert descriptive statistics, inferential statistics, regression resume_classifier and other classic analysis methods.

Teamwork: Committed to maintaining constant communication with departmental heads to ensure business goals are being met. 

Spreadsheets: Adept in the advance usage of Microsoft Excel (Power query, Power pivot, VBAs, Lookups) and google sheets for data analysis.
I look forward to a challenging environment where the priority is to solve tech driven problems of all sorts"
data engineer,"
I worked for 1,5 years in the ML Laboratory at UCU, where I performed tasks related to research, 3D computer vision, classical AI algorithms (including writing code in both Python and C++), and scientific paper writing. 
Among educational projects, I worked with Biomedical Signal Processing (more specifically, EMG processing with Python tools), Big Data (creating streaming architectures using various tools, such as Docker, Cassandra, Kafka, etc), Computer Vision (working on deep learning solutions for tasks related to MRI imaging), Geospatial data analysis and visualizations (mostly in Altair), Research (I have published papers). I also had some educational experience with microservices architecture, assembler, etc.

Currently, I am especially interested in prosthetics/orthotics development and biomedical signal processing fields, and research involving natural sciences in general.
I will not work in any company/organization which still works with russia or belarus, or which doesn't condemn the war they started against Ukraine."
data engineer,"Was able to migrate all on-prem backup data to the cloud for a Multi-national company (Bouygues construction) in Nigeria between 2020 and 2021.

Was able to rebuild the datacenter servers and network  for Bouygues Construction Nigeria after a fire incident in Feb 2021

Work in collaboration with France Office to rebuild the production servers in Nigeria for Bouygues in 2020, after a worldwide virus attack of Bouygues systems
Data Engineering:
1. Spin-up Kubernetes Clusters, loading of docker images (your APPs) that is meant to be exposed to the web using Google GKE
2. Use of minikube cluster to expose NGINX images (APPs) to the internet via service exposition
3. VEEAM backup data warehousing from on-prem Dell PowerVault robot system to the cloud.
4. Managing security of the GCP using IAM and provisioning of containers for data storages
5. Used Git and Github for version control
6. Can perform ETL of data from any and multiple data sources

Python Dev / Data analytics
1. Used python for all  my data analytics and visualization (also ETL)
2. Developed an ITSM ticket categorization model and exposed same to the web via a flask API
3. Developed a circuit breaker module for a client
4. Developed not less than 15 machine learning resume_classifier between the year 2020 and 2022 (in the following industries: medical, financial, construction, economy, academic, etc). Codes available online in my Github

Software Testing (QA/QC)
1. Testing of Simika School management system for Testlio freelancing
2. Functional Testing of Next APP for Digivante freelancing
3. Can also handle perfectly these automation test tools: Robot Framework and Playwright
Not working on anything for Adult sites and others can be talked on and revealed before contract is signed"
data engineer,"
I've been working with data collection and data aggregation for more than 4 years. Have 1-year production experience in the product company as a analytics engineer, and before that 1.5 years as a Python engineer for data collection
Experience in data manipulation, building pipelines, preparing data for ML resume_classifier, analyzing ML model's forecast results, building forecasts, and calibrating ML resume_classifier.
Have experience with plain Text, json, RestApi, csv, xlsx
From the soft skills: I'm a responsible, reliable team player, unconflicted, and always open to something new and not afraid of difficulties.
I'm a quick learner and I desperately wanna grow, so I'm looking for a project that can suit my ambitions and a team with whom I'd like to stick for 2 and more years
I'd like to work with GCP/AWS services, BiqQuery/Snowflake, python, pySpark
Also, I'd prefer propositions from Ivano-Frankivsk, because, after several years of working remotely from home, I'd like to have an opportunity to work from an office from time to time"
data engineer,"• Data Engineering Track certificate in DataCamp
• 84th top performer out of 2300 participants in the popular course on Machine Learning from ods.ai: mlcourse.ai
• Top-100 learner in educational platform stepik.org
• Top-30% in Leetcode contests ratings
• Several Math Olympiads during school time
Data Specialist who is interested in challenging data-driven tasks and looking for an experienced strong team with a culture to grow specialists and share knowledge. I am a fast learner who is ready to develop as a professional and effectively contribute to the company's development. 
Tech stack: Python, Apache Airflow, SQL, Google Data Studio.

Data Engineer | Feb 2022 – Present
• Maintained cloud-based DWHs, data solutions, storage platforms in Snowflake and AWS (Redshift, S3)
• Automated ETL pipelines and constructed DAGs making it easier and faster to fetch and deliver a data
• Increased complex pipeline execution speed by 4 times redesigning its load stage to take advantage of MPP in the Redshift
• Deployed custom connectors and sources on Airbyte which collect data from 3rd side APIs and websites
• Reduced the processing time of data parser script from 23-minutes to 2-minutes applying multithreading

BI Engineer | Feb 2021 – Nov 2021
• Built interactive visualizations, financial reports, tracking systems in Google Data Studio and Power BI
• Developed 4 new dashboards from scratch interacting with stakeholders and maintained 10+ dashboards in the collaboration with the team
• Implemented inventory management system based on ABC/XYZ analysis of clients, products, locations
• Documented the process of data preparation and dashboarding principles for an internal knowledge base
• Worked on descriptive analytics for different types of businesses such as sales, logistics, service, transport
Looking for an experienced strong team to join for long-term cooperation. I am ready to grow and learn becoming a more strong specialist. I expect mentoring from colleagues and possibilities to grow effectively as a professional. My expectations are: collaborative and supportive work environment, opportunities for continuous learning and professional development, and best practices and processes within the team/company.

Currently in Kazakhstan, Almaty. Open to remote/relocation."
data engineer,"
- 10 months (Tableau/Quicksight, SQL, Python, Apache Airflow, Amazon Redshift)
- 9 months of working as a data engineer(TT-SQL, Microsoft SQL Server, Python, Apache Airflow, Docker)
-  3 months (Python, Jupyter Notebook, Apache Spark, SQL, Matplotlib)
- 3 months retraining of data engineering, with Spark, Hadoop, Data Modeling etc.
- 9 months on the project as a backend python developer(Python, Flask, MySQL, Docker, Flask jwt extended, Boto3, logger, AWS(S3 bucket, Aurora DB), Git)
- 5 months of internship(Python, Flask, SQLAlchemy, PostgreSQL, Docker, MongoDB, Flask-Login, marshmallow, Git, PyTest, HTML/CSS , JS)
- Graduated in Computer Science in Ivan Franko National University of Lviv
"
data engineer,"
Worked on building internal and automated reporting system using Django.
Built a web scraping bot using selenium to monitor currency exchange rates and created a data pipeline that records and stores the data on a database.
Looking to expand my knowledge beyond Python to a more robust Language precisely C++ for building embedded systems.
"
data engineer,"
I have over 10 years of Data related software development and Project Management experience in IT industry. For about 4 years, I have been working as a Team Lead and Architect on various data integration projects.
Mostly my experience is related to relational data. Data design and development. Building, managing, and collaborating teams. Covering architecture roles. Most of the projects I have been involved with were MS SQL based initially and now i have solid experience in ADF/Snowflake/AWS Redshift projects.
I have been taking a role of Technical Lead on several projects.  I am experienced in Agile (XP) methodology. Have vast knowledge in Scrum.
"
data engineer,"I may explain ""how it works"" to the dev team. Yes, with examples.
As a DBA, I know how it should be, how it can be, and what the difference is. Yes, it's all about tradeoffs. And there are will be much more tradeoffs in case you want to scale your workload.
p.s.
Oh, there was a limit for the description's length. Ok, I fit it.
"
data engineer,"
- More than 10  years of experience in IT Industry , 3 in  Big data platforms having hands on experience in AWS Services

- Hands on experience on Data Analytics Services such as Athena, Glue Data Catalog 

-Performed the migration of ETLs from  premise Database  to AWS cloud using Athena, Kafka,Goden Gate  and Airflow,NIFI

- experience  of buildings ML projects with using XGboost , pandas and scikit learn

- Extract, transform and load the data from different formats like JSON, a Database, with using AWS Glue Spark

Hands on expertise with AWS Databases such as Redshift, DynamoDB 


Programming languages : Python ,SQL ,Pg/SQL
databases: Redshift , Dynamo,Oracle
AWS Services: S3,Athena,Glue,AWS lambda,  Step Function ,API getaway, Redshift ,Dynamo DB  ,
Others: Airflow ,NIFI ,Spark, Pandas, kafka,boto3,pandas,pyspark,aws wrangler
Looking for challenging projects where BIG data technology are used , opportunity for personal   grows"
data engineer,"
Over 6 years of experience in Software Engineering.
Over 2 years of experience in Data Engineering.

Interested in data processing and microservices. I enjoy working with data, pipelines, and various databases, at the same time I love building scalable and maintainable applications based on microservices. 

Skills: 
- SQL, Java, Python, JavaScript.
- Spark (Core, Streaming, SQL), Hadoop (HDFS, MapReduce) 
- Databases/Warehouses (PostgreSQL, MongoDB, Cassandra, ClickHouse, Apache Hive) 
- Kubernetes, Docker
- Spring Boot (Web, Data, Security), Reactive Programming (WebFlux), JPA
- Flask, SQLAlchemy
- Reactjs (Hooks API)
- gRPC, Protobuf, GraphQL, RESTful API
- Apache Kafka, Kafka Streams, Apache ZooKeeper, Schema Registry
- Apache Airflow, Apache NiFi
- Google Cloud Platform (Cloud storage, PubSub, Dataproc, Dataflow, Apache Beam, Cloud Run)
"
data engineer,"
Java - 4 years 
Scala - 3 years

Spark Core/Streaming/SQL, Gatling
Docker, AWS, Linux
Projects in Scala, preferably big data, high load, cloud, microservices"
data engineer,"
Skilled Data Engineer based in London with 2.5 years of experience working for outsourcing and product companies. Currently looking only for part-time projects (20-30 hours per week) as a contractor, outside of the main job. I'm not ready to be a formal part of your team and communicate with your customers through. However, if you're a product company, ready to negotiate about things described above. Not considering direct contracts as a Ukrainian entrepreneur.
My skills are the following:
Building architecture for ingesting structured and unstructured data using Athena, Amazon Glue Jobs and EMR Serverless;
Step-Functions workflows to implement complicated business logic;
Lambda functions (monitoring, orchestration, file manipulating, data processing, analysis);
REST and GraphQL APIs, using AppSync or API Gateway;
Using PySpark for writing ETLs;
Working with different column/row-oriented file types;
Implementing software using OOD;
Creating test automation frameworks and monitoring components for the data solutions;
Using Prefect and Airflow as data pipelines orchestrators;
Have strong theoretical knowledge of DWH, OLAP and OLTP;
Writing scripts for ETLs;
Docker;
Gitlab;
I don't care about the size of the company, just want to solve complex and complicated business cases as a contractor using cutting-edge big data technologies without micro-management, legacy and useless corporate things. Not passionate about mentoring someone."
data engineer,"
I have more than 9 years of BI and data management experience in different domains: finance, sales, gas&oil industry.

Was working as team member as well as single bi developer on a project. 

Have experience in building and development large DWH  - ETL and visualization of data in the reports.

Technologies (sorted by experience): 
SQL, T-SQL, Power BI, SSRS, Python, Powershell, Databricks.
I interested in Сloud solutions, so it would be great to take part in projects related with that technologys."
data engineer,"- Data analysis using SQL
- Data Modeling
- Building reports in Power BI
•	Experience of working with Excel, python3 and PostgreSQL/T-SQL (joins, subqueries, aggregates etc.)
•	I’m also familiar with SSIS
•	Basic knowledge of math statistics and Power BI
•	Languages: French, Polish, English
Ability to use my knowledge and develop my technical skills"
data engineer,"Business Intelligence  - Epam Academy
6+ years experience as SQL/BI developer / Data Engineer 

(Healthcare/Banking/OpenERP/Advertising)

Skills: MS SQL, PostgreSQL, Pentaho Report Designer, Pentaho Integration Server, Google Cloud Platform, NoSQL, Hive, Power BI, Tableau, Git, ETL/ELT, Visual Studio, SSRS, Stimulsoft, Informática, Power Center.
Solid understanding of BI.
Basic knowledge of C#, C++, JavaScript, HTML, CSS.
"
data engineer,"Created BI solution to track companies' KPI's from scratch.
I have experience in creating companies' Top and Middle management  BI Reporting and Data ecosystem from scratch.
Also I can help you setup regular reporting connected to you company valuation model or financial model. And then you and your management will be able to track performance, and act based on true actual information and company's goals.
I would like to create value  for the company by helping investors, C-level and management have a clear view of company in digits   and receive fare remuneration, including bonuses/stock options or profit share."
data engineer,"
• Experience in working with AWS (S3, Lambda, CloudWatch, Glue, Athena, Redshift, Quicksight )
• Experience using GSP
• Experience in working with Databricks (Spark)
• Development of ETL: Airflow, GCP scheduler
• Database design and implementation: Oracle, Bigquery
• SQL-tuning;
• ETL performance tuning;
• Development of Oracle PL/SQL procedures;
• Experience developing on Python(functional programming, OOP);
• Web page parsing using Selenium
• Experience using bash & PowerShell
• Experience using Linux
• Debugging
• Experience with Git
• Experience using Power Bi, Power Automate
• Troubleshooting, problem solving.
"
data engineer,"- Collaborated with cross-functional teams to identify and resolve data-related issues, providing support and guidance on data engineering best practices. 
- Designed and implemented data validation and error handling mechanisms, including logging and metadata tracking, to ensure data quality and maintainability. 
- Conducted performance analysis and optimization of stored procedures and ETL processes to improve query execution times and overall system performance. 
- Implemented and maintained Power BI dataset refresh processes, ensuring timely and accurate availability of data for reporting and analytics purposes. 
- Assisted in the migration of historical data from Excel to the data warehouse, developing data pipelines and table structures to accommodate the new data sources. 
- Contributed to the creation of onboarding and technical documentation, facilitating knowledge transfer and team collaboration.
As BI engineer I worked on the next tasks:

- ETL/ELT processes development;
- DWH/Data lake development / Designing;
- Relational Database migration / Designing;
- Data Visualization / Reporting;
As a data engineer, I would like to be responsible for designing, developing, and maintaining the ETL processes. Also to be involved in developing and implementing data storage solutions, data quality assurance procedures, and data analysis tools. In addition, I also have a passion for other data-related tasks, such as data visualization, machine learning, or data analysis. 
Overall, I am interested in exploring new technologies and tools that can help optimize data workflows and improve performance. I am excited about the opportunity to contribute my skills and expertise to the field of data engineering and to work with a team of like-minded professionals."
data engineer,"Worked as Teaching Assistant on Object-Oriented Programming course.
Created scalable architecture for data scraping.
Moved mining system from crontab to airflow.
Moved Data to s3 bucket for easy access and visualization.
Oracle Java 8 Certificate - Associate
Python developer/ 1 year / remote
- Data mining
- Handling POST requests for saving data and  for REST API authentication
- Continuous Integration on Crontab,  Apache Airflow 
- Saved Data in AWS S3
- Created Schemas in AWS Glue
- Querying Data with AWS Athena
- Used H2O AI GUI for Machine Learning tasks

Data Engineer/ 2 Years / Startup Company
- Creation of ETL data pipelines
- Data extraction with python (beautiful soup, selenium, boto3)
- performed data mining tasks 
- Transformation with python + PostgreSQL
- Created architecture for efficient data storage
- Worked on query optimization for creating various views for analysis
- Used Python to visualize data (with Altair, Plot.ly)
- Had experience with Docker / other builders
I`m keen on data mining, process automation, engineering, and visualization. Hope to find a distinctive project with passionate, hard-working team and friendly environment."
data engineer,"
Software engineer with 5 years experience in Java development (mostly e-commerce) and about 2 years experience in Data Engineering using Scala and Spark. Current role on the project - key developer (data ingestion team).
Related skills: AWS (Certified), Databricks, Data Lake, SQL, Kafka, Cassandra, Hazelcast, Spring (Boot, Data, MVC, Test, Security, WebFlux), Akka, Splunk, Maven, Git, Jenkins.
English skills: speaking - B2, writing - B2.
Looking for a contract with product company to apply my java/scala and data engineering skills."
data engineer,"Working on full life cycle of BI projects and their architecture.  
Working of Migration from SQL related technologies to BigData solutions.
Creating near real-time solution, based on SQL Server and SSAS Tabular.
Building CI/CD process for a project from scratch.
Building complex financial reporting, including deep analysis and business processes.
Working on Integration with marketing platforms: Google Ads, Facebook Ads, etc.
I have 11+ years of experience in developing database solutions, ETL processes, BI solutions and Reporting applications. I have a solid background in Big Data engineering, warehousing, data mining and complex reporting and planning systems. Very strong skills in analysis and planning of various data reporting applications as well as excellent knowledge of hardware and software needed to support such applications. I have good knowledge of SDLC and all of its stages.
Ideally Product company, ideally Ukrainian.
Ideally working on some cloud platform and maximum modern technologies. 

Project shouldn't be just to support current infrastructure, but develop something new, ideally helpful for people.

Can work as a Senior developer, but prefer to be a technical leader of the team, as I had such experience in successful project and can take such role."
data engineer,"
Improvements to current microservices related to card payments. Writing scripts to write files.
"
data engineer,"Confluent Certified Administrator for Apache Kafka
Microsoft Azure Fundamentals certification
DP-200: Implementing an Azure Data Solution (Data Engineer certificate)
DP-201: Designing an Azure Data Solution (Data Engineer certificate)
Microsoft Azure Data Engineer Associate
AWS Certified Cloud Practitioner
AWS Certified Data Analytics Specialty
Oracle Certified Associate DBA for Oracle 11g
Oracle Certified Professional DBA for Oracle 11g
Oracle Certified Professional DBA for Oracle 12C
Oracle E-Business Suite R12 Applications Database Administrator Certified Professional
Oracle WebLogic Server 12c Administrator Certified Associate
Oracle WebLogic Server 12c Administrator Certified Professional
Oracle Autonomous Database Cloud 2019 Certified Specialist
Over 12 years of Expert level Oracle, Big Data experience, Batch and Streaming system.

Over 12 years of Data Engineering with Oracle Databases, Data Warehouse, Big Data, and Batch/Real time streaming systems. Working on Cloud (Azure/AWS) and on-Premises Big Data/Hadoop Ecosystem/Data Warehouse, ETL, CI/CD and Visualization (Power BI, Tableau).
Personally love playing/watching football and swimming. DWH Methodologies: Inmon, Kimbal, Data Vault 2.0

Oracle Big Data Appliance, Cloudera, HBase, HDFS, YARN, Zookeeper, HUE, Hive, Spark, MapReduce, Impala, Apache Kafka, Confluent Kafka, Airflow, NIFI, Databricks, Delta Lake, Data Lake, Azure  Data Factory, Azure HDInsight

Cloud infrastructure setup/configure/support OCI, Azure, AWS, and GCP related to the DBA/BDA and Data engineering. 
CI/CD - Jenkins, Jira, Confluence, HP ALM, Docker
I'm looking for an interesting company in a good place. I would like to participate fast growing team."
data engineer,"Developed and maintained 80+ dashboards in Google Data Studio for a group of 10 companies.
I have experience in automation of routine processes, extraction,processing and visualization  of data from various services (Google Analytics, Hubspot, etc.)
Skilled in:
• Google BigQuery (1y)
• Google Data Studio (1y)
• Google Sheets (3y)
• SQL (1y)
• Python (Programming Language) (0.6y)
• JavaScript / Google App Script (2y)
"
data engineer,"
I am a junior data engineer with proficiency in SQL, Python, and ETL tools like Apache Airflow. I have
experience working with data warehousing and data modeling concepts. Additionally, I have experience
with data visualization tools like Tableau and PowerBI. I am currently workeing on developing ETL
pipelines using Apache Airflow to automate data extraction, transformation, and loading. I am eager to
learn new technologies and techniques in data engineering to contribute to the success of the
organization. I am a quick learner, a strong problem-solver, and a team player who is able to work
collaboratively with other members of the team to achieve shared goals.
"
data engineer,"
Experience : 
  
1. Data Warehouse Engineer in banking industry  ( fulltime )
 
    Since  september 2021 , I have been working as a Data engineer in the banking sector
 that also includes reporting , data analysis and BI tool support. I have enough experience 
 with Oracle database and
 within almost 1.5 year I have been a part of Data Management team and contributed a robust DWH of the bank. In order to
 provide ETL processing , Apache Airflow is the main tool I use.  One of my daily responsibilities contains Microstrategy BI tool support which includes satisfying 
 user requests and shaping a semantic layer behind BI processes. 
   I leverage high-level SQL  ,PL/SQL and python skills that enables me to automate repetitive tasks ,to write 
 complex ,high-speed reporting queries and to delivery demanded requests within shorter period of time.

2.  Data Warehouse Engineer in the Ministry of Science and Education of Azerbaijan (Remote/freelancer )

    Starting from August 2022 , I have been a part of this project as a middle data engineer . The project contains :
1 .  contributing an existing Data Warehouse :
2 .  integrating new data sources into DWH ;  
3 .  assuring data quality ;
4 .  Providing requested data by Reporting team ;
5 .  Establishing and monitoring ETL processes  and so on.


Skills :

Programming languages :T- SQL, Oracle SQL  , PL/SQL,  Python 
Databases : Oracle ,Sql Server 
Tools : SSIS , Git/Github , Apache Airflow , Microstrategy (BI tool) , Power BI 
Others : ETL ,DWH concepts , Data Modelling ,Data Cleansing .
"
data engineer,"•	Completion course “Программирование на Python” (by Bioinformatics Institute, Stepik), November, 2019
•	Completion course “Data Analysis: інтенсив для початківці” (by Ciklum), June, 2019
•	Participant of the Ukrainian students’ academic competition on Economic Cybernetics, April 24-26, 2019
•	Completion of English Language Dynamics Course by Michael Gott International, February, 2019
•	Taking part in National research and practical conference on “Innovative banking technologies and modern forms of money”, 2018
•	Successfully completion of a General English Course at Upper-Intermediate level (English Language School – “SpeakWell”), January 2017
Company: Multi-brand retailer in Canada
Duration: Dec 2022 – up to present
Role: Data Analyst/Engineer
Responsibilities:
1. Using AWS(Athena, jobs); 
2. Working on migration deprecated reports system to new architectural framework in the company;
3. Rewriting Python/MS SQL scripts for reports to tables/views in Athena;
4. Working with CI/CD tool – GitHub;
4. Building and improving dashboards in Tableau;
5. Direct communications with the stakeholders. 

Company: One of the largest U.S. distributors of medical devices
Duration: Aug 2021 – Dec 2022
Role: Data Engineer
Responsibilities:
1. Working with Cloud platforms (AWS);
2. Building ETL pipeline using AWS Glue workflows, triggers, and crawlers;
3. Writing complex SQL queries (Presto);
4. Reporting into Tableau with dashboards based on data;
5. Performing initial data investigation, transformation and exploratory analysis;
6. Using Python to manipulate SQL databases and data analysis. Data Analyst in ML engineering team.
Role: Data Analyst
Responsibilities:
1. Working on preprocessing, labeling data;
2. Development of graphical user interfaces for data analysis using Python patterns and Tkinter package;
3. Familiar with Python parallelism and GIL; working with multiprocessing, multithreading libraries;
4. Familiar with ML clustering algorithms and dimensionality reduction approaches.

Company: Ukrainian microfinance organization, Kyiv, Ukraine
Duration: Mar 2020 - Aug 2021
Role: Data Analyst
Responsibilities:
1. Automating regular reports in Grafana;
2. Writing SQL scripts for unloading the necessary data;
3. Data analysis, search for statistical relationships (PostgreSQL + Pandas);
4. Data visualization - graphics in Excel, Pandas, Tableau;
5. Economic analysis of clients profitability;
6. Create new rules for check information about clients.
"
data engineer,"
- Data Lakehouse design and implementation with a medallion architecture (bronze, silver, gold layers) using meta-data driven orchestration (Azure SQL/Synapse, Data Factory, Storage)
- Data Warehouse design and implementation, modeling and integration with Power BI on hybrid environment (Azure and on-prem SQL, Data Factory, various API)
- Data interfaces for Dynamics 365 
- Database versioning with Redgate Flyway
- Tableau sources integration with Hyper API
Part-time remote job"
data engineer,"
Hey! :)
I'm a data engineering and data analysis enthusiast, with almost 2 years of commercial experience. Besides that, during the past 4 years of my university studies I have worked on plenty of educational projects, which helped me to develop the key skills for working with data. 
I'm still advancing my knowledge by developing personal projects and learning more about data engineering technologies and tools, which we can discuss as well!

Commercial projects:
Data Warehouse Engineer / BI Engineer
* Developing AWS QuickSight visualization dashboards and
analysis reports for delivering analytics and insights to the end
user;
* Built Python scripts for more automated deployments and
migration of the QuickSight dashboards across multiple working
environments;
* Achieved significant improvements with the dashboards
deployment duration and reduction of manual steps involved;
* Features investigation, technical documentation writing and
team training.

Educational projects:
Bachelor Thesis Project at Ukrainian Catholic University
* Built ETL/ELT pipelines for creating a comprehensive dataset of
Ukrainian news articles for modelling and analysis;
* The pipelines worked with data from parsed news articles and the
Rest API for more detailed features retrieval;
* Was able to apply classical NLP approaches to analyse the textual
data — clustering and Text summarisation.

Movies Analysis Project (Completed as a part of the Programming course at Ukrainian Catholic University)
* Built an ETL pipeline for the IMDB dataset containing the
information on all the movies existing in the IMDB resource;
* Optimized the transformation process and was able to reduce the
time taking to load in the data;
* Created an insightful report, which included statistics on movies
from the IMDB dataset.
"
data engineer,"I`ve also finished next course and got skills:
1.OOP Java Core (SoftServe IT Academy)
Java SE, OOP, Fundamentals, Classes, Collections, Threads, JDBC , Error & Exceptions, I/O
2. Java Enterpise (Hillel School):
Servlet, Java EE, Spring Core, Spring MVC, Spring Data, Spring Security
3. QA Manul (Hillel School)
Trainee Data Engineer
(EPAM Systems, August 2022 - November 2022)
• Used Hive SQL, including internal / external tables, partitions, buckets and different types of joins;
• Get acquainted with Spark RDD and DataFrame structures;
• Worked with streaming data using Kafka and Spark Structured Streaming;
• Converted data to different formats with Spark;
• Transferred data to Kafka using Spark Streaming;
"
data engineer,"I was doing three USA Data Engineering Projects successfully with my teammates, which focused on Pure Data Engineering Projects
--Collecting Various Types of Data from Different Sources, Transforming, and Loading to Data Warehouses
--building data pipelines to ingest, transform, and load the data into data warehouses using Azure Data Factory, Python, and Talend.
--visualizing data based on the nature of the data and the respective chart type to get insights easily for the customers.
creating and publishing dashboards and reports in various data visualization tools.

Tools that i have used so far: 
Power BI,Azure,Azure Data Factory,Azure SQL,Snowflake,Python,SQL Server,SSRS,SSIS,SSAS,DBT,Excel and PostgreSQL
"
data engineer,"
I have 4 years of experience working with large sets of data (databases - Oracle, PostgreSQL, MySQL, MongoDB) using Python and its libraries such as pandas, NumPy, Matplotlib, PySpark. I'm familiar with data warehouse architecture and ETL patterns. Also have working experience in Web3.
"
data engineer,"As a academic, I can mention PhD in Information technologies, over 30 papers in peer-reviewed journals, participation in large European research projects

However I would say, that my main achievements are the ability to learn, to see the overall picture or patterns in a set of separate fragments, to find a non-standard point of view at the problem
Over 10 years of experience of work with data related to social systems and science as a very special system (publication data, temporal logs, downloading statistics, etc). 

Participated in 3 international research FP7 projects (precessor of  Horizon programs). Was a mamber of a core team responsible for writing and submitting 4 proposals for Horizon 2020 - research programs by EC

Experience of teaching and supervising at the university

Great experience in oral presentations and data visualization (in particular using complex netwoks framework)

My background is related to computer technologies in publishing - therefore, I am familiar also with design principles, color theory etc.
I would love to work with data and data visualization. Remote or part-time position is perfect for me, but I can consider also full-time job at office in Lviv - if this means an active work in a good team."
data engineer,"
I worked at Ministry of finance as mid economist about 1 year.
Then I moved to State Revenue Committee and worked there as
leading statistical specialist about 2 years. Here I used tools like Eviews and Stata, and also had a little project in ML (classification algorithms, DT and RF).
Currently I am working in Questrade Inc. company as Data Engineer/ Data Analyst and using the related tools like T-SQL,SSRS,MySQL,BigQuery, PowerBI,Python.
I have an experience of using Jira board and Git repository
"
data engineer,"
Data Analyst (financial/healthcare domain) 07/2021 - Present
Data Modeling;
Looker Modelling;
Communication/collaboration with stakeholders;
Documentation maintenance/keeping up to date;

Analyst (supplies domain) 09/2017 - 07/2021
Visiting retail outlets;
Сontrol of product representation and equipment availability;
Presentations for customers, setting goals, and holding meetings;
Collaboration with distributors on an assortment; 
Research on Competitors and Marketing Analysis; 
Control from a policy perspective;
Managing a small team;
Research of premises and control of existing ones;
I want to have a modern data environment to enhance my skills and continue practicing w/ different data concepts. Would be cool to have a some kind of mentor."
data engineer,"
I have 3.5 years in Data Analytics/Data Engineering.
I am located in Lyon, France now.
But I am going to change some another country for life to autumn.

My main tasks at work were:
- AB tests, analysis and conclusions
- Segmentation of customers and products. RFM / ABC analysis.
- statistical evaluation of tests (P value)
- dashboards on the main metrics of the company such as: Retention, ARPU, MAU, DAU, cohort analysis
- analysis of the company's shares
- Automation of ETL processes using R/Python.
- Working with databases: Maria DB, MySQL, BigQuery: window functions, nested queries, subqueries.
- Automation of reporting and distribution of messages about changes in key metrics using RMarkdown
- Working with Git to track code versioning
- creation of virtual machines in Azure DevOps.
- worked with Docker to test production services locally

Main Tech Skills: 
- Python (pandas, numpy, pyarrow, pysftp, matplotlib, seaborn)
- R (data.table, tidyverse, lubridate, purrr, ggplot2, stringi, RMarkdown, RShiny)
- PowerBI
- Data Studio
- SQL(MySQL, MariaDB, BigQuery)
- Microsoft Office
- Git
- Basic Docker
- PySpark
- Databricks
Without overtimes."
data engineer,"
Data Engineer with SQL, Python, GCP and Snowflake hands-on experience.
Key competencies: 
- modeling and development of data lakes and data warehouses
- batch and streaming code-based ETL/ELT pipelines
- data quality and reconciliation automation
- development and deployment of BI solutions
"
data engineer,"
Recently Completed Projects:


1. Report Building | December 2022 - February 2023
Description: The project's main goal was to build and update Tableau dashboards. The main responsibility was to create a couple of dashboards using data that represents employees' activities and occupations.
Environments: Tableau
Position: BI Developer
Activities:
• Employee activities analysis
• Fixing bugs
• Report building


2. Computer Vision Accelerator | October 2022
Description: The project's main goal was to create and support a Computer Vision system. It should recognize objects in a photo and label them. The solution included a web application, which was built using Python and PostgreSQL, and also some of the AWS services were used for storing data and ML.
Languages: Python, PostgreSQL
Environments: pgAdmin, VS Code
Position: Software Developer
Activities:
• Back-end development
• Fixing bugs


3. Drools Development  | July 2022 – September 2022
Description: The project aimed to improve the existing system of processing business data. The client had a list of rules according to which transaction files were validated. My task was to create new rules in compliance with the received instructions and update some of the existing rules in case of outdated instructions.
Languages: Drools
Environments: MS SQL Server, VS Code
Position: Software Developer
Activities:
• Creating and testing drools
• Fixing bugs


4. Data Engineering (internship) | January 2022 – April 2022
Description: Course for those who planned to start their career as a Data Engineer. I was learning such technologies as Hadoop, Spark, MapReduce, Airflow, AWS (Lambda, S3), GCP (Cloud Function, PubSub, BigQuery). 
Languages: Python
Environments: MS SQL Server, VS Code
Position: Trainee Data Engineer
Activities:
• Learning theory
• Building pipelines
"
data engineer,"
Data Engineer/Data Analyst/Data Modeller (financial/healthcare domain) 07/2021 - Present
Data Modeling;
Looker Modelling;
Communication/collaboration with stakeholders;
Documentation maintenance/keeping up to date;

Analyst (supplies domain) 09/2017 - 07/2021
Visiting retail outlets;
Сontrol of product representation and equipment availability;
Presentations for customers, setting goals, and holding meetings;
Collaboration with distributors on an assortment; 
Research on Competitors and Marketing Analysis; 
Control from a policy perspective;
Managing a small team;
Research of premises and control of existing ones;
"
data engineer,"Started and designed several projects as a data architect. 
Leading team of data engineers including discussion with business analytics and customers.
Data Engineer / Data Architect with solid experience in Big data 
and cloud technologies.
Have experience in presenting and delivering projects from scratch beginning from the PoC stage to the production deployment and support.
4+ years of experience in Data Engineering, Data architecture.
3 years of experience as a Java backend/full-stack engineer 
Latest projects related to Big data:
Design and start additional subprojects related to near real-time data processing. 
Involvement in pre-sales activities as a data expert.
Professional growth, interesting projects, and technologies.
Interested in involvement in data architecture, data design, and data pipeline engineering."
data engineer,"
A highly skilled and motivated Data Engineer with focus on MS SQL Server and Architecture design offering a combination of cross-functional skills with 16+ years of  experience in the IT sphere as a DBA, DB Engineer, Data Architect, Team Lead.

Key Responsibilities:
Team management and mentoring UP to 11 people (DBA, DBD, QA, BI)
Code review. Unify code style
CICD implementation
Data storage optimisation 
Big Query data transfering
GCP data transfering
Creating Reports on SSRS;
SSRS server setting up and management
Creating and supporting dashboards in Qlikview/QlikSense;
Qlikview/QlikSense server management
Configuring ETL process
SQL Query optimization
Data recalculation and migration
MS SQL server replication setup and management
Failover cluster setup
Database BackUp/restoring

 2021 - present time, Data Engineer Team lead
	Team: Up to 11 people (DBA, DBD, BI, QA)
	
New data model for analytics:
One data structure for Big Query and MS Sql Server
Increasing performance for data query 10+ times
Decreasing query complexity

Big Query:
Data Uploading
Data Monitoring
 
 2018 - 2021, DBA Team lead
Team: Up to 4 people (DBA, DBD, BI, QA)
Interviewing, mentoring
Code review 
DBA Ms Sql Server support:
Up to 20 servers
100+ databases
Setup, back up, restore
Backup automatisation

MS Sql Server Replication:
Setup, support
Up to 7 servers in one system
Hundreds of tables
Single destination DB up to 7 TB

QlikView: Support

QlikSense:
Qlik view to Qlik Sense report migration
Setup
Reports - 2+ Billion Rows
New data model for user filtering:
200+ dimensions, 300 mln rows
30+ tables as datasource
6000 result reports per day (up to 50 mln records each)

Data migration:
For scalability PK was changed in the 20 biggest tables
Full data migration to GCP virtual machines
QlikSense migration to GCP
GDPR implementation

2014 - 2018, DB Developer
	MS SSRS:
50+ reports 
Version control, CICD implementation
Simplifying subscriptions. 200+ nightly subscriptions

Data model optimisation and refactoring
"
data engineer,"•	 Created ETL processes (extraction, validation, transformation)
•	 Developed database object: stored procedures, UDF, calculation views, complicated queries.
•	 Developed Data Bricks notebooks and workflows 
•	 Implemented and supported Spark structured streaming process (Event Hub & Data Bricks) 
•	 Designed and implemented custom log structure (Dynatrace & Azure Log Analytics) 
•	 Deployed and maintained snapshots in Snowflake
•	 Designed and implemented of ADF pipelines 
•	 Designed Azure Data Lake structure
•	 Performance tuning
•	 Troubleshooting
7+ years’ experience Data Engineering
•  Building Data Lake House;
•  Building Data Warehouse;
•  Implementation of data quality process;
•  Creating ETL processes from different sources;.
•  Implementation  BI solutions 
•  Migration data to cloud
Building from scratch Data Lake/ Data warehouse.
Data migration, validation, modeling, transformation.
Work with streaming data."
data engineer,"Oracle SQL Certified Expert
IELTS (6.5)
Migration of Core Banking System
Modeling and implementing new functionalities for core banking system according to business
requirements
Making appropriate changes to existing functionalities Troubleshooting functionality issues
Stress testing and optimizing of the functionality
Used technology : Oracle PL/SQL

Created ETL jobs for data migration from different sources to DWH based on Sybase IQ
Wrote new SQL queries and optimized existing ones for ETL jobs
Created analytical and online reports using Oracle BI Publisher and Tibco Spotfire
Created Dimension and Fact tables, implementing star and snowflake schemas
Created data marts for use of various business departments Participated Credit Scoring Project of
McKinsey & Company
Used technology : Oracle, Sybase IQ, SAP Data Services Designer, Oracle BI Publisher, MS-SQL
Tibco Spotfire, Jira

Participated in DWH (Data Warehouse) development for Central Bank of Azerbaijan Republic
based on Oracle DB and created analytical reports Implemented ETL processes using ODI (Oracle
Data Integrator)
Created reports via Oracle BI Analytics
Used technology : Oracle Database, Oracle Data Integrator, Oracle BI Analytics

Automated customer contract forms using ABAP programming language Used technology : SAP
ABAP
- Interesting data related challening projects
- Friendly team"
data engineer,"
5+ years of professional experience in Banking and Health-tech product companies;
- Development of DB solutions from scratch in accordance with business requirements (created and maintained procedures, functions, views, and triggers that are responsible for the sales planning application business logic).
- Development of data migrations between databases such as PostgreSQL, FireBird, and MS SQL using Python and linked servers.
- ETL process development: obtaining data from AWS S3 and saving it into PostgreSQL.
- Obtaining JSON data via external API using Python, parsing and saving data into PostgreSQL.
- Development of reports processing automation pipeline using Python.
- Query Tuning and Performance Optimization.
- Engaged in PostgreSQL administration (backups, server settings, jobs, crontab).
- Worked in the bank's IT department. Was engaged in data quality checks, ETL monitoring, functions, and procedures development (MS SQL Server).
"
data engineer,"I have an impressive track record of writing code in Python, moreover I've had experience working on mini-projects that involve various narrower specialties but all in the same Python. 
Equally important, I have several certificates that I obtained through self-study, namely two in Python, and one in SQl.
I also worked with SQL and Python as part of my thesis work.
A hard-working and creative developer with a year of experience in solo projects, as well as teamwork experience with developers from completely different fields. Responsibility and proficiency are the best way to describe me, not only my willingness to learn and study, but also my work experience as a non-experienced developer, which will help me fit perfectly into your company and work according to your exact schedule.
"
data engineer,"
Worked for one of Big Four accounting company in British department:
-analytical and calculation views (SAP HANA);
-analytical and SQL privileges (SAP HANA);
-AO reporting.
Worked for ""National Nuclear Energy Generating Company of Ukraine"" (EnergoAtom):
-SAP HANA modeling (tables, stored procedures, functions, table-functions, CDS objects etc.);
-analytical and calculation views / analytical and SQL privileges;
-Data loading and replication via Business Object Data Services.
Internal company project (Infopulse Ukraine):
-Ms SQL scripting (tables, stored procedures, functions);
-Data loading and replication via SSIS;
-Power BI Write Back with PowerApps.
Worked for one of Big Four accounting company in USA department:
-Integration of different On-Premise Data Sources to MS Azure Cloud to provide end users with business analysis using powerful capabilities of cloud technologies;
-Development of ETL processes using Azure Data Factory;
-Azure Databricks is used to implement different tasks;
-Backup and restore procedures of different components of solution: ADLS, Databricks, ADF etc;
-Implementation of security automation process to manage access to Data Lake and AD Groups and Users;
-Using PowerShell for Azure.
"
data engineer,"
Business Intelligence Developer with considerable knowledge in Business Intelligence, Business Analysis and ERP/CRM applications. For the last five years, I have been working with different business domains: manufacturing, banking, finance, sales, retail, warehouse, healthcare, human resource.  I have hands-on experience in communicating with different customers, gathering and documenting requirements from business users.

My core skills are:

- database design and development;
- ETL process development;
- data warehouses design and development;
- reports development;
- OLAP cubes development; 
- requirements gathering

BI Platforms:

JasperSoft, Pentaho BI Suite, MS SQL Server, Microstrategy
"
data engineer,"I studied one year computer science in my home country Azerbaijan, then I came to Hungary to start my education again. After a year, my GPA is 4.86 / 5.00
I was machine learning trainee for 4 month last year. I learned there machine learning algorithms, I learned more about data preprocessing, scraping and etc. Also, I learned some stuff about SQL there. I cleaned datasets that were given to us and made a prediction about future using tools we learned.
I am willing work for any python job. Java job would also be okay for me."
data engineer,"
I have more than 5 years of working with a data engineering stack from classic DWH with RDBMS to the data lake and cloud services. 
My tasks were tied but not limited to:
- data modeling;
- writing and optimizing complex ETL/ELT pipelines with vague requirements;
- performing data analysis and exploratory analysis;
- migration of data pipelines;
- writing tests for data pipelines;

As a Platform Data Engineering, I have had the experience:
- extending Airflow functionality;
- building and adopting data lineage solution in the data lake;
- creating a linter framework for Trino;
- writing a solution for naming convention propagation;
- creating other tools and services to enhance the productivity of data consumers;
- conducting POC (gathering requirements and documentation, preparing the environment, examining solution/using the tool, preparing summary and evaluation);
- investigation of different kinds of bad practices in data pipelines. Fixing error-prone code in ETLs (automatically, if possible);
- writing documentation;
- participating in post mortems.
I will be happy:
- to have the ability to write also Scala code;
- working in a team that cares about continuous data testing/monitoring;

I'm not interested in offerings that include:
- a lot of visualization work(Tableau Power BI, etc);
- only Microsoft stack;
- only Oracle stack;"
data engineer,"
As a Data Engineer at Space International, I am responsible for leveraging my expertise in SQL, Python, and other relevant tools to manage and transform banking data into valuable insights. My role encompasses a wide range of responsibilities, including:

1. Data Pipeline Development: I design, develop, and maintain robust data pipelines and ETL processes to ensure accurate and timely delivery of data for various banking products and services. This involves collaborating with cross-functional teams to understand data requirements and implementing effective solutions.

2. Data Modeling and Optimization: I optimize data resume_classifier and database performance to enhance the efficiency and scalability of data processing operations. By writing complex SQL queries, I ensure that data is readily accessible for analysis and reporting purposes.

3. Data Quality and Integrity: I am committed to ensuring data quality and integrity throughout the data lifecycle. This involves implementing data validation, cleansing, and normalization techniques to maintain high standards and accuracy.

4. Collaboration and Communication: I actively collaborate with cross-functional teams, including Data Scientists, Analysts, and Software Engineers, to understand their data requirements and provide them with the necessary support. Clear and effective communication is key to ensuring alignment and successful execution of data-related projects.

Working as a Data Engineer requires a combination of technical expertise, analytical skills, and a deep understanding of banking products and services. By leveraging my SQL, Python and Data Engineering skills, I contribute to the overall success of the organization by delivering accurate and actionable insights that drive strategic business decisions.
"
data engineer,"
1)	Building forecast system:
•	forecast benchmark for prediction sales per sku/brand/ATC Python (Sarima, Prophet, Exponential smoothing etc.)
•	ETL process for getting and saving results Airflow (Docker version)
•	MongoDB for saving resume_classifier and turning parameters (Docker version)
2)	Building price recommendation system (Clustering, Classifier)
3)	Building telegram bot for working with IT services site:
•	aiogram + requests (Rest API) + SQLAlchemy
•	CI/CD Gitlab + Jenkins
4)	ETL process of fact sale per pharmacies Azure Databricks + Synapse 

5) Building automate analytic system from scratch:
• creating analytic base SQL Server 2008/2014 (MS SQL)
• ETL process Apache Airflow +Python (SQLAlchemy) from many sources (MS SQL (1C), Mysql, Google analytics etc.)
• deploy tabular model (SSAS)
• deploy SSIS
• deploy Power BI report server
6) Building Ad-hoc reporting system together with analytics of the company ( Power BI)
7) Building classification model (Python - sicit-learn) for predicting successes/not of buying of current goods from tv broadcast of our TV channel. Deploy web interface (Python - Flask) for model
8) Client Clustering (k nearest neighbours sicit-learn)
Web developer"
data engineer,"
I am a professional with a desire to expand my knowledge and great analytical skills to solve problems in a creative way. I am Master in Data Science at the Universidad Politécnica de Madrid with a thesis about Event Extraction on Legal Corpora using Large Language Models.

My professional experience spans the fields of Data Engineering, Data Science and Backend Developer. In all cases I have worked with the Python programming language (with more than 6 years of experience) and with cloud services such as AWS. As part of my work in Garaje de Ideas, I build microservices in the cloud to be consumed by the different applications of Multiasistencia (Allianz). Additionally, at SwagUp I collaborated in the creation of microservices that allowed communication with third-party applications such as Salesforce.

From my position as Data Scientist at Prosperia, I developed pipelines to extract, transform and load data and to get knowledge and discover patterns from it. I also worked on the creation of machine learning resume_classifier with which I gained experience in data cleaning, variable engineering and the use of different resume_classifier that could be adapted to the client's criteria without losing statistical rigor.

Finally, I am a Spanish speaker with great fluency in English and basic knowledge of French. Throughout my experience in the different companies I have worked with, I have obtained a great capacity for teamwork and experience with tools such as Python, Pandas, Scikit learn, MongoDB, SQL, AWS, Databricks, among others. As part of my academic studies I have also worked with Tensorflow, Apache Spark, Apache Flink, NetworkX and Gephi. I have experience with other languages such as C# and Common Lisp and basic knowledge of Scala and Java that can be corroborated in my Github projects.
"
data engineer,"- Led and developed numerous BI projects that were used to find anomalies, to optimize data warehouse / ETL processes, logistics and resource management for DAX companies
- Saved costs and operational overhead by supporting migration from Power BI, Tableau, Cognos and Superset to Qlik Sense
- Received positive feedback and 5* reviews from the happy clients on Clutch
- Led several business development initiatives to help the company expand their client portfolio
PROFILE:
I have 4+ years of building Data Analytics for different business departments of the DAX companies as well as 5 years of working with complex data in Academia (Postdoc and PhD). I am interested in building Cloud and Big Data Analytics solutions

SKILLS:
My expertise includes identification of requirements, architecture, data modelling, DWH, ETL and visualization from the concept phase to the production. Qlik Sense, SQL, AWS,
GCP (Data Analytics stack), Dataiku
I am looking for remote / part time projects in Data Engineering with a possibility to develop my skills futher. Industry is not very important, however Pharma/Biotech would be preferable. My core competence is Qlik Sense (ETL, DWH, data modelling, data visualization), however I am searching for projects, where i would be able to improve my skills in AWS / GCP / ML"
data engineer,"More on LinkedIn
- Spark & PySpark  / DataBrikcs / SQL / Python / PyTorch / MxNet
-  Data preparation, manipulation, transformation, visualization and interpretation 
- ETL processes, experience in Data Warehouses, Data Lakes (Azure)
- ML solution for data anomaly detection, predictions, time series, classification, regression, market segmentation etc. 

Certifications: 
*Databricks Certified Associate Developer for Apache Spark 3.0 (Python) 
*Harvard CS50’s Introduction to Artificial Intelligence with Python
*Amazon training and certification 2020
*Cisco Networking Academy® self-paced course

Primary education:
*Finance & Credit
*MBA
Small projects not mote than 20 hours per week!"
data engineer,"I take pride in building reliable data pipelines ""from scratch"" for several international companies.
Self-educated in Python & Big Data, so whenever I join a new project - I can learn its new technologies and adapt all by myself, no baby-sitting required))
Skills: Data Processing and Analytics (10+ years)
SQL (SQL Server, MySQL) – 4 years
AWS (S3, Athena, Redshift, Databricks) – 2 years
ETL pipelines (SSIS, Databricks, PySpark, Airflow) – 2 years
BI dashboards (Looker, PowerBI, Tableau) – 3 years
DBA (SQL Server) – 1 year
Foglight, Powershell, Google Colab, Jira/Confluence, Git, Jenkins.
NLP and Text Mining (spaCy, gensim), web parsing (requests, selenium).

Collaborative, flexible, passionate to evolve, face new territories and challenging tasks.

Some of my projects: 
- top-10 computer parts vendor
- top-10 Online Video Platform
- major Forex broker with worldwide coverage.

I prefer projects which do not just utilize my superpowers, but also allow me to master new ones and grow as professional. 

My passions: fine-tuning convoluted systems, finding bugs in them, polishing them to perfection. 
Studying data to discover hidden patterns behind it, improve understanding of real processes and people, and ultimately provide valuable insights which can lead to tangible results.
Challenging data-related tasks; 
Opportunities & motivation for self-development and learning new cutting-edge big data technologies;
Chance to extract valuable insights from data, which will influence decision-making;
Efficient & fast communication with the team, ability to ask many questions in order to get deep understanding of the product and business."
data engineer,"
Database developer with 10+ years of hands-on experience in DBMS, including high-volume DB. 
Strong knowledge of data manipulation and programming, creating, maintaining and optimizing 
database elements, database design, performance tuning, backup/restore technologies, ETL, 
DWH, requirement analysis

Additional experience:
 - Azure Cloud services
- Python (Pyspark)


Latest project:
July 2019 – April 2022
Position:
Senior Database Developer
Development of a market analytics platform 
Responsibilities:
• Designing and developing technical solutions using various MS SQL concepts and objects 
such as stored procedures, views, functions, triggers, temporal tables, in-memory OLTP, 
partitioning, SQL jobs, SSIS packages, linked servers, replications, log shipping 
• Preparing SQL queries, reports, and data extracts in an ad-hoc and regular manner 
• Performing and managing daily database maintenance, index/backups strategy, monitoring 
and performance tuning tasks, optimization and automation of existing procedures and 
processes 
• Moving database related project environment from on-premises to Azure virtual machines, 
implementing Azure Blob Storage solutions in accordance with the project needs 
• Developing ETL solution based on Databricks, PySpark, Azure Data Lake
Technologies and tools 
• MS SQL, SSIS, Azure, Databricks, PySpark

I'm interested in data engineering , SQL/DB development

Thanks!
"
data engineer,"Developed and built an internal cloud analytics system from scratch (stack: Apache Kafka - Apache Nifi- dbt -
BigQuery - PowerBI)
- Deployed a secured instance of Apache Nifi on Google Compute Engine vm
- Development of various dashboards in Power BI 
- Modeled DWH using dbt (Kimball approach)
- Connected to various third-party APIs to collect and enrich the data
- Generating and testing various hypotheses for customer acquisition and retention
- Assisting management with everything that concerns data
- Automated report Email send out with Google App Script
Google Cloud Certified Professional Data Engineer (02.01.2023) 

Below is the list of tools and technologies I used for various purposes. 


Languages: Python, SQL, DAX (Power BI) & M (Power Query)

Technologies: Apache Nifi, Kafka, Airflow, Apache Beam, dbt, GCP (BigQuery, Compute Engine, Cloud Functions, Dataflow etc.), Git, Docker

BI: Power BI
What I am interested in is working in a company that understands the value of modern cloud analytics and either already has implemented such a system or strives to do so. Ideally, I would like to keep working in GCP as I am quite well-versed in the system. However, if your cloud platform is AWS, I am okay with that - just keep in mind that my experience in this system is not as expansive as in GCP."
data engineer,"I have won hackathons organized by prominent companies like banks and Microsoft, where I developed a prototype of a device that enables visually impaired individuals to play sports. My innovative solutions and dedication to problem-solving helped me secure first place in these competitions.

As a programming instructor at a coding school, I created a course that helped train and prepare aspiring developers for a career in IT. Thanks to my course, 10 students were able to secure roles as backend and frontend developers, analysts, and UX designers, making a meaningful impact in their careers and the technology industry.
As a data engineer with 3+ years of experience, I specialize in designing, building, and maintaining data pipelines and infrastructure. I have expertise in various data warehousing tools, including ETL processes, data modeling, and data integration. I aim to optimize data processing and flow to ensure accurate, timely, and secure data delivery, enabling data-driven decision-making.


Projects in SAAS-based company 

• Engineered and supported 50+ completely automated ETL jobs using Airflow, Python, and SQL (Clickhouse, MySQL, Postgres, ElasticSearch).
• Redesigned data pipelines that helped to optimize execution time by 50%
• Provided sales and marketing teams with required data for different campaigns. The churn rate decreased to
12%
• Led documentation of processes and data solutions

Projects in the outsourcing agency (the largest partner of Google in APAC region) 

• Built a series of Marketing Dashboards for Korean automobile manufacturers using Data Studio/Looker.
• Implemented statistical analysis of the website events to identify key business actions that drive the business using GCP (BigQuery mostly), GMP (CM360, DV360, SA360, Google Analytics (UA + GA4), and GTM)
• Assisted in implementing a machine learning model to identify users with a higher probability of conversion.

Projects in the telecom company

• Enriched DWH with 100k houses data on competitors’ technical capabilities by integrating a self-designed scalable web crawler.
• Collaborated with the marketing and data team to satisfy analytical needs by building both large systematic reports and small custom pieces.
• Developed and supported data infrastructure and ETL processes
I prefer not to work with companies in the gambling, betting, alcohol, and tobacco industries due to personal values and beliefs."
data engineer,"
Experience
•	5+ years of outsource experience in software development (including Regulatory Reporting Projects and Reference Data Services)
	10+ years experience in the IT industry
•	Maintenance  and implementation of ETL processes (data extraction, transformation, loading, cleaning, validation) .
•	Development with experience and knowledge in all stages of project life cycle.
•	Experience in Database Architecture and Data Modelling
EMPLOYMENT

Oracle/ETL Developer, Jun 2013 - currently Luxoft, Ukraine.
 
	Projects: Client onboarding, Regulatory and Control, Reporting system
Description: The project mission is creation of a unified system for client onboarding.Migration from legacy systems.Loading regulatory client information. Creation of a reporting system.
Duties:
o	Development of the real-time and batch ETL processes according to the specifications:
 		on Informatica side (mappings/workflows);
 		on database side (packages/procedures/functions etc.);
o	Integration with external system using Informatica ,(external source –Oracle DB, SQL Server DB, Salesforce, SOAP and REST WS, files in different formats;
o	Data modeling including the design and development of Oracle and ETL processes;
o	Non-PROD environments testing support;
o	Performance tuning;
o	Incident and defect analysis and fixing;
o	Environments setup and synchronization (sanity and smoke checks);
o	PROD rollout preparation and  support;
o	Creation of the Control-M jobs/packages for workflow scheduling;
o	Communication with local and remote/distributed teams (UK, US, India).


Oracle Developer, Oct 2012 - Jun 2013 Miratech
Ukraine

	Duties:
o	Design, develop, maintain billing system ;
o	Create a logical database structure, scripts, queries, packages, views, jobs,
Tuning of SQL queries
o	ETL development using PL/SQL;
o	Communication with technical and analytical specialists from customer side;
o	Ad-hoc reporting;
o	Technical support of L3 and L2 services, technical customer support


   Oracle Developer, Jul 2008 - Sep 2012  Internal Revenue Service

	Duties:
o	developing of applications framework for Tax Accounting and Reporting Department (Visual FoxPro, Oracle);
o	developing a server-side software for data aggregation and data selection (PHP, Oracle);
o	Tax system support.
"
data engineer,"
Core Technologies and Tools:
SQL, PL\SQL, Oracle
Python 3 Core, Java Core, OOP
DWH, ETL, Informatica PowerCenter
JMS, Kafka, RabbitMQ, XML, JSON
Jenkins, TeamCity
SVN; GIT 

Knowledge and Skills:
Oracle – knowledge of Memory Structure; Database objects; Partitioning; Materialized view; Locking; Transactions.
Performance tuning - experience in processing of big data; Execution plans; Tables, Indexes; Join methods; Hints; Statistics gathering, AWR reports.
SQL – Writing queries of varying complexity; Data manipulation; Analytical and Grouping functions, Regular expressions, Hierarchical queries.
PL\SQL – Procedures, Functions and Packages; Collections and records; Bulk loads; Cursors; Exception handlers; Dynamic SQL.
Python scripting – Python 3 Core
Java – Java Core, OOP principals
Informatica – Mapping and Workflow development and support
JMS, Kafka, RabbitMQ, XML, JSON
GIT, SVN – Version Control Systems
JIRA, Confluence – Bug-tracking systems
"
data engineer,"- resolving migration issues
- performance enhancement
Experience: Database Development

Area: DWH, ETL, Cloud

Tools/Technologies: SQL, Talend, Python, AWS Data, Azure Data, Snowflake

Projects: 
- on-premise -> cloud data migration
- migration between different DBMS's
- data migration
- DWH design / development / management"
data engineer,"Our team have successfully designed / communicated / executed DWH migration from on-premise servers to GCP technologies for the latest client. Additionally we have implemented Bi system based on BigQuery, Looker, Connected Google Sheets. Everybody within a team contributed equally to the project,  so we share the achievement between us.
I would like to say hello and discuss the scope of work done for the last few years. I spent the last 3 years working for an IT outsourcing company in a Data Engineer capacity. I was assigned to their client - a European online retailer of fashion apparel.

My journey with the client will be over soon. I will start looking for long term opportunities starting September 2022. In the meantime I am up to part-time or project work. If you temporarily short on Data employees I can lend a helping hand on a task or few.

Scope:

1. Supporting Old DWH and BI System
Contributed to supporting technology stack: SQL Server, SSIS, SSAS, SSRS, Excel BI

2. DWH rip and replace migration from on-premise servers to GCP.
Developed data pipelines from on-premises servers to GCP using Apache Kafka / Kafka Connect (Debezium source connector, Pub/Sub sink connector), Pub/Sub, Dataflow (Apache Beam), BigQuery.

Developed data pipelines for external REST APIs using Cloud Functions (Facebook, Microsoft Ads etc.). As well as setup GCP native integrations for Google products (Analytics 360 Export to BigQuery, Google Ads etc.)

Designed and implemented DWH structure for BigQuery:  
datasets / tables, unified table / view schemas, partitioning / clustering options, table / partition expiration policies, group access policies including column-level security, sensitive information masking etc.

Set up data pipeline performance / data consistency monitoring and alerting policies via Google Monitoring

Developed ETL routines in Kafka Connect using SMTs and Dataflow. As well as developed the ELT part using BigQuery Scheduled Queries and Views.

3. Developing New BI System
Developed data sources for Looker, Connected Google Sheets.
Contributed to developing LookML Models, Explores, Dashboards, access policies.

4. Time-series Forecasting
Developed automated routine for creating annual Financial Budgets (from Gross Sales to EBIT) and operational forecasts.
The routine is a Facebook Prophet model retrained regularly using Vertex AI scheduled notebooks.
It forecasts data on a Category / department / season code granularity level.

I can tell more and share details on request.
I would like to be on an international team of dedicated professionals. I would prefer to be in e-commerce (fashion / apparel) or energy industries, since I have relevant experience in those areas.
I would avoid stale projects with no perspectives of evolution built some years ago. Nevertheless, I will happily assist you in modernizing your system if you have reached such a decision.

My preferences in terms of technology stack: 
Python
SQL
Kafka / Kafka Connect
Apache Beam (Dataflow runner)
GCP technologies (since I have GCP Professional Data Engineer Certification, but can also look into other options)
Kubeflow Pipelines and TensorFlow Extended (I recently started looking into it)"
data engineer,"Professional Data Engineer and GIS Analyst with nearly ten years of experience with ArcGIS software for developing GIS databases as well as Editor data creation / editing tools and snapping tolerances. Demonstrated skills in customer interaction marketing and basic sales work. Continuously looks at ways to add value to take the initiative in improving processes and identifying business opportunities.
Data engineering, Data analytics, GIS, Implementation of geographic information systems, Business analytics, Spatial analysis, Remote sensing

-Python
-SQL
-Git
-GIS API
- structure data and algorithm
-gis (arcpy)
-math ( numpy, pandas, matplotlib)
-Google API
-MapBOX
-Google Colab
- ETL
Data engineering. GIS. Remote sensing."
data engineer,"Reporting automation
•‎ Develop and maintenance of analytical systems (OLAP-cubes) MS SQL Server Analysis Services (SSAS). 
•‎ Support the establishment of procedures and MS SQL Server Integration Services (SSIS). 
•‎ Develop reports in MS SQL Server Reporting Services (SSRS) and PowerBI. 
•‎ Develop and maintenance of DWH. 
• Development and maintenance using the C#, PowerShell, PowerQuery M, PowerPivot, DAX, Jira, T-SQL, CosmosDB, MS Synapse
•‎ Developing, publishing, and scheduling Power BI Dashboards/Reports as per the business requirements including drill through, drill down to SSRS reports;
•‎ Advanced Hands-on use of Power BI & Power BI Tools & Power BI Premium
•‎ Develop visual reports, dashboards, and KPI scorecards using Power BI desktop
• Connect to data sources, importing data, and transforming data for Business Intelligence
• Create and maintain SQL scripts and objects (functions, stored procedures, views, etc.)
• Use embedded analytics like Power BI Service/Embedded
• Integrate Power BI reports into other applications
• Work with Power BI Client API and Power BI Rest API
• Capable of implementing row-level security on data along with an understanding of
application security layer resume_classifier in Power BI
• ETL (Extract, transform, load)
• Design and implement the Data Flows and Data Warehousing
Development of reporting in SSRS and POWER BI. Part-time or hourly payment"
data engineer,"Confidently completed courses at the IT school Hillel on the course ""Basic Python"".
Successfully passed the training course from Nix Solutions ""Python education"". I've worked with building different database schemas, fill them with data from several sources via PostgreSQL. Also, I touched Apache Airflow and created several ETL services, worked with libraries such as pandas, PySpark and wrote unit tests via Pytest. Learned many tools like Git, Docker and etc.
On the course were many challenging tasks that required to find interesting approaches for them.
The last experience was the tiny project of downloading raw information from the website. Parse date once, upload to the S3 bucket and use Airflow to schedule and download new information that appears during the day. After that, the processed information is uploaded to the database (PostgreSQL) and also uploaded every day according to the schedule. Transformed to the desired format and combined with other data according to certain criteria.
I have expireince in Big Data, built ETL and want continue work with it, but this pleasure teach and use new tools."
data engineer,"
Built ETL that ingested data from a website to Postgres database.
Designed and implemented data pipeline to process data from multiple sources using SQL, Prefect, Terraform and GCP.
used docker to dockerize and containerize python script for data ingestion.
Created a Flask API.
Data Engineering Intern motivated by data and results, ready to join a team to build a data stack from scratch.
"
data engineer,"Currently I'm looking for internship of the Business Intelligence developer role. I'm in progress of self-learning SQL and Azure Data services
During last five years i was working as security systems installation and configuration engineer in Euroinstal company (Warsaw, Poland). Company is specializing on video monitoring, access control and fire safety.
"
data engineer,"• Certified AWS Machine Learning Specialist (Amazon, Validation Number: YJJTVLDKHM111B5P) - Exam passed successfully
  • Certified AWS Solution Architect PROFESSIONAL (Amazon, Validation Number: Q5YFD7EC3FQQQ23Y) - Exam passed successfully
  • Certified AWS DevOps Engineer PROFESSIONAL (Amazon, Validation Number: TMQ7ZVW13NB41ZW6) - Exam passed successfully
  • Certified AWS Security Specialist (Amazon, Validation Number: EP84L54KGNB41VKF) 
  • Certified Azure Solution Architect Expert (PROFESSIONAL) (Microsoft, Certification number: H218-2646) - Exam passed successfully
  • Certified Google Cloud Architect PROFESSIONAL (Google, Certification ID: 6zTqfr) - Exam passed successfully
  • Certified AWS Developer (Amazon, Validation Number: 9T4NM16CBBQEQT9C) - Exam passed successfully
  • Certified AWS Architect (Amazon, Validation Number: 6GSY8YS1DMQQQG9R) - Exam passed successfully
  • Certified Kafka Developer (Confluent, 12263104) - Exam passed successfully
  • Certified Spark & Hadoop Developer (Cloudera, License: 100-017-901) - Exam passed successfully
  • Certified Cassandra Developer (DataStax, Certificate ID: 16f135c3-a433-4363-8518-de1f4a18c070) - Exam passed successfully
  • Certified Hadoop Administrator (Cloudera, License: 100-017-901) - Exam passed successfully
  • Certified HBase Developer (MapR, License number: ipupzqv6nzj6) - Exam passed successfully
  • Certified MongoDB DBA (MongoDB, Certification Number: 565804091) - Exam passed successfully
  • Certified MongoDB Developer (MongoDB, Certification Number: 996745314) - Exam passed successfully
  • Certified Neo4j Professional (Neo4j, License number:  16681227)
  • 5 years’ commercial experience in BigData
  • 6 years’ commercial experience in AWS
  • 6 years’ commercial experience in Scala
  • 12 years’ commercial experience in Java (including concurrency and multithreading), Python, Ruby 
  • 7 years’ commercial experience in e-commerce
  • 2 years’ commercial experience in C++
-   17 years’ commercial experience in software development
-   several business domains
-   hands-on architect but consider the following roles: Architect | Developer | Engineer | DevOps
- 	focused on cutting-edge technologies
- 	experience confirmed by certificates issued by Amazon, Google, Microsoft, Cloudera, Confluent, MapR
Fully remote projects. FinTech is preferable but any business domains are considered."
data engineer,"- Implemented and partially designed a few complex long-term big data projects from zero to release (i.e., lambda architecture in AWS for near real-time brokers market analytics, and modern Lakehouse/Warehouse for telecom ML/BI in Azure)

- Did architecture discovery, assessment, MVP in role of Big Data Architect / Consultant. As part of this did significant impact on projects architecture and success.

- Tried myself through all end-2-end project lyfecycle: presales, discovery/assessment, team & processes setup, implementation, ongoing support & tunning. Achieved some kind of balance in technologies & approaches between depth (long-term projects) and width (short term projects).
Many Big Data projects, in engineering and architecture roles, everything in CV. 

Among them complex big data projects (lakes, dwh, batch, streaming, migration) from ground up, requirements collection, design, architecture discovery/assessment, implementation, team leadership, etc.
- Legacy systems where all decisions made before me
- Purely managerial roles, where despite Lead/Architect Title 80%+ of time is spent on managing others and their expectations"
data engineer,"I made a bounce from ETL Engineer to Data/ML Engineer within three months, mostly by self-development and self-learning.
I've successfully completed all the projects in which I was engaged, and I am sure that any of my former/current managers or customers will give a positive reference in case if it is requested. 
As well, I enhanced my English to the level of fluency which allows me to work with any customer without predicaments.
ML Engineer. Former DB/ETL developer with 6 years experience. 

Experience:
Grid Dynamics 
Data Scientist/ML Engineer 
June 2018 - Present 
Working in a small team on the chatbot platform: - Developing and improving resume_classifier for NLP (Intent classification, NER, RE) - Developing Text preprocessing - Gathering data for resume_classifier and a catalog - Developing REST microservices - Handling data extracting and loading
Technologies: - Python (flask, requests, numpy, sklearn, keras) - JSON, HTML, CSV - MongoDB, ElasticSearch - Git, Jira, Scrum

Oracle DI/ETL Engineer March 
2017 - May 2018 (1 year 3 months) 
• Processing, integration, validating the big amount of data (100M+ rows) from different sources into Oracle DB (Exadata) • Designing and developing DB objects (tables, views, procedures, packages, indexes) • Tuning SQL queries, solving performance issues • Designing near real-time and batch ETL processes • Learning and practicing in Data Science
   
Technologies and tools: 
• Oracle Data Integrator • Oracle DB (Exadata) • MS SQL • BMC Control-M • Python • Pandas, Numpy • Matplotlib, Seaborn • Scikit-learn, stats • Git, TFS

Bell Integrator 
Senior Oracle Developer December 
2014 - March 2017 (2 years 4 months) 
Working on projects of ""National settlement depository"" (Russia) • Developing, maintaining, debugging PL/SQL procedure packages for complex depositary IS • Designing DB objects (tables, triggers, indexes, views, functions) • SQL query tuning • Developing ETL integration processes between different DB (Oracle, MS SQL, web services, files, e-mail)
Technologies and tools: • Oracle Database • SQL, PL/SQL • Oracle Data Integrator • Jython • TeamCity • Redmine • Git

I possess profound expertise in Data Analysis, ETL, Oracle, and Databases. I am seeking an opportunity for relocation and professional growth with the company.
Any relevant opportunity will be considered within adequate rates and conditions.
I will be happy to chat and stay in touch for further opportunities.
I am a dedicated employee, and I expect to dip into the environment in which I could be able to grow with the company and improve my skills. 
Also, I will be more than happy to work with a manager/team leader from whom I could learn."
data engineer,"- has built custom MLops tool for automatic deployment, scale and monitoring gathering for TOP 3 Singapore bank in 6 month; 
- led a team of 4 middle data engineers; 
- successfully built Data Operations pipeline and/or infrastructure for over 10+ clients capable of handling TBs of data flow, preprocessing and storage;
Seasoned Python developer with 7 years of experience in extracting, manipulating, and storing large data streams using cutting-edge tools such as Spark, Airflow, and Kubernetes, alongside Pandas, Numpy, and TensorFlow. With a strong background in linear algebra, statistical and probability theory, has contributed to open-source KNIME workflows and is an active Kaggle contributor. As a certified Alteryx (SaaS) developer, possesses practical expertise in statistical techniques such as linear/logistic regressions, multivariate regressions, random forests, XGBoost, LSTM, and ARIMA. Has implemented MLops best practices for enterprises, including automated model deployment, model versioning, data drift, and performance monitoring. Has extensive experience in building ETL pipelines and has participated in public MLops events.
Can consider any type of products/services and become in alignment with team."
data engineer,"
NDA - data warehouse and data science platform:
  Participated in the architecture of an on-premise data warehouse and data science platform, starting from defining technical specs for the bare metal machines.
 Created initial design of ETL migration from an in-house build system to open source
 Helped to set up development processes e.g.: code quality, the review process, on-boarding
 The main impact - I’ve initialized company “ETL”-mindset transformation, which helped to have much-complicated
analytics and insights generation.
 Impact: ​the whole ETL process had been improved, scaled up and automated, which gave the company the possibility
to scale up/down as much as it wanted. Performance for the running ETL processes increased in 2 times

NDA - machine learning productization system:
 Took lead in creating of the initial architecture
 Designed CI/CD pipeline to work with bare metal machines
 As a result of this project - the company started to use the on-line evaluation of the ml resume_classifier instead of off-line one
 Impact​: Developed framework which helped to decrease ml model way to production from months to couple hours.
Insights Venture Partners - analytics platform:
 I played the main roles in system designs and was responsible for team building/hiring
 Helped to create ETL system which was generating meaningful insights for future investments

NDA - data aggregation:
 I played a key role in knowledge and project transfer from the other vendor, helped in code assessment and understanding of the definition of done for it.
 I was responsible for ETL processes revitalization - as a result I’ve increased speed and performance of the ETL into couple time
"
data engineer,"Exam 761: Querying Data with Transact-SQL (2020)
15+ years of IT experience 

6+ years experience with huge databases (telecom and bank sector) 

 - New program modules development
 - Reports development
 - Troubleshooting legacy code
 - Data Base migration
 - ETL modules development
 - Data quality checking mechanisms development

 
Experience with small businesses databases (CRM systems)
 - SQL-Server administration (creating users, server audit, DB maintenance)
 - Development and optimization of server-side logic
 - Data Integration
 - Ad hoc reports
"
data engineer,"During previous project I was implementing LIBOR replacement package for several clients, who were pioneering new SOFR and SONIA based rates for asset evaluation.

Took part in 24/7 Support Service during Winter Youth Olympic Games 2020;
Assisted our Dev team during Sprint Demos;
Facilitated the retrospective sessions on behalf of PM;
Maintaining existing ETL-pipelines;
Created custom Azure Data Factory (ADF) pipeline to pull latest updates from a 3rd party provider into the ADLS(2) storage with pure ADF tools only, without relying on external notebooks or .jar libraries
Automated MS SQL backups recovery from local .bak files into .bacpacs and further downstream into Azure SQL Server via PowerShell scripted VM
Run some PoC researches on various data processing solutions like Dask, Databricks and Trino.
Prepared Power BI dashboard for the data platform on top of StarBurst cluster (a.k.a. Trino // a.k.a Presto) capabilities demo with underlying sql materialized views


Another project purpose was creation of universal Data warehouse for sport related events and organizations. We have communicated a lot with International Olympic Committee and their sport dictionaries. As a data analyst I was reviewing existing DB records, updated our Knowledge base on Confluence regarding new changes in Competitions and provided new mapping/ transformation rules for our Dev team.
To further develop as a Python Dev or Data Engineer. Look forward hands-on experience with industry accepted tools and approaches like micro services, blue green deployments, CI/CD pipelines and orchestrations."
data engineer,"- Databricks Certified Data Engineer Associate 
- Astronomer Certification for Apache Airflow Fundamentals
- Microsoft Certified: Power BI Data Analyst Associate
- Microsoft Certified: Azure Fundamentals
- MongoDB Certified Associate Developer
- ITIL® 4 Foundation CPD
- Dataiku Core Designer
Experienced Data Engineer and Power BI specialist with a strong background in data modeling, ETL processes, data
integration and data visualization. Skilled in designing and implementing data pipelines and ensuring data quality and
integrity. Proficient in programming languages such as Python, SQL with expertise in various database technologies
(RDBMS, NoSQL). Collaborative team player with a track record of delivering data-driven solutions that drive business
growth and improve decision-making processes.

- 9 Years experience in retail banking
- Dozens of data pipelines (SSIS, Pandas, Airflow DAG's)
- Dozens of API's used
- More than 100 of Power BI, SQL and SSRS reports in production on Sales, Employee performance, Financial performance, Products, various KPI, Partners and etc.;
- Various automations in IT and business processes via Python, Telegram, command line.
What I do not expect:
- Any kind of work that demands frequent focus change between completely different spheres that is not even connected to Subject matter."
data engineer,"
- Development of ETL/ELT pipelines 
- Integrating CI/CD practices (GitLab to Airflow)
 - Data visualization, creation analytical web applications/dashboards and tools
 - Creation of web services for data visualization and work with them
 - Producing prototypes to demonstrate concepts and ideas
 - Creating of internal web service for automation of tasks and optimization of work of SEO and marketing specialists with wide functionality
- Development of a data extraction system with 170+ connectors
- Development of systems for building reports
Prefer product companies with large amount of data
Remote working possibility
Flexible working schedule
Team of highly qualified professionals"
data engineer,"
More in CV upon request

Current Position:
Marketing Data Connectors SaaS Product Developer

Develop and maintain data connectors for a marketing-focused SaaS product.
Utilize Python, SQL, and Apache Airflow to create and manage ETL pipelines and orchestrate data workflows.
Previous Positions:

ETL and Data Streaming Pipeline Developer

Participated in the design and implementation of ETL and data streaming pipelines using NATS and JetStream.
Contributed to the open-source NATS JetStream Library, enhancing its features and improving its performance.
Developed infrastructure for efficient data storage and processing.
Async Telegram Bot Developer for an English School

Designed and developed an asynchronous Telegram bot for an English language school using Python and the aiogram library.
Hosted the bot on AWS and used PostgreSQL with SQLAlchemy for data storage, as well as Redis for caching.
Implemented complex testing and booking functionalities to streamline the user experience for students and administrators.
"
data engineer,"Made batch and streaming loading Data Warehouses.
I've built ETL systems from start to finish in both Clouds oriented (GCP) and Server oriented formats.
Configure development and production server for the backend data pipeline on AWS instances (S3 bucket, RDS, Redshift).
Created the application architecture and DataBase that laid the foundation of the new app.
Certified Google Cloud Platform Professional Data engineer (Date of receiving January 11, 2022) 
The main domain of experience: Production, e-commerce, medical.

Worked on:
- Designing and creating ETL/ELT process and data infrastructure for product companies.
- Development of data pipelines and pre-processed data with dbt(Data Build Tool), Airflow including Hadoop.
- Made data analytics, building complicated reports with forecasting and data visualization (Metabase, Looker, Chartio, Tableau).
- Mainly work with Google Cloud Platform (BigQuery, Task, Functions, SQL, Dataflow, Spark, Composer, Pub/Sub) also, have experience with AWS (EC2, S3, RDS, Lambda, Redshift).
- Had been implementing logic for the Python backend (Django, Flask, Cloud Function) and JS frontend.
- Database and Data Warehouses design (PostgreSQL, MongoDB, BigQuery, MS SQL).
- Strong SQL data manipulation and processing expertise.
- Have working experience with Terraform and Ansible

Familiar with Agile and Scrum project management methods.
Worked with teamwork systems: Trello, Jira, Assana.
Used in teamwork: Slack, Google Meet, Discord, Microsoft Team.
I`m looking forward work with experienced software and data developers, and learning and implementing modern tools and technologies.
Expect to work with: GSP, Python, dbt / Airflow, engineering/tuning data pipelines, and data analytics."
data engineer,"-- PostgreSQL, Clickhouse, MySQL, Snowflake, BigQuery, Redshift, Airbyte, DBT

-- Modeling relational databases (ER modeling, Relational schema, Normal forms), data modeling 3NF/Dimensional (Star/Snowflake)

-- Python, Django, FastApi, Pandas

-- k8s(Helm), Docker, Git (GitHub, Actions), Airflow, Metabase, Nginx, AWS/GCP 

-- Hadoop(MapReduce), PySpark(Datarbicks), TableFormats (Iceberg, Delta table), FileFormat(Avro,Parquet,ORC), Storage(S3,ADLS,GCS,HDFS)
TOP SKILLS:
-- PostgreSQL, Clickhouse, MySQL, Snowflake, BigQuery, Redshift, Airbyte, DBT, Dagster
--  Modeling relational databases (ER modeling, Relational schema, Normal forms), data modeling 3NF/Dimensional (Star/Snowflake,DataVault)
--  Python, Django, FastApi, Pandas
--  Metabase, Looker, Tableau
--  K8s(Helm), Docker, Git (GitHub, Actions), Airflow, Nginx, ElasticSearch, 
--  AZURE(Synapse, data factory, adls…)
--  GCP(Dataflow, PubSub, DataProc, Instance, GCS, BigQuery, …)
--  AWS(Glue, S3, EC2, Lambda, ElasticBeanstalk, IAM, VPC…)
--  Hadoop(MapReduce)Streaming, PySpark(Datarbicks), TableFormats (Iceberg, Delta table), FileFormat(Avro,Parquet,ORC), Storage(S3,ADLS,GCS)



---- September 2022 till now _ Itransition
---- Data Engineer role
1. Converting BigQuery queries to Snowflake after two companies merged
2. Doing ELT(dbt) with Snowflake-tasks, and for some SQL using Airflow. It depends on which team has 
created a ticket. 
3. Preparing semantic layer for Looker, so that Bi engineers(including me) can build dashboards 
and do some aggregations.
4. Building Looker reports and attaching them to business-related boards.
Python, Apache Airflow, GCP/AWS, Snowflake/BigQuery, Looker(LookML), SQL, DBT


---- September 2021 — September 2022 _ EPAM Systems
---- Python Engineer role
Extracting and data validation for RSS feeds.
Extending functionalities of existing lambdas.
Python, AWS(Lambda), S3, Elastic Beanstalk, VPC, EC2


---- August 2020 — September 2021 _ AlifTech
---- Data Engineer role
- Improved initially working with MySQL Tableau performance by replacing it with MongoDB.
- Wrote data pipelines using Apache Airflow (including data retrieving from several MySQL 
instances, processing them on Pandas, and storing them into MongoDB as data warehouse).
- Automated daily manual jobs with Airflow including sending daily reports to other departments.
- Automated ML model builds by refactoring code into Airflow DAGs and Tasks.
Airflow, python, pandas, FastAPI, Tableau
"
data engineer,"A lot of automations that were made by me, really simplified and improved workflows on my projects.
I worked on different ETL processes, DB migrations, automatisation, many tasks were related to DevOps part.Have a pretty good knowledge of AWS.
I want to work in result driven collective/company where everyone wants to make an impact, to achieve goals."
data engineer,"Looking for a remote job.
1 year in total of working experience as [big] data engineer
2 years of experience in technical support
1 year of working experience in front-end development
Looking for a job as big data engineer, would like to work the big data stack like: python, aws, spark, kafka"
data engineer,"
Skilled in data engineering, machine learning and cloud engineering. Some of my previous work included:
- Designing and building data architectures and ETL pipelines on AWS
- Building and testing machine learning resume_classifier for computer vision tasks, natural language processing & time series predictions
- Developing and deploying web apps in Python/ Django
- Data mining, cleaning and analysis
- Writing technical and end user manuals and documentations
- Configuring and managing cloud resources on AWS & GCP
- Configuring and managing distributed databases

Technologies/ tools: Python, AWS, Tensorflow, Apache Spark, PostgreSQL, deep learning, Git, 

Certifications: Coursera Deep Learning Specialization, Tensorflow certified developer (ongoing)
Education: MSc Electrical & Computer Engineering
Personal & professional growth"
data engineer,"Graduated Bachelor of Suleiman Demirel University (2020)
I am studying for a master's degree in Computer Science.
Successfully completed the ""Big Data Engineer"" training from Alfa Bank.
Currently, I work in the field of Data Engineering at Kaspi Bank. Developer DWH. We are consolidating data using big data tools like Pyspark, Apache Kafka, Hadoop. My task is to stream data from various sources and send it to DWH or another database.
We also work with cloud services like Google Cloud and AWS.
I want to develop further in this area and learn new technologies.
"
data engineer,"Master's Degree in Computer Science. A Data Engineer with experience managing digital media assets and data. Tools that I'm using the most: Python, Airflow, Azure, Snowflake, dbt, Postgres, MS SQL Server, Kubernetes, and Docker.
Product company (gambling field), Data Engineer
Oct 2022 - Present
1. Develop ELT pipelines for data source integration (Python, Airflow, Azure, dbt, Snowflake, Postgres, MS SQL Server, RabbitMQ, Kubernetes, Docker).
2. Develop data integration Python libraries (Python, Azure REST API, Power BI REST API).
3. DevOps responsibility: maintain and support K8S clusters, deploy microservices, setup and maintain CI/CD pipelines (Docker, Kubernetes, AKS, kubectl, helm, Lens, Gitlab, Azure)

One of the biggest outsourcing companies, Software Engineer
Feb 2022 - Sept 2022
Customer - the big UK company.
The goal of the project is to transfer data from external CMSs to the customer's own custom CMS. That requires writing rules using query transformation language and configuring pipelines for the data migration.
Participation:
1. Participated in the data migration cycle.
2. Developed JSONata transformation and mapping scripts for automating the pipeline.
3. Developed XSLT transformation and mapping scripts.
4. Performed testing phase for each of the process steps.
Database: MongoDB
Tools: jsonata, docker, mongoserver, GitLab, google services, Slack, Jira, Confluence
Technologies: YAML, JSON, git, go

One of the biggest outsourcing companies, Data Quality Engineer
Apr 2021 – Jan 2022
Customer - One of the biggest property and casualty insurers in the United States.
The goal of the project is to migrate the Customer’s DB2 databases to the Customer’s instance of AWS.
Technologies: Java, AWS, DB2, Spring boot, PostgreSQL, Bamboo.
Participation:
1.  QA/QC support of cloud migration activities
2.  Testing of databases objects: keys, sequences, triggers, tables, stored procedures, and user-defined functions
3.  Tests results analysis
4.  Regression testing
5.  Bug hunting and reporting
Less Talk More Action!"
data engineer,"TD Feed Generator
- Service for generating various product feeds for integration with advertising accounts
TD Marketing Services
- Business process automation services of the Marketing Department
RealWeb — Python Developer (Part-time project) [Apr 2022 - present]
– Development of applications and services in Python
– Receiving and processing data from different sources

Technodom — Data Engineer [Jun 2020 - present]
– Development and support of existing ETL processes
– Collect and store data from analytic tools (Segment Stream, Google Analytics) and advertising systems
(Facebook, Google Ads, Yandex Direct and etc.)
– Import and export data from/to Google BigQuery, MSSQL, PostgreSQL, MySQL, SAP HANA and
BW
– Preparation and sending data to automated marketing systems
– Automation of business processes of the marketing department by developing new tools

Technodom — Loyalty Specialist [Dec 2018 - Jun 2020]
– Execution of the regulations of the ”TechnoBonus” Loyalty Program, automation of business processes
of the Loyalty Program in 1C CRM
– Development and control of execution of instructions and scenarios for the Loyalty Program in retail
stores and online stores
– Regular tracking and prompt correction of incorrect information in the CRM database
I want to: 
- Engage in tasks that include receiving, processing and storing data.
- Build a data pipeline and data warehouse architecture;
- Do tasks that include working with data visualization tools"
data engineer,"EDUCATION
Kryvyi Rih National University
Master's degree with honours in Software Engineering
Graduated December 2022

CERTIFICATES
Google Cloud Certified Professional Data Engineer, September 2023
Relevant skills: BigQuery, BigTable, Dataflow, Dataproc, GCP, Pub/Sub

EPAM Systems Data Software Engineering (Big Data) Lab, October 2022	
Relevant skills: Apache Airflow, Apache Kafka, Apache Spark, HDFS
Audio-based identification of beehive states
• Developed a custom features extraction method with librosa and fine-tuned convolutional neural network with Keras to classify beehive states, achieving a classification accuracy rate of 99%.
• Used React to implement frontend and Flask for machine learning model deployment.

Excavator teeth condition control system
• Using Python with OpenCV and PyQt5 built a system that determines shovel teeth condition and informs the user regarding the status of each tooth.
• Gained fundamental knowledge of image processing by applying Gaussian blur, Canny edge detector, template matching, morphological operations, GrabCut algorithm.

URL shortening service
• Created a full-stack web application that allows users to shorten URLs and then manage them.
• Built a fronted with React, Redux, and Material-UI using modern practices with functional components and hooks.
• Utilized Firebase tools, such as Cloud Functions and Cloud Firestore for backend, Authentication to authenticate users to the app, and Hosting for deployment.

Sudoku Game & Solver GitHub
• Built an Android app on Kotlin to play Sudoku.
• Utilized knowledge of the Room persistence library to store Sudoku board data.
• Implemented a solver feature that uses a backtracking technique to find a solution to a game.
• Fulfilled features, such as autosave, undo, pencil marks, and auto error-checking.
"
data engineer,"
Outsourcing company, remote position - Backend Python Developer
Writing efficient, clean, scalable code. Identifying and fixing errors. Writing tests. Working with databases and version control systems. Communicating with the project team. Release preparation and management.

There are more experience, which is described in detail in the CV.
"
data engineer,"From sales to engineer, passed the selection and got into Demium (incubator) but unfortunately after a while the incubator closed due to the war in Ukraine, I came up with a project and created the MVP when I worked in a startup, the project got into the top 3 projects, but due to a number of reasons, I left the company and the project passed into other hands.
Worked in a startup (fintech 60 employees, the main product was a proxy for encrypting sensitive data) where I started from the position of sales, at the same time took technical tasks from engineers (expanded the functionality of the cli tool, wrote bash release automation on github, wrote integration templates with our product) , after which I was able to moved to the position of a customer support engineer, where worked with technical issues of our customers (integration with our product, documentation, finding solutions to problems with a team of engineers), then moved to the position of a software engineer, worked mainly with existing services (microservise system) written in Python (flask) the main tasks of fixing bugs, expanding functionality, writing tests (pytest unittest), working with data warhouse, API integrations, writing python automation bots, then moved to the position of data engineer worked with the following tools:  Matilion (ETL), Argo (ETL) (now also Airflow) - write new ETLs, integrations with data sources, maintain existing, writing alert system (with message pushed to slack); Redshift as a Data Warehouse,PeriscopeData as data visualisation and Redash (for custom requests from non-technical colleagues), worked with Docker, Circle CI, Rollbar, Kubernetes, Git, REST API, also on this position also got some expirience with  AWS (Athena, Crawlers, S3, Lambda), after which I continued my way and decided to try to work in large companies, where I worked on a such projects: HealthCare (working with customer data, obtaining and searching for useful information) here I faced with Snowflake, dbt, Fivetran
I am looking for opportunities to develop within a small company, I am very interested in companies where everything is just beginning, and I can participate in various development processes, now I am studying machine learning, I also wanted the product to be related to this area."
data engineer,"I share github profile with my projects:)
My professional interests are in the field of data engineering and data processing:

- Design, implementation and maintenance of solutions for automation and time reduction data processing (writing Python scripts (preferably Pandas), functions in Google Sheets).

- Writing GUI applications for convenient use of scripts.

- Working with Google API for data manipulation and/or formatting to .csv, .xlsx.

- Searching, extracting relevant data using regular expressions.


And in my free time I am interested in:

- Building data pipelines on Google Cloud Platform.

- Optimization techniques SQL queries (preferably PostgreSQL).

- Approaches to writing efficient Python code: multiprocessing, multithreading, delayed functions.

Goal - step up to data engineer and grow up my knowledge of big data and data pipelines.
russian-loved companies - do not disturb"
data engineer,"
Work Experience:
        Completed projects involving GAN-based anomaly detection, using advanced techniques to identify irregular patterns within datasets.
        Contributed to software development projects, integrating APIs to enhance overall functionality and user experience.
        Currently hold a key role in a cross-functional team, actively collaborating on refining data-driven solutions.
        Utilized Python proficiency for tasks including web scraping, automation, and data analysis.

    Anomaly Detection Expertise:
        Applied GAN-based anomaly detection techniques to identify outliers and irregularities in complex datasets.
        Leveraged deep learning resume_classifier to achieve accurate anomaly detection results, enhancing data quality.

    Software Development and API Integration:
        Played a pivotal role in software development projects, focusing on API integration to expand features and capabilities.
        Utilized Python programming skills to interface with APIs, enhancing software functionality and user experience.
Remote work is a priority for me due to its flexibility and the potential to achieve a better work-life balance.
    In the event of a relocation, I would expect the role to provide clear benefits and opportunities for professional growth that would make the transition worthwhile.
    Regardless of the work arrangement, I am looking for roles that value innovation, collaboration, and ongoing learning.
    I am not inclined towards roles that lack a clear remote work policy or don't provide adequate support for remote employees."
data engineer,"• Successful deployment of telegram bot on Digital Ocean.
• Coding knowledge and experience improvement.
• Experience in remote work and communication with IT professionals.
• Successful Integration and setup of new software and hardware systems.
• Excellent cooperation with international teams.
Python developer / QA engineer	 Jan 2021 - Dec 2021
Self employed  Remote, Ukraine           	
	• Development of Python-based Telegram bot for English vocabulary quiz game with 5000 words database (Development logging, bug-tracking and bug fixing)
	• Regular meetups and remote communication with the CEO partner, experience in remote work and communication with IT professionals 
	• Continuous coding knowledge and experience improvement 
	• Successful deployment of telegram bot on Digital Ocean Cloud Service 

Digital and Radio Equipment Ships Officer	 Jan 2015 – Mar 2022 
Shipping Co. Worldwide            	
	• Provided maintenance and administration of ship’s computer software, hardware and local and satellite networks
	• Excellent cooperation with international teams in extreme working environments
	• Helped train new hires on company’s policies, operations, and day-to-day responsibilities
Work"
data engineer,"
Junior Data Engineer (Mar 2023 – present)
Forecasa
- Created and deployed data pipelines using Airflow, Python, and Docker;
- Implemented new dbt resume_classifier, and modified existing ones;
- Monitored Python and dbt pipelines performance, addressing and resolving failures promptly;
- Designed and implemented new PostgreSQL tables and sqlalchemy ORMs;
- Implemented ad hoc modifications for a PostgreSQL database;
- Performed numerous data transformations from raw JSON and CSV to PostgreSQL tables using pandas;
- Created tools to make the transformation process easier, faster, and more efficient;
- Created a testing suite for data transformations that checks data quality and integrity;
- Implemented benchmarking pipeline for OCR system, improved OCR process accuracy;
- Conducted frequent code reviews.

Junior Data Mining Engineer (Nov 2022 – Mar 2023)
Forecasa
- Built scrapers using selenium and requests, uploading the collected data to AWS S3;
- Processed and cleaned collected data using pandas prior to uploading it to PostgreSQL database;
- Implemented improvements for custom scraping framework that decreased development time, increased
efficiency, and resulted in a better code readability;
- Worked on code documentation and assisted in onboarding new members of the team
I am most interested in fields such as Data Engineering, AI, Software Engineering with Python."
data engineer,"
Python, Pandas, NumPy, SciPy matplotlib/seaborn (dataframes, aggregation, queries, data manipulation), sk learn resume_classifier.
PostgreSQL (aggregation, joining, grouping, case)
PowerBI/DAX
Have coding resume with examples.
Excellent English and good German. Robust experience in international IT business.
I look for gigs in data science and need support/education from employer"
data engineer,"Implemented a complex stream processing project from scratch. It's consist of two main parts: 1) AWS DMS task to collect changes from database in realtime and push them to a Kafka topic; 2) Apache Flink stream processing job to filter, aggregate and apply events as small batches to a target database.
Supported and improved existing ETL pipelines including requests to a third-party API providers on a daily basis, added new data source for the pipeline which uses PySpark for processing, Refactored and improved stability of big pipeline which queries internal resources and push the data to third-party API provider, Managed a team of Python Developers (10+ team members): onboarding, mentoring, reviewing.
"
data engineer,"Data warehouse migration from Teradata to Snowflake/Airflow.
Design data quality rules, bad orders collection. 
Tuning Data warehouse performance.
Mentorship of interns and juniors.
Supply chain, MRO supplier, E-commerce (US), 2019-now
- Data warehouse on Teradata/SQL, ETL/WhereScape, SAP, shell scripting, Kafka
- Data engineer with Python, Snowflake, AWS, Airflow, CircleCI.

Travel aggregator (US), 2017-2019
- Data warehouse on Teradata/SQL, ETL with Informatica, Hive, Qubole, QlikView, Jenkins.
"
data engineer,"Open source commits
Currently, I am working on a cybersecurity project with ML fraud prediction.

My main responsibilities are:
1. Developing and maintaining scalable microservice architecture while using FastApi/Django/gRPC, asyncio, protobuf, AWS services, and Apache Kafka.
2. Data modeling, query and speed optimization, and cost reduction. For databases Postgresql, Redis, Cassandra, Neo4j, and Elasticsearch.
3. ETL/ELT pipeline developing and maintaining using pandas, pySpark, Hive, Databricks, and Go programs.
4. Participation in ML development. ML ops.

The most detailed information is in my CV.

I like to share my knowledge and experience while applying them to develop non-trivial architecture. And my true passion is to learn new technologies to enhance system robustness and performance.
"
data engineer,"No special needs
I have more than 7 years as a Software Developer in C# & other languages 
AND 4 years experience as Data engineer 

In Summary:
.NET  Experience			5 Years
•	C#, .NET, CORE
•	REST, JSON, XML
•	Unit Tests, Integration Tests, TFS, SVN, GIT, Entity Framework

ETL / BI Experience			4 Years
•	Designing ETL data flows with different data sources
•	Designing and deployment of reports 

Cloud Experience			3 Years
•	Azure (DevOps, Event Bus, Data Lake, Blob Storage)
•	AWS (EC2, RDS, SNS, SQS, SES, Glue, Athena, Lambda, API Gateway, S3, CloudFormation, CloudWatch)

T-SQL / Databases Experience	3 Years
•	Installation, Configuration and Updates of SQL Server
•	MSSQL, MySQL
•	T-SQL developer skills including Stored Procedures, Indexed Views etc.
No special needs"
data engineer,"Over 7 years as Software Engineer.
Master's degree in Data Science (UCU master's program).
Google Cloud Data Engineer certification.
Built many distributed data platforms.
Geek and hacker. I like to read tech books and investigate new (old) technologies.
Good father :-)
Several projects:
- A distributed application that analyzed Instagram data (Text/photo/meta info) and produced the most important features for engagement rate (correlation features). Stack: Python/TensorFlow/Scikit-learn/Kubernetes/GCP
-  Distributed cloud application that makes anomaly detection and forecasting sales in retail data. Stack: Python/Scikit-learn/BigQuery(1TB of data)/online re-train
- Cloud stream infrastructure (and partly application) for sentiment analytics tasks and fake detection. Stack: Python/Apache Beam/Kubernetes
- Distributed cloud application (batch and stream) and infrastructure for crawling web pages.  Stack: Python/Scrapy Cluster/Apache Airflow/Apache Spark/Apache Kafka/Hive(as a data warehouse)/Kubernetes/GCP.
- Provided new approaches for improving data validation and monitoring data processing (near real-time). Stack: Python, Apache Airflow, Apache Spark, Grafana/PostgreSQL/Kubernetes/GCP.
- Distributed event source architecture. Stack: Java, Apache Kafka, Kafka Streams, Kubernetes, GCP.
I'm more interested in itself project that I'm solving than in various ""cookies"" in the company. For example, I would prefer a small and little-known company that solves a cool and challenging problem instead of a well-known company with a ton of ""cookies"" and boring tasks.
I prefer small teams.
I like to know business information about a task. What business goal am I solving, will this solution really solve it, how can I check this in the future, what is the specific benefit of the current task, and so on? The ratio is 80 (technology) / 20 (management).
I prefer projects in which I can do something new, but not more than 30-40% of all time."
data engineer,"
EPAM (Jun 2021 - now) - Software Engineer
Part of data migration squad. Sites migrating (British client)
Tools & Technologies: ETL, JSON, JSONata, XSLT, Docker, Mongodb, GitLab, Python, YAML, Kanban, GCP overview.

PWC (Apr 2020 - Jun 2021) - 	Business Systems Support Specialist
Supported different internal business processes for CEE region
Tools & Technologies: SQL, Visual Studio, SSRS, SSIS, SSAS, PowerBI, Google Tools, Scrum,
Python.

PWC (Dec 2019 - Apr 2020) - Data Analyst
Worked with data (extract, transform, load) to simplify the audit processes.
Tools & Technologies: SQL, Python (data transformation), Alteryx (ETL tool), KLIK Portal, iPower, MS Excel
"
data engineer,"
I've been working as a Data Engineer for the last 2 years designing and developing ETL data pipelines with Airflow, data scraping projects in Python and Node.js (using scrapy and puppeteer), data gateway API for validating and pre-processing all incoming data files (written in TypeScript with NestJS) as well as doing data processing with pyspark in Azure Databricks. I have experience coaching junior team members, doing code reviews and assigning tasks. 

For 1.5 years before that I had been working as a Backend Software Engineer mainly developing APIs in Node.js. Worked with  ElasticSearch, MongoDB, Redis, PostgreSQL, Lerna monorepos.  

I have limited experience deploying projects with Docker and Kubernetes, also some experience writing CI pipelines (CircleCI).
I'm looking for part-time projects, either remote or in Vilnius.

Will not work with blockchain/cryptocurrencies/online casinos."
data engineer,"* I learned Rust language in my spare time and used it to contribute to Firefox. Eventually, I got accepted to the program, where participants were paid for their open-source work.

* I worked in the large enterprise projects and tiny startups, solo and as a part of a team. I managed to meet expectations in all those setups. My work was recognized by the clients and by my teammates. I think this proves that I'm able to find a common ground with different kinds of people.
What I've been up to recently:
* Worked on an open-source Rust project
* Developed batch data processing pipelines(ETL/ELT) with Python, SQL and other tools
* Maintained legacy systems
* Designed meaningful analytical dashboards
* Trained end users, researched new tools, worked on proof of concepts
* Worked in client-facing roles quite a lot
I am looking for a team where I’ll be able to grow along with my colleagues, as well as utilize my existing skills and be useful. 

Besides Data Engineering, I'm also considering roles in Release Engineering/Ops/Infrastructure, as these are the fields that I'd like to explore more."
data engineer,"Scraped 28m records during 2 weeks from the site which bans users every 20 requests.
Rewrote old repository with 432 scrapers from python2 to python3.
Deployment of Apache Airflow and synchronizing its DAGs via Amazon S3.
Deployment of TOR as a microservice for proxying requests using HAproxy and Polipo.
Accelerated calculation and processing of geospatial data by 20 times
Worked as Tech Lead in a separate team.
Soft and polite scraping of floorplan images from different data sources (14m+ of images, 1.4+ terabytes).
Proposed and implemented architecture refactoring for P&L calculation which made it stable and 30x faster.
Worked with OpenAI LLM integration (using langchain) to create a chatbot which can use tools to query Elasticsearch and recommends some real estate properties based on users' needs.
Experienced Software Engineer with 4 years of expertise in Python, ETL, microservices, and web scraping. I have contributed to backend application and API development, handled large-scale data scraping projects, and worked on diverse data science initiatives involving satellite image processing, geospatial data, and data collection. Proficient in service architecture and event-driven design. Worked on the refactoring of legacy code. Additionally, I have actively participated in various CI/CD tasks.
Looking for a mixed (opportunity to work remotely and in the office) job, ready to work with backend tasks, data extraction, transformation and load (ETL), and additionally interested in R&D tasks."
data engineer,"
24yo
Study:
- National university of food technologies, master degree of computer science
- ITEA - Python for data science

Work expirience:
- Call center IT support  - sql developer, developing/support team
- Raiffeisen bank - data analyst, controlling department
- Microcredit company - Developer (sql, python) / data engineer - IT department

Work skills:

Spoken SQL (MYSQL, SQL Server):
- complex optimal queries, reports working with millions rows
- query / processes optimizing, bugfixing, crm / system db bottleneck searching
- Business processes monitoring (Grafana)
- SQL Server productivity monitoring, optimizing
- Jobs, SSIS/python data transfer (post/get), processing, datafilling
- adding back-functionality to existing erp/crm

Python3 (basics):
- Pandas, numpy, scikit-learn
- Data science/ machine learning basics (Certificated, ITEA)
- http requests  (basics), json

Additional:
- RDLC, BIRT reports
- grafana
- SSRS
- SSAS (Data mining, olap)
- knowage bi
- jira, confluence
- postman
- excel

Expirienced in:
- Bank data (DWH)
- credits data, loan calculations
- call center data
- integrations

Open for study/self-study, new technologies (like hadoop, nosql), new DB.
Not interested in only report developing / fixing /editing  work.
Not interested in DB administration.
Interested in machine learning tasks, big data, nosql, or other developing tasks."
data engineer,"
Hello, I have been a data engineer for the last year. I have experience in building storefronts, in building etl processes and orchestration, I work with ms sql, PostgreSQL, oracle sql, greenpulm from kafka and talend etl tools.
"
data engineer,"
Passionate Database Engineer with extensive background in different roles.

Worked with different databases Oracle, PostgreSQL, MSSQL, MySQL, etc. 

Passionate about using data to drive insights and improve business performance. I enjoy working in a fast-paced environment, comfortable managing large datasets and complex data structures.
"
data engineer,"
• DB:
   o T-SQL Development (MSSQL Server 
      2008/2012/2014/2016) and administrate experience;
   o PL/SQL Development (Oracle 10/11/12);
   o Experience with MariaDB/MySQL, HP Vertica;
   o Experience with OLAP and ETL tools (SSAS, SSIS)
• Reporting:
   o Power BI
   o Fast Report;
   o Microsoft SQL Server Reporting Services (SSRS);
• Python
• Web-Servers administrating:
   o IIS 7/8 (Windows Server 2008/2012);
   o Nginx (CentOS/RHEL);
• Development Dynamics AX and SSRS reports 
  (Microsoft Dynamics 365 for Finance and Operations);
• Version Control Systems: VisualSVN Server, GIT;
• Atlassian products (JIRA, Confluence, Bitbucket);
"
data engineer,"
• Building and management of IT infrastructure of the company
• Building saas aplication for sturtup (new version credit-buro), implement scoring system in company
• Working on projects integrating services and applications with DWH
• Working with large amounts of data, OLAP, ETL, API
• T-SQL, SSIS, SSAS, SSRS, PL/SQL, Power Bi, MongoDB
• Management reporting within the project (Jira, Trello)
• Development of the project documentation (technical and reporting)
• Python, PHP
• Management of an international remotely team of 6 people
"
data engineer,"* I'm very attentive to detail. I can usually spot a bug or two in every PR I review, saving from couple of hours to tens of hours on debugging for the team.
* I'm able to spot inconsistencies and defects pretty quickly when interacting with production systems. When I joined current company, I spotted a weird metric inconsistency in our pipeline software. This had turned out to be a bug that went unnoticed for 2 years and made nightly data pipeline run 20% slower.
* Played key role in migration of Big Data pipeline to the Databricks on AWS and Airflow. After migration, data pipeline started to run more that 2x faster. Wrote tooling and maintained progress dashboards, coordinating efforts of multiple teams.
* Love writing automation for routine tasks. Wrote a tool to parse and convert 650 Hive tasks to Spark to speed up the migration. Wrote a tool to execute airflow task locally by automatically setting up local environment and modifying DAG on the fly. It's used by a hundred developers.
* Authored multiple customizations to Apache Airflow in order to achieve business objectives. Some of these were open-sourced to upstream
* Proficient at tuning performance of Spark jobs and getting the maximum value out of your compute and storage.
* Write tooling, automation, documentation to help the organization use the data platform most effectively
I have extensive experience with Data-Intensive systems and Backend Applications since 2011.
Written microservices and monoliths for e-commerce, hospitality, banks, telecom. Provided Big Data solutions for telecom, retail and largest online library.
I'm leading teams of developers since 2017. 
In recent years I've worked with Spark and Hadoop ecosystems. Have a lot of experience with Scala, Java. Know some Python, Go.

I would like to deepen and broaden my expertise with cloud and big data tech. I would like to try my hand at machine learning.
I would like to work for the socially-conscious company that provides opportunities for all people and maximizes the good as much a possible.
Would like to avoid working on: gambling, advertisement, banking."
data engineer,"
Senior specialist
Arkon Consulting Company LLC 2010-2015
consulting
conclusion of sales contracts
insurance
keeping work documentation
Achievement: improved communication skills, improved sales, skills of working with VIP clients, solving problem situations.

Freelancer
2015-2023
consulting
insurance
support of purchase and sale agreements
Achievement: the ability to make quick decisions, take responsibility, learned to keep calm.
"
data engineer,"Certified in different cloud technologies(AWS, Azure) with demonstratable work experience in Node.JS development as well as a full-stack developer.
Excellerent - Nov 2021 - Present -  Data Engineer | Node.JS Developer
 Working as a team lead, leveraging Fivetran & ADF as the EL tool, and Snowflake as the Warehouse, automatically loading
data and transforming data utilizing DBT including normalization, type casting, aggregating, and further transformations for
different client customer data built an automated orchestration system using airflow in Kubernetes environment.
 Developed a reliable method for ingesting large datasets into an enterprise-scale analytical system using Fivetran, Snowflake,
and Looker which increased customers by at least 10%.
 ETL/ELT platforms were constructed using a variety of tools, including Azure Blob storage, S3, Redshift, and Snowflake.
 Designed a custom pipeline per client needs using Apache Kafka, PySpark, and Airflow using python as the primary
development language with event grid trigger from azure with a 14% increase in overall efficiency.
 Modeled a highly scalable back-end app using Node.JS for an automated skill and experience extraction system from a
client's CV which made the hiring process much easier.

10 Academy - Web3, Data Engineering, and Machine Learning Tutor (Contract)  Aug 2022 - Dec 2022
Designed curriculums and gave various tutorial sessions for the top selected 50 students from the entire Africa.
Presented 10+ workshops on Data Engineering, Deep Learning, and Machine Learning Operations and how-tos on Python, Git, Docker, CML, DVC, MLflow, GitHub actions, Airflow, DBT, and, Kafka.
Gave sessions on the fundamentals of blockchain, Dapp’s on Ethereum, and Smart Contract writing using Solidity & Algo.

Addis Software - Oct 2021 - Nov 2021 - Full-Stack Developer

TECHIN - Feb 2021 - Oct 2021 - Web Developer | Data Scientist

AIESEC - May 2020 - Feb 2021 - Node.JS developer | Partner Manager

MN ale Addis - Sep 2020 - Nov 2020 - Node.JS Developer
"
data engineer,"creation of end-to-end data pipelines in on-premises as well as in cloud environment
build maintainable, scalable and efficient ETL solutions for different business cases using relational SQL databases as well as distributed cloud solutions (Spark, Databricks, Azure Blob Storage, Azure Data Factory etc.)

apply statistical and machine learning techniques using Tensorflow, Keras, Scikitlearn, Scipy etc

Projects that I have been working on belong to the following areas:
* Fraud detection on top of the General Ledger (Accounting)
* Transition of Applications to Azure Cloud  (Finance)
* OCR/Computer Vision (Accounting)
* Development of Customer Life Time Value algorithms (eCommerce)
* Valuation of derivatives (Banking)
"
data engineer,"Automatisation regular tasks, which are part of production. Implementing algorithm for outliers detection, good performance, high roc auc score.
Junior data scientist
* Comparing advantages of usage preventive maintenance for high loaded vehicles, other than while equipment failure occurs.
* Project using Bayesian networks (health field)

Technologies: Python(pandas, numpy, matplotlib), openAiGym, Reinforcement learning.

Big Data Analyst / Engineer
* Exploratory data analysis
* A/B tests
* Visualization
* AWS data services
* Building tools for data quality and monitoring
* HTTP
* Monitoring data completeness, reporting and fixing gaps
* Setting up Monitoring dashboards

Big Data Engineer
* Building ELT/ETL pipelines
* Creating Airflow dags
* Dockerizing solutions
* Spark
* AWS (EMR, S3, Athena, Redshift, ECR, ECS)
* Gitlab CI

Technologies:
Python(pandas, matplotlib, numpy), Grafana, Airflow, Spark, AWS ( Athena, Redshift, EMR, ECS, S3), PostgreSQL, MySQL, Gitlab, Jira, Confluence.

Pet projects:
* weather service (Django, postgresql, docker), 
* Analysis car prices (SQL, bigquery, beautifulsoup, pandas, bumpy, matplotlib, sklearn).
"
data engineer,"
Experienced with: Python (np, pd, re, matplotlib, sqlalchemy, scikit-learn, Flask), PostgreSQL, Supervised
Machine Learning, Feature Engineering, Data Mining, Data Analysis (descriptive, exploratory, predictive,
statistical), Hypothesis Testing, Algorithms and Data Structures, Creating and using REST APIs, Apache Kafka,
Linux, Git, Jira, Confluence.

Basic knowledge of: C++, C#, Java, OOP, HTML, JS and Software Engineering in general.

Data Scientist at Amazinum, Jul 2022 - Jun 2023 (1 year)
Skills:
- Data analysis
- Hypothesis testing
- Feature engineering
- Selection and training of supervised ML resume_classifier
- Writing scripts in Python
- Writing complex SQL queries
- Processing hundrends of millons of rows of data with PostgreSQL
- Creation and filling of relational databases
- Deployment of ML resume_classifier with Flask
- Experience with Apache Kafka

Developer of trading strategies with Binance REST API in Python, Self-employed, 1 year of experience
Skills:
- Data analysis
- Hypothesis testing
- Plotting of charts
- Complex algorithm development from scratch 
- Writing a large amount of scripts in Python
- Cleaning and processing big datasets on local machine
- Building algorithmic and ML resume_classifier for market price forecasting
- Using Binance REST API methods to automate workflow
Main workflow:
- Collection of trading data from Binance
- Creation of strategies and testing them on historical data
- Creation of programs/robots that trade according to a given strategy/algorithm on Binance
"
data engineer,"Several courses in the internet connected to data science, data engineering, machine learning.  ##########################################################################################################################
No experience, only online courses. Гражданин Украины.
Data science, data analytics, data engineering, visualisation, mathematics, statistics, machine learning, sql."
data engineer,"
•	Work in a dev environment managing purpose-built, highly available, distributed, scalable cloud services;
•	Good problem-solving skills, with attention to detail and focus on quality;
•	Solid understanding of mathematical foundations behind Machine Learning; algorithms, and comfortable with discussing them in details;
•	Good understanding and background in probability theory, random process, statistics, and optimization techniques;
•	Solid knowledge of linear algebra;
•	Previous experience with either IR, NLP, CV, Text mining, machine learning or big data mining is highly desirable;
•	Proven track record delivering enterprise-grade, scalable, secure & reliable software systems;
•	Good experience in developing highly scalable machine learning/deep learning-based applications and services;
•	9 + years of experience in analysis, design, and development of client/server, web-based and n-tier application. Expert in developing windows applications, web applications, windows services, and web services;
•	Ability to work on data mining, data science projects with an application; engineering, quality engineers and product management.
"
data engineer,"– experience with Database Management (MS Access, MySQL, SQL, CA ERwin,
PostgreSQL, T-SQL)
– experience with graphics editor (Photoshop, Corel Draw, GIMP)
– experience with Python (Scikit-learn, NumPy, Matplotlib, Pandas, Seaborn,
Keras)
– experience with Wolfram Mathematica, MathCad
– basic knowledge Java, C++, HTML, CSS
2021-Present Data Science Department Specialist PUMB

2020 – Present Master‘s Degree, Applied Mathematics
Kharkov National University of Radio Electronics. State
– Data Mining
– Stochastic Programming

2021 Coursera: Machine Learning
Broad introduction to machine learning, data mining, and statistical pattern recognition

2020 GlobalLogic: GL ML BaseCump
NLP, treets, forest, boosting, clustering, neural networks

2019 Cisco: Programming Essential in Python

2016 – 2020 Bachelor‘s Degree, Applied Mathematics
Kharkov National University of Radio Electronics. State
– Basics of Machine Learning
– Databases and Information Systems
– Probability Theory and
Work with data, interesting tasks, work on real projects, trainee, improve  my skills"
data engineer,"
4 years of experience as a Data Scientist and Data Engineer.
Have various experiences:
 - creating/researching resume_classifier and deploying them;
 - creating an analytic platform from scratch;
 - creating data pipelines and strategies for live trading.

Also, have some management experience.
Main language - Python. 
Prefer AWS as a cloud service.
I'm looking for a friendly, life/work balanced environment focused more on interesting challenges rather than simple support of the existing applications."
data engineer,"Developed a Medical Information System for a clinic last year.  It’s in production.

Got a Master's degree in Computer Science last year.
Develops software over 20 years as a lead, developer, application architect. Last year switched to full stack web development and works with JavaScript/ReactJs for front-end, and Java EE PostgreSQL for backend. 

Participated in scientific researches, used statistics methods and data mining. Has scientific publications.

Has comprehensive background in business application development.

Has technical leadership background - seven years.

Now is interested in growing data analysis skills and looks for a team to work with.
Looking for a comfortable team to work with. Looking for a part-time job."
data engineer,"
I began my career in 2019 as a Business Analyst at Air Astana, the largest airline in our country. I got this opportunity through a competitive internship organized by the National Welfare Fund of the Republic of Kazakhstan. During the internship, I worked on implementing processes for the Purchasing and Ground Services Departments using the ELMA BPM system.

In 2020, I interned as a Data Analyst at Kolesa Group, a leading company in the classifieds business. I created useful dashboards and conducted tests to improve conversions, such as achieving a 2.5x increase in Auto Credit sign-ups.

Later, I joined KazMunayGas Engineering as a Software Engineer in the Big Data sector. I focused on managing data in the MySQL database and developing backend features for monitoring company performance.

Currently, I work as a Data Scientist at Technodom, a major electronics retailer. During the work we leverage diverse customer data, including external data parsed from various sources. I have executed projects encompassing NPS classification, churn prediction, next purchase prediction, and demand forecasting. These initiatives involve comprehensive data analysis to generate valuable insights and predictions, enabling informed decision-making. We also utilize AWS for efficient data storage and analysis.
• Working with an international team
• New challenges
• Learning new technologies
• Solving big scale problems
• Looking for data science/data engineering/data analyst positions"
data engineer,"
I have 5+ years of experience working in data science and machine learning field. My experience covers different industries, such as Banking/Fintech, AI consultancy, Food Delivery/Quick Commerce, Talent Management, Cyber Security, Software Development, etc. During these years, I gained valuable hands-on skills in data science, machine learning, and software engineering fields.

Few projects I've worked on:

Cyber Security
- Anomaly detection model to identify possible abnormal behaviour observing windows events.

Search Engine:
- Build and productionized search engine where I extract tens of thousands of data points in real time and rank them using different measures. Here, I mainly do data engineering stuff.

Matching Engine
- Build matching system to match job description to job candidate and vice versa.

Anomaly Detection Model
- Build real time anomaly detection model for time series data
- Build time series similarity measure model to identify ""sister"" (similar) time series.

Question-Answering Model
- Build and deployed QA model to process PDF documents and provide answers to the pre-defined list of questions.

Credit Scorecard Model
- Build credit scorecard model to automate loan disbursement process
- Conducted market basket analysis to reveal customers new segment

Find a lot more info in my CV.
I'm looking for new challenges. Where, I will gain new skills, will learn new technologies, and will grow professionally in every direction. The place and the team, that have can do and everything possible attitude. They share their experience and help team members to improve."
data engineer,"- Kaggle: Santander Product Recommendation [TOP 15%],
House Prices: Advanced Regression Techniques [TOP 4%]
- Have built a web-crawling agent that allows to retrieve automatically Text of all job openings found on indeed.com and linkedin.com websites with respectto user request (job Title, place, Salary, etc.). Advanced filtering involves NLP (stemming, lemmatization, Ngram) and regex for Text cleaning, TF-IDF algorithms for feature extraction. The result - one can plot, for instance, top-X skills required for any position in any city or country in the world. Steck of technologies: Python (nltk, BS4),
PhantomJS, Selenium.
- Deployed from sketch Hadoop 2.x ecosystem on multi-node cluster: HDFS, YARN, Pig, Hive, Hbase, Sqoop, and Spark.
Key expertise:
• Python: 5+ years, Hadoop ecosystem: 2+ years. 
• Fluent English, Intermediate Italian and French.
• Machine learning (classification, regression, clustering): 2+ years. 
• Web crawling (Python, requests, BS4, PhantomJS, Selenium, NLTK, regex).
• Design and implementation of Batch jobs using Sqoop, Hive, MapReduce and SPARK.
• Development and running of SPARK applications on Hadoop YARN multi-node cluster using Python (analytics on data, ML tasks).
• AWS EC2. 
• RDBMS (MySQL), NoSQL (MongoDB, Apache HBase). 
• PhD in Engineering (Italy).
• Currently employed in EU.
- Challenging projects using latest steck of Big Data technologies and frameworks 
- Excellent working environment
- Location of the office: UA (Lviv or Kyiv)"
data engineer,"
Strong data scientist with experience in software development. 

Professional skills:

- Good experience in predictive analytics
- Comprehensive knowledge of data validation and data analysis process and techniques
- Broad experience in the financial services industry and multi-disciplined wealth management expertise
- knowledge of PM-book standards for developing project management documentation, Jira, Scrum, Agile

Technical Skills:

- Proficiency in statistical and mathematical packages: SPSS, SAS, R, R-Studio, Python
- Python, Tensorflow, Keras, PyTorch, Pandas, scikit-learn etc
- Experience in DB: Sybase IQ, MSSQL, PostgreSQL, MySQL
"
data engineer,"
Passionate developer with 4+ years of experience in Python programming, focused on Data Science for the last 3 years, mainly in the biomedical industry and wearable devices.

Worked on R&D of DL\ML\classical algorithms for biomedical signal analysis(mainly 1D-like signals like Respiration, ECG, PPG, etc), working on tasks of data exploration and visualization, segmentation, classification, tools for data labeling, classical algorithms optimization with Cython, report generation, testing tools, working with SQL databases.

Have experience turning algorithms into microservices (Flask\aiohttp\gunicorn\docker), some minor experience with GCP\Microsoft Azure.

Having wide experience in different fields, I can be a Data Scientist, Data or Backend Engineer, or both of these roles.
"
data engineer,"
Analyse data • Collect data • Prepare regular reports •  Design and develop reports, BI dashboards • Provide 
Top Management team with deep insights regarding company and applicable activities • Support in decision-making • Data analysis to detect the reasons for deviations in the main and secondary metrics • Bringing early attention to issues or changes in trends related to KPIs • Ensure that data is tracked in the proper manner • Looking for alternative data sources and tools • Collect data from different place• Design and develop dashboards, repots (MS Power BI) • Create visualizations and presenting them to the team and management

Analysis • Power BI • Data Analysis • SharePoint • Power Automate • Power Apps • Tableau • Statistica • SAP • MES • SPS • GIT
"
data engineer,"
Data Scientist Network, Lagos, Nigeria.
Data Scientist 
	Built an end-to-end computer vision model that could classify Community pharmacies (CP) and Proprietary Patents and medicine Vendors (PPMV) stores in local areas in Nigeria using Python, TensorFlow, Keras. 
        Built a Retail audit Computer Vision model using YOLO architecture to recognize over 500 FMGCs products in Nigeria, determine the visibility and provide detailed analytics based on the detected FMCGs products (PyTorch, TensorFlow, FastAPI).	 
        Design and implement end-to-end data engineering pipelines to acquire, prepare, and store data in the data warehouse using data engineering standards (ETL/ELT).
	Develop a detailed project plan identifying the key milestones, resources, deliverables and track project KPIs over time.
	Develop real-time analytic dashboards using Power BI for insightful decision-making and to track project progress.

MeasureIT Analitika, Ibadan, Oyo State, Nigeria.
Data Analytics and Visualization Associate
	Designed and developed an interactive dashboard to track and evaluate key performance indicators using Tableau, Microsoft Power BI, Microsoft Excel, and Google Sheet
	Prepared and coordinated monthly analysis and visualization.
	Reviewed, cleaned, and validated converted datasets for possible insightful analysis. 
	Supported the development of the Multi-source Data Analytics and Triangulation dashboard.
"
data engineer,"
For the last year, I've been working in the R&D department of a Ukrainian retail company as a Data Scientist.
Before that - two years as a freelancer Data Analyst / Data Scientist via Upwork. Earlier - 8 years in scoring resume_classifier' development, risk management, and banking analytics.
My main tools are Python and SQL, used to work with R. 

My last projects:
- an auto-replenishment algorithm, with a short-term sales forecast for supermarkets. Python, Jupyter Notebook, Time Series Forecast, MS SQL Server, Azure DevOps;
- KPI metrics for employees and supermarkets. Power BI, MS SQL Server;
- customer journey analysis, marketing attribution resume_classifier. BigQuery, Python, SQL, DBT, BitBucket, Hubspot;
- ad-hoc analysis and dashboards on retention, conversion, sales, customer structure, helpdesk, and Email campaigns, designing experiments in prices, discounts, and free products, developing mix marketing, attribution, and customer churn prediction resume_classifier. Python, BigQuery, Google AutoML, Periscope / Sisense, R, Zendesk, Fullstory, Mixpanel, Shopify, Stripe, Blendo, Xero.
"
data engineer,"
Data Engineer/Data Modeller/Data Scientist(healthcare domain) 03/2022 - present
- Data Modeling;
- Looker refactoring;
- Communication with stakeholders;
- Data Science tasks(e.g. classification, topic modeling);
- Documentation maintenance;
- Hands-on technology stack: AWS, Snowflake, Looker, dbt, Snowshu, AWS SageMaker, Great Expectations, Confluence, Jira, Bitbucket, DBeaver, Docker, Python(pandas, numpy, sklearn, nltk,TensorFlow/Keras, PyTorch,XGBoost, seaborn/matplotlib and etc.), SQL, Anaconda.

Data Analyst(financial domain) 09/2020 - 03/2022
- Analysis of Macroeconomic data across the globe;
- Provide Data/Customer support using ITIL framework;
- Researching data, insights on the internet;
- Communication with Customer/Data providers on their/our needs as a service provider;
- Data Engineering;
- Extra mile: successfully passed internal Data Science Mentoring program and able to do: EDA, Hypothesis Testing, Regression/Classification tasks, Clustering, Anomaly Detection, NLP(Text), Time Series Forecasting, NN and CV, NN and NLP(Text)
- Hands-on technology stack: AWS, ServiceNow, Confluence, Jira, Swagger, Rundeck, Hbase, Postman, Kibana, Python(bs4, selenium,pandas, numpy, sklearn, nltk,TensorFlow/Keras, PyTorch,XGBoost, seaborn/matplotlib and etc.), SQL, Azure ML;
"
data engineer,"Azure Data Scientist Associate

Coursera specialization:
Machine Learning & Data Analysis Specialization
Deep Learning Specialization
Algorithms specialization by Stanford University
Data Scientist/Data Engineer with math background and 5+ years of experience in building data-driven solutions (7+ years in software development in total). Passionate about solving different problems alongside learning something new.
For most of my relevant experience, I worked for an outsourcing company(Big Data and Analytics). As a member of the R&D department, I worked on mostly short/mid-term projects and internal ones for the company itself. 
My experience is balanced between Data Science and Data Engineering, sometimes dealing with both roles on the same project. you can find details on the latest projects below.

Data Engineering Projects – Databricks Engineer
- Project result: Speeding up an existing e2e solution by 10x times using big data  tools and frameworks
- Designing and implementing ETL workflows for collecting structured and semi-structured data
- Building Quality Control framework for data cleansing
Tech stack: Python, PySpark, SQL, Databricks, Azure platform

Sales Forecasting Project – Data Scientist/ Data Engineer
Project result: developed helper tool for the sales department, which is used for decision-making process 
Designing and implementing ETL workflows for collecting data
Building ML solution to forecast product sales based on early market lifetime
Tech stack: Python, ​Pandas, Scikit-learn, XGBoost, Azure platform

Employee Leave Forecasting – Data Scientist
Project result: developed PoC helper tool for the HR department
Parsing and collecting data from different sources (Databases, excel files, third-party APIs)
building ML resume_classifier to predict employee leave
Tech stack: Python, ​Pandas, Scikit-learn, XGBoost
"
data engineer,"
I had experience in commercial projects as Unity Developer (C#). Along with good code-style practices I also gained experience in the product development cycle and working in a team. 

When it comes to working with data, I have experience with Python (including relevant tools for engineering: Kafka, PySpark, MongoDB, and as well for data analysis and machine learning: Pandas, Numpy, Scikit-learn, Scipy)

I have a few pet-projects working with all those technologies, including:
- Image Segmentation (Research and comparison of algorithms, working with CNN + PyTorch implementation)
- Wikipedia Data Stream Processing (Implemented system which processes Wikipedia stream data with the further access to it from REST API endpoints using Apache Spark Streaming, Kafka, Cassandra, FastAPI)
- Matrix factorization (collaborative filtering) for recommendation systems (explore the use of matrix factorization techniques for collaborative filtering in recommendation systems - implementation of Singular Value Decomposition, Non-negative matrix
Factorization and Alternating Least Squares algorithms.)

I'm interested in growing in this direction and learning new things.
"
data engineer,"
Production Engineer intern
Azertexnolayn 

Observing technological processes in the workplace, recording and reporting problems that occur during production process, controlling equipment, and other necessary actions providing the continuity of production and operations.

Data Analyst intern
AZBADAM LLC | 18/10/2020 - 10/04/2021

 I used analytics tools such as Power BI and Python to create forecast analysis and reporting.
 My responsibilities were:
Interpreting data, analyzing results, using statistical techniques and providing ongoing reports.
Analyzing data and present data through reports that aid decision-making.
Locating and defining new process improvement opportunities.
"
data engineer,"Worked as the Chief Support Operations Lead with 200+ agents in stuff, setup and optimised the whole support pipeline as the service.

Was a part of business consulting team, worked on 280 business processes: design, analyse, optimise, process re-engineering.
As a part of cross-functional team, managed to create the approach for Customer Analytics (Customer Feedback River), using data-driven approach (SalesForce, R, Google Cloud Platform : NLP, Tableau) along with Customer Satisfaction Metrics were developed as the outcome, based on that the whole customer acquisition pipeline is optimised. 

Developed more than 250+ reports using R + Tableau with fully automated process. 

Continuously working with 3-rd party API's in order to pull, push the data between the services like: FastSpring, WorkFront, Google Cloud Platform, LiveChat, SalesForce, other custom CRM systems with private/public API.
Worked as Business Consultant (Business Analyst) / Data Scientist (R Developer). 
- 7 years of overall experience working in IT
- 3 years of work as Data Scientist, Data Analyst, Data Engineer
- 3+ years on freelance with 10+ projects as BI Specialist, Data Engineer

Helping analyse business-related problems using data-driven approach in relative domain fields.
Building clear visuals and insights (R and Tableau) that drives business to go forward.
Providing data processing using R with Python pipelines (using 'reticulate' framework), data cleaning, data pre-processing, data wrangling, data visualisation using Tableau (Desktop+Online), R (Shiny Dashboard), data mining. 
Using predictive modelling to increase and optimise business outcomes as well as executive operational excellence (Sales Revenue Prediction, Customer Segmentation, Optimising Customer Support pipeline, etc). As well as analytics in: 
- Billing and Finance
- Customer Support Excellence
- Employee Utilisation rate
- Customer LTV Analytics / Cohort Analysis / RFM / Churn Rate
- Retention Analytics 

Experienced user of 3-rd party API's for data analysis:
SalesForce v.36-v.49, WorkFront, Google Cloud Platform API (NLP), FastSpring API. Custom web-scraping techniques including Text mining and data harvesting (PDFs, web-pages, custom forms, etc), Tableau API, working with datasource, report management, administration.
All project are implemented, using version control (GitHub).

As a part of cross-functional team, helped to implement data resume_classifier and monitoring metrics and outcomes, building new business rules and operational flows using data-driven approach.
Looking for interesting projects based on new technology, it could be either product company or other, working with developing and implementing AI / ML solutions, with social security offer included.
Want to be a part of professional team. Ready to fit the company's needs, continuously learn and self-develop, by acquiring new skills and knowledge. Want to focus on building complex AI / ML processes (ready-to-deliver), using existing technology or by creating the one as the part of the team.
Want to be able to apply my knowledge of R, data visualisation techniques, data-modelling skills, in order to build the great product, service or just to bring the expected result to the client dealing with real-world problem."
data engineer,"
15+ years of experience working with data.
Programming languages: R, Python, SQL;
Skills: geospatial analysis, prediction resume_classifier, behaviour prediction, process mining, recommender systems, NLP, deep learning, web scraping;
Сloud computing: AWS, Google Cloud Platform;
Databases: PostgreSQL, MySQL, Mongo, Redis, DynamoDB, AWS Athena/PrestoDB, ElasticSearch, etc;
Frameworks: Tidyverse, Scrapy, Numpy, Pandas, Sklearn, Tensorflow, Keras, Pytorch, Airflow, etc;
Web: Flask, Django, CoffeeScript, Flexdashboard, Shiny;
Cluster computing: Apache Spark;
Geospatial: Postgis, QGis, R packages, geopandas;
Project management: Scrum, Kanban, LeanDS;
Other: Docker, Git, Jira/Trello, Nuclino.
"
data engineer,"
Data Scientist with a strong statistical background and 2+ years of experience in predictive modeling, data processing, data visualization, and data mining algorithms to solve challenging business problems. Highly motivated in delivering data-driven and customer-centered software applications. Proficient in Python programming, SQL, data visualization with Tableau, statistical modeling, and applied machine learning in production. Experienced in preprocessing, cleansing, and validating structured and unstructured data for further analysis.

I would describe myself as driven, hardworking, resilient and resourceful individual who maintains a positive, proactive attitude when faced with adversity.

KEY COMPETENCIES:
•Python(Programming language)
•C++(Programming language)
•R(Programming language)
•Java(Programming language)
•SQL
•Data visualization with Tableau
•Statistical modelling
•Applied Machine learning in production
•Deep Learning using Tensorflow and pytorch
•Web scrapping using beautiful soup and Scrappy 
•Data structures and Algorithms in C++/Python
•Big Data Analysis and big data tools(Hadoop,Apache spark,pig and Hive)
•Cloud computing-Amazon AWS
•Natural Language Processing (NLP)
•Data warehousing and ETL tools(Talend)
•NoSQL DBMS -Mongo DB and Hbase
"
data engineer,"
I have 1.5 years of experience in the field of financial analytics. For the last year I have been engaged in in-depth study of machine learning on a python, mainly computer vision.

Now I'm working on pet project. It's about object detection and trajectory tracking using YOLO and KalmanFilter.
For a now I am open to any projects, especially in a friendly and cheerful team."
data engineer,"Got a chance to be an intern at Google this year, helping to solve a challenging but important problem with abuse fighting.
Awarded a partial​ scholarship during the first year at university based on previous achievements.
Top 3%​ among U​krainian students ​based on entrance tests results (199/200).
Work Experience:

Summer Trainee Engineering Program Intern at Google, Zürich, Switzerland (July – October 2022)
As part of the Privacy, Safety, and Security team, developed a visualization tool that tracks abusive accounts and provides visualizations of their actions and characteristics. This tool is used by analysts to simplify their day-to-day work, resulting in increased quality of the abuse detection system in the long run.
Conducted research of potential solutions for the problem, as well as technical aspects, which led to results being of high quality.
Technologies Used: Google development stack, Python, Pandas, GoogleSQL, HTML, Typescript & Vega.



Speaking about my projects:

Image Stitching (February – June 2022)
Used Python, OOP & Linear Algebra Principles to implement an algorithm that composites multiple overlapping images captured from different viewing positions into one natural-looking panorama. 
Implemented from scratch horizontal and vertical stitching for several images using the K-Nearest-Neighbors algorithm & David Lowe's ratio test for Feature Matching, Homography estimation with Ransac algorithm, and SIFT algorithm for keypoint detection.

Sportify, Project Manager (February – June 2022)
Managed a team of 7 students. Performing competitive analysis and market research, defining a unique selling point (USP), and creating a strategy and implementation plan. Developed high-quality benchmarking, technical documentation, and mockup using Figma, resulting in an easy-to-use marketplace for coaches and sports clubs' clients, which increased demand for sports activities by 23%, according to a survey of coaches and gyms.
 
Spam Classifier (October 2021)
Used Python, Pandas, NLP principles, OOP & Multinomial Naive Bayes algorithm to develop a spam filter with 96% accuracy. 

Musical Telegram (April – May 2021)
Used Python & correlation ML model to develop a Telegram bot with Spotify API integration that simplified finding new music based on preferences for 300+ students.

Friends on Map, February 2021
Used Python & HTML to create a web application for friends’ locations map generation of any Twitter account by incorporating Twitter API and Flask framework.

I am an active volunteer with broad experience organizing and assisting in major events. Speaking about some of them:
Designer & Website developer at TEDxUCU
Organizer of Apps Summer Camp
Tech-Supporter for Lviv Data Science School
I would like to work in an open team where you can develop and gain experience from each other.
Passionate about gaining more knowledge and experience in Data Science, Python engineering, Computer Vision, Artificial Intelligence, and NLP."
data engineer,"As a diploma project made research in ""Automated architecture search of deep neural networks for image classification"". Model based on searching CNN architectures within a continuous domain using gradient descent (solving bilevel optimization problem).
I have a Master's degree in Data Analytics. In Data Science for 4+ years. Before worked as a python developer (2 years) and Game developer (developed augmented reality games in Snapchat for 2 years).

Past projects:
- Building databricks workflows for big data to find car signal anomalies. Transform and organize big data from data lake. Build ml pipelines to find anomalies. (Spark, Databricks, mlflow, XGBoost)
- Video recommendation service. 
Developed pipeline on Apache Airflow with the switcher of resume_classifier: deep recommender and two-step model. (user-content based resume_classifier), A/B testing
- Service to tag invoices (NLP) 
Build AWS Sagemaker pipeline with different resume_classifier (classification, week-supervision).
     Also, I led the team and worked directly with the client on an experimental stream doing various things: PU learning, Multiple-Instance learning, Anomaly Detection, extract not biased subsamples
- Demand forecast for suppliers to optimize logistics.
Developed a flexible pipeline that is able to be trained on different data for different markets and countries. Optimize warehouses logistics (Databricks, Spark)
- Service to recommend an optimal price for hotel rooms to maximize income (time-series)
There was a lot of work customizing architecture for the encoder-decoder model, preventing leaked features (monotonic constraint), and finally extracting the desired features dependence. 
- Predict returning of the customer (Big data, Big query)
- Service for lungs diagnosis. (Computer vision)
The service segment lungs, and is available to detect 10+ diseases, and segment pneumothorax
- Anomaly detection service for scraping company (Semi-supervised)
Complex feature engineering work. (fighting with aggregated-correlated features)
- Service to predict policy price using minimal input data for insurance companies. 
Besides standard work with the model and service, there was done analysis on feature importance, using SHAP and own techniques.
I am currently located in Europe."
data engineer,"I have done many experimental researches on various segment on machine learning.
I have built data engineering structures for 2 companies I worked from scratch.
Hello, 
I have 4 years of experience to work as Data Scientist and Data Engineer
- I can and did build Text summarizing, QA, Chatbots,  Named Entity Recognition on scale from end to end including devops processing.
- I have experience with various computer vision tasks such as Image classification, Text to Image, Image segmentation, etc
- I have experience in data engineering wir building ETL pipelines using Python, DBT, databricks, airflow, etc.
- I have experience with customer focused Machine Learning products such as churn prediction, conversion rate optimization, recommendation, anomaly detection, etc.

Tools&Stacks;

Coding; Python.
Data engineering; DBT, AWS, GCP, databricks, Airflow, Tableau.
Databases; MySQL, PostgreSQL, Bigquery, mongodb.
ML frameworks; Pytorch, Keras, Sklearn, transformers, Opencv.
A friendly and challanging enviroment"
data engineer,"IEEE VAST Challenge 2018 MC2, Identifying patterns and anomalies within spatiotemporal water sampling data. Our team won an award for ""Elegant Design of an Interactive Display"" (presented on the conference IEEE VIS 2018 in Berlin). My responsibilities included association rule mining (Apriori algorithm), correlation analysis, and visualization [R, SQL,Tableau];
Graduate of a German university with 4+ years of professional experience as Data Engineer and 2.5 years as Data Scientist. Highly skilled in SQL, Python, Machine Learning, and Data Visualization.
I'm a citizen of Kazakhstan, currently based in Almaty.
Work experience:
Data Scientist (Germany) - half year.

Paraphrase generation for coupon titles by using neural networks (Pytorch, LSTM, Bert, Spacy, Flask).

Research Assistant (German University) - 2 years

Data quality assurance, data cleaning, and data visualization (Python, Pandas, Numpy, Matplotlib);
Twitter Sentiment Analysis using Python and KNIME;
Geospatial data visualization using JavaScript (D3.js, Leaflet) and PostgreSQL (PgRouting, PostGis);

Data Engineer (Kazakhstan) - Data Warehouse Project:

Worked on all phases of data warehouse development lifecycle, from gathering requirements to testing, implementation, and support.
Data Sources: Oracle DB, WAY4 OpenWay, SAP ERP.
Developed ETL pipeline (Pentaho Data Integration, PL/SQL).
Used various aggregation functions for the BI reporting systems.

AML and Anti-Fraud Project:

Analyzed the current legislation for fraud detection in financial data.
Developed various PL/SQL subprograms like Stored Procedures, Functions, Jobs, and Packages.
Performed optimization and tuning of PL/SQL code and SQL queries.
Designed scenarios to detect suspicious transactions.
I would like to focus on classical machine learning and analytics rather than deep learning and computer vision"
data engineer,"
Experience:
• 2+ Amazon Web Services (S3, Glue, Athena, Lambda, CLI, Management Console, Systems Manager, Lake Formation, RedShift, IAM) + Terraform
• 2+ PostgreSQL (perfomance tuning)
• 2+ Apache Airflow (Python), Apache NiFi , Apache Zeppelin, Apache Kafka
• 2+ Grafana Dashboard Monitoring, Git (CI/CD with GitLab), Prometheus, Kibana, Docker, Kubernetes (Lens, Rancher, k9s), Tableau (Service Management)
• Integrations:
 Microsoft Dynamics with Oracle
 Salesforce with PostgreSQL (SOAP API)
 Salesforce with Amazon Web Services (Lambda)
 Google API to S3 Transfer

• 3+ Oracle Database 11/ETL (SQL, PL/SQL, performance issue resolution)
• Data Warehousing ETL experience of using Informatica Powercenter
• Troubleshoot and tune complex database query performance issues
• Handling numerous enhancement and maintenance requests, data updates, bug fixes
• Full cycle of SDLC / ITIL (GIT, Jira/Trello, Mantis, UML)
• Backlog / Incident / Change Request management (PagerDuty)
• Conduction meetings with USA / EU stakeholders and presentation the results in BRD / TDD.
• Gathering, analysis and documentation of functional and business requirements (Confluence)
• PM/Business Users' assistance in optimizing the scope, benefits and risks of proposed projects.
• Strong communication skills with USA, EU, Indian clients


Key skills
Backlog / Incident / Change Request management, SDLC establishment and follow-up, Application / Support development, Work with local and remote/distributed teams, Fluent English/French (C2/C1).

Amazon Web Services, Apache Airflow, PostgreSQL, Oracle, PL/SQL, SQL, Trade Data Warehouse (DWH), GIT, ITIL, Informatica Powercenter
"
data engineer,"- Led a project for the transition to a new banking system;
- Building Churn resume_classifier;
- Develop an analytical platform for searching potential clients;
- Increasing the conversion rate of approved loans for current resume_classifier;
- Creation and optimization of the logic of collection processes, process modeling, forecasting the volume of processing, analysis of collection processes, approval process, risk indicators of the loan portfolio;
Main skills:
Databases: Oracle, MS SQL, My SQL, PostgreSQL;
BI systems: Qlik Sense/View, Oracle BI 11/12, SAP BO;
Data analysis, ML: pandas, numpy, sklearn, MATLAB, A/B testing, Ad Hoc, Research;
Tools: ETL(informatica), Oracle Enterprise Manager, DWH(structures);
Languages: PL/SQL, SQL, Python, OOP, VBA;
ability to develop skills for data science;
learning new technologies;
from Kazakhstan looking for remote job."
data engineer,"
In spite of graduated CAD department of the university and having good enough understanding how electricity via silicon highlights rgb points behind the glass, I moved to software development area. 4 years I worked as a c++ developer in a different area, every new project gave new portion of knowledge, not only technical.

From 2004 being a member of amazing team of initially several people created and still improving one of the best solutions in Transport Freight Audit area, responsible for Business Analysis, Data model and business process on it, System Administration, DB Administration and Development, EDI and DevOps activities, Level 2-3 support. 

ControlPay company with than 250 members after 16 years joined Transporeon company to provide more efficient different services in Freight area as one team.
"
data engineer,"I worked with Telecom project and Financial project
•	Created procedures and functions in the database
•	Calculation of various KPIs
•	Created pipelines for the ETL process
•	Development of dashboards
•      Development of DWH
•      Optimization of scripts
"
data engineer,"I'm quick learner and do not afraid to get my hand dirty in code and learn new technologies.
Sep-2022 - Dec-2022 - data analyst, EPAM Systems
Customer: PennyMac
Customer Description: CATEGORY
Production - SW Development
SOLUTION COMPETENCY
IE/Big Data
INDUSTRY COMPETENCY
ISP/Financial Services
EPAM Project Description: MISMO ODS Build and Migration Services
Team Size: 5 Developers and 2 Quality Assurance (QA) specialists
Project Roles: DB Developer
Responsibilities:
• Was responsible for adding new data points, ensure data integrity and proper data
transformation of corresponding procedures
• Tested data transformation procedures
• Using git version control to implement and document my changes to code base
Tools and Technologies: MS SQL Server, ms SQL Management Studio, git, T-SQL
Feb-2022 - Aug-2022 - Data Analyst, EPAM Systems
Customer: Canadian Tire Corporation Limited
Customer Description: Retail & Distribution
EPAM Project Description: Analyze and propose a delivery roadmap for P2C
Team Size: 2 Developers and 1 Data Analyst (myself)
Project Roles: Data Analyst
Responsibilities:
• Responsible for creation documentation (Diagrams and spreadsheet documents) to cover
project data flow. Prepare presentations and demo to create most optimal and easy way to
read diagrams for developers and analytics team
• Collaborated with a team of developers to document data flow of ITL & ILT activities
• Developed data transformation diagrams to illustrate flat tables data sources structure,
Created general data flow diagram to represent overview of data flow activities that was
engaged in our project as well as detailed diagrams with data sources and data points
description with relations
• Organized Metadata Descriptions (MDDs) for datapoint that was involved in our project
Tools and Technologies: Oracle, DataFrame - Apache Spark, Apache Hadoop, Microsoft Visio
Microsoft Excel
Databricks Notebook
PySpark, Spark SQL, SQL
PySpark
Data Modeling
Jun-2021 - Nov-2021 - Database Developer, Voicenter
Customer: Voicenter
Project Description: Internal web services
Team Size: 2 dba/db developers 3 backend developers and QA team and Team Lead
Project Roles: Database developer
Responsibilities:
• improving performance of procedures and functions
• implemented fixes for discovered bugs in database functionality
Tools and Technologies: MySQL, MSSQL., Datagrip, MySQL Workbench, JIRA, GIT., SQL
Mar-2020 - May-2021 - Database
I preferer to work in data engineer role."
data engineer,"- development of data bases /dwh architecture
- development of etl, bi
- development of web applications
- quickly learn/use new technologies
- result oriented
MySQL, MS SQL Server, SSIS, SSRS, SASS, VBS, JavaScript, Sybase ASE, Sybase ASA, SQL, SQLite, Scala, Android, jasper, ireport, pentaho, dwh, neteeza, Python, Tableau, looker, Power BI, PostgreSQL, redshift, bigquery,  mongodb, documentdb, django, python
"
data engineer,"
After I've finished studying in SoftServe IT Academy i have been working as a Freelancer. So, I do have work experience with creating Parsers, Web-sites and work with Data-Bases.
I have been using technologies like Django, Flask, BeautifulSoup, requests, etc.
I have been working as DataEngineer with PySpark methodologies and binded technologies for a year. 
I'm also interested in DataScince direction(have finished some online courses).
Looking forward to getting an interesting experience.
No doubt I'm looking for strong team that need a team-player."
data engineer,"Worked with a big legacy project without documentation

Taught my friend programming now he's a developer:)
Experience in writing different tools and scripts using C#/Python, code refactoring
Creating tables, procedures, functions, jobs, and indexes for MS SQL.
Working with modern tools, preferably cloud solutions, challenging tasks, a small number of routine tasks"
data engineer,"- Development of ETL pipelines
- Creating interactive reports in Data Studio

- Development of class of trading bots
- Third party library upgrade
- Taking part in searching and fixing trading strategies' weaknesses
6 months - pure data analysis
2 years - development

ETL Engineer/BI specialist:
- Automation of data acquisition from different services/sources:
  Google Ads, Google DV360, Google DoubleClick, Google Analytics, Facebook Ads and other
- Storing, ensuring  integrity, checking correctness of data:
  Google BigQuery
- Creating reports for internal and external usage:
  Google Data Studio
	
Python Developer:
- Development , testing, and supporting third party library - 
  Standardized REST and WS interaction with multiple trading platforms
- Web scraping
- Implementation of trading algorithms
- Monitoring and maintenance of trading bots

Data Analyst
- Testing different hypothesis
- Monitoring and analysis of CRM system
- Creation of templates for data analysis in Tableau
- Analysis and evaluation of the results of A/B test
Not interested in web development. 

Preferably looking for Data engineer positions in strong team with opportunities for learning new technologies.

Remote positions are highly appreciated.

Also interested in junior data science vacancies."
data engineer,"
- Worked with various data sources that include databases like Vertica, SQL Server, MongoDB, flat files to push data into DWH using ETL Tools;
- Designed and developed ETL mappings to move data from multiple sources into common target areas such as Data Marts and DWH using lookups, source qualifiers, filters, expressions, aggregator, join, normalizer, update strategy from varied transformation logics ;
- Development / refinement of databases: tables, queries, forms, packages, reports, procedures and functions;
- Experience in designing, creating and processing of cubes using Modrian Server. Created and configured Dimensions, Cubes, Measures, Calculating Measures
- Support and add new functionality to existing applications;
- Working with analysts and customers in ascertaining business requirements/needs further translating this
  into technical specifications with further development;
- Development and automation of ETL processes based on dblink and Pentaho;
- Calculation of aggregates and creation of showcases for further use by the BI application
- interested in the vacancy of Data Engineer or BI developer"
data engineer,"- I have practical experience with backend developing, ETL processes on data analytic, cybersecurity projects.  
- Developed heavily loaded service with not standard (not flask or django) frameworks for API creation for more faster response to user
- Mentoring juniors.
- For now I have 4 own hobby projects which in total have coverage of about 3,000 people.
- Created many scrappers for different ecommerce services
- Big Experience on code optimization for fast work.
- Big experience with redis (include async reddis), can write fast LUA scripts. etc
- Familiar with Flask, django API and other more fasters frameworks (blacksheep and FastAPI for example)
- Good knowledge in PostgresSQL, aiohttp, Kafka, Scylla\Cassandra DB
- Knowledge in Elastic
- Experience in working with many different API services, Spotify, ChatGPT, Telegram, Slack, Smart Devices API for example.
- Mentoring some colleagues
More interesting cases, development, improvement of familiar technologies and learning new ones. Growth within the company. Lack of stagnation. An interesting team and cookies in the office =)"
data engineer,"
I've been working as Data Engineering Lead for US-based product company for almost 4 years. 
Started from moving from Elixir data pipelines to 
Fivetran+Snowflake+dbt (now Airbyte). Have experience working closely with Data Analytics team, Product Owners, Data Science team.

Worked on Data Governance and Data Quality processes in collaboration with CTO.
"
data engineer,"
Data Engineer
GraiLabs — VoxCroft
Addis Ababa, Ethiopia
Sep. 2015 – Dec 2020 Oct 2021
October 2021 – Present
South Africa, Capetown - remote
 • Built a a machine learning orchestration tool for automating an active learning workflow that includes model training, model evaluating and model updating using python. Integrated Dagster for scheduling ML tasks and used MLlfow for model tracking and model versioning.
• Wrote an AWS CloudFormation configuration file for a simple way of provisioning and managing AWS resources.
• Setup a CICD pipeline to automate product delivery using AWS CircleCI.
• Built a data collection tool using React, NodeJS, PostgreSQL database on Amazon RDS and an S3 data lake
• Wrote a python package for managing data collection workflows
• Setup schema design and management of PostgreSQL database on Amazon RDS handling 1000+ of transactions per day


Software Engineer Feb 2021 - May 2021 WHIZ KIDS WORKSHOP Ethiopia, Addis Ababa
• I collaborated with a team of software engineers to create a ruby on rails web application that allows users to express themselves through haiku poems
• Wrote unit tests and integration tests using Ruby on Rails testing frameworks
• Contributed software engineering expertise to the Software requirement specification (SRS) and software design
specification (SDS) processes


Software Engineering - Intern Feb 2019 – July 2019 Addis Ababa Institute of Technology Addis Ababa, Ethiopia
• Participated in a project called Wheely to develop a self driving and cleaning robot
• Using Python OpenCV, I created a computer vision module for the robot’s perception
• Prepared a dataset and implemented model detection using transfer learning, YOLO3 and Darknet
• Contributed to the implementation of the Ackerman steering mechanism using C++.
"
data engineer,"- easy-to-manage analytical processes with reusable results and code, knowledge sharing, and customer focus
- data-driven approach
- the central data warehouse data model author
- development of light wear ETL command-line tool
- BI replacement approach based on google sheets
- classic ML feature engineering approach based on Weight of Evidence (classic bank scoring technics)
- ML resume_classifier (credit scoring, churn prediction, fraud detection, deal detection)
Worked with different data sources and business processes in a wide range of contexts and roles, from high-level strategy, and roadmap to day-to-day tasks. In Banking (15 years) and E-commerce (6 years).

- Data  integration, processing, cleaning, ETL
- Data Warehousing and Data Lake concept implementation
- Postgres, PrestoDB, ClickHouse, Oracle, Teradata, MS SQL, MS Access, MySQL, SQLite, MongoDB, Couchbase, Spark/Thrift
- SQL knowledge, procedures, functions, usage of analytical and window functions, development of agreement of usage names. Author of central database model in warehouse project. 
- Programming in Python (command line interface tools, web applications, data crawlers, data analysis notebooks)
- Strong in classic machine learning tasks like churn, scoring, and fraud detection (pandas, scikit-learn, IBM Modeler/ SPSS Clementine, SAS enterprise miner) also could work with part of NLP tasks
- BI tools and spreadsheets for building reports (Google spreadsheets, Looker studio, MS Excel, Tableau, Oracle BI, Power BI, Qlik, etc)
- Leaded teams of up to 10 people

I want to find a job related to data/data science area, where, on the one hand, there would be useful and could apply my old skills, but also where there is an opportunity to learn new approaches, learn new tools, and technologies, solve a new type of tasks.
"
data engineer,"I started my career in tech in 2009 and since then held multiple high-level engineering and lead positions while always staying hands-on.

I also had my own team of engineers which helped me with R&D activities for the business my partner owns.
My main area of responsibility is building, coordinating, providing technical guidance and mentorship for tech teams in various set-ups.

Provided different level of technical management for teams of different forms and sizes: from direct management of 1-2 teams to high-level coordination across 8+ teams.

Hands-on experience with top 3 clouds(AWS, Azure, GCP).

Worked with a variety of stacks and languages:
I've spent a good part of my career working with JVM languages, but am also very much interested in other stacks, from Python to Rust.
I also worked closely with Nodejs stack while holding an Application Architect position and set up a project that leveraged Google goodies - Golang and Dart(for Flutter) and Python(Coding against 3D engine on top of Blender API).
"
data engineer,"
I have been working as an Engineering Manager for more than five years. Before that, I had much technical background as a software engineer. 
My current role is Head of Data Engineering in a fintech startup ($150m+ raised, data & ai headcount 30+, total headcount 300+). 

I joined this Company ~6 months ago, and my main goal is to evolve the data analytics platform to the next stage. I work directly with the CTO and Head of Data & AI to bring changes in both cultural and technical aspects. The Company is going to become data-driven and needs a robust, scalable and effective data analytical ecosystem as a ground base. My main tasks at this point are
- Hire and grow data engineering team (previous vendor has left)
- Ensure business as usual tasks depending on data performed seamlessly
- Work with platform users and stakeholders on feedback and requirements management
- Make data analytics platform as an internal product
- Drive platform evolution to desired state
- Cost optimisation

During my career, I have performed different roles, from Java Software Engineer at the very beginning to Data Analytics Competence lead in outsource company (see LI for more details). 
I like building interesting and complex products, work with the professionals who enjoy their job and working on self-improvement. I work hard, expect same from the team but respect their borders, and it's very important for me to achieve results.
"
data engineer,"I have 15 years of relevant experience in automotive service and customer demand analytics.
I have 7 years of experience in analytics using Excel. I completed a Data Science course and have a great desire to implement new knowledge and skills.
During my studies, I made a bunch of projects. In them, I used various methods of machine learning, clustering, and others.
I am looking for my first job in IT and am very interested in developing and contributing to your company"
data engineer,"
As an IT Support 1st Line professional at NL International in Kyiv, I played a
pivotal role in delivering top-notch technical assistance to internal stakeholders,
ensuring seamless operations and enhanced user experiences. Through my
proactive approach and exceptional problem-solving skills, I contributed to
maintaining a productive and efficient IT environment that aligns with NL
International's goals. My experience as an IT Support 1st Line professional at NL
International allowed me to develop strong technical skills, a customer-centric
approach, and the ability to thrive in a fast-paced environment. Working
collaboratively with cross-functional teams, I contributed to maintaining a
reliable IT infrastructure that enabled smooth operations and user satisfaction.
This role has not only solidified my technical abilities but also ignited my
passion for providing impactful IT solutions that contribute to the success of the
organization. I was part of an extensive engineering project to analyze and identify the most suitable locations for solar panels in central Europe. As well as pet/project writing games there and gaining experience in C#.
"
data engineer,"Ph.D. degree. 
AWS Cloud Practitioner. Lightbend Reactive Microservices Certified Expert.
Over 15 years of experience in IT.
Java Core 8, 11, maven, gradle.
JDBC, Hibernate, JPA, Spring Data, Mongo, PostgreSQL, MySQL, ElasticSearch.
Hadoop, Apache Spark, Apache Hive.
Linux, Terraform, Ansible, AWS (Cloud Practitioner certificate), Kubernetes.
Small experience in JEE, Spring, Hibernate, JPA.
Scientific Research experience, PhD degree in Mathematical simulation and computational methods.
Base education in Computer Science and Computer Engineering.
Over 13 years experience in IT-education (In higher education, Delivered special Courses on Java Core, Linux, IT Security).
Also experience in technical writing (handbook and manuals), supervising student development and research projects.
Senior Java developer, BigData Engineer, researcher.
Senior researcher."
data engineer,"Here is a well-rounded Java Software Developer with different commercial experience in Java and a Java-oriented technology stack, object-oriented design, and continuous integration; with some expertise in Data Engineering, building, and maintaining of Data Warehouse Event-based systems.
Was working on:
- The Supply-Side Platform for video ads
- Datawarehouse service of the video ad system
- Data warehouse platform for data collection and analysis of the gambling business
- SIP telecommunication server and networks
- SaaS-based ERP/CRM systems
- High-performance or real-time data processing
​​- Java-based Data Engineering for analytical platform
- Distributed microservices-based systems relying on k8s and messaging broker
- CI/CD

- No time tracking
- Minimum Scrum
- No micromanagement
- No permanent rush
- Well-being atmosphere is a must."
data engineer,"
Currently I am a part of learning project at EPAM systems. I have experience of using apache spark for both batching and streaming jobs, apache kafka as message bus, minio as local S3 compatible storage. I have basic knowledge of SCRUM principles, and can work productively as a part of team.
"
data engineer,"- Publications in scientific journals and participation in a scientific conference
- High academic performance
- Academic scholarship of a President of Ukraine
1. Project: Intelsat
Company: N-iX
Description: US-based company that provides In-flight Internet and entertainment services to the aircraft across the globe, serving more than 2,900 commercial over 6,600 business aircraft. At this moment we cooperate on Business Intelligence, Data Analysis, and BigData directions and look for talents who can contribute to the complex data management and analysis projects.

Duration: 08/2020 - 04/2022
Tools and technologies: Python, Scala, AWS, Spark, Spark Streaming, MariaDB, Oracle, MS SQL
Server, DynamoDB, Athena, Redshift, Accumulo, Spark, Spark Streaming, YARN

Project responsibilities:
- Developed Spark data transformation processors and integrated them into AirFlow pipelines, prepared data for further analysis
- Developed data batch transform pipeline using AWS Sagemaker
- Performed EDA on multiple data sources
- Developed multiple pipelines using AWS Lambda functions
- Worked with different AWS services (RDS, EMR, EC2)


2.Project: Services for telecommunications companies
Company: N-iX
Description: Global company that provides technology and business services for a number of telecommunications companies

Duration: 02/2020 - 06/2020
Tools and technologies: Scala, Apache Spark, Hive, Scala test

Project responsibilities:
- Wrote data transformations, unit, and integration tests - Solved problems with partitioning


3. Project: R&D Incubator
Company: SoftServe
Description: POC for Raspberry Pi-based camera device for emotion recognition during business meetings
Tools and technologies: Python, TensorFlow, Scikit-learn, Movidius NCS, OpenCV

Project responsibilities:
- Trained neural network for facial expression recognition on a large dataset
- Developed pipeline for processing camera feed with two neural networks and clustering,
optimized it to run on Movidius NCS
Interesting tasks, opportunities for professional growth, friendly team"
data engineer,"Designed and managed a personal MySQL database for efficient data organization.
Consistently ranked in the top 10% on coding challenge platforms.
Self-motivated learner with a strong dedication to skill improvement.
Skills and Goals: 

Proficient in MySQL.
Basic web development with HTML, CSS and JavaScript
Aspiring software engineer seeking opportunities to gain hands-on experience and grow in a professional team.
Eager to learn industry-standard practices and tools.
MySQL proficiency for efficient database management.
Quick learner and problem solver.
Strong coding skills in JavaScript.
Team collaboration and communication abilities.
Commitment to continuous improvement and professionalism."
data engineer,"
Writing stored procedures and complex queries in SQL

Cleaning and transforming data using Python

Creating Dashboards in Power BI, Tableau

Creating ETL solutions:
Azure Data Factory:
web -> csv -> Azure Data Lake Storage Gen2 -> transforming -> upsert to Azure SQL Database

AWS:
S3 -> SQS -> Lambda -> DynamoDB
"
data engineer,"
Currently studying at DataCamp in the direction of Data Engineer.
"
data engineer,"Greetings, I am an IT specialist and a 
final year undergraduate student 
majoring in computer science. 
In my current position, I proved myself to be a good troubleshooter, optimized the administration of statistical data and the work of scripts, created several dashboards to illustrate analytics.
""NovoBox"" 
IT specialist 
( 10.11.2022- Present ) :
• Data administration (Graylog) 
• Writing Python scripts ( Pandas, 
Numpy, Plotly ) 
• Creation of new postmachines and 
change of already working ones 
• Updating data for dashboards (Google 
Sheets, Looker Studio)

""InfuseMedia"" 
Online Data Verifier 
(19.05.2022- 26.08.2022) :
• Verificating and acceping data which 
suitable for the client's request 
• Working on research projects 
• Working based on manualand team lead 
experiens
No any calls and sales"
data engineer,"
01.2022 — Present
PrivatBank
Database analyst
- Business process automation
- Writing scripts to optimize work
- Locate and identify data discrepancies,
investigate for solutions using data analysis
practices and prepare analysis reports
- Creating data visualizations
- Data cleaning and preparation

02.2021 — 01.2022
Teleperformance
Data analyst
Responsibilities:
- provide analytical support to the operations team
- perform analysis to come up with some insights to
help improve the main operational metrics
- writing scripts to optimize work VBA
- visualization Power BI
Achievements:
- analyst at the largest project in Ukraine
- power bi implementation
- automation of important reports

05.2017 — 05.2020
Abogados
Lawyer's assistant
- Execution and registration of applications
(inheritance, divorce, purchase and sale of
property)
- Cooperation with government agencies

06.2013 — 05.2016
E-COM
IT-specialist
- Uphold integrity of all internal processes (incident
management, major incident management,
problem management, change management, etc.)
- Working with the Development. Operations and
Business teams in the software development
lifecycle for all Electronic invoices and related
technology projects
- Work with providers, retailers and engineering
peers to keep abreast of trends, products,
frameworks, and applications
"
data engineer,"Got a bachelor`s degree in computer science.
Passed courses: ""Deep Learning Specialization"", ""Data Warehouse Concepts: Basic to Advanced concepts"", ""The Complete SQL Bootcamp 2022: Go from Zero to Hero"".
Bachelor of Taras Shevchenko National University of Kyiv.
Specialization: Computer Science.
Developed Telegram Bot for interactive meet organizing for 
a coursework using Python and popular conference creation APIs.
Developed Telegram Bot for a qualification work, using computer vision and the Google Colab cloud environment.
In addition to my university tasks, I was self-educated and took some courses such as machine learning using Python, SQL, Data Warehouse Concepts.
"
data engineer,"• Data extractor (xls, docx): I have developed a data extractor for laboratory documentation (the program reads Word file, with regex it takes necessary data and writes it to csv). All code is written in python using pandas, re, docx, math and os libraries.

• Web Scrapers: I have done several crawlers for websites with Scrapy library; the spiders crawl websites and download the required data, including images. 

• Data processing: Experienced in processing raw data with pandas and spark dataframes
Currently, I am involved in ETL project with the following technology stack:
- Scrappy
- AWS EMR
- AWS S3
- AWS Athena
- Elastic Search
- Jupyter Notebooks
- Pandas
- Spark
- regex
My responsibilities include: scrappinh  websites, processing the obtained data and loading it to Athena and Elastic. 

Right before this project I studied at EPAM Big Data Lab where I worked with such tools: 
- Apache Spark / Spark Streaming
- Hive
- Kafka
- Elasticsearch and Kibana
- Airflow
I am looking for a data engineering job with possibilities to learn new things and grow continiously"
data engineer,"- Bachelor's degree on Computer Science
- Basic data engineering, data visualization skills.
- Well-acquainted with SQL, databases and DWH
- Basic knowledge of implementation of ETL solutions using on-premises
software (SSIS) and cloud-based services (AWS and GCP)
- Basic Python skills
Currently, I am studying for a master's degree at the Lviv Polytechnic at the Faculty of Computer Science.

Eleks
Juniour DB Developer
Tools: T-SQL,SVN, Postman

Epam Systems Trainee
This intership was divided into 2 parts: first,  theoretical, where I had BI basics, and second, where I use my gained knowledge to practice on Google Cloud Platform. 

Second part:
- Performed project requirements gathering and analysis
- Created a cloud function that runs when a CSV file is uploaded to the bucket and uploads the data to Google Cloud SQL
- Performed ETL process from Cloud SQL to BigQuery on Apache Airflow
- Performed data warehouse modeling
- Migrated data from SQL Server to MySQL
- Created file uploader app to GCP storage on Python
- Organized CI/CD processes

Team - 4 Developers

Database:
MySQL, MS SQL Server, BigQuery

Tools:
Cloud Storage, Cloud Function, Cloud SQL, Cloud Composer,
BigQuery, Apache Airflow, Tableu, MySQL Workbench, GitHub, Jira

Technologies:
Google Cloud Platform, Python, Pandas, SQL, Git, JSON


First part:
- Performed CRUD operations in on-premises database MS SQL Server
- Worked with different data sets using stored procedures and built-in functions.
- Carried out performance optimization: managing indexes, partitioning, execution plans analysis
- Explored different data warehouse design approaches
- Implemented ETL solutions with SSIS (building data flows using different transformations: derived column, lookup, merge)

Database:
MS SQL Server

Tools:
Microsoft SQL Server Management Studio 18, SQL Server Integration Services(SSIS), Visual Studio 2019, Jira

Technologies:
T-SQ, Python

LVIV POLYTECHNIC NATIONAL UNIVERSITY
Project:
Hospital Clinic Application

- Created client appointments application for a clinic as a degree work.
The application includes a relational database with the data of
clients and doctors, client's appointments and different
aggregated data from the application.
- Migrated specific data to an AWS Cloud. 
- Created dashboards both for individual departments and for
the entire clinic

Database:
SQLite, DynamoDB

Tools:
PyCharm, AWS Cloud (Lambda, SNS, S3, DynamoDB) , SQLlite Studio, Microsoft Power BI
Technologies	Python, SQL
"
data engineer,"
Data Analyst - Full-time
aug. 2021 г. – current time
Skills: Clickhouse · API · Tableau · Python · PostgreSQL · BigQuery · Git
-Automated reports using Google Sheets API and Python
-Increased report generation through automation
- Created Tableau reports
- SQL queries using different databases (PostgreSQL, ClickHouse, BigQuery)
- ETL

Data Analyst - Trainee
jun 2021 г. - jul 2021 г. · 2 month.
- SQL queries for data analysis
- Created a stored procedure for further data analysis
- Created Power BI dashboards

Education
- 2016-2020 Cybersecurity, National Aviation University - Bachelor degree
- 2020-2022 Cybersecurity, National Aviation University - Masters degree
"
data engineer,"
**Data Engineer in Apollo Factor**.
I have been working for six month in Apollo Factor. I was a part of small team and my main tasks were writing scripts for automated preprocessing big data, cleaning, filtering and preparing it for further processing. My work involved a lot of manual data input processing and handling strings.
"
data engineer,"Highly motivated and result-oriented person with skills and knowledge in following areas:
• T-SQl
• Python, PySpark, Pandas
• Azure Cloud (Azure Storage, Azure Data Factory, Azure Synapse Analytics, Azure SQL, Cosmos
DB, Azure Databricks etc.)
• AWS Cloud(EC2, S3, DynamoDB, Amazon RDS etc.)
• Data visualization with PowerBI, Tableau
• ETL/ELT
• Linux, Bash scripting
• English C1
• Higher education in field of Computer Science
• Graduate of EPAM Data Analytics/Engineering educational program
Certified as:
• Microsoft Certified: Azure Data Engineer Associate(DP-203)
• Databricks Certified Data Engineer Associate
• Azure Data Fundamentals(DP-900)
• AWS Certified Cloud Practitioner
Completed a bunch of additional professional courses in the last year.
Looking for initial job position as Intern/Junior Data Engineer.
• Design and implement data pipelines: Implementing data pipelines with SSIS and Azure
Data Factory
• Data modeling and database design: Designing normalized and denormalized database
schemas
• ETL development: Developing and maintaining ETL processes using AWS RDS, Azure Data
Factory and Azure Databricks
• Data analysis and reporting: Developing and maintaining reports and dashboards to
provide insights into business performance and trends using PowerBI and Tableau
• Performance tuning and optimization: Optimizing database and ETL performance to ensure
timely and accurate data processing.
• Continuous learning and improvement: Staying up-to-date with the latest data engineering
technologies and best practices, and continuously improve skills and knowledge in following
areas: T-SQL, Python, PySpark, Azure Cloud, AWS Cloud, Azure Databricks etc.
Looking for initial data engineer position more or less relevant to my skills"
data engineer,"
Enhance and maintain data queries, advanced formulas, and data sources for scalable one-of-a-kind reporting, ensuring flawless generation of data visualizations and reports, work with large data sets and develop data pipelines that move data from source systems to data warehouses, data lakes, and other data storage and processing systems, also develop and maintain data APIs, ETL processes, and data integration systems.
"
data engineer,"I am a certified Data Engineer and Data Analyst. I know how to implement a full data life cycle process, which includes collecting, cleaning, transforming, storing, analyzing and reporting data.
I have 6 months of practical experience. During my internship I gained hands-on skills that included Database Development (SQL), Data Engineering (ETL + DWH) and Data Presentation (PowerBI).
I'm looking for a friendly collective that will share the common work responsibilities :)"
data engineer,"Unfortunately, I haven't got any achievements yet, but I want to correct this.
I have been working with arduino in tinkercad.com with my university's mentor. He gave me tasks like a 'Simulation of a dice', which I did in tinkercad. I created schemas of microcontrollers and wrote codes for tasks.
During the Python course, I was doing various tasks.
I have telegram-bot with rating system and the parser, which gathers informations of advertisements, such as Name, date, views, district etc, and adds it to data base (PostgreSQL).
I really would like to work in Game Development in the future. However, I know it's impossible for me right now, nonetheless, I will learn everything I need.
Now I want to become Data Engineer and work with Python & SQL."
data engineer,"Completed Coursera course about machine learning and SQL
I have experience participating in Hackathons working with a team of machine learning engineers. I was mainly busy with deploying ml onto servers and partly with the tuning resume_classifier
I don't have experience working in large companies but I really want to start.
"
data engineer,"
Optimization of algorithmic trading strategies
For 6 months I was developing my own application for analyzing data from cryptocurrency
market. Main purpose of this app is optimization of algorithmic trading strategies. It is
especially focused on working with raw data from two sources: parsing of market by trading
terminal and price action data collected via API of exchange (Binance).
"
data engineer,"Good communication skills, thoroughness and ability to self-organize. Mastered tools such as SQL Server 2019, Visual Studio. Advance knowledge in SQL and T-SQL. Experinced with ETL in AWS. Build dashboard and report in Tableau and Power BI.
Project:EPAM [OnlineUA] DWBI\DQE Autumn Program.
The purpose of this project work was to learn DWBI\DQE's technologies, tools and how to use them. Created database, analyzed given database and created SQL queries, stored procedures, UDFs, triggers and transactions in SQL Server. Created SSIS solution that contains three Master Packages. Performed requirements testing, created checklists, test cases and bug reports in Jira. I worked with AdventureWorks2019, AdventureWorksDW2016 and used such tools as Microsoft SQL, Server Management Studio 2019, Visual Studio 2019 (SSIS), Jira
I would like to work with interested project full-stack or part-time. I can work with MS SQL Server or another database that uses SQL language. And I would like to work with data integration or data visualization."
data engineer,"1. Built data ingestion from Postgres DB in Cloud SQL into BigQuery using Federated Queries, Cloud Composer 2 (Airflow) and Python.

2. Recovered financial debt amounting to #40 Million naira at BuyPower Inc using analytics skill of SQL. Furthermore, I collaborate with software engineers to fix the issues that led to debt, thus ensuring Data compliances and boosting trust with our clients and stakeholders.

3. Ensured clean energy sustainability at low cost for Smarterise clients that resulted in more than 30% reductions in annual operational cost by culling insights from their energy data.


MSc Financial Engineering at WorldQuant University Feb 2022 (95/100)
UPWORK
Role: Data Engineer Freelancer
1. Working as an external Data Analytics Expert contractor to help multinational clients get data-driven insights using various tools such as python, Metabase, DBT, Google Data Studio, BigQuery, GitLab etc. (Èquipe Totem Inc.)
2. Building Data pipeline to migrate data from google sheets, Mixpanel, and Postgres DB into Data Warehouse using Google Cloud Composer 2 (Airflow), python, federated queries for a tech startup. (WebVerif)

Company: BuyPower ABUJA, NG
Role: Data Engineer [buypower.ng(YC 2017)]
Timeframe: SEPT 2021 - PRESENT

1. Lead a team of three Data Engineers, and collaborate with Engineering management on continual improvement of the ETL process, Documentation, data pipeline architecture and system architecture design to minimize operation cost by 40%.
2. Design and implement data ingestion pipelines (batch and stream) for data replication automation of request logs from an unstructured database(MongoDB, Firebase), relational database (MYSQL in Amazon RDS) to Data Warehouse (BigQuery).
3. Build multiple observability ETL systems (DBT)  that give several teams insight into their operations, increasing transparency of information and reducing time to insight from days to mere seconds.
4. Culling Insights and preparing multiple Google Data Studio dashboards for various teams and stakeholders from Data warehouse to answer business questions and consolidate the company’s long-term goals and visions.
5. Implementing several schema and query optimisation techniques which reduced the overall time required to execute queries by up to 70%
Tools used include Bigquery, SQL, Python, Google Data Studio, GCP (Cloud Functions, Scheduler, VM Instance, Cloud SQL, Cloud Storage), Google Colab, MySQL, MongoDB, RestAPI, Firestore

Company: Smarterise LAGOS, NG
Role: Data Analyst
Timeframe: APR 2020 - AUG 2021
1. Performed ETL on big energy data (Millions of rows)
2. Presented summarized data insights on energy consumption and power quality to save more than 30% of the annual operational budget of our clients.
3. Built from scratch a comprehensive KPI sales report for a Multinational client to track their sales records and make data-driven decisions to increase their annual sales.
4. Applied ML model in predicting energy load profile for research and recommendation purposes.
Tools used include Python, Excel, Power BI, Google Colab
"
data engineer,"
Work Experience: 

Cloud Support Engineer, Devoteam G Cloud, Warsaw: 
• Developing features on top of Open Source projects and their integration to Devoteam’s Support Department Environment (Python, Terraform, Flask API, Docker, GCP, Cloud Run, Cloud Functions)
• Contribution to local support automation projects (Developing Cloud Functions)
• First line support for Clients with the Google Cloud based infrastructure (Google
Kubernetes Engine,Cloud DataProc, Cloud Dataflow, Cloud SQL, Big-Table, BigQuery)

Student Data Engineer/DevOps, Abbott, Warsaw:
• Supporting AWS Infrastructure (Debugging Issues related to resource Provisioning,
Deployment and Exploitation)
• Creating ETL Glue Jobs and AWS Lambdas for Data Processing and Automation
• Assisting in Deployment, Updating and Creation of IaC Cloud-Formation Templates
• Creating Bash scripts for automation of AWS Processes

Intern Software Developer, CASE, Warsaw
• Writing automated jobs in Python for extracting and processing data from different
resources
• Managing, tuning and maintaining EC-2 Virtual Machines for running jobs in AWS
Cloud
• Filtering data in the database by using SQL


Certifications:

• Associalte Cloud Engineer
• Terraform Associate
• GCP Data Engineer (Upcoming)


Personal Projects: 

• Batch Data Pipeline for extraction of Cryptocurrencies' data and calculation of price
differences between various cryptocurrency categories on different time ranges.
(GCP, Spark, Airflow, Pandas, GCS, BigQuery. Terraform, Docker, Java, Cloud Run, NodeJS)

• Building a Serverless Data Pipeline by using Python, AWS Lambda, Amazon S3 and AWS
Cloud-Formation for processing CSV files.

• Building a real-time Data Processing/Visualization pipeline for processing Web
Application’s access logs (Python, Pandas, Flask, PostgreSQL, SQLite, Django)

• Developing a Web Scraper for Ebay Website (BeautifulSoup, Python, Pandas)


On my previous positions I have been mostly working as an Intern and a Junior Specialist. Want to develop myself as a Cloud Data Engineer (either with AWS or GCP) and Data Architect. Want to learn more about the Hadoop ecosystem, Stream Processing, MLOps and Cloud Native solutions for Big Data. 

Will be happy to contribute to projects with my existing background and impact company's success with my passion to Data, Cloud and Programming
"
data engineer,"
Worked as BI engineer, Data Scientist, Data Analyst. Have a strong mathematics background.

Implemented ETL process, worked with configuration files, requests, API, parse files and texts (SQL, python), creation of site parsers (python). 

Created and optimized SQL queries, complex procedures in SQL, index building, Olap cubes, MDX queries, developed BI reports (in local tools, ssrs, power Bi). 

Data processing (use pandas dataframe, regex expression, lambda functions etc.). Work with time series.  

Knowledge OOP principles, knowledge of algorithms and ML theory. 

Creating machine learning resume_classifier (linear resume_classifier, decision trees, etc.) for solving regression, classification, clustering problems. Optimize algorithms (hyperparameter tuning, cross validation etc.).

Statistical analysis, visualization (searborn, matplotlib).

Strong interest in Data Engineering, Data Science and Machine Learning.
"
data engineer,"Computer Vision Trainee (1-month internship) 02.2021
Developed a module for detection of the floor in indoor pictures taken with iPhone XII (Python, OpenCV, ML)

Analytical public project 07.2020
Analysis on spatial planning documentation in Ukraine - ArcGIS (web app accompanied by Article  with 11 000+ reads 'New territorial administrative division is approaching – some thoughts about the situation in the field of spatial planning in Ukraine') 

 Kaggle educational competitions (as of 10.2020):
- House Prices: Advanced Regression Techniques - in the Top 31%
- CIFAR-10 - Object Recognition in Images - in the Top 23%
- Plant Seedlings Classification - in the Top 54%
- Titanic: Machine Learning from Disaster - in the Top 26%
Computer Vision Trainee (1-month internship) 02.2021
 Project for detection of the floor in the indoor pictures taken with iPhone XII (Lidar, Python, OpenCV, ML, NN)

Data Science Trainee (Apprenticeship) 08.2020 – 12.2020
DataRoot Labs Kyiv, Ukraine
 Machine Learning module and API for data preprocessing and prediction for the Kaggle's competition 'House Prices: Advanced Regression Techniques' (Python, Flask).
Docker image for the project – svparhom/mlapi
 RESTful API for handling GET, POST, PUT, DELETE requests to a database (Python, Flask, PostgreSQL).
Docker image for the project – svparhom/apifour.
 Kaggle educational competitions:
- House Prices: Advanced Regression Techniques - in the Top 31%
- CIFAR-10 - Object Recognition in Images - in the Top 23%
- Plant Seedlings Classification - in the Top 54%
- Titanic: Machine Learning from Disaster - in the Top 26%
 Supervised and Unsupervised ML algorithms from scratch (Python):
- Linear regression, Polynomial Regression, Logistic Regression, NBC, KNN, SVM, Neural Network;
- K-means, PCA;
 Snake Game with pure Python in the Gym environment.

Project Manager 06.2018 – 10.2020
Deutsche Gesellschaft für Internationale Kyiv, Ukraine
Zusammenarbeit (GIZ)
U-LEAD Programme „Support to the decentralisation reform in Ukraine“
 Coordinated national and international implementing partners in the full cycle of the projects related to spatial planning/public infrastructure topics of national scale of total value more than 1 000 000 EUR and durations more than 1 year. Participated in the development of new projects.
 Led a remote team of 5 external experts.
 Initiated, developed, and maintained a data management system for the Programme’s projects monitoring.

Assistant of Architect 05.2017 – 05.2018
Architectural Bureau Smolakowski Wroclaw, Poland

Assistant researcher (Erasmus+ traineeship) 10.2016 - 04.2017
European Urban Research Association (EURA) Dortmund, Germany

Urban Planner Intern 11.2015 - 02.2016
GIPP Porto, Portugal
"
data engineer,"
Have experience in Python OOP, Django framework.
Worked with Airflow and Sql flow as administrator. Got a strong knowledge of operating systems including Windows, Linux and Virtualization Technology (Docker, Citrix, VMware).
In future i want to gain proficiency in Python OOP, machine learning, big data.
"
data engineer,"Best Thesis Award 2020 at the American University of Central Asia

My thesis topic: Servant Leadership and Turnover Intention in Central Asia: A Test of Implicit Leadership Theory.
Work Experience:
- Data Scientist at Central Asian Bureau for Analytical Reporting for 8 months. My tasks include:data scraping, data cleaning, exploratory data analysis and Natural Language Processing in Russian and English languages.
Technology and lanugages worked with: Python(numpy, pandas, matplotlib, beautiful soup, TensorFlow), NLP libraries( NLTK, spaCy, scikit-learn, Natasha, Dostoevsky) HTML,  Jupyter Notebooks, Google Colaboratory, Visual Studio Code. 

- Poject Manager at Arniro Web Development Studio for 1 year. My tasks include: setting project timelines,  monitoring the status of project deliverables, updating relevant stakeholders and team members on the project's progress and delegating tasks to project team members.
Technology and methods used: Agile Scrum methodology and Trello for workflow

Education/Courses:
- Bachelor's degree, Business Administration, Financial concentration from the American University of Central Asia
I covered the following courses on Codecademy platform:
- Data Scientist career path
- Build a Machine Learning Model with Python
- Analyze Data with Python

Please find more information about my work experience  and education on my LinkedIn page: Atobek Rahimshoev
"
data engineer,"I was proposed a paid internship by German Research Center and to continue my study at Leibniz Universität Hannover, but I couldn't participate because of the COVID-19.
Being a freelancer, I have a lot of practice communicating (speaking and writing) with English speaking clients from different places.
I have almost a year of full-time job experience working with databases and data manipulation. My key experience is in maintaining and optimizing databases, data extraction and manipulation, bug reporting and validation.
I have worked as a freelancer for about 9 months with companies from the health industry, entertainment and gaming, education, etc. I was mainly working on data analytics, manipulation, storage and data patterns.
I participated in different educational programs such as Machine Learning Bacics Educational Program course provided by GlobalLogic and Introduction to Data Analytics and Artificial Intelligence provided by Kyiv School of Economics.
I am looking forward to working with skilled specialists to upgrade my practical skills in a working environment.
I prefer to have the opportunity to work parttime remotely, although I am planning to spend my first months on a new job from the office."
data engineer,"
Here is a summary of my qualifications:
    • Database Experience: Over 8 years of experience in utilizing Oracle (SQL) for various tasks, including troubleshooting subscriber issues, creating complex queries, running procedures, and data updates.
    • Cloud Knowledge: Azure and AWS services, holding certifications such as DP-203 Azure Data Engineer Associate and CLF-C01 AWS Certified Cloud Practitioner.
    • BI Tools: Experienced in working with PowerBI (PL-300 Microsoft Certified: Power BI Data Analyst Associate), Tableau, and Sisense for data visualization and analysis.
    • ETL Tools: Experience in working with Azure Data Factory and SSIS for efficient data extraction, transformation, and loading processes.
    • Technical Proficiencies: Skilled in utilizing JIRA, Confluence, Postman, Kubernetes, Jenkins, Oracle, Cassandra, MongoDB, Microsoft SQL Server, PostgreSQL, and GIT.
    • UNIX Administration: Experience in administering UNIX-based systems.
    • Container Technologies: Experience in working with Kubernetes for containerization.
    • Python: Experience in using Python for file and database manipulation tasks.
    • Telecom Knowledge: Experience with Charging and Billing Systems in the telecommunications industry.
    • Collaborative Experience: Worked effectively as a member of globally distributed and cross-functional teams.
    • EPAM Data Analytics and Data Engineering Internal Lab: Currently a member since 06.2022, involved in various projects including the development of ETL pipelines in Azure Data Factory, implementation of ETL processes using SSIS and Python with PostgreSQL in AWS, and creating reports in Power BI and Tableau for data analysis.

I am highly motivated to further enhance my skills and expand my knowledge in the field of data engineering and analytics. I have obtained the following certifications:

DP-203 Microsoft Certified: Azure Data Engineer Associate
CLF-C01 AWS Certified Cloud Practitioner
DP-900 Microsoft Certified: Azure Data Fundamentals
PL-300 Microsoft Certified: Power BI Data Analyst Associate
Academy Accreditation - Databricks Lakehouse Fundamentals
I am looking forward to the opportunity to further enhance my skills, engage in an interesting project, and collaborate with a friendly and supportive team."
data engineer,"At the moment I am developing my first pet-project - ""Brand Analysis"".  It will search articles and posts with brand mentions and return overall statistic regarding the sentiment classification. Interface - ChatBot.
I am currently studying Python at Beetroot Academy. It helps me structure my learning. But the main emphasis is on self-study: I ​​read documentation, books, solve problems. I took several courses on interactive online platforms: Code Academy, LinkedIn learning, Prometheus.

I implement in code some interesting data science tasks and study how different approaches and methods work.
For the moment on my ""portfolio"" GitHub page you can find such projects as:
- CV: Objects contour recognition on video with OpenCV
- ML: Sentiment analysis of reviews with SciKit-learn
- ML: Fake news recognition with SciKit-learn
- ML: Autoencoder/Denoiser with TensorFlow (Keras)

I chose Python because I am most interested in Data science. I believe that in this area I will be able to find a large number of interesting projects for myself.

Also I have a lot of knowledge in 3D space and objects (aviation, automotive area) due to 15 years experience as Design engineer.
1. Growth
2. Growth
3. Growth
...
n. Growth"
data engineer,"
2021 --- Student Trainee of EPAM.
1)	Project about departments and employees. In this project we can add new employees to the department and edit employee data. We also have the ability to filter and display a list of employees in the departments with an indication of the Salary for each employee and a search field to search for employees born on a certain date or in the period between dates.
Technologies: Django, HTML5, CSS3, BOOTSTRAP, GIT, BASH, PYTEST, SQLite, JQuery, Gunicorn, Python.
2)	Game store web application with role-based access control. Admins or Managers can add, delete games or change game attributes (Name, description, price, genres and quantity).
Technologies: Django, HTML5, CSS3, BOOTSTRAP, GIT, BASH, PYTEST, SQLite, JQuery, Gunicorn, Python.
"
data engineer,"
1. Developed and maintained a microservices-based architecture for operating large amount of data (about 2kkk rows per day) with Python, Docker, Kubernetes, PostgreSQL, Kafka for a betting platform during my GR8 Tech tenure.
2. Designed and implemented REST APIs with usage of asynchronous Python, FastAPI, Aiohttp, Kafka, K8s.
3. Observed comprehensive system state with Grafana, Prometheus and Kibana monitoring tools. 
4. Affirmed code quality by means of writing efficacious functional and unit tests.
Big projects with a variety of used technologies, opportunities for learning (both from courses and peers). Preferring remote positions."
data engineer,"
I am working for two localization teams on voluntary basis right now. My tasks are performing localization reviews and due reports in the first project. Also I am using my knowledge of literacy editing in another project to produce high-quality translations for voice lines of video game characters for the Ukrainian dubbing team. I have expierence of executing online and offline activities as well.
I`m looking for interesting and challenging projects. І would be happy to be a part of the team with encouraging culture and promoting professional growth and development. My main interests are software testing, data analysis and visualisation, advertising and game development"
data engineer,"
I have practical experience (about 4 years) in the development and maintenance of large databases as a SQL developer in the field of auto parts sales and call center maintenance.
Daily work includes writing queries of varying complexity, views, common table expressions, functions, procedures, triggers, as well as analyzing the database for errors, inconsistencies and their elimination.
I have experience with MS SQL Server, MySQL, PostgreSQL, T-SQL, SSIS, ETL, SSRS.
Microsoft Office, Visual Studio Code, HeidiSQL, DBeaver, SQL Server Management Studio, Microsoft Visual Studio - Power User.
Familiar with HTML, CSS at a basic level.
I have completed courses and have theoretical knowledge of DWH, SSIS, ETL, SSRS.
I would like to do T-SQL, DWH, ETL, SSRS.
I don't want to administer the database"
data engineer,"
Hello!

I'm an aspiring Data Engineer who is looking forward to gaining my first commercial experience. You can find my skills listed below.

In late April, I completed my first project where I built a small ETL pipeline using Python, Pandas, and SQL to extract data about rocket launches from an API, transform it, and load it into a database for analysis.

Currently, I am focusing on developing my core skills (Python, Pandas, and SQL), and have created a roadmap to guide my learning process.
"
data engineer,"Google Cloud Certified Professional Data Engineer 
Google Cloud Certified Professional Cloud Architect
Apache Beam code contributor

Problem-solving
Direct communication to business stakeholders
Strong mathematical/algorithms background
Feature request -> Feature maintenance
Quality First
""Doing the Right things"" > ""Doing things Right""
Experienced Lead Big Data Engineer with a strong skillset in Google Cloud Platform (GCP), Python, SQL, NoSQL, Graph Databases, Data Warehouse, Data Lake, ETL/ELT, and building Enterprise Data Platforms. 
Proficient in developing efficient Data Processing Pipelines for Artificial Intelligence, Machine Learning, and Analytics systems.

Extensive experience in Product Development and Consulting for multiple customer projects with complex business domains, showcasing the ability to quickly grasp new concepts and tools while maintaining a deep understanding of on-site business processes.

Google Cloud Certified Professional Data Engineer
Google Cloud Certified Professional Cloud Architect
Apache Beam code contributor

Responsibilities:
Responsibilities:
• Developing, reviewing, refining, and managing a scalable enterprise data architecture
• Designing and implementing Analytics Data Warehouse.
• Designing, implementing, maintaining, and integrating a Graph Database into the platform.
• Implementing third-party data integrations
• Developing ETL/ELT processes and procedures
• Reviewing, optimizing, and implementing the platform Data Service Layer API
• Building, designing, and managing data infrastructure technologies
• Building, designing, and managing data integrations and data pipelines
• Managing data quality and minimum viable sparseness
• Informing database design, management, and architecture
• Contributing to data-related Cloud Operations (DevOps/CloudOps)
• Providing robust APIs to other parts of the team
• Integrating ML relevance and ranking resume_classifier developed by the Data Science/ML group

Soft Skills
• Ability to work as part of a self-directed
and self-managed team
• Ability to collaborate across time-zones
• Strong team player - the ability to work
directly with others exchanging ideas,
knowledge, experience and thoughts.
• Have a thirst for learning, continuous
improvement, sharing and working in a team environment
• Passionate about data quality
• Pragmatic proactive problem solving
• Eye for detail and identifying problems
• Leadership skills
• Not afraid of challenging assumptions, but
humble enough to recognize and adopt the views of others

Full profile on linkedin.com
Product Company
Artificial Intelligence
Data Science
Distributed Systems
GCP or other clouds
Graph Databases
Data Lake / Data Warehouse
Data Processing at Scale
Orchestration

RSU, Stock Options preferred"
data engineer,"
Data Engineer with strong experience in Big data technology stack and cloud technologies.
Solid experience in leading cross-functional teams 
Projects full life cycle management: requirements collection, architecture, design, documentation, implementation, testing, deployment, support. 
Experience in mentoring and developing team expertise according to the required demand Hands-on experience with GCP and AWS
Have experience in presenting and delivering projects from the POC stage to the production stage Implemented solutions for customers from the finance sector(Hedge funds, Trading companies) 
High interest in utilizing and further developing my skills in Machine Learning, software architecture, projects, and team management.
As far as non-engineering skills are concerned, I have an upper-intermediate English speaking/writing level
"
data engineer,"Have successfully built IT solutions and infrastructures for 10+ companies, varying in size from 5 to 150 employees.
1. Performed data mining and data analysis for Oil&Gas industry.
Stack: python.

2. Built NLP processing algorithm from scratch to enhance marketing SMS conversion (for a telecom).
Managed a team, researched reinforcement learning applications for the same task.
Stack: python, bash, PostgreSQL, Keras, GCP.

3. Built multiple data pipelines and automation routines as a replacement of manual labor performed by 10 employees.
Currently in the process of constructing a fully autonomous system to be integrated in the business. The system has replaced an approximate of 5 employees and covers data exchange and processing, system monitoring and reporting for executives.
Stack: python, GCP, bash, PostgreSQL.

4. Led a team of data scientists and data engineers in one of the top live streaming platforms, to deliver data driven recommendations to the viewers. Included ML resume_classifier, A/B testing, working with GCP. Stack: BigQuery, Cloud Functions, Dataflow, Container Registry, Cloud Run, App Engine.

5. Developed from scratch adtech solution for an auctions participant to detect fraud. Stack included: Snowflake, ClickHouse, GCP Cloud Run, Cloud Functions, Dataflow.

6. Developed and maintained from scratch an IT division at a commodity trading enterprise, including management of on-premise and cloud-based infrastructure, integration of existing solutions, research and development of ETL pipelines, BI and analytics setup, development of Django-based internal solutions for operational activities. Stack included: Azure AD, Azure Virtual Machines, Azure Power BI, Azure Flexible SQL, GCP Dataflow, GCP Cloud Functions, GCP SQL, GCP Storage, GCP BigQuery, GCP Cloud Run, GCP Domains.
"
data engineer,"I've implemented two pretty complex and data intensive projects from the same scratch. Participated in all architecture significant decisions/discussions, hope to have a full end-2-end picture and reasoning behind each of decisions and alternatives.
9 years of commercial development experience, 4 of them in Big Data area. Hands-on experience in R&D, design and implementation of Big Data solutions (last two projects from the same scratch). 

Spending a lot of spare time over reading books and articles about Cloud, Big Data, and other technologies beyond listed in CV. Eventually got interested in making arhitecture “right”. 

Looking for a complex system to be implemented with a big freedom in decision-making. Ability to take part of architect duties and be mentored by experienced architect would be a great plus.
Ideal opportunity is one that allows to gain and improve architecture skills. Greenfield project, lead position with part of architect responsibilities, technical pre-sales, mentoring from experienced architect would be a great plus."
data engineer,"Successfully accomplished projects:

- to migrate existing banking infrastrucutre from propietary tools to Cloudera Ecosystem (Hadoop, Hive, Spark) and reporting UI. Results: significant cost reduce due to Windows Server and High Power Computing grid decommission.

- creating new data infrastructure for global Fintech holding allowing close to real time reporting of more than 1k KPIs within a very limited budget that included but not limited to: risk analytics, regulatory reporting, marketing automations, top management reports; Results: company volume growth by 600% within 2 years.

- creating & partial migration of infrastructure from Microsoft ecosystem to open source stack to: reduce costs, improve efficiency and concurrency, introduce AI and advanced analytics, improved scalability and automations.
Building end to end Fintech Data and BI solutions (Team/Tech Lead) on top of Microsoft and AWS stack.
- Python (PCAP Certified);
- AWS Aurora, AWS Redshift,  AWS SQS, AWS S3, EC2, Boto3 SDK;
- Microsoft SSAS Tabular modeling, DAX, Power Query (MCSA Certified);
- Microsoft SQL Server 2008-2019, SSIS (MCSA Certified);
- Power BI (both cloud and on premises), SSRS;
- ETL & ELT Development;
- Prefect Orchestation, Docker;
- CI/CD Using TFS (Azure DevOps Server);

- understanding data design principles;
- establishing data processes and pipelines from scratch;
- establishing CI/CD and team as working unit;
- deep dive into domain to understand business operations better and focus on deliverable items leading to increase of income;
Not interested in short term projects.
Preferrable domains are: fintech, trading, gambling, gaming."
data engineer,"
Hard skills (including but not limited):
Python, R 
ETL & ELT
Airflow 
StreamSets
Talend
GCP (GCS, BQ, Firestore, Composer, Dataflow, PubSub, Data Transfer, Dataproc, SM, Cloud Functions, Cloud RUN, VM, Looker)
AWS (S3, Redshift, Lambda, Step functions, EMR, Athena, Glue)
Azure (ADF, Synapse Analytics, Databricks)
Kafka
ClickHouse
BI dev (R shiny app)
Sisense
Power BI
Tableau
Redash
A/B testing (t-test,  ANOVA,  Mann-Whitney,  chi-square  test)
ARIMA, GLM forecasting
KNN 
LGBM 
RDBMS (PostgreSQL, MySQL8, ClickHouseSQL, Standard SQL)
Regex
"
data engineer,"
I am a Lead Data Engineer with a strong background in building and optimizing data infrastructure for e-commerce start-ups. With over 7 of experience in the field, I have a proven track record of driving data availability, observability, and efficiency while controlling cloud infrastructure costs. 

In my current role at Accel Club, I have successfully improved the existing data infrastructure by migrating from an external vendor's solution to an in-house made one. This transition has not only increased transparency and reliability but also reduced development time and costs. Additionally, I have focused on enhancing local development and deployment transparency by implementing a CI/CD flow from scratch, leading to improved efficiency and collaboration within the team.

Previously, at Cian, I contributed to the support and improvement of data flows on an on-premise Hadoop cluster. I successfully migrated mass ETL jobs from Hive to Spark, optimizing data processing and leveraging SparkSQL. Furthermore, I developed templated Kafka and custom API ETL jobs, facilitating data integration and supporting the Data Science team in achieving their objectives.

During my tenure at Innotech, I played a vital role in developing business data marts and automating reporting processes for one of the largest banks in Russia. I utilized Hadoop, Spark, and Hive to handle large-scale data processing and analysis. Furthermore, I gained expertise in business and system analysis, providing valuable insights for future development.

At MERLION, I conceptualized and built a Data Vault Enterprise Data Warehouse (EDW) from scratch. I ensured adherence to development guidelines and standards while also mentoring junior data engineers. Additionally, I contributed to the implementation of Apache Airflow for workflow management and utilized Vertica for high-performance data storage and retrieval.

Throughout my career, I have gained expertise in a range of technologies, including Google Cloud Platform, Python, Git, Docker, and SQL. I am skilled in data warehouse architecture, cloud computing, DataOps, and DevOps methodologies. Furthermore, I possess strong proficiency in tools such as Apache Airflow, Apache Spark, Apache Kafka, Grafana, Redis, and Luigi.

Feel free to connect with me to explore opportunities in the field of data engineering, cloud computing, or related domains.
"
data engineer,"Implemented DWs with dimensional modeling best practices providing the customers with a reusable data model, well structured for consumption by data analysts, and highly performant.
I have experience in DW development from scratch, leading projects.
Interested in building new solutions, primarily in Snowflake Cloud Data Platform."
data engineer,"
I'm from Minsk, Belarus. But currently, I live in Georgia (temporarily). Open only for positions with relocation to the EU/the UK/the USA/Canada.

Current position and responsibilities:

LEAD DATA ENGINEER
Sep 2021 - Present
Company: iDeals
Reorganization of work with data (transition to Data Mesh), design new architecture and data pipelines for Data and Analytic Platform, work with sensitive data, gathering data from API's using Python and GCP tools.

Previous positions:

SENIOR DATA ENGINEER
Nov 2019 - Sep 2021
Customer: Big European Bank
Project: Data migration from old Oracle DWH into Snowflake using Azure Data Factory, Azure Blob Storage and Apache Airflow.
Responsibilities:
Creation and preparation dashboards in Metabase for monitoring users activities in Snowflake and for monitoring of Snowflake cost allocations;
Work with Snowflake streams and tasks for delta logic maintenance;
Creation of Snowflake tasks for incremental tables storing;
Creation of Snowflake UDF for automatic DDL preparation;
Preparation of Python script for Microsoft CDM data model getting and transformation it in database objects;
Creation custom packages in Python for work with HashiCorp and different sources such as Azure Blob and FTP;
Creation of custom Airflow sensor for checking objects appearance in Azure Blob;
Creation pipelines in ADF for data migration from Oracle into Snowflake; creation pipelines for daily data uploading from Blob into Snowflake;
Development of application in Oracle Apex for collateral management.
Tools:
Snowflake, Azure, ADF, Airflow, Gitlab, CDM, Dataverse, Oracle APEX, Oracle 12c, Jenkins, Docker, Metabase, Metabase API, Nexus PyPi, Python, HashiCorp

TEAM COORDINATOR (Jun 2019 – Nov 2019)

SENIOR DATABASE ENGINEER (Jun 2018 – Nov 2019)

DATABASE ENGINEER (May 2016 – May 2018)

ETL DEVELOPER (Sep 2013 - May 2016)

DATABASE DEVELOPER (Aug 2012 - Aug 2013)

DATABASE DEVELOPER (Jul 2010 - Jul 2012)
"
data engineer,"Created CI/CD flow from scratch using Jenkins
optimization of existing logic of objects in database structure
	performance tuning of queries
	building mdx queries (ssas) 
	new development/modifications in SSIS package according to business requests
Would like to work more with new technologies"
data engineer,"- Professional Cloud Architect, certified by Google Cloud
- Professional Data Engineer, certified by Google Cloud
- skilled in data engineering, technical leadership, web development, backend development
- participated in Software Requirements and Product Architecture creation
- participated in project stuffing
- played a tech lead role
- played team lead role
- experience with major cloud providers: Google Cloud Platform, AWS, Azure
- have experience working with SQL and NoSQL storages
- have experience with CI tools and CI/CD process
- worked at both product and service companies
- worked with Agile methodologies
- worked with different teams from small local (4-5 people) to big distributed ones (80+ people)
Open to positions that involve:
- one-time consultancy, advisory, pre-sales activity
- technical leadership & development"
data engineer,"During one year, designed and developed two business solutions from scratch, taking care of all communications with stakeholders, defining requirements, designing the technical solutions, etc. - all the way to the market.
- Professional Data Engineer, certified by Google Cloud 
- Microsoft Certified: Azure Solutions Architect Expert
- Microsoft Certified: Azure Data Engineer Associate
- played a tech/team lead role
- designed data processing and data analytics pipelines (BigTable, Google Cloud Dataflow, Apache Spark, Apache Hadoop)
- designed and developed data pipelines from ingestion to insight (Apache Spark, Azure Databricks, Delta Lake)
- participated in Software Requirements and Product Architecture creation, 
- established effective communications between business and tech stakeholders
- experienced in Analytical Data Modeling and Design (DWH, Lakehouse, Streaming Solutions)
- have experience with CI tools and CI/CD process
- proficient in the testing of data pipelines and want to grow in the direction of Data Quality, Data Discovery and Observability - and Data Governance in general.
"
data engineer,"AWS and Spark certified
Interests: Big Data, Clouds, Distributed Systems, DevOps.
1) Big Data or Highly Distributed System
2) Possibility to learn and develop during work process
3) Work life balance"
data engineer,"Developed and maintained core components of a large-scale Lambda-style data platform with up to 2.5M RPS.

Introduced monitoring and 24/7/365 support process via Datadog and OpsGenie for all the parts of the project, starting from the data ingestion and finishing with the possible reporting data issues and delays.

Was playing a crucial role in building the Big Data competence in my current company and participated in a lot of pre-sale / up-sale activities related to Data Engineering.
I've worked on large-scale data pipelines with rates up to several million RPS.
I love Big Data stuff, Python, Scala and Spark, and I still like to use Bash and Groovy for simple and/or monotonous tasks. I have experience with setting up large reporting sinks (from MySQL and PostgreSQL up to Vertica, Clickhouse, Oracle and anything else in between) and with Linux and BSD.

I'd like to work for a big product company with lots of data.
I'd love to improve my skills and expertise in new business domains, as well as solve complex problems in a simple way.

Priority-wise, I'm interested in Salary, new technologies and a big product company is preferable to me, compared to service companies and outsource businesses.

If possible, I'd like a project without the direct need to babysit the team."
data engineer,"Oracle Certified Associate, Java SE 8 Programmer
Oracle Advanced PL/SQL Developer Certified Professional
Oracle Database SQL Certified Expert
Primary skill is a develop server-side applications, web services, REST-APIs, databases structure design, data-warehouse, and ETL systems in various domains (telecom, banking, gambling industries). As a server-side developer during the last period of time, mostly used Java, Spring/Spring Boot, Hibernate, Oracle, Apache Kafka, ClickHouse, Tableau, Liquibase, PHP, Docker, Maven, Jenkins as well as plenty other things from the servers side ecosystems. Also, have experience in front-end/web development. For the current project, we are using plain JavaScript as well as TypeScript with Angular, HTML, CSS.
I am interested in long-term employment in a stable (preferable product) company with a good compensation package. 
I would like to have a clear understanding of the company goals and the needs of end-users.
The project should allow me will use my best experience as a serverside/database/data warehouse developer. Partly participation in front-end development not be considered as cons."
data engineer,"
- Coding using Java, Python, Groovy
- Designing and implementing ETL processes for data pipelines using Glue, PySpark, Spring Batch
- Building predictive resume_classifier using machine learning techniques (XGBoost, Random Forest)
- Design and development of web services / microservices for ETL steps (Spring, Falcon) and ML resume_classifier (scikit-learn, PyTorch) and its CI/CD integration (Docker, Kubernetes, AWS, CircleCI, Git)
- Anomaly detection using SparkML
- Designing and developing various data mining applications for natural language processing: web crawlers (HtmlUnit/HttpClient) and search systems (Lucene)
- Database knowledge: SQL (MySQL, Aurora, Postgres) and NoSQL (OrientDB, MongoDB)
"
data engineer,"Creation of Machine Learning based services that helped to automate a lot of human work and made company more profitable and competitive.
Creating of data warehouse that made data access fast and easy, and helped to make a lot of valuable data insights.
Creation of  high performance hybrid PPC/RTB advertising platform with effective analytics components.  Building data pipeline.
Creation of FISOAP: traffic tracking/scoring system.
Creation of useful platform components that lead to an increase in KPI.
Effective cost lowering by refactoring hardware and system design (e.g 70+ to 10+ servers)
7 years of experience in web applications design and development.		
6 years of experience in Machine Learning (NLP), Data Science/Engineering/Analytics domains		
3 years of experience as Project Lead in digital advertising building PPC/RTB arbitrage and data-oriented applications/components.		
1 year as Security Officer in digital advertising company in malware/fraud protection		
1 year as Head of Technical Support in digital advertising company		
1 year of experience in financial/business analytics		
		
- Passionate about The Data, able to work within deadlines, self-motivated, team player, self-studied person. Have desire to accept new challenges.		
- Ready to investigate and find solutions for issues with different levels of complexity.		
- My hobbies: self-education, reading books, playing guitar, yoga and everything that can help being happy and healthy.
Interested in:
Working in a data-driven company, developing data-centric products, 
in Machine Learning, Data Engineering teams,
building data pipelines, AI-based services, training resume_classifier.
Python/Go/JS stack;
Possible relocation: EU"
data engineer,"Implementing ML part of a photo editing app (from research to deployment) on one's own.
Bachelors diploma with honours.
Was invited to be a Mentor&Judge at hackathon.
Development of a system for analysis characteristics of cortical folding patterns on graphs (DL). Detecting anomalies financial data time series. Building a recommender system. Text tagging and speech-to-Text research. ML part of a photo editing app. Music generation, Text classification. Creating an automated trading agent (Java).
Expect the company mission to be inspiring, including interesting and complex tasks. I am interested in studying new technologies, meeting new people and public speaking."
data engineer,"Designed a serverless ML cloud architecture from scratch (AWS Cloud Formation/Terraform)
FleetyFly:
- Deployment and automatization of ML pipelines that optimize fleet efficiency and increase
clients revenue
- Design serverless ML cloud architecture from scratch (AWS Cloud Formation/Terraform)
- Prepared CI/CD pipeline for ML projects (AWS CodeBuild)
- Strategic decision about company data architecture
Wind Mobility GmbH, Department: Data.
- Created e-scooters rebalancing model that increases the revenue for the rebalanced e-scooters
by 50%
- Creating serverless ML architecture in AWS cloud (AWS Cloud Formation)
- Creation and deployment of ML resume_classifier to production environment (Python, SparkSQL, AWS
Fargate)
- Data Engineering: moving data from operational to analytical DBs in serverless fashion (AWS
Glue, AWS Lambda, DataBricks), extensive unit/integration testing
- Optimization of complex logistics systems that increases numbers of rides in a city by up to
25%
Vueling Airlines, S.A., Department: Data.
- Created a model that predicts price and pax of every flight in the 3 months period with daily
accuracy below 10 % MAPE
- Developed and implemented end to end data science pipelines from scratch for supervised
and unsupervised ML resume_classifier (PySpark/Python)
- Was part of Data Engineering team that deploys and automates ML resume_classifier to production
env. Provided logging and monitoring pipelines (Python, MLFlow)
Lidl Stiftung & Co. KG, Department: Advanced Analytics.
- Developed ML algorithms (regression and classification) (sk-learn)
- Performed exploratory data analysis and business analysis, data visualization (R, Python,
SQL)
- Improvement of sales model (association analysis etc.
Most important is to work on interesting projects, and hands on work on developing ML solution, deploying architecture for ML and data. I would prefer keep the meetings to minimum (of course some meetings are necessary, but some aren't :) ). Working with experienced people from which I can learn, or with whom I can brainstorm is beneficial of course, but I also have experience with working in a startup where I was the first DS/ML Eng."
data engineer,"
Data Engineer at Amazon
• Distributed data quality framework design and development (personal project)
• Working with Cloud Development Kit for automated resources deployment
• Internal fleet load investigation and optimization

Data Engineer / ML Ops at Receptor.AI
• Parallelization of machine learning and biochemical computations
• Managing public cloud and an own cluster
• Building startup infrastructure from scratch

Big Data Analyst at Kyivstar
• Constructed datasets using Apache Spark, Hive and Oracle DWH
• Look-alike model development
• Created analytical reports using Power BI (B2B, B2G)

Junior Fullstack developer at Prophy science
• Developed REST API via Flask Python3 web framework
• Frontend development with React JS framework
"
data engineer,"I оffered our team project to change the user's portrait and offered a feature to collect donations.
1. Conducted requirements analysis
2. Developed a check-list and test cases
3. Determined test design approaches and techniques
4. Tested the API using Postman and Swagger
5. Conducted Functional, Non-Functional and Change-related testing Made Bug Reports in Trello
6. Made a part of the presentation from the QA team
7. Took part in SCRAM meetings of the Developers+QAs team
To work with a company that sells its product to russia or have employees from russia"
data engineer,"I have developed a custom analytics solution based on Kafka, Bigquery.
Hi there, 

I am looking for an eCommerce or SaaS company that needs help with product and marketing analytics. 

Skills: 
- SQL: Google Bigquery, Clickhouse, Snowflake.
- BI: Power BI, Tableau, and Looker Studio.
- REST API: any ads and analytics APIs. 
- ETL: any data pipeline development.
- Designing a data warehouse and data lake structures. 
- Python: Pandas, NumPy. 

Expert skills in Google Tag Manager, GA4, Firebase and AppsFlyer.  

Designing KPIs for different departments and business units in a company. Using new technology as server-side solutions. 

I have no problem leading a department or being part of an existing team. I was an SEO and PPC specialist in the past, so I understand the challenges before marketing. 

I would be happy to help your company fall in love with analytics.
I'm Ukrainian, but now in Warsaw, Poland.
"
data engineer,"
Software Developer with over 3 years of commercial experience in software engineering, data engineering and data mining. Most of my current work is related to design, implement and deploy BigData processes, ETL processes, creating data tools for analytics. In the past I developed web crawlers and web scrapers for the financial industry.

- Participation various POCs to see if technologies meet the business goals.
- Key contribution in the design, development, deployment and maintenance of the data processing solutions.
- Active collaboration with data architects and data analysts.
- Building data-marts for supporting analytics.
- Mentoring junior developers.

I mostly work with Azure cloud platform, but don't mind switching to AWS.
"
data engineer,"Implemented one process using AWS Lambas reducing runtime from 2.5+ hours to 20 minutes (in total); Helped my team to set up Pandas in AWS Lambdas reducing development time for new processes almost in half.
N-iX, full-time, 2 years, Aug 2021 - present
Data Engineer
Develop new ETL processes in AWS. Work with Apache Spark/Scala, and Pyspark implementing high-performance applications with TBs of data.
Use AWS lambdas with Python 3+ and Pandas, sometimes Java 8 for daily processing.
Also, implemented a few EC2 applications (backend-only) using Java 8 + Spring Boot.
"
data engineer,"
2 Years of DBA experience at EY, have used VBA, MSSQL, as well as RPA(Corezoid) and PowerBI.
7 month at SPS Commerce as Data Engineer. My responsibility was to maintain current pipeline using Snowflake, C#, SQL, we have used AWS as cloud provider.
8 month at SoftServe as Data Engineer. With team we have developed new pipeline for educational company using AWS, Databricks, Apache Airflow
9 month so far at 3Shape as Data Engineer, currently we are working on brand new pipeline using Azure Databricks, Kafka, Delta Lake
"
data engineer,"
Data Engineer with more than 2 year of experience. Mathematics graduate passionate about data engineering. Highly-capable leader, excellent at multitasking and analytical thinking. Proficient in a 
range of modern technologies including SQL
To work in a learning and challenging environment, utilizing my skills and knowledge to do the best of my abilities and contribute positively to my personal growth as well as the growth of the organization."
data engineer,"
Experienced IT professional with 4+ years in the IT industry(3+ years as a Business/System Analyst and 1.5 years as a Data Engineer)
Currently, I've worked as a Middle Big Data Software Engineer with practical experience in Databricks, AWS, Azure, Apache Spark, and Airflow.

The outsourcing company, Feb 2022 - Present
The last project: 
Fintech domain, Implementation of Databricks for the customer
Responsibilities:
- created and tested Delta Live Tables(DLT) pipelines and jobs
- performed bugfixes and improvements
- developed notebook for automatic DLT pipelines and jobs creation through Databricks API for MySQL, MariaDB, and MongoDB sources
- developed notebooks to ingest big tables(3 TB) from MariaDB source
- developed Airflow DAGs to call Databricks jobs through Airflow
- prepared configuration files for DLT pipelines
- performed data validation of Delta tables
- communicated with internal client teams to resolve issues
"
data engineer,"
I am a skilled Data Engineer with experience in collecting and processing data from various sources, building data pipelines, and developing data marts. I have a strong background in Python and its related technologies, including Pandas, Celery, and Airflow, as well as experience with SQL, Redis, Django ORM, SQLite3, and MySQL databases.

In my role as a Middle Data Engineer, I am responsible for collecting data from different sources and building data pipelines and ETL processes. I work with various technologies and tools, including Heroku, Dokku, and AWS for deployment.

In addition to my work as a Data Engineer, I also have experience as a Python/Django Developer, where I was responsible for writing backend programming in Python, developing and maintaining the website, and using Beautiful Soup for web scraping.
I am currently seeking a position that will provide me with an interesting and challenging project, where I can utilize my existing skills and learn new technologies. I am passionate about my work and am always looking for opportunities to develop as a professional and take on new challenges."
data engineer,"
I have experience in developing and maintaining an efficient process of loading data from various sources such as Apache Kafka, RabbitMQ, MySql, PostgreSQL and Oracle Database using ELT tools Oracle Data Integrator and Apache NIFI. I also have experience in writing scripts and packages according to client's requirements using Oracle PL/SQL. Additionally, I have experience supporting DWH servers and administering Gitlab, Jira, and Confluence. I designed and implemented CI/CD to automate the installation of updates in production environments. I also have practical skills in working with Google Cloud Platforms and BigQuery, and NoSql DB Redis using Oracle Data Integrator and Apache Airflow. Additionally, I have experience writing scripts and packages as per customer requirements using Oracle PL/SQL. I also have experience maintaining DWH servers and administering Gitlab, Jira and Confluence. Development and implementation of CI/CD to automate the installation of updates in production environments. I have practical skills in working with Google Cloud Platforms and BigQuery, and NoSql DB Redis.
"
data engineer,"•	AWS Certified Cloud Practitioner
•	Databricks Certified Data Engineer Associate
•	Microsoft Azure: Fundamentals
ISP/Media & Entertainment domain
Stack of technologies: Python, GCP (BigQuery, Storage), SQL, Jenkins, Apache Nifi, Terraform, Bitbucket, Jira
I created resources on the Google Cloud using Terraform, optimized queries, and configured the process of loading data from sources into a table.

E-commerce domain
Stack of technologies: Python, Spark, Spark Streaming, Spark SQL, SQL, GCP (BigQuery, Composer,Storage), Databricks, Airflow, Git, Jira.
My responsibilities included connecting new data sources to our system. Creating, documenting and testing pipelines. Refactoring code, investigating and debugging issues. Creating various queries (identify issues, resolve them, and create datasets for reports). I was also involved in the creation of jobs (by pyspark or SQL) to improve cleaning, data transformation or data loading. I was working with big international team.

Rental Domain
Stack of technologies: Scala, Spark, Kafka, Cassandra, ELK stack, Airflow, Docker, Azure, Git, Jira.
Created adaptable and efficient data visualization pipelines to aid businesses with management, and was involved in every phase of the project's development. Collaborating with a team consisting of 9 developers, 1 Scrum master, and 1 lead, I utilized a variety of technologies throughout the process.
"
data engineer,"
My current responsibilities include:
• Troubleshooting and resolving technical issues within data processing systems, including performance tuning and optimization of Spark applications and SQL queries. 
• Collaborating with development and data teams to understand data requirements, implement data transformations, and review code quality 
• Developing efficient code within Databricks Notebooks to support data processing and analysis tasks 
• Creating delta tables using data from multiple sources 
• Engaging in Spark development to create scalable and reliable data processing applications 
• Optimizing ADF pipelines 
• Addressing and resolved bug fixes 
• Crafting SQL queries to extract, analyze, and manipulate data from various databases 
• Conducting research to identify, evaluate, and implement potential solutions

Technologies:
• Apache Spark 
• Databricks 
• Microsoft Azure Cloud 
• Azure Data Factory 
• SQL 
• Scala/Python 
• Grafana

My previous experience contains the following responsibilities and technologies:
Responsibilities:
• Created delta tables using data from multiple zones 
• Developed code in Databricks Notebooks 
• Spark and Python development 
• Worked with ADF pipelines 
• Performed different bugfixes 
• Created SQL queries 
• Searched for possible solutions 
• Implemented Python tool to parse existing codebase and build data lineage based on SQL code 
• Implemented tool for interactive visualization of SQL data lineage 
• Implemented unit tests.

Technologies:
• Apache Spark 
• Databricks 
• Microsoft Azure Cloud 
• Azure Data Factory 
• SQL 
• Python

I am dedicated to advancing as a proficient big data engineer by continuously enhancing my technical skills in data processing, distributed systems, and cloud platforms. Through hands-on experience in real-world projects and active engagement with industry trends, I aspire to contribute effectively to large-scale data processing.
"
data engineer,"Not sure that statements like ""performance increased by 42%"" are worth to be added here, cause no one knows what and how something was implemented before.  

Anyways, I've got some certifications in Python, SQL and Data Engineering fields from DataCamp in my LinkedIn profile.
- Participated in designing and implementing real-time ELT pipeline to process semi-structured data from Apache Kafka to PostgreSQL using Python and its libraries;
- Wrote lots of SQL procedures and functions in Postgresql that process, enrich the data received from Kafka;
- Created APIs using PostgREST in order to data in database was accessible from outside;
- Developed a generic Python application that loads data from a database and puts it on either Email or SFTP server in CSV format;
- Created pipelines that archive data from PostgreSQL to Google Cloud Storage daily;
- Utilised Apache Airflow as a tool for triggering data pipelines periodically and continuously;
- Implemented real-time ETL pipelines that loads data from Kafka to Google BigQuery;
- Participated in designing data warehouse solution in Google BigQuery and wrote lots of SQL procedures and functions that loads data into the DWH;
- Implemented a Python application with Pandas that loads data from multiple data sources, compares it and sends the results of comparison to Email;
- Created many reports and dashboards using  JasperReports tools which are sent to the clients' emails everyday.
I want to improve my skills in Data Engineering field, especially focusing on the cloud products. And if your company provides any help with preparing or/and getting any AWS/GCP certifications then I will consider them at first."
data engineer,"
QA Responsibilities:
• Product quality providing at all stages of SDLC
• Automation scripts creating (Batch files, Python)
• Requirements Analysis, Traceability Matrix maintenance
• TC's creation, execution and maintenance on all environments
• Data Quality analysis
• Jira to Azure DevOps TC's migration
• Initial and Incremental load testing

Dev Responsibilities:
• writing complex T-SQL queries
• some experience with SSIS jobs
• writing stored procedures for testing purposes in MS SQL Server
• improving query execution performance
• reports creating using PowerBI and SSRS

DBA Responsibilities:
• creating objects (tables, views, CTE's, Stored Procedures, Triggers) in MS SQL Server
• some experience with job scheduling using Crontab
• Batch script creating for quick objects deploying
• some basic experience with backup creating in MS SQL Server
• Indexes creating

None-projects activities:
• Participating in Python Course as Assistant and Code Reviewer
• Interviewing (11 sessions on 7 July 2020)
• Soft Skills Course passed

Tools:
• Microsoft SQL Server Management Studio
• PyCharm
• Azure DevOps
• Jenkins

Certifications:
• Microsoft Azure AZ-900
• Google Cloud Platform Associate Cloud Engineer (Coursera)
• Datacamp Data Engineer

Technologies and Languages:
• Python, PySpark
• Golang
• T-SQL
• Batch
• RDBMS, DWBI (Data Warehousing & Business Intelligence), ETL
"
data engineer,"I finished Data Engineering courses from IBA group company that was concentrated on creating Big Data solutions using Apache NiFi, Python, PySpark, PyKafka and IBM Cloud.

Courses on Coursera:
Big Data Analysis with Scala and Spark
Big Data Essentials: HDFS, MapReduce and Spark RDD (Yandex)
Introduction to Designing Data Lakes on AWS
Dynamic and creative software developer with over 1 year of experience in creating data engineering solutions. Eager to support the big data dev team with good data preprocessing skills. In previous roles, helped with physics data for Landviser company, and built data lake solution for Tire manufacture.
"
data engineer,"- Designed, led and co-implemented a switch to new aggregation key in one of our projects. It required data resume_classifier changes and affected more than 50% of one of our spark projects. The feature has been smoothly delivered in time.
 - Significantly improved an algorithm of files import. We had issues with 1TB+ files import, cluster scaling didn’t work. After the improvement it easily runs on 40 m5.4xlarge EC2 instances.
4 years of Java experience, 3 years of Spark experience. 2 years working with Python as a secondary language. Have a short experience and willing to learn Scala.

Databricks certifications.

Last position:
 - Implementing of data pipelines based on Airflow + EMR 
 - Taking part in project architecture/design
 - Taking part in short/long term project planning
 - Performance tuning
 - Writing unit and integration tests

Technologies:
Java, Spark, Airflow, Python, Spring Boot, AWS s3, AWS EMR, AWS Athena, PostgreSQL
Cloud based (AWS or Azure) project using Apache Spark with Java or Scala"
data engineer,"Giphy:
Improved CTR in the Giphy Slack App for 10 percentage points

HolyWater:
Built marketing analysis system and data warehouse from scratch
Setup ETL processes for data importing from different platforms
Created analytics dashboards that helped the marketing team to improve our company ROAS from 80% to 120% on day 180
In the Giphy I was working on creating a multimodal recommendations system and improving existing resume_classifier, researched state-of-the-art resume_classifier to work with different types of unstructured data.
Stack: Python, Pandas, Numpy, PyTorch, AWS, Databricks, SageMaker

Currently, in my role at HolyWater as ML & Data Engineer, I build and maintain crucial data-related processes for our marketing team. I regularly add new data and experiment with different approaches to provide a complete picture of creatives' and campaigns' performance.
Stack: Python, Tableau, GCS (BigQuery, Cloud Functions, Administration, etc), Airflow, Firebase, AppsFlyer
Interested in FinTech and Crypto/DeFi sphere, not interested in gaming"
data engineer,"
Built and deployed REST, Full Stack applications to Kubernetes environment. 
Worked with SQL/NoSQL databases using SQLAlchemy.
Used Apache NiFi and Apache Spark to build pipelines used for fetching, transforming data.
Worked with NLP and Speech Recognition projects, deployed resume_classifier using Flask, FastAPI. Created CI/CD pipelines for continuous integration and deployment.
"
data engineer,"- Optimization of big data data flow to decrease costs up to 400 times (1TB to )
- Development of resume_classifier is capable of turning cash flows up and costs down.
- Creation of resume_classifier with a higher level of metrics than previous solutions, required by business
- Automating handwork to cost optimize processes in business, saving thousands dollars per week. 
- Leading developers to accomplish tasks and documenting business processes to bring light of business vision to development team for right direction.
- 400+ solved leetcode problems
- 150+ solved strata problems
Experienced Software Engineer with 3 years of Machine Learning and Data Engineer, specialised in Retail, Banking, Telecom, Creative fields. And have experience as Data engineer with rolling big data for pipeline (ETL, ELT). 

As a machine learning engineer, I have experience in developing end-to-end machine learning solutions. I am familiar with the entire machine learning pipeline, from data collection and preparation to model training and deployment.

My team was realising scoring resume_classifier, business reports, monitoring churn resume_classifier. A data-driven data scientist with highly adaptive skills in analysis and investigation. Have a strong problem-solving skills, critical skills and the ability to accomplish complicated tasks.  Easily can speak with international clients to complete End-to-End Machine learning projects.
I expect experienced CTO, responsible and mature colleagues, who can take ticket tasks and accomplish them. I respect team, who is thinking of one's turn to read their code (do not forget about documentation). The team is able to delegate tasks and strive for knowledge.

Deployment and designing of resume_classifier for highly data intensive workloads (end-to-end). Working with AWS, maintaining Big Data."
data engineer,"Successfully leaded the research, Deep Learning resume_classifier training and resume_classifier deployment in the Neural Machine Translation area using OpenNMT-py framework.
Python3, OOP, PEP8
iPython, Google Collaboratory, VS Code, PyCharm
Machine Learning (Scikit-Learn, Pandas, Numpy) 
Data Visualization (Matplotlib, Plotly, Seaborn)
Deep Learning (TensorFlow, Keras)
Basics of MLOps (TFX)
Basics of Airflow
SQL (subqueries, stored procedures, window functions) 
Basics of Docker, Kubernetes, Minikube 
Basics of GCP, AWS
Professional Data/ML team, career planning, established processes, professional working environment."
data engineer,"Numerous ML related projects. Bringing ML resume_classifier to production. Expertise in Video Processing, Edge Devices, MLOps, Realtime recommendations systems.
Started with different small Data Science project. Evolved into building IoT systems based on Computer Vision algos. Implemented MLOps for such systems. Then switched to cloud based ML systems with strong focus on MLOps side.
Product company with ML-centric product."
data engineer,"Has developed and designed a many projects  (include international), creating a data warehouse, the implementation of a CRM system.
25 years of experience working with databases (Oracle, MS SQL).
DB structure design (tables, indexes, views, stored procedures, functions of any complexity).
Strong knowledge of T-SQL.
Creating and supporting SSIS packages.
Creating and supporting reports SSRS.
Creating and supporting OLAP cubes SSAS.
Have experience in implementing CRM.
Have experience in Data Warehouse Solution Architecture.
Data analysis skills.
Requirements Analysis skills.
Prefer the development of complex projects on MS SQL, ETL development, SSRS, DWH.
Full time.
The ability to work remotely."
data engineer,"Pet-projects (2021-2023):
- Oracle Database 23c (PL/SQL, UTL_HTTP, XML, JSON (Data Types, Schema, Relational Duality));
- Amazon RDS (PostgreSQL, MySQL, MariaDB, Oracle, Amazon Aurora (MySQL), Aurora (PostgreSQL));
- MS SQL Server 2022 (T-SQL, CLR C#, CLR, OLE), Azure SQL Databases (T-SQL);
- SSRS (*SQL DB), SSIS (*SQL DB, XML, JSON, CSV -> MS SQL 2022);
- PostgreSQL 15 (PL/pgSQL, PL/Python);
- Docker, Kubernetes, Docker Desktop (*SQL DB, MongoDB, Apache Cassandra);
- C# .NET 7 (WPF, EF Core (*SQL DB, *NoSQL DB), WinForms, ADO.NET, JSON, XML);
- С# ASP.NET Core 7 RESTful Web API (EF Core, MS SQL, Azure SQL);
- Java 20, JavaFX, Swing, Maven, JasperReports, JDBC (*SQL DB, MongoDB, Apache Cassandra);
- Python 3, PyQt6, DB (*SQL DB, MongoDB, Apache Cassandra).
-----------
* SQL DB (Oracle, MS SQL, Azure SQL, PostgreSQL, MySQL, MariaDB, IBM DB2, IBM Informix, Firebird, SQLite). 
* NoSQL DB (MongoDB, Azure Cosmos DB, Apache Cassandra).
- 10+ years of experience in the development of banking automation systems (PL/SQL, SQL). 
- ETL and API projects: Team lead, developer on implementation, migration, confluence of the core banking system in banks and financial org. (PL/SQL, SQL);
- Integration of banks with Neobank, Corezoid, DWH (Oracle, MS SQL), web-services (json, xml);
- Report building systems: SSRS, Report Builder, JasperReports, Fast Report;
- Data modeling, architecture/structure design;
Not DBA, DWH and PM position"
data engineer,"Helped establish MLOps practice in one of the biggest fashion retailers in the world.
6 + years as a back end developer.
6 + years as data engineer.
Worked with all three major cloud providers as well and with different on premise Hadoop distributions (Mapr, horntonworks). Wong time worked in data science consultancy where lead crosfunctional team of data scientists, data analysts and data engineers
"
data engineer,"IBM, Oracle, AWS certificates
Certified​ ​data​ ​engineer​ with​ ​13 years​ ​experience​ ​in​ ​designing​ and developing​ ​ DWH, Data Lakes, Data Mesh and ​Big Data solutions. I have an experience of building Data Infrastructure from scratch for a unicorn company.
"
data engineer,"
Junior Python Developer 
3 yrs 1 mo
- Built 2 REST APIs for American education company.
- Key Technologies: Python, Flask, AWS, Dynamodb.

- Built and maintained multiple plugins for American monitoring company.
- Key Technologies: Python, GCP, Docker.- Built 2 REST APIs for American education company. - Key Technologies: Python, Flask, AWS, Dynamodb. - Built and maintained multiple plugins for American monitoring company. - Key Technologies: Python, GCP, Docker.
Skills: Software Development · Python (Programming Language)


Software Engineer
 6 mo
- Built and maintained multiple plugins for American monitoring company.
- Key Technologies: Python, GCP, Docker.

- Build and maintained internal services for American monitoring company.
- Key Technologies: Python, JS, Vue js.

- Covered the project with QA automated end-to-end tests.
- Key Technologies: JS, Nightwatch js.- Built and maintained multiple plugins for American monitoring company. - Key Technologies: Python, GCP, Docker. - Build and maintained internal services for American monitoring company. - Key Technologies: Python, JS, Vue js. - Covered the project with QA automated end-to-end tests. - Key Technologies: JS, Nightwatch js.
Skills: Software Development · Python (Programming Language) · JavaScript


Middle Software Engineer
 1 yr 10 mo
- Built 2 REST APIs for German logistics company.
- Key Technologies: Python, FastAPI.

- Built a part of a new data pipeline for a complex ML algorithm at the logistics company.
- Key Technologies: Python, Pandas, NumPy. 

- Built and maintained internal services for German logistics company.
- Key Technologies: Python, FastAPI, Docker.- Built 2 REST APIs for German logistics company. - Key Technologies: Python, FastAPI. - Built a part of a new data pipeline for a complex ML algorithm at the logistics company. - Key Technologies: Python, Pandas, NumPy. - Built and maintained internal services for German logistics company. - Key Technologies: Python, FastAPI, Docker.
Skills: Software Development · Data Engineering · Pandas · Python (Programming Language) · Data Science
I expect a project with decent work processes implemented like: agile, code reviews etc (or I'm allowed to build these processes).

Cooperative team.

I'f it's a failing project that's going down and you want a a developer who will overwork and overperform to save it for another month or two pls don't contact me."
data engineer,"Latest achievements:
-> Built a data analyst/engineering department from scratch.
-> Developed analytics lib, and notebook orchestration framework.
-> Implemented automation of web application release processes.
-> Significantly improved performance of  ETL pipelines.
I have more than 10 years of experience in IT. In recent years, I have been engaged in data engineering. The core stack is Python, PySpark/Databricks, SQL, Azure. Lately, I've been interested in ML engineering topics. Also, I have an experience in .NET. I am able and willing to work on the complex projects. During the last few years, I performed the duties of team lead-a of the Ukrainian development team. Open to interesting projects and proposals!
Here are some projects I worked on:

1. Data analytics software platform.
Used for analyzing customer data to perform further optimizations in the company’s workflow which leads to increasing margin and reducing unnecessary losses. The solution consists of three main parts. ETL pipeline, which processes customer data, a backend that provides API for querying data, and a frontend that aims to present results to the end-users.
Responsibilities: Data engineering(
ETL pipeline development, development of analytics library, development framework for pipeline orchestration), CI/CD, team management.
Backend development: REST API, GraphQL API, Integration with Azure services).
Technologies: Python, PySpark/Databricks, C#/.NetCore, Azure, MS SQL Server/Graph databases. 

2. Broadcast traffic system
Windows application broadcast traffic system designed for long and short program planning, asset licensing rights and finance, transmission scheduling, secondary event scheduling, and media library management.
Responsibilities: windows application development, database development.
Technologies: Delphi, DevExpress, Firebird PL/SQL 

3. Video archive
Web application for describing video content with further fast and flexible search.
Responsibilities: Web application development frontend and backend parts.
Technologies: Python/Django, Angular JS/DevExpress, Delphi, Sphinx search engine, Firebird, Nginx

4. Media asset management system(MAM)
Distributed client-server system for building workflows for processing video files, including monitoring of different sources (FTP, SMB, etc.), transcoding, and relocation.
Technologies: Delphi, FFMPEG, ANGULAR 4, HTML, CSS, DevExpress.
"
data engineer,"
Data Engineer/Python developer
SynergyOne
Apr 2019 – Present
5 mos
Kiev Region, Ukraine
- Development and deploying of the credit-scoring module.
- Embedding predictive resume_classifier in the scoring module.
- Building and optimizing data pipelines.
- Work on optimizing the calculation of input parameters.
- Development of a testing module to verify the accuracy of calculations.

PJSC Ukrgasvydobuvannya
Full Stack Developer
PJSC Ukrgasvydobuvannya
Aug 2018 – Apr 2019
9 mos
Kiev Region, Ukraine
- Development WEB-application monitoring of the gas and oil well indicators.
- Development Dashboards and analytical reports.
- Support for the quality of gas and oil well monitoring data.
- Deployment and ensuring the smooth functioning of the well monitoring.
- Collection of data from various sources.

Vodafone
System Administrator
Vodafone
Feb 2014 – Aug 2018
4 yrs 7 mos
Kiev Region, Ukraine
- Supporting of workability and administering Unix\Windows servers, monitoring systems.
- Development of network monitoring applications.
- Building analytical Dashboards.
- Building ETL processes.
"
data engineer,"
Data Engineer and Backend Developer
Technodom· Full-time
Sep 2021 - Present · 1 yr
Almaty, Kazakhstan

Data Engineer: 
• Backend development (Python) 
• Scrapers (Python Selenium, Python Asyncio) 
• Crawlers (Python Appium, Python Asyncio)
• ETL (Apache Airflow, Python) 
• REST API development (Python FastAPI) 
• Web Application development (Python FastAPI, JavaScript, HTML)
• Process Automatization (Python OpenCV, Python Aiogram, Cl/CD) 
• Big Data Processing (PostgreSQL, SAP HANA, MongoDB)
• Linux Servers and VM Administration (Ubuntu) 
• Services Administration (Kubernetes, Docker) 


Automation Specialist
Alisa · Part-time
Nov 2019 - Sep 2021 · 1 yr 11 mos
Almaty, Kazakhstan

Responsibilities:
1. Designing, developing and implementing business-automation solutions.
2. Developing and updating a well-structured database in order to store production data.
3. Creating a bespoke CRM-like software to ease the work processes of employees.
4. Automating the compilation of routine periodic reports.
"
data engineer,"
I have experience working on two projects as a Software Engineer in Test. I have a Bachelor's degree in Software Engineering. I am looking for a new role for me: data engineer as the opportunity to challenge myself.

Responsibilities that I have done:
- Collaborating with developers and product owners
- Automating test scenarios using the frameworks
- Setting up the infrastructure to run tests
- Developing tools to assist manual and exploratory testing
- Creating requirements for the tools
- Code Review
"
data engineer,"
Practical Database Engineer possessing in-depth knowledge of data manipulation techniques and computer programming paired with expertise in integrating and implementing new software packages and new products into system. Offering background managing various aspects of development, design and delivery of database solutions. Tech-savvy and independent professional bringing outstanding communication and organizational abilities.
Company NIX
"
data engineer,"
Python Data Engineer, Ph.D., with over 7 years of successful experience in different industries with data ETL, ML, big data, data processing. For the past years has been working with capacity and performance of banking applications, delivering resume_classifier for market risk data, developing and deploying pipelines for data of diverse origin.
Analytical and detail-oriented.
"
data engineer,"significantly upgraded produced ETL quality and reduced fraction of trash data.
RUS/ENG. Location - Moscow, Russia

1. Admitad: Developed a contract for data supply on apache kafka based on google protobuf; also used logstash and clickhouse
2. Sbermarket: Tuned recsys in production: developed ETL pipelines; airflow DAGs, stored recommendations in redis. stack: yandex cloud, airflow, spark, grafana, clickhouse, docker, gitlab ci/cd
3. Ozon: 50/50 development/ML. wrote airflow dags/etl pipelines, trained neural nets. stack same as above + hdfs
4. freelance web development / ml projects, such as web service for image segmentation data labeling & solution on this task.

Education: Moscow State University named after Lomonosov, Computational mathematics faculty
i prefer small teams with startup atmosphere, but enterprise does not annoy me too.
also prefer working solo on my subproject, but sharing tasks is ok for me too."
data engineer,"Developed Android client for Python based application.
Developed several integrations using SOAP, XML-RPC.
Implemented and configured task tracking process.
Automated development process with internal tools including Slack bot.
Developer REST API from scratch.
Developed scraping scheduling and data post-processing from scratch.
Switched LDAP client libraries with integrations.
Migrated an old REST API to the newer with FastAPI.
Developed several integrations with external APIs (Domo, Marketo, Hubspot, etc)
Developed ETL pipeline on AWS from scratch with terabytes of data in Redshift. Managed Redshift cluster.
I have 10+ years of commercial development using Python, Projects I was working on relates to the different areas:  network/security, ERP, ETL, and Data processing. I prefer using the standard library to using modern external packages.
Resolving the issue with high quality in predefined terms means more than the desire to try modern technologies for me.
No Django development."
data engineer,"Projects: 
Building Decision Making Systems from the ground up on Django (DRF , SQLALchemy, Pydantic, numpy, pandas) plus alternative solution developed demo on (Spark , Kafka) 
Building transactional history and  notifications clients for bank (Postgresql , Python , RabbitMQ)
Building reporting systems and dashboards infrastructure from the ground up on open source.
Took part deploying liveness solution on AWS
As an experienced backend developer, I have a strong background in Python (API integration data and ml resume_classifier on DRF) .
SQL, and database programming (DBA , ETL jobs,  production decision making system and transactions) , as well as proficiency in RabbitMQ, Docker, and server administration. 

For the last few years worked building Big Data for banks in Ukraine as team/ tech leader of developers teams.
 
I am eager to leverage my skills to tackle challenging projects and contribute to the success of your team.
python development , devops , full stack"
data engineer,"
About 10 years of experience with Python and 3+ years as big data engineer.

Worked in different domains, including telecommunications, precise agriculture, mining and other.

Have some mentoring experience, limited DevOps, CI/CD, shell scripting. Worked with different clouds but more familiar with GCP.
Looking for python backend developer / data engineer role rather then leading position.

Don't want to work with odoo or front end."
data engineer,"
Data Engineer - NDA(June 2022 - nowadays)

Python Developer - SoftJourn (August 2021 - June 2022)
Working as Backend Engineer in analytical team. Working with AWS services

Python Intern – Luxoft (February 2021-August 2021). Work in Automotive Production team, which deals with compilation map. 
	Development of internal web and desktop applications for automation process of compilation map and other internal processes. (Web – Flask, Django, desktop – wxPython) 
	Preperation environment for compilation maps
	Support process of compilation
	Visual testing
	Report for the customer at the end of the iteration

Startup AllMemes at the Odessa I.I. Mechnikov National University, work within the KeepSolid  laboratoty (September 2020 – nowadays): AllMemes – social network, which specializes of classification images for  the convenience of users
	Database design and support (PorsgreSQL)
	Python backend (Django, Django Rest Framework)
	Development of parsers from Internet, other social networking (Python, work with outside API, requests, bs4)
	Development of neural networks for classification images (Python, Keras)

Freelance (September 2019 – nowadays):
	Web parsers (Python, requests, bs4)
	Telegram bots (Python, aiogram),
	Rest web application, backend (Flask, Django, Django Rest Framework, PostgreSQL, MySQL),
	Two games on UnrealEngine 4
Software Engineer - OPP (Odessa Port Plant) (July - September 2019):

	System programming (Python)
	Web and desktop application for automation work with database for admins (web – Django, Flask, desktop – C#, WinForms, database - MySql)
"
data engineer,"First of all, I have improved an ETL processing backend using threading and generators that allowed my team to work with a lot bigger amount of data and to spend less time on already existent workflows using a new approach.
Second, as talking about hacks, changed Django Connections behaviour in order to make dynamic connections update possible in runtime and to exchange it via the cross-services connection.
Mostly worked with backend-part of web applications.
Implemented integrations with different services and API's (AWS Cognito, Salesforce instance, client-related API, and a lot more)
Taken part in developing and support of self-made ETL  service.
Implemented threaded approach to data-processing backend.
Worked on SQL scripts and procedures that were supposed to gather, transform or send data from one source to another.
Supported and modified AWS Glue workflows, with a bit of Terraform use.
Handled database modifications and schema tune-up.
As for now, I have a combined role of backend developer and a data engineer.
First of all, I want to increase my qualification as a data engineer.
Would be great, if it would be possible to take a part in ML projects and get expertise in that field.
The use of the multiprocessing or threading approach is a great possibility because I`m getting a lot of pleasure while working with that tools.
I have some basic experience with golang, so I`m interested in combining the python position with it.
To be honest, I`m pretty language-agnostic and looking to increase my expertise and understanding of other tools.
Also, I`m pretty okay with legacy code.
I'm not interested in doing routine work (e.g view adding) all the time, without any new challenges or the possibility to work on other project parts."
data engineer,"For my training projects, I did telegram bots, did implementation of microservice architecture using REST API. Helping out my younger brother with developing of his snake game for android devices
I have a varied professional background. I started my career as an Automation QA Engineer, where I focused on automating backend tests for business. I also contributed to the development of APIs for testing purposes. Currently, I work as a Data Engineer, worked in few projects was responsible for maintaining and improving Data Warehouse. This involves using technologies like Drools, Azure, SQL Server, Azure DevOps, and MongoDB. Also for training purposes and improving technical expertise tried out PySpark and Hadoop

I also have an interest in data science and like staying informed about the latest developments in the field. I find working with complex datasets to extract meaningful insights intriguing. 

Furthermore, I'm proficient in utilizing tools like pandas and NumPy for efficient data handling, along with Matplotlib and Seaborn for creating visualizations. These skills allow me to bring value to the projects I'm involved in.
"
data engineer,"
Personal projects include, but not limited to: Analyzing international news headlines after the start of Covid-19 pandemic, and several data science projects from Kaggle. Most of the data science projects have been developed using Pandas, Scikit, matplotlib, numpy libraries. I have experience in databases such as MongoDB, PostgreSQL. Moderate experience using Spark (PySpark), Hadoop, Docker, Django, Redis.
"
data engineer,"My social activities:
- Active member of BEST (European Board of Students of Technologies) 2014-2016
- de:coded '2016 organizers team member
My experience:
1. System for collecting and processing data using scraping, open data sources and NLP tools.
Achievements:
- integrated several third-party APIs;
- created Selenium-based LinkedIn scraper;
- refactored 50% of the codebase on the project
Stack: Python 3.8, Selenium, BeautifulSoup, Postgres, Postman

2. Cloud-based system for collecting, storing and processing data (Rakuten Catalog Platform at
rakuten.com)
Responsibilities and achievements:
- created scripts for web scraping;
-created a component for extracting and processing data from ElasticSearch and
publishing messages to Google Pub/Sub topics with Dataproc job;
- tuned Google Dataproc clusters and jobs;
Stack: Python 3.6, asyncio, aiohttp, GCP services, Elasticsearch, PySpark, Kafka, Redis

3. Scraping system from scratch with Django-based web application for system management (part-time).
Application features:
- asynchronous scraping in threads;
- integration with third-party services (like Dropbox API);
Stack: Python 3.7, BeautifulSoup, Django, MySQL, MSSQL, Redis, Docker, AWS

4. Flask-based web application from scratch with REST API (part-time)
Application features:
- asynchronous task queue;
- communication with Google Cloud Storage;
- fully containerized with Docker;
Stack: Python 3.7, Flask, PostgreSQL, Redis, Celery, Docker

My expectations:
I have experience working with different domains (web, scraping, data engineering). Now I am searching for a project where I can work as a Data Engineer/ Big Data and want to switch to this domain completely. 
Another important thing for me is that I want to be a part of a team. At least a few people working together on the same project.
Not interested in working with people/projecs with Russian or Belorussian connections. 
Also, I am not really interested in full-stack positions."
data engineer,"
have 9+ years of overall experience as a software developer (java/python)
have worked as JAVA developer for B2B web application for 7+ yeaars (Spring/Hibernate).
along with JAVE have worked as a python developer (majority of project I've worked as data engineer)

Having worked as Data engineer for several years, I have developed a deep understanding of data collection methodologies, scraping techniques, and API integration. My have strong experience wiht lib/packages like Scrapy, Beautiful Soup, and Requests to extract and process data from various websites and APIs. I have successfully created and deployed numerous spiders and scripts for different types of websites, including e-commerce platforms, online dictionaries, and news websites.

In addition to these projects, I have experience dealing with captcha-protected websites, including solving captchas using anti-captcha APIs. I am adept at working with a range of frameworks/packages, including Scrapy, inline_requests, Requests, Selenium, and various databases like Oracle, PostgreSQL, MySQL, SQLite, MongoDB, and DynamoDB.

I am always eager to learn and adapt to new technologies, making me well-equipped to contribute to your team's success.
""white Salary"". possibilities to work with cutting-edge technologies.
don't work with russians teams/clients!"
data engineer,"Diploma for 2nd place in the English Olympiad
I am a Python developer with strong communication skills. In just one week of studying, I coded the Django project, DRF project was created after studying this framework just for a week and a half. In addition, at my previous job, I grow from the position of analyst to the head of the logistics department in six months. Therefore, you can count on me.
"
data engineer,"In case of technical challenge I have developed Ip blocking system(business logic based) . But most difficult situation I was ever in is about fixing a bug in production without knowing anything about the project and what the bug is about. (project hosted in kubernetes and no access to repo)
I have worked with different technologies. Ranging from simple front end web pages, ending with infrastructure. But I can say most of my experience is related with backend development, mainly microservices
"
data engineer,"Achievements:
Impovement and support company's platform for working with large datasets, data visualization and analysis;
Took part in development company's product as logistic platform;
Development and testing of a Data Services component;
Successfully developed factor-criterion model of bonuses for managers
I am a curious Software Engineer. In my most current job, I have inploved in a Software Engineering where I take part in a development a Cloud-Native Applications using Python/Flask, integrate various services (databases, storage, queues) into cloud applications, create tests for web applications. I have strong background in Python as well as Oracle, Flask, Docker. 

Achievements:
Impovement and support company's platform for working with large datasets, data visualization and analysis;
Took part in development company's product as logistic platform;
Development and testing of a Data Services component;
Successfully developed factor-criterion model of bonuses for managers

Tools: Python (flask, pandas, numpy, scikit-learn), Docker, SQL, git, AWS
Techniques: Backend engineering, data analysis
"
data engineer,"
Python engineer/Data engineer
I have more than four years of experience with Python. I have experience maintaining backend solutions and working with microservices. Dealt with building data pipelines. Worked with platforms such as GCP, AWS.
"
data engineer,"Development and automation of the data testing process 
Writing a UI Automation Framework 
Performance test execution
QA Lead, QA Data Engineer, QA Automation
March 2016 – Present
Business analytical platform for solving problems in
the retail sphere.

QA Engineer, QA Automation
February 2012 – March 2016
A set of tools for automating of business processes.
Product company preference"
data engineer,"Maiia Zbitnieva. I have profile in LinkedIn.
Education:
-  Diploma with distinction on specialty of software for computers and automated systems. (Now it is “Software engineering”). Kharkov National University of Radio Electronics. Programmer engineer qualification.
1994-1999
- Diploma of candidate of technical sciences (Doctor of Philosophy - PhD), specializing in automated systems of control and progressive information technologies in the field of automated systems of dispatching control of electric networks. Kharkov National University of Radio Electronics.
1999-2002
- Diploma of graduation in enterprises economics. Kharkov National University of Municipal Economy.
2011-2014
---------------------------------------------------------------------------------------------------------------
Additional education:
- FrontEnd course with distinction (2020);
- Basic Web and SQL for software testing course (2019);
- Linux administration (2021); 
- Build, configure and maintain a PC on a local network (2019);
- English courses (1996 – 1998, 2011).
-------------------------------------------------------------------------------------------------------------
Skills:
- .Net programming; 
- Java programming; 
- ANSI SQL; 
- C#;
- FrontEnd: HTML, CSS, Javascript, BootStrap, jquery, TypeScript, Angular, Less, Prepros, SASS/SCSS, Git, GitHub, Gulp, methodology BEM - Block Element Modifier;
- System administration, Linux System Administration;
- Software testing. Quality assurance;
- Databases administration;
- Technical support;
- Technical writing;
- Adobe Photoshop; 
- Agile, Kanban, Scrum;
- English;
- Sales.
-  Associative professor in Software engineering area – 13 years 
2002 – 2015, Kharkov National University of Radio Electronics
- System Administrator – 8 years 
2015 – 2022, Kharkov Humanitarian Pedagogical Academy
- Sales (freelancer) – 3 years
"
data engineer,"- Designed and developed data infrastructure and pipelines for drug discovery in one of the world's leading pharmaceutical companies (Merck & Co./MSD)
- MSc with honors in Biomedical Chemistry from Czechia's largest technical university
- Dean's List award for outstanding research results
In my role as a data professional, I've designed and implemented robust data processing pipelines and infrastructure for the life sciences and pharmaceutical sectors, with a focus on designing cloud-based ETL/ELT pipelines, integrating chemical and biological principles into transformative algorithms for data curation, and leveraging machine learning techniques for insightful data interpretation. Embracing adaptability, I continually enhance my skill set with emerging technologies, fostering team performance and demonstrating proficiency with a range of DevOps tools, including CI/CD pipelines, containerization, and Infrastructure as Code (IaC) for the streamlined setup of cloud-based data infrastructure.

Here's a snapshot of my technological competencies:
– Programming & Scripting Languages: Python, R, SQL, Bash, TypeScript (minor experience)
– Machine Learning Libraries: TensorFlow, Scikit-learn, Keras
– Data Processing & Management: AWS (Lambda, StepFunctions, Redshift, DynamoDB, Neptune, SageMaker), PySpark, Pandas, SQL (PostgreSQL, SQLite), Airflow, Galaxy
– DevOps & Infrastructure: Terraform, AWS CloudFormation, AWS CDK, GitHub Actions (CI/CD), Jenkins, Docker
– Version Control & Agile Tools: Git, GitHub, GitLab, BitBucket, Jira
– Data Visualization: Seaborn, Matplotlib, Plotly
– Software Best Practices: Test-Driven Development, Object-Oriented Programming
– Cloud Platforms: AWS, OpenStack
I am considering opportunities in data-focused roles exclusively within the pharmaceutical, biotech, medtech, or life sciences sectors. Specifically, I am interested in positions such as Data Engineer, Data Scientist, or Machine Learning Engineer, where I can leverage my technical expertise and industry knowledge."
data engineer,"Some number of completed tasks. Details under the NDA.   :-)
Scala, Spark, Kafka, Java, Hadoop

1.5 years with Scala
8-10 years of total experience with Java
Misc. experience with SQL and NoSQL databases, Linux, web.
To work with Big Data and/or Scala technology stack.

Remote. No relocation."
data engineer,"
I'm passionate about distributed computing and cloud technologies. I dream to join an innovative company where I
could contribute at best with my skills, share experience and learn from industry gurus.

Experience
Apache Spark Engineer
at CyberVision, Inc.
from Dec 2018 - Present (3 years 7 months +)
Responsibilities:
- Support and developing data processing applications.
- Performance tuning.
- System and security integration.
I consult customers to integrate Spark applications into cloud platforms and on-prem. Plan and support migration from on-prem clusters to cloud. Help to solve compatibility/dependency issues.

Skills:
- Certified Google Data Engineer Professional
- Scala and FP programming
- Streaming services (Spark Streaming, Pub/Sub, Kafka, AirFlow, Akka Streams)
- Batch/SQL services (BigQuery, SparkSQL, Hive)
- NoSQL (BigTable, Cassandra, Mongo, HBase)
- ML (SparkML, GCP ML Suite)
- Orchestration/Resource management (Yarn, Kubernetes)
- Other (Akka/AkkaHTTP, gRPC)

Customer Services Engineer
at Ciklum
from Dec 2015 - Dec 2018 (3 years 1 month)

I was responsible for integrating of the BI product into complex and security hardened BigData
processing environments.
I did software customization, extensions, POCs. Then in collaboration with other teams we integrated some features into core product based on customer feedback

Skills:
Programming: JavaCore, JUnit, Maven, Git, SpringBoot
Cloud providers: AWS (EC2, S3, API Gateway, Lambda, IAM, SNS)
"
data engineer,"
Mykyta takes programming and engineering with a passion. He is a big data middle engineer working in the field for the past 2 years specializing in distributed solutions and cloud deployments. His main tools are Spark for developing efficient data lakes and Kubernetes for deploying solutions in the cloud. He uses Kafka for microservice communication, Cassandra as a distributed database, and Redis as distributed cache. His language of choice is Scala. He uses Akka to leverage concurrency and ZIO as an effect system for writing functionally pure applications. Mykyta is open to challenges and eager to deliver solutions that fit customers’ needs.
"
data engineer,"Developing big data application for web data analysis.
The project that has helped to the big financial company to conquer a new market.
Working with ES/CQRS architecture.
Building a team and developing a successful MVP for a US start-up.
Being involved in a pre-sale activity for a large enterprise customer.
Java/Scala Software Engineer with 20+ years of experience in enterprise- and web-scale software development mostly acquired in Ukrainian outsourcing companies. Has been working at the positions of a Senior Developer and Tech/Team Lead. Has been performing functions of an Architect and PM.
Solid working experience with Cloud Computing (Amazon EC2 etc.), Big Data (Spark, Akka, Hadoop, Kafka), Information Retrieval, Business Intelligence, Machine Learning, Functional Programming, Event Sourcing architecture (CQRS).
Solid working knowledge of JavaEE stack (EJB and no-EJB technologies), SQL and no-SQL databases, scalable high load application development (Messaging and Streaming frameworks, MongoDB, HBase).
Working knowledge of Linux configuration, virtualization, containers (Docker etc.). Have experience in machine learning and Data Science.

Experience in telecom, travel, retail, security and financial domain areas.

Experience as a Java developer – 13 years.
Experience as a Scala developer – 2.5 year.
I would be especially happy to find a vacancy with technically challenging tasks - Data Science, Machine Learning, Eventually Strong consistency clustering etc."
data engineer,"Automated calculation of bonuses for all sales departments with detailed reporting.
Development new business features, extending and supporting BI solutions: ETL(SSIS and Python), DWH(SQL Server and Aurora), Tabular(SSAS) and Power BI reporting. I looking for Senior Data Engineer or Technical Lead position.
"
data engineer,"in personal communication
About 10+ years of experience in DB design and Data engineering.
Current workplace:
* design Data Warehouse in SQL Server 2019 and MySQL
* designing complex BI solutions with near-real time data processing
* data manipulation and API integration (GoogleAPI, FacebookAPI, Binance, Kraken, CoinMarketCap etc.), Kafka, communication with external DB
* Python scripts developing 
* ETL, SSIS, Prefect flows development
* Design and develop Tabular resume_classifier with complex DAX logic
* Power BI and SSRS reports design and development
* machine learning and data segmentation using Python (TensorFlow, sklearn)
* Master Data Management
stable Salary"
data engineer,"
I am a Senior BI developer and Data Engineer with around 7 years related experience with Microsoft and Amazon Data Tools and Technologies.
"
data engineer,"
4+ years of Scala and BigData stack development experience in Global bank, 3+ years SQL developer experience, 5+ years Accountancy & Finance experience in banks, goal-driven, have excellent communication skills , ability to plan.
Secure and challenging position where I can effectively contribute my skills, possessing competent Technical Skills."
data engineer,"Implemented overall 4 various Big Data Products from scratch in different architecture patterns (Spark Batch Processing, Event-Driven Architecture et cetera). This always resulted in bringing the single source of truth for the data i provided with my work, dramatic increase of Data Quality transparency and measurements and decrease of AWS bill. 
Also I sucessfully mentored several colleagues, both interns and non-technical guys.
Big Data Engineer with an extensive experience in end-to-end design of reliable, maintainable, scalable Big Data batch processing and ETL pipeline design. Have 4 years of experience in relevant position and 8 years of experience in IT overall.
Interesting and challenging projects focused on Big Data intensive processing."
data engineer,"
I am a big data developer oriented on building scalable and reliable distributed solutions using JVM stack (Java, Scala) or Python
I have experience with various processing engines (Apache Spark, Flink, Beam), messaging systems like Kafka and PubSub, SQL and NoSQL solutions(Cassandra, Presto, BigQuery, Hive), DevOps, monitoring and alerting tools(Terraform, Jenkins, Datadog, Grafana).
Have experience with all 3 big cloud providers: AWS, GCP, Azure
"
data engineer,"
Python/TypeScript/PySpark/SQL/AWS CDK/docker
AWS Sagemaker, Spark, Hadoop, MSSQL, Snowflake

Work a lot in close cooperation with the Data Science team. 
Data aggregation/preparation/validation/etc. 
Data pipelines with AWS Sagemaker. 
Experience in Hadoop cluster creation and tuning via Ambari.
Windows Server administration (AD, DNS, DHCP...)


I mostly worked in close cooperation with the Data Science Team.
"
data engineer,"
Student of EPAM University

March 2020 to July 2020

During this course I gained awesome experience with Java Servlets, Junit, Mockito and JDBC

Junior Java Software Engineer at Eurosoftware UA, LLC

August 2020 - April 2021

Big E-commerce product

Working on implementation of complex business logic

Java, Spring, Hibernate, JasperReports, Mockito, JUnit, Tomcat, Maven, Oracle Database, REST, Git, Eclipse, Docker, Gitlab, Nexus, Confluence, Jira, Jenkins, Postman

Big Data retraining program at EPAM Systems

April 2021 - May 2021

Working on training tasks with big data technologies

Java/Scala/Python, Hadoop, HDFS, Hive, Spark, Kafka, NiFi, StreamSets, Docker, Kubernetes, Airflow, Kibana, ElasticSearch, Logstash, MongoDB, Jenkins
 

Middle Big Data engineer at EPAM Systems at <Top secret> project

May 2021 – December 2021

Implementing etl pipelines, extending datasource libraries, such as ""cobol"", working with terraform, loading data to Snowflake.

Scala/Python, Spark, Snowflake, Kubernetes, Terraform, Jenkins, AWS Lambda, AWS Step Functions, AWS Glue, AWS SQS, AWS SNS, AWS DynamoDB, AWS EKS, AWS EventBridge


Middle Big Data engineer at Intellias at <Top secret> project

January 2022 – September 2022

Project is about adapting OSM format to new more quality format.

Implementing new jobs, adapting existing jobs, verifying data.

Java, Spark, Jenkins, Ansible, Hue, Zeppelin, OSM.


Senior Big Data engineer at N-ix at <Top secret> project

September 2022 – September 2023

Project is about taxi/delivery.

Implementing ETL/ELT pipelines, creating architecture, refining requirements with customer.

      Python, PySpark, AWS, CircleCI, Redshift, Quicksight, Terraform, Serverless, Kubernetes, Docker.
Want to have experience with new technologies and in new areas"
data engineer,"
Has experience of consulting work for AWS and GCP migration projects.
Experienced as BigData Architect Consultant for building internal company framework of customer and project engagement.
Highly experienced in end-to-end building of distributed data pipelines and microservice systems.
"
data engineer,"
Professional with a 10+ years working experience on database technology solutions and designs delivery
	10+ years of Oracle PlSql DB development
	2+ years of PostgreSQL development
	Proficiency with SQL, PlSQl, PgSQL
	Hands-on experience with query tuning
	Performs database development using best practice solution to ensure optimal operation of database environments
	Familiarity with Agile/Scrum/SAFe and Waterfall methodologies
	Actively participate in Development Standards, Architecture review, code review, QA support and deployments.
	Extensive experience with highly loaded, highly available, near real-time solutions.
	Good team player
	Excellent in mentoring and coaching team members
	Love to learn new

Programming Languages/ Technologies
	Software Architecture Design/ UML/ Reverse Engineering/ 
	SQL/PlSQL/PgSQL
RDBMS 
	BigQuery
	Oracle
	PostgreSQL 
	Derby 
Methodologies
	Agile, SCRUM, SAFe
	Waterfall
	Test Driven Development

Development Tools 
	PyCharm
	DBeaver
	SQL Navigator
	PlSql Developer
	Toad
	Oracle Workflow Builder
Cloud providers
	GCP
	AWS

Virtualization Tools
	Docker
	OpenShift
	Kubernetes Engine
"
data engineer,"On a project stared as a developer, after 6 months started new team as Tech Lead
AWS certified Solution Architect (Associate Level)
» Architecture proposal and alignment
» Applications migration to the cloud (AWS)
» AWS infrastructure setup, optimization
» Real-time, batch data processing (ETL)
» CI/CD set up
» Team management (5-6 people)
» Business and technical presentations
» Writing of technical articles
» Development and support of Python applications
» ML data preparation
» Base experience with ML/DL
Looking for challenging project, preferably cloud/Big data/ML related"
data engineer,"
Java/Scala, Apache Spark, Databricks Runtime, Databricks Delta, Iceberg, TimescaleDB, Cassandra, Kafka, Trino, basic knowledge of machine learning (Spark MLLib) and Python
"
data engineer,"
I have 7+ years of professional experience in software development and maintenance of
systems with standard and custom solutions;
Strong experience in data processing and implementation of ETL processes;
Strong expertise in software lifecycle (design, development, delivery, deployment, and support);
Experience in the development of healthcare and business process management applications;
Experience in communicating with clients in order to collect requirements, offer a solution, and/or show a demo;
Interested in working with data and data modeling.

My responsibilities: architecture development; development of a new approach; implementation of business logic; data analysis; production support; writing the unit and integration tests; bug fixing, troubleshooting; communication and collaboration with the client; working in a distributed team; performance tuning; conducting code review; conducting sprint demo; writing documentation.

The most recent projects are related to the development of the data processing workflow (staging, data warehouse, data mart) including but not limited to writing spark jobs for data processing, automating, and optimizing business processes (Camunda BPM), and data analysis.
I'm primarily interested in projects related to data processing and Big Data."
data engineer,"Managed to conquer new in-demand areas of expertise in reasonable time.
Had experience in technical leading of projects from business ideas to working products.
9+ years' work experience, more than 10 projects.
Strong background in mathematics.
Domains: Machine Learning (namely NLP), Data Science, Big Data.
Programming Languages: Java, Python, Scala, R, …
Software:	Spark, Hadoop, Elasticsearch, …
Databases: PostgreSQL, Oracle, MySQL, MongoDB, Cassandra, HBase.
Operating Systems: Windows, Ubuntu.
Foreign languages: Russian - fluent, English - C1.
Professional growth. Challenging tasks."
data engineer,"1. Creating interactive dashboards to rise the key indicators of business performance and control them continuously.
2. Continuously improving OLAP pipelines performance of the company for reducing processing costs in Analytics.
3. Applying various techniques of Power BI performance tuning to reduce refreshing times of dashboards and processing costs.
Work experience:
- 5+ years experience as a Data Analyst (BI engineer)
- 2+ years experience as a Data / Analytics Engineer
- 1+ years experience as a Lecturer on ""Data Analytics"" course (Teaching Data concepts, SQL, Power BI)

Fields worked:
- 3+ years experience in production company
- 3+ years experience in SaaS company
- 2+ years experience in Fintech company

Tech stack experience:
- 5+ years experience in SQL
- 5+ years experience in Power BI (All components - Power Query, Data Modelling, DAX)
- 5+ years experience in Excel
- 3+ years experience in Python (including PySpark SQL package)
- 2+ years experience in DBT
- 1+ years experience in HEX
- 1+ years experience in Narrator AI

Technologies worked with:
Programming languages - Python, C#, SQL
RDB - MS SQL Server, MySQL
DW - AWS Redshift, Google Big Query, Snowflake
ETL - Fivetran, Stitch, Segment Streaming, Zapier
1. Challenging project
2. Fair Salary rate
3. New learnings and professional development"
data engineer,"The supreme attestation commission attached to the President of the Republic of Azerbaijan by the Decision dated February , 10, 2016 Confers on the academic degree of Doctor of Philosophy (PhD) on mathematics
I am an Oracle, MS SQL SERVER and Postgresql RDBMS specialist with over 15 years of experience as Developer and DBA.

I currently administer about 40 different databases.
    - In the Oracle database (approximately 4 Cluster and 8 dataGuard)
    - In the SQL SERVER database (these databases are many but the most critical are 30 fail over Cluster database),
    - In the PostgreSQL database (there are PostgreSQL Cluster with patroni of 6 different systems).
     
My experiences;
  	• Troubleshooting, monitoring, tuning, debugging, data security and database auditing
	• Mirroring, Replication, High Availability, Clustering, Log Shipping
	• Database backup & restore
	• Database upgrades and migrations
	• Database security and roles management
	• Data synchronization to central database with T-SQL coding and Agent Jobs
	• TDE - Transport Data Encryption
	• Migration Database from MS SQL Server to PostgreSQL
	• Migration Database from Oracle Server to PostgreSQL 

My Certificate and Training
	Oracle Database 12c Administrator Certified Professional 
	Oracle Database 11g Administrator Certified Professional 
	Oracle Database 10g Administrator Certified Professional
	Oracle Database 10g Administrator Certified Associate
	VMWare VTSP-Server virtualization 
	IBM Storwize Family Technical Solutions V4 
	Servicing the IBM POWER8 System
	POWER_7 - Servicing the IBM POWER7 Systems
	Servicing the IBM Storwize V3500
	Servicing the IBM Storwize V3700
	Servicing the IBM Storwize V5000
	Servicing the IBM Storwize V7000
	Servicing the IBM FlashSystem 900
	Servicing the IBM FlashSystem V9000
	Servicing IBM System Storage DS3000 Family Test v1.2
	LC_Power_8348 - Servicing the IBM Power System S812LC
	LC_Power_8335 - Servicing the IBM Power System S822LC
"
data engineer,"Created database (Oracle) and backend (Java - rest API for metadata) for KYC application of one European bank
Over 20+ years of professional experience working as software developer.
Extensive hands on experience: Data Modeling, Database Architecture, building DWH, Data Migration, integration of applications, developing applications.

RDBMS: Oracle 11g/12c, MS SQL 19, PostgreSQL (AWS)
ERP: Oracle E-Business Suite 12/11i (OM, AR, PO, INV, GL, CM, AP)
Reports: Oracle Reports, Crystal Reports
Case: Power Designer 16.6
Others: ETL, DWH, OLAP
Solve customer tasks, problems and provide solutions.
Partial or complete remote work."
data engineer,"Professional performance optimizer and troubleshooter, refactored and optimized huge legacy databases with 1700+ stored procedures, optimized cloud and on-prem hosted databases used by 2000+ clients.
15+ years of MSSQL experience including different projects using SSIS and Azure migration. Typical responsibilities: - database performance tuning and optimization for different on-premises and cloud hosted clients; - assisting with urgent production and data quality issues, participating in incident management calls; - refactoring of large legacy databases, writing unit tests using tSQLt and Python; - SSIS packages and SSRS, Power BI reporting development; - migrating databases to SQL Azure, migrating SSIS packages to Azure Data Factory;
- Developing Airflow dag's to process data;
 - onboarding new developers, mentoring and conducting code reviews.
Looking for Lead/Senior Database Developer/Data Engineer position"
data engineer,"PoC, presales, team leading in few leading it outsource companies
I am a proactive data engineer, certified MCSE and Azure DE, with an interest in Big Data, NoSQL databases, and cloud platforms. Having around 5 years of experience in data engineering, I developed hybrid Big Data/DWH systems, ETL/ELT pipelines, reporting systems, and interactive visual reports, established data quality procedures, analyzed business requirements, tuned the performance of slow-running queries, and many more.
Data visualization"
data engineer,"
I am working as senior data engineer.
I am using sql, pl/sql, python, spark on my current work.
I completed multiple tasks on establishing ETL pipelines using my sql, python skills.
I am always on search for new technical skills
"
data engineer,"
Full cycle data science engineer with 10+ years experience. Strong theoretical background in ML & fin-tech/economics. Passioned in everything related to AI and new technologies. Best areas of potential involvement are DE/ML/DS. Accounts: pharma, logistic, fintech, gamedev, betting, retail, marketing. I have experience in R&D work, production code writing, DevOps within data science and data engineering pipelines. Practical side of this include experience with: neural networks, recommendation systems, chat bots, regression and classifications tasks, building and maintaining data pipelines, AI agents development, DB architecture, software architecture, fraud analysis, BI tools creation, web application creation, maintaining legacy code, technical documentation writing.  Beside working projects maintain  constantly  game & VR app development as a hobby. Involved as well to the teaching activities within and outside of the company.
"
data engineer,"Built a data analyst/engineering team from scratch.
Improved application release processes with big time saving (from  1 full day to few clicks)
Improved performance of ETL pipelines from 2 days to 2 hours.
Hello! I have more than 10 years of experience in IT. In recent years, I have been engaged in data engineering. The core stack is Python, PySpark, SQL, Azure. Lately, I've been interested in ML engineering topics. Also, I have a big experience in .Net development. I am able and willing to work on the complex projects. During the last few years, I performed the duties of team lead-a of the Ukrainian development team. 
Open to interesting projects and proposals!
"
data engineer,"Help customers to scale their databases to cloud
• 10+ years of Data Engineering experience
• Clear understanding of Software Development Life Cycles(SDLC), Requirements Analysis
* Cloud: GCP, AWS
* Container: Docker
* Orchestration: Airflow
* Programming: Python, PySpark
* Databases: Oracle, PostgreSQL, MS SQL
* Configuration Tools: Git, Jira, BitBucket, Confluence, Jenkins, TeamCity, AutoSys
Challenging tasks."
data engineer,"Master’s Degree in Computer Science and Information Technologies
Kharkiv National University Civil Engineering and Architecture
2020 – 2021
December 2021 – Present
Scala Data Engineer
Created raw, ingest and product pipelines on Azure Data Factory and ran jars on Azure Databricks.
Investigated POC about Azure Cognitive Search and POC for CosmosDb(Cassandra API).
Wrote unit and load tests;
Communicated with foreign teammates;
Mentored newcomers.

June 2020 – December 2021
Java/Scala Software Developer
Faced with optimizing DB performance issues for PostgreSQL;
Worked on ETL(Extract, Transform, Load) process;
Wrote unit and load tests;
Communicated with foreign teammates;
Mentored students.

April 2019 – June 2020
Trainee/Junior Software Developer
Participated in the full lifecycle of web application development using Spring 
Framework.Developed new features, fixed bugs and wrote tests.
Reviewed and refactored code
"
data engineer,"Core bank architecting , DWH from scratch , Migration projects, query optimization,PL SQL architecting, Development of data pipelines with Airflow and  Spark. Migration to AWS .
Experienced in Data engineering & Database development
with 10+ years of experience in a wide variety of systems .
Experience across Oracle , MS SQL Server , PostgreSQL Data Warehousing projects with different ETL tools .
Data pipelines with Apache Airflow.
Data engineering with Python and PySpark.
Good Python programming skills. Good understanding of Spark core concepts.
Experience with Docker,Git.
AWS services S3, DMS, RDS, Redshift  experience.
Database architecting. Deep knowledge of Oracle Database architecture and concepts.
Data&Software migration experience. Software architecting and development of software with intensive SQL & PL/SQL
calculation.
Leadership skills.
Challenging project using : DWH / ETL / Python data engineering / Big data / PL SQL programming for big business."
data engineer,"
I worked with migration projects from old system to new one, used different ETL's which were written on Python and SQL. Created a new data pipelines for future migrations. Right now I'm working as Data Engineer and my responsibilities are: validate data and create reports for invalid data, create new and modify existing data pipelines
I would like to work with modern technologies and don't want to work with legacy. I would prefer product IT companies."
data engineer,"Build ETL and DWH projects for Wninteractive and UP Games from zero point
- 10+ years of professional experience in the Information Technology industry, 6+ years in database development, 5+ years as Senior Data Engineer;
- Proven ability to translate complex business requirements into efficient, scalable, and high-performance data resume_classifier. Proven record of success in data migration, ETL procedures
development and support, and DB performance optimization;
- Python development experience with the integration of various data sources and API,  built data pipelines, development DB benchmarks, and other helpful tools and scripts for automation processes.

Aim: I wish to develop the area of work concerning the design and development of big data solutions, and get helpful job experience while solving new tasks within large and interesting projects.

Projects: Playtica “Slotomania”, Wininteractive ""Slots Craze DB, DWH"", Up games DWH, Choreograph. I have experience with the following technologies: Mysql, Postgres, Vertica, Kafka, Google Cloud, Hadoop, Hive, Presto, BQ, Airflow, Talend, Pentaho, Python, and other
"
data engineer,"Creating/designing new solution from scratch.
10+ years of professional experience in the Information Technology industry;
Rich experience in Data Modelling/DWH development/ETL;
Rich experience in the design and implementation of DWH/Data Lake solutions;
Experience in data migration;
Experience in using Cloud (AWS) for ETL;
Domains of expertise: financial, tourism, and media;
Development and expertise highlights:
    – Data modeling (Snowflake, Hive, Oracle, PostgreSQL);
    – ETL development (Python + AWS);
    – Data integration and ETL processes design;
    – Performance tuning / SQL query optimization;
    – Python development;
    – SQL, PL/SQL development;
    – Database migration, reverse engineering.
Experience working in large distributed teams.
Interesting projects, flexible schedule, new (for me) modern technologies.
I am now only considering options which imply B2B cooperation using my Polish PE."
data engineer,"Data Engineering: Etl pipelines architecture design
Python Dev: developed RESTful applications, deployed apps in k8s cluster, crawled different web resources, developed ML detection technics, big data pipelines implementation 
Data Science: developed and implemented ML resume_classifier based on upper-confidence-bound (ucb1), imbalanced (RUSBOOST, NearMiss, Smoteen etc.), factorized machines (TFFM), boosting (XGB, Catboost, Lightgbm etc.), Time Series (ARIMA, Prophet, luminol, winsorizing) algorithms
1. October 2020 - present 
    Senior Software Engineer at Geeksforless
    Data pipeline development and support ingestion data process, reporting system implementation and maintenance using Python, AWS Glue, Redshift, Athena, S3, Lambda, Step functions, CloudFormation, CloudWatch, Terraform, serverless, Pyspark etc
2. March 2019 – October 2020
    Python Software Engineer at LOHIKA
    Applications development, ML resume_classifier implementation, DevOps
    via Docker and Kubernetes 
    Technologies: Python 3, Django, Flask, Docker, Kubernetes, 
    Helm, AWS, Crawling (Scrapy, Selenium, Beautiful Soup), 
    Opencv, Git
3. August 2018 – March 2019
	Data Science Engineer at Together Networks
	Data science engineering, researching, algorithmic resume_classifier
        prototyping, production implementation 
        Technologies: Python 3, Jupyter Notebook, Numpy, Pandas, 
        Matplotlib, seaborn, Sklearn, imblearn, SciPy
4. January 2017 – August 2018 Data Engineer at EPAM SYSTEMS
	Exploratory analysis, data mining, automation process with 
        Python, anomaly detecting, Python scripting, ML modeling 
       (time series field), Amazon service, taxonomy data building, 
       researching 
5. 2014 – 2016 Data Analyst at ControlPay
	Complex data analysis, data mining, technical writing, 
        calculation logic implementation & Python scripting
6. 2009 – 2014 Leading Specialist (Analysis & Control observance) 
        at Kyiv Municipal Administration
	Analysis and consulting within municipal improvement field, 
        reporting & recording data, KMA’s controlling legislative & 
        executive functions support, Python enthusiast
7. 2008 – 2009 Leasing / Marketing Specialist at ILTA
	Marketing analysis, leasing schemes calculation, marketing 
        activities, leasing sales support
Main goals:
•	Software engineering (Python)
•	Data driven direction within ETL pipeline building 
•	Self-development within software skills improvement"
data engineer,"
3 years l 2: Spark, Airflow, AWS

Technologies: Scala, Spark, ZIO, EMR, Airflow, sbt, doobie, Clickhouse, Postgres, AWS, Kubernetes

1. year l 1: Designed and maintained Scala gRPC, REST microservices in the environment with more than a thousand of them.

Technologies: Scala, Bazel, MySql, MongoDb, Kafka.
"
data engineer,"Improved bidding system performance ~10-20x before Black Friday. Allowed to save costs and change bids much faster at the same time.
7 years in Java, 4 years in data engineering. 

Experienced in building, maitaining and performance tuning of data pipelines with Spark on AWS, Dataflow on GCP. 

Designed and implemented high-volume data platform with Spark 3 & Hive metastore on Kubernetes, using Kafka as ingestion source.

Developed data management systems based on S3, Cassandra, MongoDB, BigQuery and Redshift, using Kafka as message broker. 

Implemented CI/CD integrations on Jenkins with Docker, ensuring full software development release cycle. 

Performed as ad-hoc mentor and project lead.
Interested in building data platforms, data pipelines performance tuning, exploring Lambda, Kappa and other big data architectures, using up-to-date data engineering tools and best practices.

Enjoy communicating with stakeholders and solving real business problems."
data engineer,"I have worked mainly with Marketing data for the past 4 years and proficient with marketing tools APIs for custom data ingestion (Salesforce, Hubspot, Marketo, Contentful, Iterable, Linkedin Ads, Bing Ads, Google Ads, Facebook Ads, Zoom, Livestorm, Bizzabo, Accelevents). I have setup numerous marketing pipelines end to end ingest-trasformation-outgest or visualisation.
A self-driven Data Engineer who enjoys working in a team and is madly in love with the data. I have 6+ years of experience on both sides of the Marketing/Sales process: as a part of the Sales team and as a part of Business/Data Engineering. If you have lots of data and questions that need to be answered, let’s talk!
"
data engineer,"NRT data processing
Event Driven data processing in AWS
• AWS Certified Data Analytics – Specialty
• Experience in working with AWS (S3, Lambda, StepFunctions, SNS, SQS, KMS, SSN, DynamoDB, CloudFormation, CloudWatch, Kinesis, Glue, Athena, Redshift)
• Experience in working with Qubole (Hive, Presto, Spark)
• Development of ETL: Prefect 2.0, Airflow (MWAA 2.0), dbt (Data Build Tool), IBM DatatStage, Informatica, MS SSIS;
• Database design and implementation: Oracle, MS SQL, Teradata, DB2, Hadoop Hive, Snowflake;
• SQL-tuning;
• ETL performance tuning;
• Development of Oracle PL/SQL, Microsoft T-SQL, Teradata SQL PL program units;
• Experience developing on Python;
• Experience using bash & PowerShell scripting;
• CI/CD tools: Jenkins, GitHub Actions, Terraform
• Debugging & Unit Testing;
• Experience with Git, SVN, Perforce; TFS;
• Troubleshooting, problem solving.

===========================================
Senior Big Data Developer
EPAM Systems · Contract
Jun 2021 - Present
Customer: EF Tours

AWS services used for data lake hydration using DMS, S3, Glue, Lambdas, Step Functions etc.
Data loading into Snowflake (Cloud Data Warehouse)

Projects pipeline:
- Implement tooling for task orchestration and data transformations using AWS Managed Airflow, dbt (Data Build Tool).
- Ingest and process additional data flows into Snowflake
- Near real-time replication to hydrate data lake and Snowflake DWH

Tech stack: 
- AWS: Lambda, StepFunctions, S3, SQS, SNS, DynamoDB, DMS, KMS, SSM, Athena, CloudWatch;
- Snowflake
- DBT (Data Build Tool)
- Prefect
- Fivetran
- Rivery
- Python
- Terraform
- GitHub Actions
- Docker
AWS
Snowflake
dbt
Python
FiveTran / Rivery"
data engineer,"Worked on all phases of data warehouse development life-cycle, from gathering requirements to testing, implementation, and support.
Big Data Project with (Apache beam,Apache Airflow, Apache Spark).
Good knowledge Google Cloud Platform (Dataflow, Bigquery, Dataproc, Bigtable etc..).
Good knowledge PySpark
•	Exceptional background in analysis, design, development, customization, and implementation and testing of software applications and products. 
•	Demonstrated expertise utilizing ETL tools, including SQL Server Integration Services (SSIS), SQL Server Annalise Services (SSAS), SQL Server Reporting Services (SSRS), Data Transformation Services (DTS), and DataStage and ETL package design, and RDBM systems like SQL Servers, Oracle. 
•	Strong understanding of data warehousing concepts, OLTP, OLAP, Normalization, Star and Snow Flake data resume_classifier.
•	Create OLAP Cubs.
•	Strong leader with experience training developers and advising technical groups on ETL best practices. 
•	Excellent technical and analytical skills with clear understanding of desig
I am Freelancer :-)"
data engineer,"
Certified AWS Associate Developer,
Certified AWS Associate Solutions Architect,
Certified GCP Professional Data Engineer,
Took part in Snowflake certification prepering tranings

>1 year of freelance experience (web-dev, not considering it anymore)
~3 years of commercial experience, 2 big projects:
1. On-premises ETL migration project
Stack: IBM DB2(source), MariaDB(target), Talend Studio for Data Integration (ETL tool), Liqubase, Jenkins, Docker, Python (for automating manual tasks), Git/GitLab, Jira

2. Building Data Lake in cloud environment
Stack:
AWS Services: Redshift, Redshift Spectrum, S3, Athena, Glue, CloudFormation, SNS, Lambda, CloudWatch, SQS
Other: Python, PySpark, DWH, PostgreSQL (Redshift version), Jenkins, Git/GitLab, Jypyther Notebooks, Jira

Other activities: technical interviewing (~30), mentoring (1 to 1 in company, 1-to-1s on a project KT (6 times), mentoring team after company Data courses), experience in community speaking (1 company event, couple of project-related events), driving and facilitating SCRUM meetings

Detailed description available in CV, just message me)
Cloud-based BigData projects (AWS, GCP, Azure)
Not interested in on-premises tech stack"
data engineer,"
Data Engineer with a proven track record in designing, building, and maintaining efficient data pipelines and solutions in companies of various sizes including start-ups and established international companies.

Proficient in a variety of programming languages, including Python, Scala, Rust, and SQL, and experienced in utilizing big data technologies such as Hadoop and Spark. 

Strong analytical skills and a talent for translating business needs into technical solutions. Adept at collaborating with cross-functional teams, including data scientists and software developers, to deliver data-driven products and insights.

Worked in consulting in Europe for multiple years. Have successfully enabled small and medium size companies to utilize their data at scale with a data ecosystem built both in the cloud and on-premise. Managed small to medium size teams. Domains: Fintech, Proptech, PharmTech, ChemTech

I have an MSc degree from the University of Leeds and did my bachelor's in Moscow. Playing chess is my hobby.

Able to work B2B from Latvia or Azerbaijan. Open to relocation and/or travel. Remote, or hybrid preferred.

• Python, Hive, Spark, RabbitMQ, Airflow, Git, Bash, Jenkins, Travis, Docker
• Understanding and adhering to concepts of FP and OOP depending on the use case
• Experience in working with AWS services such as Amazon S3, Lambda, EC2, Glue,  EMR, Redshift, DynamoDB, Athena, and IAM
• Experience in working with GCP. Cloud Storage, BigQuery, Functions, Data transfer, Cloud Functions, Firebase, Firestore, Google Analytics
• Experience with Databases, MySQL, Oracle, Microsoft SQL Server, MongoDB, Firestore
----------------------------------------------------------
"
data engineer,"Snowflake certified
Data Engineer with experience in Snowflake cloud database, AWS, GCP.
ETL tools: Matillion, Talend, SSIS
Reports : Power BI, Tibco.
Programming skill: Python.
Snowflake certified.
Snowflake cloud database, AWS"
data engineer,"
Data Engineer with hands-on knowledge of SQL, Python, Airflow, & AWS cloud. 

5+ years of experience in different data roles (recent 4 years as data engineer). 

Key expertise: code-based data pipelines (ETL/ELT), data warehouse (DWH)/data lake development, data modeling. I also have experience with productionization of ML resume_classifier (tabular/time-series data).
NOT interested: 
- overtime work
- work at ukrainian holidays, weekends, late or night hours
- employee tracking soft (time trackers, bossware)"
data engineer,"Experience with Data Mesh implementation.  
Experience with development of the Medical Device Class 3 software product. Development of the 3D graphic software product.
Dear all, 
I had a lot of projects as  a Senior Software Developer in different business domains (finance, healthcare, logistic) and with different technologies (Python, Spark, Snowflake, Dbt, Databricks, AWS Cloud, Terraform). 
I'm searching for Data Engineering opportunities. 
Currently learning linear algebra just for fun so will be interested in projects with ML part (ML ops).
I'm searching for Data Engineering project, as currently doesn't have any project at current company."
data engineer,"• Design and creation of new data pipelines and maintenance of existing ones
• Optimization of existing mechanisms for speed and cost-efficiency
• Ensuring pipeline stability and best code practices through Unit/Integration testing and CI/CD
• Design and implementation of back-pressure mechanism for log fetcher to avoid data loss
• Design and implementation of disabling mechanism for stuck workers based on statistics collected during particular time window
• Design and implementation of throttling for producers cluster to decrease lag between producers and consumers
• Design and implementation of re-enabling mechanism for failed workers using exponential retry and jitter
• Have added integration with new third party log sources for fetching
• End-to-end delivery of highly scalable cloud native product
• Deep knowledge of modern software development technologies and frameworks
• Experienced in performing optimisation, performance profiling and debugging
• Expertise in designing of scalable software applications from scratch and based on existing solutions
• Understanding of object-oriented and service-oriented application development methods and theories
• On-hands experience in production with SQL/NoSQL databases and DBMS
• Solid knowledge of the most popular Version Control Systems
• Good knowledge of modern operating systems and command line
• Ability to handle a team of software professionals, appropriately dealing with problems inside the team
• Ability to deliver technology solutions which meet business expectations in shortest terms
• Experienced in integration of best engineering practices into existing processes
• Dedicated with proficiency to work independently and in a team
"
data engineer,"
Development, implementation and maintenance of resume_classifier for evaluating credit risk
Aggregation of the data from different systems, development and maintenance of data marts, ensuring data quality  and reconciliation, regulatory and business reporting.
Migration of ETL(ELT) pipelines to open source: performing PoCs of new tools & environments, learning new tools, performing MVPs, defining rules and approaches, migration and development of pipelines using new tools.
DWH (Sybase IQ) & Data Lakehouse (AWS S3, AWS Glue, AWS Lake Fomartion) building: rules, formats, governance, ingestion.
Leading a migration of On-Premise processes to Cloud solutions: helping stakeholders with onboarding to cloud; gathering of requirements and dependencies; data modeling; construction of a migration plan.

Stack: AWS, S3, Athena, Glue, Lake Formation, SQL, Python, NiFi, Spark, Airflow, Docker, Linux, Java.
I am a competitive person, so I love challenges, learning, all kinds of games (sports, computer, board), math and especially data. There is always a way to do more or better with data. And that's why I'm here to improve my skills and put them into good use in projects that socially matter."
data engineer,"
Experienced Senior Data Engineer with 5+ years of expertise in creating strong data pipelines, efficient data warehousing, and ETL processes. Skilled in diverse technologies, optimizing data architecture, and ensuring data quality. Proven in project delivery, cross-functional teamwork, and driving data-driven decisions. Seeking to enhance data infrastructure and uncover insights through innovative technologies
"
data engineer,"
Experience in IT for over 30 years. Behind many projects using different technologies.
I would like to work in a team of professionals where everyone could develop as a specialist and help the company and a friend to achieve the set results.
I would not like to work in a company where young sprouts of innovation are buried under mountains of papers and reports."
data engineer,"
Highly skilled data engineer with 11 years of hands on experience in DWH implementations on both SMP and MPP systems. Interested in high volume cloud projects, data integrations and sophisticated cloud implementations. 
Expert knowledge of MS SQL implementations and performance tuning;  highly skilled in Teradata with hands on experience with Exadata, Oracle, Postgres etc. Currently working with Postgres.
"
data engineer,"
I am a data engineer with almost 2 years of commercial experience in data engineering. I have solid data engineering expertise in different domains such as rental, sport, health care, and e-commerce. During my career, I worked on 1 training project and 3 commercial projects (one of them was an R&D one).

Training project - rental domain
Technologies: Scala, Python, Spark (Batch and Streaming), Kafka, Kafka Streams, Cassandra, Docker, Airflow, ELK stack.
Team: 1 lead engineer, 1 senior engineer, and 5 junior engineers (including me).
Project responsibilities: Implementation of big data solutions to optimize the work of the international service for the rental of electric scooters.
	Implemented scalable and high-performance pipelines for data visualization and assistance to enterprises in management
	Participated in each stage of project development and worked with all these technologies

R&D project - sport domain
Technologies: Scala, Java, Kafka Streams, Apache Druid, Debezium, Metabase.
Team: 1 lead engineer, 1 senior engineer, 1 back-end engineer, 1 data scientist, and 2 data engineers.
Project responsibilities: Working with a Ukraine-based team of data engineers, data scientists, and back-end developers to implement machine learning solutions in the sports betting system.
	Developed ETL pipelines
	Maintained integration with data sources, orchestrating jobs, testing code, and visualizing data
	Worked in teams with data scientists

Health-care domain
Technologies: Scala, Spark, Kafka, HDFS, Hive, Cloudera
Team: a big international team of data engineers, DBA, and others
Project responsibilities: 
	Developed ETL pipelines for customer subprojects
	Built CI/CD pipeline
	Tested code and troubleshoot different components
	Orchestrated jobs

E-commerce domain
Technologies: AWS (S3, Glue, EMR, DynamoDB), Python, Spark, Airflow, Snowflake
Team: 5 Data Engineers, DQA Engineer, BI Engineer
Project responsibilities: 
	Building Data Lake
	Communication with stakeholders 
	Building and orchestrating ETL pipelines
"
data engineer,"
I am Harun Mbaabu Mwenda. I am a data engineer with 3+ years of experience designing, building, and maintaining large-scale data systems where I combine my knowledge of software development and data science to build data-driven products that can solve problems and derive insights from the customers and developers.

A considerable amount of my time is spent doing community work on things that I hope will help humanity in some way and my open-source work is supported by the community.
"
data engineer,"
I have been working in IT for more than 9 years. My primary language is Java, but I also have good experience working with Scala and Python and know that languages at the intermediate level. During the first four years of my career, I worked as a software engineer where I worked in a large distributed team with big monolithic applications, developing REST API, and working a bit with UI technologies. Also, I act as a mentor for newcomers and do technical leadership for a small team of people. Then I switched to the data engineer role, where I have extensively worked on building data lakes from scratch, extending spark to accelerate its query execution with custom accelerators that were written in C++. That includes extending the spark catalyst rule-based engine and extending the tungsten code generator to generate C++ code and C++ code integration to the scala/java codebase. In data lakes, I mostly worked with batch-processing jobs based on the spark, but I also have a bit of experience working with stream-processing pipelines. In addition, worked a lot in spark job optimization.
"
data engineer,"
I’m a Data Engineer with 10+ years of experience in Data Warehousing and Analytics. I help companies to design and implement complex Data+BI solutions, maintain and improve existing platforms and assist on revealing the full potential of data.

The most vivid achievements so far:
• As a Senior Data Engineer/Architect at Arkadium, I helped to migrate the DWH with more than 10 TB of data and up to 50 pipelines from on-prem SQL-Server to the cloud Lakehouse Databricks. We have achieved 40% cost reduce on data processing while ingesting two times more data. 

• As a BI specialist at Huawei, I supported DWH and BI reports of complex billing system for mobile operators

I get excited about opportunities where I make data accessible and useful for decision makers and help companies to manage their data.
"
data engineer,"Designed a Delta Lake and all-around infrastructure that replaced an expensive Redis cluster.

Designed a scalable fault-tolerant Apache Flume setup and migrated it from Cloudera Manager to the AWS Kubernetes service.
Initiate performer with hands-on experience in developing large scalable big data pipelines using trend software and techniques. Strong AWS user and administrator.

Flexible to take the next responsibilities: QA, DevOps, BI, intern training, team lead substitution, etc

Worked on-site in Silicon Valley and several EU locations for months.
"
data engineer,"
- Knowledge of Hadoop ecosystem and different frameworks inside it – Hive, HDFS, YARN, MapReduce; - SQL-based technologies (e.g. MySQL/ PostgreSQL/MSSQL); - Assembling large, complex data sets that meet business requirements; - Developing ETL processes using Talend; - Troubleshooting performance issues. - Developing of DWBI solutions; - Developing SSIS ETL packages; - Building reports in SSRS, Tableau, PowerBI, TIBCO Spotfire; - Designing and developing of Data warehouse and Integration services; - Troubleshooting performance issues; - Developing of dimensions and cubes within the Analysis Services project; - Writing relational and multidimensional database queries; - Creating complex functions, scripts, stored procedures, and triggers to support application development; - Optimizing database systems for performance efficiency.
Want to work with new technologies. I don't like boring tasks and am ready to have challenges."
data engineer,"Took part in school Office and Programming competitions(regional). Participate in University programming competition.
Knowledge of Python, JavaScript, SQL, C, HTML5, CSS3, Oracle, Kivy.
I have learned programming languages in university disciplines such as WEB development, Object-oriented programming, Data Bases, Python.
Practical experience was gained in laboratory and course works.
Want to find friendly and experienced team."
data engineer,"Most of my achievements are related to my experience because I don't know, where and when my work was easy.
Oracle and Snowflake master.
Machine Learning Padawan.

In more detail way:
- Developed some custom and generic pipelines on Airflow
- Developed bunch of custom operators, sensors and plugins for Airflow
- Developed application for webscrapping
- Developed application for comparison of customers based on similarity algorithms and ML resume_classifier
- Developed ETL process and Star schema in Snowflake for application which is making decision based on execution of ML model
- Optimised a lot of Oracle sql-scripts and pipelines
- Developed several APEX applications
- Developed ETL pipelines for Oracle/Snowflake/Teradata using: Airflow, Azure(data factory, batch, storage accounts), Python, SQL
- Reports for BI Analytics based on Snowflake data, using Power BI and Metabase
Developing data pipelines (ETL/ELT) and data warehouses."
data engineer,"
- ORACLE - senior level - more than 10 years experience.
- Amazon REDSHIFT - senior level - 2 y experience
- POSTGRESQL - regular level - 1 year experience
- AWS Data Services (S3, Glue, RDS, Athena) - middle/junior level 
- Python - junior level
- JAVA - junior level 
- ADDITIONALLY have experience/familiar with Linux, shell scripting, ERWin, Power designer, Git, MS Project, Jira, Mantis, Airflow, Jenkins, 
------------
- PROJECT MANAGEMENT - work experience more than 4 years, experience in full cycle of software development and delivery, experience in managing a development team (10-15 people)
---------------------------------------------------------------------
2020-2022 - Senior Data Engineer (AWS)
2019-2020  - IT manager\ Project manager
2018-2019  - PostrgeSQL  developer/ junior Java developer  
2015-2018  - ""Luxoft"", senior Oracle developer \ teamlead
2014-2015  - ""DAXX"", senior Oracle developer
2010-2013  - ""Юнитех+"", Software Architect \ Project manager
2004-2010  - ""Юнитех+"" , senior Oracle developer
2001-2004  - ""Укрзалізниця"", Delphi developer, Orace developer
"
data engineer,"• Developing and supporting REST API
• Optimization and refactoring of code to improve performance and lead to uniform style
•  Changing the structure of the database
• Creating reports on the work done and monitoring the timely updating of product documentation
• Conducting staff training, documentation maintenance
Project within treat intelligence portfolio, we as a core of operation team provide the following services:
 - DBA administration:
 - Maintains and support the monitoring
 - Maintains and support the infrastructure 
 - Manage user management in AWS
US company
Project Role
Senior Data Engineer, DevOPS
Responsibilities
    • Help DevOps team to improve the SQL request
    • Help to design the MySQL DBA
    • Analyzing of the database structure, making recommendations on changing the structure
    • Add new or update JIRA projects, issue types, screens, fields, schemes
    • Update workflows
    • Create, update, delete users and roles, setting project permission
    • Write Python scripts for behavior
    • Create notification
Tools & Technologies
MySQL, JIRA, Webex, Confluence, AWS, Prometheus, Grafana
2)DB Optimization
The project required performance audit after migration from other database to MariaDB and validation migration correctness
US company
Responsibilities
    • Analyze queries, identify slow queries, optimize queries
    • Analyze indexes, collations, engines, configuration files, general parameters and optimize
    • Provide report of redundancies in DB schema, compare different schemes, validate migration correctness
    •  Created auto tests for slow queries
    • Estimation, prioritization, and distribution of tasks
    • Create pipeline
    • Analyzing of the database structure, making recommendations on changing the structure
Tools & Technologies
Linux; Windows PowerShell; Ubuntu; MySQL; Python, Apache Airflow
Percona (pt-duplicate-key-checker, pt-query-digest, sysbench), vSphere; pfSense, Webex, TargetProcess (for reporting tasks); Scrum
3) Software and hardware system to introduce a new generation of medical infusion devices
The project required database analysis for schema separation into smaller databases for use with microservices as well as creating those smaller databases along with their builder code (C# .Net)
US company
Responsibilities
    • Database analysis and mapping
    • Requirement’s analysis and clarification
    • Architecture and design of new database tables and recreation of the previously
I prefer a long-term contract, I like to work with a product company.

Ready to relocate to another country"
data engineer,"- World Yandex Algorithm competition final 2011 (12-th rank)
- ACM semi-finals SEERC 2010 and 2011 in Romania (4/5-th rank) - TopCoder and other online competitions
- have written own chess engine)
Python, Spark, Big Data, ElasticSearch, Solr, CouchDB, Django, Jenkins, Redis, Java, AWS, SQL
- high technically skilled team
- good challenges
- communicative team
- good management"
data engineer,"* Developed dozens of services and apps of various kinds
* Successfully leading 5 sub-projects simultaneously, 14 remote devs in total
* Have experience of taking ownership of huge infrastructure with 100+ polyglot microservices (new team creation, platform upgrade, reducing costs for hardware, migration to open-source solutions)
* Participated in the creation of several BI departments for different products
* Successfully collaborated with the Data Science team to integrate several Machine Learning solutions into the production
Recently participated in the development of:
* Microservice-based systems (Spring Boot, resilience4j, Kafka)
* Batch processing ETLs (Spark / AWS Glue, AWS S3, Parquet)
* Streaming ETLs (Spark Streaming / Storm, Kafka, Scala)
* Automated data transfers between S3/Kafka/RDBMS/MongoDB/etc (Java / Scala)
* REST API for analytics data querying (Java)
* REST API for Machine Learning, Model as a Service (Python)
* BI data flows and ML training automation using Jenkins (Groovy pipelines) and GitLab CI
* Remote only (business trips - ok)
* Product company / startup
* Big Data or Data Science related project is preferable"
data engineer,"- Spark ELT frameworks from scratch 
- Spark Streaming applications 
- Data lakes on AWS
Seeking for positions to work with architecture and big workloads, big data, optimizations, devops. Product companies are preferred.
NO to frontend, full stack, enterprise, legacy"
data engineer,"- Rated TOP 20% in McKinsey x Tinkoff and TOP 35% in McKinsey x Alibaba hackathons. 
- Certified Alteryx designer.
Self-directed and driven Data Engineer with a background in Data Science, having good problem-solving skills.

- Years of experience: 5
- Domain: banking, fintech, healthcare.

- Technologies stack: Python (Spark, Trino, SQLalchemy, Airflow, FastAPI, MinIO, Boto3), Postgres, Kubernetes, Docker, AWS (S3, Athena, Lambda, Glue, QuickSight, LakeFormation), Dask.
- BI Tools: SAP, Alteryx, PowerBI, Tableau
- Version control: Git, BitBucket
- Documentation: GitLab, Confluence
- Pipeline: ArgoCD
- Monitoring: Grafana/Loki

- Education: Carnegie Mellon University (2019)
- Major: Data Analytics/Data Science
"
data engineer,"Implemented Datamart for Sales system from the scratch. Passed Microsoft certifications 70-761, 70-762, 70-464
Senior DW/BI Engineer with 10 years of experience. Participated in 5 long-term projects. 
Qualifications: 
- Data Engineering
- Database Design 
- DWH 
- ETL 
- DB Development
Work with Cloud (AWS, Azure)"
data engineer,"
Software/Data Engineer with expertise in
- architecting & implementing data pipeline components in Python and Scala
- gathering requirements and building PoCs for novel parts of the tech stack
- maintaining AWS-based ETL pipelines and large-scale Snowflake warehouses
- setting up data incident management workflows from scratch
C1+ English.
Looking for an awesome data team in a product company :)"
data engineer,"
Software Engineer at FAANG, have a solid background in Data Engineering and Data Pipelines. Worked on different projects with Big Data and Fast Data technologies. Has a good knowledge of building distributed and large-scale systems.
Was responsible for leading a team of 10+ people and mentoring newcomers.
"
data engineer,"
Software Engineer with 9 years of Big Data related experience. During this time I've got an extensive experience of using Hadoop ecosystem frameworks and cloud computing platforms. Highly interested in projects where high performance and scalability are needed. Also my area includes research and POC implementation.
"
data engineer,"Got certificates in DB direction: DB development (IT Academy), BrainBench (sql server administration), hackerrank (sql, python), oracle (LinkedIn), MongoDB (mongoDB university), Sql. 

Also I am certified by AWS Certified Cloud Practitioner (CLF-C01), Microsoft Certified: Azure Data Fundamentals (DP-900), Astronomer Certification for Apache Airflow Fundamentals, dbt fundamentals.

All information you can see at my LinkedIn profile.

In 2011 I was 4th in the rating of students in mathematics in Ukraine.
Some company [2020-present]:
Current project: Having joined the team as data engineer for two years now, I grew up to the position Team Lead, so for 9 months now I’ve been leading the team and migration for legacy products into brand new platform. Moreover, my responsibilities included delivering releases on time, team planning using OKRs, hosting all discussions, maintaining ETL and creating new validation areas for customers, supporting legacy applications, participating into on-call activities.
Tech stack: AWS, MSSQL, T-SQL, MySQL, Java, ASP.NET, F5, DataDog, Splunk, Gerrit, PowerShell, project specific language to validate data.

Previous project: My main responsibilities included developing and maintaining DWH, ETL, CI/CD pipelines, generating test data based on customer needs, sustaining support activities.
Tech stack: Azure, MSSQL, T-SQL, Python, Apache Airflow, Jenkins, Docker.
------
My way was started from IT Academy in 2019, then the job offer was received. I was immersed into DW techniques and methodologies, practiced with MSSQL, Power BI, built and deployed SSIS packages. I learned how to operate with Oracle, PostgreSQL, MongoDB and Vertica.
I’m eager to be challenged in order to grow and further enhance my skills related to databases (RDB's and NoSql) and Scala development. So, if company is not promoting and offering professional growth I will not join the team."
data engineer,"
The main stack: Snowflake, AWS, MS SQL Server,  Git, Bitbucket, Kibana, Jenkins, TFS, PostgreSQL, PowerShell, SSIS

Was involved in developing of different OLTP/ETL projects on Snowflake and MS SQL stack as Tech Lead

Currently, Team Lead in Data Engineering team

Ready to grow up in data-related technologies (Azure Database, AWS stack, Spark, Python)
Several technologies related to DB development and opportunity to work with cloud stack (AWS stack, Snowflake, Azure Database for ex.)
Small team
Ideally - product company
Main responsibilities is development 
Clear requirements for development
Career opportunities
Possibilities to change development processes"
data engineer,"– Built mobile app Analytics SDK with 20M DAU (daily active users) sending 1B+ events. 
– Built analytics function and key data resume_classifier in 1500+ ppl company (team, architecture, data modeling for CRM, SaaS, sales/finance, growth forecasting, product analytics, machine learning use cases).
Built analytics function in 4 companies: 2x as an employee, 2x as a consultant. 
– 6 years in the data and analytics space (engineer, architect). 
– 6 years as a full-stack software engineer before that.

Implemented in production:
– Designed analytics data stack and architecture (2 large projects).
– Created data resume_classifier for SaaS analytics, product analytics, usage behaviour analytics, growth forecasting, marketing and lead pipeline, finance.
– Built analytics teams (from 2 ppl to 20).

I value:
– good communication, measurable outputs
– code quality, source control, CI/CD
– data model performance, data quality testing
– repeatable data pipelines, proper snapshotting
– planned and well-communicated data migrations
Ideal company: 
– SaaS startup which needs help building dbt SQL data resume_classifier.
– Looking for full-time or part-time remote data modeling jobs.

Preferred tech stack: 
– ELT, Fivetran/Stitch, dbt, Snowflake, SQL."
data engineer,"
Data Engineer with more than 15 years of overall commercial experience in IT, including Data Engineering, Support Engineering, 1C development, and System/Database Administration. Solid knowledge of developing solutions using Business Intelligence, ETL, and DWH. Has experience in troubleshooting and resolving database problems. Always willing to learn new tools and technologies. Responsible, detail-oriented, and results-driven team player with good organizational skills.

Certified Azure Data Engineer.
"
data engineer,"Google Cloud Certified Professional Data Engineer
Google Cloud certified professional Data Engineer with more than 2 years of hands-on experience in developing ETL processes and modeling DWH with cloud-based and on-prem solutions. 

Active skills:
- Cloud providers: GCP, AWS
- Programming languages: Python, shell, JS (Google AppsScript), R
- BigData technologies: Hadoop, HCFS (GCS, S3), Spark, Cassandra, MongoDB, Hive, Google BigQuery, Zeppelin
- Data streaming: Apache Kafka, Cloud Pub/Sub
- ETL: Google Cloud Dataflow, Talend solutions
- Pipelines orchestration: Apache Airflow
- Business Intelligence: Google Data Studio, Tableau, Google Sheets, BusinessObjects, Looker
- Machine Learning resume_classifier deployment: Kubeflow, Cloud ML Engine
- Data Science basics: Supervised/Unsupervised resume_classifier (mainly with sklearn), Deep Learning (Tensorflow, Pytorch), Natural Language Processing (bert large resume_classifier, Text classification), resume_classifier tuning (GridSearch)
Looking for a part-time project as a Data Engineer or Data Analyst"
data engineer,"Performance tuning that achieved to optimize long-running queries from several hours to several minutes
I have 22 years of experience as a Software Developer, primarily in databases technologies (OLAP, data warehouse, OLTP, BI, ETL, database design performance tuning). 
I had been taking part in several projects as Senior Software Developer, including projects based on large data warehouse (terabytes of data).
I took part in all development stages: from discussing requirements with customer and designing requirements specification to the end software and its maintenance. I was responsible for logical design of database, development, deployment, maintenance, database administration, designing user interface, analytical reports development, data analysis, users’ support.
Good problems solving and analytical skills and independent development, quick learner.
Looking for project with large amount of data.
Flexible work schedule.
Remote work."
data engineer,"Through two successful stack migrations, I have honed my ability to learn new technologies at an impressive pace, taking just 0.5 months instead of the typical 1 month timeframe. My passion for rapid skill acquisition has resulted in a diverse skillset, enabling me to adapt to ever-changing technology landscapes with ease.

I have also taken the initiative to create a comprehensive project on-boarding guide for newcomers. This guide, which covers everything from opening their laptop to making their first commit to the corporate repository, has led to a remarkable increase in project involvement speed. The average time frame has been slashed from 2 months to just 1 month, resulting in significant improvements in productivity.

Additionally, I have developed a Proof of Concept (POC) titled ""Spark: Comparison of data storage formats"", which has increased overall productivity by an average of 20%. My dedication to innovation and efficiency has been a driving force behind my accomplishments, and I am always eager to tackle new challenges and push boundaries.
With a wealth of expertise spanning 5 years, I have successfully implemented Big Data solutions for leading retailers and media companies in the United States. This includes the development of projects from scratch, building of data pipelines with batch and stream processing, and adjustment of infrastructures while integrating external systems. My proficiency in Scala, Python, and Spark stacks based on GCP, AWS, and Airflow have enabled me to deliver tailored solutions that meet my clients' unique needs.

My skills extend beyond Big Data, with experience in Back-End IoT software systems utilizing Scala stacks and developing low-level protocols. I have also amassed 8 years of experience in electronic devices, further bolstering my technical expertise.

Mentoring engineers comes naturally to me, and I take pride in developing the next generation of experts. I have received positive feedback from my teammates, who acknowledge my contributions as a collaborative team player.
Preferable main languageы Scala, Python.
Nice to have: IoT, HealthCare domains"
data engineer,"Microsoft Certified Solutions Expert (MCSE): Data Platform
Microsoft Certified Solutions Expert (MCSE): Data Management and Analytics
Microsoft Certified: Azure Data Engineer Associate
Certified Scrum Master (PSM I)
Databricks Introductory Learning
11+ years of experience in data engineering. Strong experience in data modelling, data migration, data warehouses/lakehouses, ETL, performance troubleshooting & tuning. Able to work in a team or independently. Have experience in mentoring junior developers, performing technical interviews, presales and audits. Able to participate in full cycle of data integration from source data to end user report.

I mostly focus on Microsoft BI stack (SQL Server, SSRS, SSIS, PowerBI) and Azure Cloud (Synapse, Data Lake, Delta Lake, Data Factory, Databricks, SQL DB). I am certified in Microsoft Data Platform as well as in Azure Data Engineering.

Right now located in Turkey while working through Ukrainian business entity (private entrepreneur).
- A technical role, not people management role;
- Projects with MS BI and especially MS Azure stack;
- An opportunity to get familiar with new technologies;
- Projects that are not on the phase of support or testing;
- Not interested in opportunities with on-call support, shifts, etc.
- Remote only;"
data engineer,"
I'm professional Database developer with 15+ years of experience in IT industry.
All time I'm looking for interesting projects and always ready to non-standard challange/solutions.
I expect the long-term relationship with my clients and I will be glad to provide the qualify services in the term-of.

I'm experienced in:
1. Database development and design:
	- MS SQL Server, MySQL, PostgreSQL, Amazon Redshift, MongoDB, Cassandra, SnowFlake, BigQuery;
	- Index tunning, query optimization, performance optimization;
	- Database design;
	- Stored Procedures, Functions, Triggers, Views etc.;
	- T-SQL;	
2. Python:
        - Machine Learning (OpenCV, Keras, NLP);
	- Web scraping (Scrapy, BeautifulSoup, Selenium, XPath);
	- Web development (Django Rest Framework, Flask);
	- Data processing/ETL (CSV, Excel, Pandas, NumPy, SciPy, PySpark);
	- Other (GZip, Celery, SQLAlchemy, Redis, Multi-Threading, Regular Expression, RESTful, JSON etc.);
3. ETL development:
	- Python, Matillion;
	- MS Integration Service (SSIS);
        - Apache NiFi;
        - Talend;
        - Airflow;
        - Azure Data Factory;
4. BI (Business Intelligence) and tools:
	- QlikView, Tableau, Power BI, MS Reporting Service (SSRS);
5. Other:
	- Google Cloud Services;
	- Amazon Web Services, S3, EC2;
        - Miscrofost Azure;
"
data engineer,"Ported CI/CD workflows for building containers from bitbucket pipelines to aws codebuild

Developed and deployed data pipeline 
""incoming webhooks -> snowflake"" using AWS Api gateway, SQS, Lambda, Kinesis datastream/firehose, snowpipes.

Implemented a feature for an internal tool that allows it to build data warehouse replica containers for both amd64 and arm64 architectures.
Data Engineer/DevOps (ad-tech domain) 
Apr 2022 - present
Working with internal tools to support analytics team Main such tool was a sampling engine to create Snowflake sampled replicas in a form of a Postgres container for local work.
Deploying cloud infrastructure on AWS using terraform. Services used: S3, IAM, Lambda, Kinesis, Kinesis Firehose.
Developing containerised ETL jobs to move data from anywhere to snowflake dwh using snowpipes.
Implementing CICD for purposes of testing and building containers.
Working on snowflake stored procedures used for compliance with PII related regulations.
Performing admin-related Snowflake tasks (roles, schemas, integrations) using Schemachange

Data Platform Engineer, contract, (retail) Apr 2023
Architecture planning (GCP: Cloud Run Jobs, Bigquery, Looker, Pub/Sub; Other: DBT, Slack integrations, Google Sheets integration).
Deployment of the above (Terraform).
Porting workflows from excel reports to the cloud.
Training staff on working on this platform.

Data Engineer  Nov 2020 - Apr 2022
Developing python scripts for semi or complete automation of data collection and preparing for ingestion into proprietary system using various sources.
Financial data sets analysis.
Writing and maintaining documentation of implemented solutions and user workflows.
Communication with customer’s stakeholders, clarifying task requirements and making changes according to business needs.
Communication with data providers across the globe
 New employee onboarding

Data Analyst Mar 2018 - Nov 2020
Developing regular and ad-hoc reports in Excel and Power BI
Establishing data pipelines for automated reporting
MSSQL DB administration
Power BI server management
"
data engineer,"15+ years of development experience; Over 25 successful projects were in areas: software R&D, Internet-portal, bank, stock exchange, wholesale / retail trade, logistics, warehouse;

Ability to learn complex technical material and troubleshooting complex problems; Ability to work independently; Willingness to learn new skills; Interest in technical progression and growth; 10-finger method skill;
* Data modelling, Designing DB schemes
* Google Cloud Platform (GCP, BigQuery, Storage)
* Python, Linux, Git, PostgreSQL, Apache Airflow
* SQL, MySQL, Oracle PL/SQL, T-SQL 
* OOP, API, XML, JSON, Backend, HTML, CSS 
* ERP, CRM, ETL, DWH (data warehouse), BI
Adequate management. Professional growth."
data engineer,"*AWS and Microsoft certifications
*Coordinated DE students' education stream: interviewing, mentoring, and guiding other mentors
*Reimplemented the ETL logic to improve the data quality and performance of the scripts (f.e. from ~20 minutes to ~1,5 minutes)
*Automated the deployment process of a few applications on the last project
Data Engineer with 5 years of versatile experience. Performed a wide variety of tasks: wrote SQL, Python, and Java code;
designed database structures; have some experience with CI pipelines configuration; created Terraform scripts (IaaC tool).
Have solid experience with AWS cloud. Achieved Microsoft and AWS certifications.
Apart from that, constantly participate in non-project activities.

# Analytics Platform
• Wrote Python and Java code for processing data for the analytical resume_classifier
• Worked with AWS: Aurora, EC2, Lambda, Step Functions, Cloud Watch, ECR, IAM, S3, and other services
• Supported existing data load process from external sources to the database
• Created Terraform scripts
• Configured Concourse pipelines
• Led a subteam
• Communicated with POs and other teams
• Wrote documentation for the project components and processes

# Applications for USA banks
• Developed and managed MS SQL Server database objects (tables, stored procedures, views, etc.)
• Created new reports and provided performance tuning and query optimization for existing reports
• Analyzed existing ETL (SSIS) and worked on ETL reimplementing
• Constantly communicated with stakeholders
• Took part in grooming, planning, and estimating new features

# Healthcare application
• Developed backend logic for applications using T-SQL and custom framework
• Reimplemented existing reports to improve performance
• Created database objects (tables, stored procedures, functions, etc.) for reports and applications
• Communicated with the customers to clarify business logic for the new features
• Used SQL Server Agent jobs
Highest priority: Python
Would be great: Cloud (preferably AWS)

Do not consider companies that work in the RU domain"
data engineer,"
More than 8 years of experience in developing projects in various fields, such as Big Data/ETL/Data mining, Blockchain, Web Dev.
Worked in startups, product company and leading outsourcers.

Key technical skills and technologies: Python, Kafka, Airflow, PySpark, AWS Serverless Stack(Lambda, Step Function, Fargate, Athena, RDS etc)
Databases: PostgreSQL, Snowflake, Elasticsearch
Frameworks: Django, Flask, Falcon, aiohttp, Scrapy
DevOps tools: Jenkins, Octopus, Jfrog, AWS Codebuild, Docker
Paradigms and concepts: microservices architecture, event-driven applications, serverless, micro batching
Full remote, work-life balance, flexible working hours, lack of unnecessary meetings"
data engineer,"
I have extensive experience with BigData and cloud, especially in performance tuning and optimizing data pipelines.
E.g. on my current project in last 2 years:
I built all the infrastructure for Aiflow(MWAA) + EMR in AWS.
Rewritten and optimized all data pipelines.
Which resulted in cost savings on AWS infrastructure more than 20k USD per month.
Wrote the POC and moved all configs we have to Vault. Configured and introduced a staging environment (before we had only prod).
Wrote a POC to sync data from GCP(BigQuery) to AWS.

Used technologies:
• Scala, Python, Java
• AWS (S3, Redshift, EC2, EMR, MWAA, Lambda, RDS, SQS, Athena, Glue)
• GCP (BigQuery, GCS, PubSub)
• Akka, Play
• Cats, Cats Effect, Shapeless
• ScalaTest, ScalaMock, Mockito, Cucumber
• Sbt, Maven, Gradle
• Postgres, Cassandra, MongoDb, Oracle, SqlLite, MSSQL
• Spark, DeltaLake, Presto, Druid, Hive, Impala, Zookeeper
• Etcd, Vault
• TeamCity, Jenkins
• Kafka, RabbitMQ
• ELK, Grafana, Prometheus, Graphite
• Ansible, Terraform
• Airflow, Luigi
• Docker, Kubernetes
For last 3 years working in startups
"
data engineer,"Designed and implemented data lake with S3, Python, Airflow, and Athena.
Designed a fully automated ad account management system.
Productized AI-based microservices.
Scaled Airflow for ETLs with K8S.
I am Python Data Engineer with 5+ years of working experience,
strong mathematical, algorithmic and Data Science background,
5 years of experience in AdTech domain, ability and passion to learn and addiction to solving complex non-trivial tasks.

My primary stack is Python (+celery, FastAPI, Airflow), AWS (S3, EC2, Athena, SQS), Postgres, MongoDB, ELK, Docker, K8S, Terraform, Jenkins.

My latest major projects were: data lake design & implementation; data collection from multiple sources and processing it to unified report; scaling Airflow for ETLs with K8S;
designing Elasticsearch schema and performance testing; productization of AI-based microservices; leading R&D projects aiming to optimize product performance.

I love optimization, math, and algorithms, so I'd like to work with big data where O-large complexity is extremely important.
Join the team of highly skilled professionals, work with big data and ideally develop AI on top of it"
data engineer,"
Senior Data Engineer with 6+ years of experience in developing Business Intelligence as well as Big Data Solutions in various data stacks, such as SQL Server Data Stack (T-SQL/SSIS/SSRS/SSAS).  Extensive experience in different programming languages such as Python and C# and monitoring tools such as Apache Airflow. Hands-on experience in Report development using MS Power BI and SSRS. Strong hands-on experience in developing solutions using both on-premise tools and cloud technologies, such as MS Azure Stack (ADF, ADLS Gen2, Azure Synapse, Azure Databricks) as well as Snowflake & dbt (data build tool).
"
data engineer,"
Scala 4 years, Spark, 3.5 years
Java, 10.5 years, Reactive programming (Akka Streams and RxJs), 3 years
Kafka, 3 years, Cassandra, 2 years
Kubernetes, 2 years, Docker, 3 years
Azure 2 years, incl. Data Lake, Event Hubs, Data Factory
GCP 1 year, incl. Storage, BigQuery, DataProc and DataFlow
Prometheus, Kibana and Grafana, 2 years
Design and develop high-load/big data systems"
data engineer,"I managed to create from the scratch an analytic system in a big international company. It is still successfully working and developing there.
Currently I am working with such technologies as Amazon services, Databricks, Apache airflow, Delta lake. This year I was working on migration company's DWH to the cloud. Also as a team we  participated on moving big Ukrainian bank to big data solution.
"
data engineer,"
- Over 6 years of hands-on experience of applying Machine Learning techniques in E-commerce, Investments, Retail, AdTech,  FinTech etc.
- Designing, developing, integrating and maintaining complex AI-powered solutions
- Extensive experience of delivering high-performing and scalable Machine Learning applications into production
- Able to translate business requirements into clear technical specifications (ML roadmap)
- Strong problem solving skills and business value oriented
- Capable to interpret model performance and suggest ways to improve it, not just report metric
- Making use of A/B testing to support data-driven decisions
"
data engineer,"
Senior Database Engineer with over 10 years experience in development and administration of MS SQL Server. Proven experience working with Azure SQL and DB administration tasks. Proficient in complex stored procedures and useful functions to facilitate efficient data manipulation and consistent data storage using T-SQL. 
Hands-on experience in using Microsoft BI studio products like SSIS, SSAS, SSRS, Power BI for implementation of ETL in datawarehouse. 
Good experience to automate tasks and performance optimization. Effective team player, punctual and purposeful. Fundamental background in Computer Software Engineering and MBA background.

Experience
Senior Data Engineer
September 2021 – Present
Responsibilities:
 Worked with customers on projects required data engineering and management
 Used MS SQL, Azure and Snowflake
    Recent Projects: 
	Developing custom BI software and DWH (Snowflake)
	Developing DWH, ETL and creating reports (Azure Synapse, Power BI, Azure Data Factory) 
	Administration of DWH in Synapse
	Administration of MSSQL servers
	Migration of on premises MSQL to Azure (Azure SQL, Azure MI, VM) 
	Building near-real time DWH staging layer in Azure


Gold Mining Company
Senior Database Administrator
July 2013 – August 2021 Largest Gold Mining company.
 Supported bunch of systems in One of the largest Gold Mining companies
 Worked with MS SQL (2000-2019), SSIS, SSAS, SSRS
 Managed Migration to MS Azure projects
 Written new solutions with Azure
 Built new Transaction Log shipping backup system and standards
 Developed standards and check their implementation in T-SQL scripts
 Responsible for all production systems and databases
 Consulted Developers team in code review and developing new scripts
 Migration of all company's products in to newer servers and DBMS
 Administration of Servers, managing backups system, Optimization of network load (Veeam
Backup, Office 365, Hyper-V, AD, MS Exchange, Cisco, Fortigate, Juniper) 
 Managed server loads, memory, CPU, Hard drives, etc.

Tools and technologies used: Azure, T-SQL, MS SQL, SSIS, Always-on, SSAS, SSRS, Power BI, SQL Database, PowerShell, C#, Hyper-V, AD, MS Exchange, Cisco, Fortigate, Juniper.
"
data engineer,"Participated in DWH Project for the Oschadbank - biggest stated-owned savings bank in Ukraine, Teva - the big worldwide pharmaceutical company in Israel, Pixellot sports video startup VOD/OTT and Raiffeisen Bausparkasse in Austria.
Experience in Pharmaceutical, Manufacturing, Sports and Banking industries.
Data Warehouse Developer/Data Engineer with 10+ years of experience in building solutions in ETL, BI and Analytics.
Participated in DWH Project for the Oschadbank - biggest stated-owned savings bank in Ukraine, Teva - the big worldwide pharmaceutical company in Israel, Pixellot sports video startup VOD/OTT and Raiffeisen Bausparkasse in Austria.
I'm looking for a Data Architect, DWH Architect, Lead Data Engineer, Lead DWH Engineer positions with a focus on modern solutions and tools preferably in Cloud environments.
"
data engineer,"
ETL: 3+ years
Snowflake, Azure Synapse, Postgres, 
Airflow, Informatica, Kubernetes, Docker
AWS(RDS, S3, EKS, ECR, DeepLens, EC2)
GCP(BigQuery, CloudStorage, Compute Engine ...)_
-----
Backend: 2+ years
Java, Go, PHP,
GWT, Symphony, Mysql, Postgres, Docker
I would like to work on ETL projects and BigData stack.
I'm interested to gain practical experience with AWS or GCP services"
data engineer,"
Have over 7 years of practical experience in development including enterprise systems for the web, more than 3 years in development using Spring and SQL. Hands-on experience in Java SE/EE. Also, have experience in Scala, MongoDB, and other technologies.  Work on the Scrum and the Kanban methodologies.

* Working in distributed teams;
* Mentoring;
* Implementing PoCs;
* Feature estimations and designing.
"
data engineer,"As team leader on few projects, besides work by Agile and closing iteration green, we were able to:
-create multifunctional deployment script with different options such as deploy, restart, archive logs and so on
-setup automatic nightly builds using Maven and Jenkins 
-establish full continuous integration process
-use best development practices in yet-non-production project
-use main development principles SOLID, KISS, software patterns, TDD
As developer I was able to:
-overcome huge performance database issues (sql were rewritten using indexes, force plan, logical i/o stats, hints for the optimizer)
-overcome huge performance issues with export
-get rid of database deadlocks
-integrate application with external systems(AD,ETL)
8 years of experience with Java SE, Java EE, Servlets, ActiveMQ, Spring, Mybatis and following DB: OrientDB, Cassandra, Sybase, MySQL.
3 years of developing on Flex from Adobe.
Currently I'm practicing Hadoop technologies, mostly for proof of concept projects.
I love all performance challenges! Will be happy to solve some! Every possible optimization inspires me!
Currently I'm looking for open-minded companies to work remotely due to life circumstances.
If you need a person who is responsible, disciplined, motivated and love to make his job in time - that's me for sure."
data engineer,"Built a big data pipeline for petabytes of raw data from scratch, starting from architecture to implementation, from low-level data structures to high-level apis
Built a set of high-load applications for 100k rps
Software developer, Tech lead, Architect with almost 15 years of Java and 5+ years of Scala experience in ad-tech, e-commerce, banking, and travel projects within product and startup companies. Building heavy BigData pipelines for in-house and/or AWS clusters, developing high-load applications for hundreds thousands RPS. Proven mentoring and problem solving skills, ability to lead a project through full development lifecycle. Experience working with agile teams. Excellent negotiation, issue resolution skills.
Looking for a relocation to USA (currently outside of Ukraine)"
data engineer,"
Big Data Software Engineer with 5 years of experience. Have wide range hands-on experience in commercial software development starting from business idea, design and implementation, to support and maintenance. My main area is Big Data software engineering (Hadoop and Clouds) with Java, Scala and Python. Have experience with back-end Java and Python solutions.
- Looking for BigData stack. Java, Scala or Python. Clouds, preferably AWS or GCP.
- Interested in Senior, Team Lead or Tech Lead Position.
- Interested in new projects, for example, which are in development not more than half-year.
- Considering working from the office or remote."
data engineer,"- Successful delivery of the project from Proof-Of-Concept stage to production
- Designing and implementing separate modules
- Performance tuning of Hadoop cluster and Spark jobs
- Mentoring junior colleagues
- Leading a small-size team of senior developers
- Java 8 and GCP certified
11 years of commercial experience in IT, including: 
 - 8 years with Scala
 - 6 years with Big Data, Hadoop and Spark.
 - 1.5 years as a Team Lead/Tech Lead.

Have experience of working with Cloud (AWS, GCP)

Strong advocate of the best practices in software development, such as Unit/BDD Testing, code reviews, pair programming etc. Also have experience in mentoring junior colleagues and conducting job interviews. 

Have always been eager to create software which is well architected, tested and documented, easy to maintain and meeting all the performance requirements.
Looking for a job as a Lead developer in Scala/Big Data project.
Interested in opportunities for professional development and learning new technologies."
data engineer,"Apart from the technical expertise I have plenty of experience in training and teaching. I've been conducting technical training for different audiences for the last 4 years.

For 5 of the last 7 years I've been living and working in 2 European countries and the US, therefore I have good skills in working in foreign environments and cultures.
I'm an experienced Senior (Lead/Architect) Data Engineer with 10+ years of experience designing and building high-load data systems.

I have 3+ years of experience as a team/tech lead, including working on-site in the US and leading a customer's team of 5 developers. 

For the last 2 years, I've been working in the Product Company where I worked both on improving existing systems and building new internal products from scratch.

Currently, I'm open for a Senior/Lead/Architect position related to Data Engineering in product companies.
I don't consider returning to work in outsource/outstaff."
data engineer,"
An experienced and result-orientedData/ML/Software engineer,  product leader, BA, and Data Analyst with a proven track record of using a data-driven approach to concept, design, build, integrate, improve, and deliver effective products to customers.
I have been building products from scratch using my background in SW engineering, business development,  commercial, and business administration areas.

Ukrainian military service obligations: Last autumn honorably discharged from service as a veteran.
I have permits to work in Germany and EU countries.
I'd like to be proud to take part in creating innovative services to help people be better informed to act smarter."
data engineer,"Developed architecture and implemented social network for swiss businesses, enterprises and tenders.
Created efficiency and correctness monitoring tool for data deduplication engine. 
Extended media content delivery system and integrated it with legacy systems. 
Aided migrating two large enterprise systems to microservice architecture.
Implemented system for tracking malicious user behavior.
Implemented and maintained numerous data engineering tools.
Python: Django, Flask, FastAPI, SQLAlchemy, Celery
JavaScript: React, Vue, Backbone
DevOps: AWS, Terraform
NoSQL: Redis, Elasticsearch
SQL: PostgreSQL, MySQL
Testing: pyunit, pytest, nose
ML: SciPy, Scikit-learn, Tensorflow, PyTorch, NumPy, Pandas
Big data: Hadoop, PySpark
Applications: Apache Server, Nginx, Varnish Cache
Message brokers: Kafka, RabbitMQ
Continuous integration: Jenkins, Travis, GitlabCI
Schedulers: Airflow
Preferable backend with possibility of work with DevOps, Big Data, Python, ML, highload, data intensive applications."
data engineer,"PhD, Computational Intelligence; 
AWS Certified Solutions Architect - Associate.
I am a scientist, an engineer, and a competitive Machine Learning enthusiast.
My favourite thing about software engineering is scaling products and processes.
I shine at building cloud-based data platforms for products focused on machine learning.

I am currently located in Wroclaw, Poland.
Looking for a Software/Data Engineer position on Machine Learning/Data Science project."
data engineer,"Able to work independently, directly with customers and take responsibilities for the workflow.

Honors & awards

- 1st place on Beeline hackaton (SkyLab)1st place on Beeline hackaton (SkyLab)

- Employee of the year 2015
Issued by Chocofood · Dec 2015

- 1st place internal Choco-hackaton Winter 2015. Issued by Chocofood · Feb 2015
I am a passionate Software engineer with 9+ years of experience. I have excellent knowledge of Python programming language and AWS ecosystem. Recently, I was focused on entertainment, content production, development and publishing own products. I strongly believe that creative positive and fun environment helps to creat great products.

My Interests:
- Data Engineering
- Cloud Computing
- Distributed systems
- Web Development
- Product development
- Hight load services

My key strengths:
- I am a strong vision holder and inspiring leader. 
- Exceptional communication & organizational skills. 
- Able to work with a variety of roles starting from artists to engineers 
- Extensive knowledge of Django, AWS , Python, React, Docker
- Extensive knowledge of Online development including server infrastructure and architecture.

Worked on:
- Co-Founded a game-dev studio
- Builded internal marketing analytical platform
- Created ETL tasks to fetch and transform data from Partners API
- Integrated and developed services using AWS 
- Created a complete pipeline for data processing on AWS Lambda Functions

Skills: Product Management · Company development · Business Strategy · Cross-function Team Leadership · Budgeting · Agile Methodologies · Scrum · Kanban

Stack: Python · Django · AsyncIO · React · PyTest · Docker · PostgreSQL · MySQL · Reddis · AWS Lambda Functions · AWS Fargate · AWS RedShift · AWS Kinesis Stream · AWS Kinesis Firehose · AWS DynamoDB · AWS CloudFormation
Full-Stack tasks, Data Engineering tasks. Leaned toward constant evolving in both managerial and technical tasks. Ready for challenges."
data engineer,"Engineering data-intensive database applications for more than 10 years. Those include:

Allyo.com - designed from scratch, spearheaded and launched two analytics services, developed integrations with Tableau Analytics

Lifedojo.com - designed hundreds of apis for online learning provider, provided AWS integrations

Leadrouter - designed reporting for Fortune-500 company in real-estate, a db-heavy project for US.

Rebelmouse.com - social-network publishing platform.

Certified AWS Cloud Practitioner.
I am a web developer working with multiple languages, but mostly Python during the last 10 years.
AWS Certified Solution Architect Associate.
For the last 6 years I've been taking part as a Python developer in a series of large SAAS projects/startups. Specifically

rebelmouse.com - social media platform
lifedojo.com - online learning
allyo.com - AI chatbot automation for recruiting

I like to deal with DATA, engineer it, develop ETL and data workflows, process it asynchronously and store it in the most optimal and safe way.

There were more projects, sentiment analysis apps, marketing apps, sports  betting apps, online auctions and web site generators.
Personal relations in a good team.
Preferring long-term projects.
Quality over speed.
Some resemblance of 37 signals or trying to match similar values.
60% conservative, 40% innovative.
Team Mates to read important IT books, not just articles."
data engineer,"I developed high-load application with support mobile (iOS, Android) and web clients. Also, I developed in-house solution of advertising with targeting, limits, statistics, recommendation and etc. (like Facebook Ads) and integrate then to application.
I had responsibilities of all backend part of project (architecture, development, agreement with other teams like mobile, front-end, marketing, etc., knowledge sharing) I was the lead of backend development. Tech stack: Django, PostgreSQL, Airflow, Celery, GCP, GCF, BigQuery, Redis, DataDog, RabbitMQ, Firebase, third-part services. I supported API (REST) and backend logic in AI platform for marketing (fetching data, preprocessing, validate recommendation, business logic). I had experience with Facebook, Adwords, Slack endpoints/SDK. Tech stack: Flask, PostgreSQL, Celery, AWS S3. Also, I code reviewed, wrote documentation and developed tests with Pytest library. I also implemented and supported: - Microservice to get information about uploaded images (full ownership). Tech stack which used: FastAPI, Celery, Google Vision, AWS S3. - One of the main modules and implement integration with another inner platform (Asyncio, Aiohttp client). - Web application (Internal CRM service) with control metrics, data quality metrics and support tools (Django, PostgreSQL, Jira API endpoints). - Building web-scraping applications for complex websites using Scrapy and API endpoints (backend development for application on Aiohttp) or broker (Kafka). - Scraping data from various types of websites. Bypassing various anti-bot measures used at websites to prevent automated data scraping (Scrapy)
I’m looking for a company with interesting and challenging projects where I could improve my skills and professional knowledges."
data engineer,"- As a result of a POC project for a new customer, the company that I worked for got a long-term contract for ~50 employees.
- I was the only Big Data engineer (out of ~30 senior engineers) to successfully go through a series of technical interviews with a very important and valuable new client.
- I've optimized cloud costs for a customer by 50%.
- Being a team leader of 7 engineers I've set up a bunch of processes that allowed us to deliver a huge milestone faster than it was originally planned and without any critical issues. We saved a lot of money for the customer and they extended the contract with my company for more teams for another year. 
- For the last couple of years I've mentored ~10 interns that successfully passed the internship, got hired, and got very good customer feedback on their projects.
- For only the last year I've done ~50 tech interviews.
I'm a software engineer with 9 years of experience in enterprise software development.
I have great expertise in developing various kinds of software of different complexity and scale for the top US enterprises.
For the last 5 years, I've mostly been doing Big Data analysis and engineering.
Currently, I work at one of the FAANG (a contractor) as a Senior Data engineer.
I'm tired of out-staff companies.
Now I'd like to try working in a startup or a product company.
I consider relocation to Western Europe."
data engineer,"Developed web application from scratch using AWS lambda with my teammates, participating in developing a buy and sell platform using a variety of frameworks (Solar, Finatra, Spark etc).  Conducted interviews with newcomers and mentoring of beginners in Scala.
Total development experience: over 10 years.
Last 7 years I've been working as a Scala / Java developer.
I'm passionate about functional programming. 
The scope of my responsibilities includes collaborating with a customer, discussing different approaches with team members, mentoring  Scala beginners, and writing code. Worked as both backend scala and big data engineer (with Scala and Python). 

Have experience with AWS infrastructure. 

Scala frameworks used:  Cats / Cats effect, ZIO, HTTP4S, Akka Http, Finatra, Slick, Kafka,  Spark, Scala Test, Scala Specs.
Ability to deliver high-quality features to a customer.  Ability to write true functional Scala code. Ability to choose technologies based on my expertise.  Mentoring (if needed). Working with open-minded strong experts, leading. Direct collaboration with a customer."
data engineer,"1.Telecom billing database, tables with millions of rows. Optimized complex Oracle SQL query from 3 minutes(cost 13000) to 2 ms(cost 50), rewrote with advanced analytical functions, applied correct indexes with hints.
1) Remote Contractor - Senior Software & Data Engineer
1.Development of microservices
2.Development of data pipelines(batch, streaming)
3.Integration and testing of different 3rd party APIs

Technical stack:
Programming: Java 17, Python 3, Kotlin, Spring Boot stack
Data engineering: advanced Oracle SQL, PL/SQL, optimizing complex SQL queries, Postgres, MySQL, ETL, BI reporting, Kafka, Apache Spark, Spark Streaming, Solr
Cloud: AWS RDS, Aurora, EC2, ECR, Lambda, S3, IAM, SQS, SNS, Athena, CDK
DevOps: Docker, Kubernetes, CI/CD, Jenkins

2) Senior Java and Big Data Engineer
I was one of the core team members in European Bank's GDPR related Big Data project. 
1.Developed near real time big data processing pipeline with Kafka, Apache Spark, Spark Streaming, Solr and HBase
2.Developed microservices with Spring Boot
3.Developed artifact and configuration deployment scripts for DevOps engineers.

Technical stack:
Programming: Java 8, microservices with Spring Cloud 
Big Data: Cloudera Hadoop cluster on AWS, Apache Spark, Spark Streaming, Kafka, HBase, Solr 
DevOps: CI/CD pipeline with Jenkins, Docker, Ansible.
Atlassian stack (JIRA, Confluence, BitBucket)

3) Telecom Company-Head of Datawarehouse Unit
My role was 50% technical and 50% managerial. I was responsible for Oracle Exadata based 51 TB corporate data warehouse system. DWH was highly critical reporting source for company’s daily operations and financial reporting.
Responsibilities:
 1) Collaborating with business units to understand, analyze their reporting & analytics needs
 2) ETL development, data modeling(dimensions, fact tables, data marts), BI reporting with SAP BO tools
3) Supervising team members, driving prioritization and delivery
We maintained Cloudera Hadoop cluster, implemented different telecom specific Big data use cases.
I’m interested in Java, Python, Kotlin, AWS, microservices, Oracle, Postgres, data and cloud engineering projects"
data engineer,"
In my current work, I work with Java and related technologies. My main responsibilities are to build web services and create data pipelines to support our data platform. In total, I have more than 5 years of experience in different industries, like banks, startups, and a company selling AI products.
"
data engineer,"
Siebel developer, integration with third systems (JSON, SOAP) - 6 years experience, SQL (Oracle, MS SQL), AWS and Python (junior/beginner). 
Projects - Vodafone, Agromat, Konecranes (developer and integration engineer). At the moment - team lead.
"
data engineer,"I am committed to delivering high-quality work and have consistently strived for excellence in my development projects. One notable accomplishment was when I successfully led a team to develop a complex web application using cutting-edge technologies within a tight deadline. My open-mindedness has allowed me to learn new techniques and adapt quickly to changing project requirements. For example, during a recent project, I quickly picked up a new programming language and implemented it effectively, improving the overall performance and maintainability of our codebase. I am always eager to explore new possibilities and contribute to innovative proposals in the workplace.
A highly skilled data engineer with over 6 years of experience in the software development industry, I'm committed to delivering top-quality work and ensuring client satisfaction. My expertise spans data engineering tools like Airflow, Snowflake, EKS, EMR, GitHub actions, and Jenkins. With a strong background in Python and its web frameworks like Django, Flask, and FastAPI, I have a proven track record of delivering exceptional results for successful companies such as Revelio Labs, Airbyte, and Contora.
Career growth. Friendly team. Competitive Salary. For non-remote, it is also important for the office to be located in the center of Kyiv."
data engineer,"
Built advanced data analytics and machine learning products for global companies in finance, retail, marketing&sales domains.
Backend (mostly Python), cloud (AWS - Lambda, DynamoDB, S3, Kinesis, etc.), Data engineering (Spark, Redshift)
"
data engineer,"The client put the whole team (except myself) on hold for ~3 months due to COVID.
During that time I had fixed existing issues, implemented new API's, successfully performed the rollout for the first production release and set up monitoring/alerting.

---

Set up new Hadoop cluster, prepared architecture for migration from old Hadoop cluster and implemented metadata and data replication between clusters, so that the migration can be done without impact to the business (no downtimes).

---

Implemented several operators for in-house execution engine (not SQL, but semantically close), along with the migration of dimension model from row storage to columnar storage (this was required by execution engine).
Currently (last 2 years) - tech&team lead. Main responsibility is long term architecture (no hard switch - new services should work along with legacy services during the long transition period), planning short-mid backlog and release/production rollout.

During the last ~9 years worked (lead and senior positions) almost exclusively on data and performance(latency/throughput of api/db/query engine) centric projects.

---

I would like to be less of a jack-of-all-trades and focus more on data-related and/or performance-critical parts of the project.
Not interested in team/tech lead positions.

----

Any of the following on the project would be a plus:
- Apache Arrow 
- Apache Arrow Datafusion (yes, I know it is in Rust)
- Apache Iceberg
- Presto/Trino
- Spark
- Observability (tracing/metrics/logs)

If the project is performance-centric (low latency and/or high throughput) - this is a big plus."
data engineer,"In my previous role as a data engineer, I was responsible for ensuring the efficiency and reliability of ETL data pipelines, using Airflow, DBT, Postgres, and GCP. I also supported the maintenance of Python web scrappers, and developed new features for an application implemented with NodeJS, React, and TypeScript. My skills include Backend and Frontend development, I have experience with CI/CD and Cloud solutions like AWS and GCP.
- Data Engineer | Software Engineer
- Python Developer | JavaScript Developer | Full Stack Developer

## Skills

- Python [6+ years], including libraries and frameworks for
  - Data ETL/ELT Pipelines (NumPy, Pandas, Airflow, DBT) [1~ year]
  - Web Development (Django, DRF, Flask) [3~ years]
  - Mathematical Simulations (Matplotlib, SciPy, SymPy) [3+ years]
- SQL and NoSQL databases (Postgres, GraphQL) [3~ years]
- CI/CD (GitHub Actions, Docker, NGINX, Sentry) [2+ years]
- Cloud (GCP, AWS) [2+ years]
- JavaScript popular frameworks for Web Development (Node, React, Redux, Vue, Express) [2+ years]

## Background

- Fluent English
- Data Transmission Systems Ph.D.
- Computing Engineer Ms.C.
- Solid mathematical background
- Deep understanding of Digital Signal Processing, and Wireless/Optical data transmission systems
- Former experience with HPC and intensive mathematical simulations (MATLAB, Python, CUDA)
I am seeking a challenging role to solve complex problems. I am interested in data or software engineering, an innovative company culture, and growth opportunities."
data engineer,"
I have more than 9 years professional experience in Software Development (Requirement gathering, analysis, system design, coding and testing using programming language: - Java, Mobile App Development, PHP, ASP.Net, C#, Python, XML, JSON and SQL and framework Laravel and Liferay) and Software project management (ERP, Digital ID System and so on). And I have good experience in web site and portal development using WordPress and Liferay.
Also I have good experience in data/database management and visualization:  MySQL, SQL Server, MS Access, SQLite and MS PowerBI
"
data engineer,"- taking part in implementing CI process(vCenter+Jenking+ODI+Oracle 11g)
- implementing unit testing process
- reducing product time-to-market with automation of development tools and processes
- heavy, but effective refactoring of transaction system
- building docker swarm based infrastructure for micro services from scratch
- creation of half automatic documentation handling system
PL/SQL, SQL(oracle, postgresql), ODI, Jenkins, Gitlab, Bash, APEX, BI, DWH, CI/CD, development automation, Java(basic), Python, script and processes optimization, large scale databases, transaction database, docker, markdown, taking care of documentation, testing, performance testing and tuning, working with and maintaining AWS serverless stuff using cloud formation
Looking for job with plenty of new features and opportunity for professional growth. Ready to learn new features(supposedly getting DevOps/backend developer experience)"
data engineer,"
Build and maintain ETL pipelines. Extract and load structured and semi-structured data. REST API services development. Working with serverless computing. Working with documentation. Automating processes.
"
data engineer,"
More than 1 year of  experience in the big tech telecom company as a python software developer. Developing tools for increasing efficiency of internal workflows. Mostly, tools for parsing excel using Pandas, creating GUI with wxPython.
"
data engineer,"Created datamart (MS SQL Server) that reduced the time of calculation of turnover from 5 days to 1 day.
Automated execution of procedures in MS SQL Server and refreshing of Excel-reports that reduced the time of daily routine from 2 hours to 15 minutes.
Created system of validation of the data that helped ensure the data completeness.
BI analyst (T-SQL, Excel, Python),
Data Engineer (Redshift, Tableau, Hive),
Data Engineer (Redshift, AWS (Athena, Glue).
Position of BI Analyst / Data Engineer with conceptual tasks solving and distinct responsibilities of each team member."
data engineer,"
BI DEVELOPER (remote)
Sompo International Holdings Ltd. | New York, NY | Jan 2019 – Present                                                                                               
	As a key member of BI team, delivered the best results to the development needs, accomplished successful outcomes by working with T-SQL, Power BI, SSAS, SSRS, SSIS, ADF2, and other Azure Cloud tools;
	Daily scrum meetings with Project Managers, Business Analysts and end users/clients to gather, analyze and document the business requirements and business rules; discuss the project specs in order to understand the business needs;
	Implemented performance tuning on complex stored procedures, views, user defined functions and queries by creating proper indexes and optimizing the existing ones, creating and updating tables and checking the database consistency by executing DBCC commands;
	Built reports for different domains such as Finance, Accounting, Management, and other client teams;
	Created Power BI reports on account information, research reports using ad-hoc reporting to be used by clients and advisors. Created reports on both cascading and standard parameters. Worked on Power BI and Tableau in creating dashboards and reports and visualizing data;
	Helped clients to convert Excel reports for Accounting Department into Power BI reports by automating the whole lifecycle of report generation. Generated OLTP database objects. designed Data Warehouse. and created all Power BI reports. Created ETL processes to automated data flow;
	Accounting Reports included Year-To-Date, Month-To-Date, Period Close reports, Transaction and Snapshot reports for General Ledger, Budget reports, etc.;
	Developed filters, reports, dashboards and also created chart types, visualizations and complex DAX calculations to manipulate the data in Power BI;
BI DEVELOPER (remote)
DataSite Technology LLC | Tashkent | Nov 2017 – Dec 2018
	Actively participated in meetings with different Departments, Managers and End users to capture and document system requirements;
	Created complex analytic queries on huge data sets; developed T-SQL stored procedures, triggers, and user-defined functions. Handled data manipulations as required;
	Croreeated joins and sub-queries for complex queries involving multiple tables from different databases; 
	Normalized the tables and maintained Integrity by using Primary and Foreign Keys;
	Created Views to enforce security and data customization;
"
data engineer,"
3+ years of experience as Oracle Database Engineer, 2 years as Software Engineer. I have implemented complex business logic in Oracle Database using SQL, PL/SQL, XML, JSON. Maintained packages, procedures, triggers, indexes etc. Proficient in queries optimization and database performance tuning. Created complex reports using Python and SQL, experience working with BI tools such as Tableau. 

Skills:
- Proficient in SQL, PL/SQL, Oracle database architecture, query optimization, performance tuning.
- In-depth knowledge of data modelling, data structures.
- Experience with Python, Tableau, data analysis.
- Experience with Apache Spark, Apache Impala, Starburst.
I am looking for SQL Developer / Data Engineer / BI Developer position."
data engineer,"Hello,
I would like to express my interest in applying for a remote full-time data specialist position in your company. Currently, I am based in Kharkiv.

I've worked as the data engineer for more than a year on a remote position for a Poland company. During that time, I've engaged with different activities of data analysis and collection to help to improve the product and services. Beside that, my work experience also includes the solution developer position where I practiced my skills for banking/financial sector.
What I liked the most about my jobs is that they gave me the opportunity to learn and be creative, and it looks like this position would do the same. I feel that I could be a valuable asset to your team, and I would bring to the table all of the skills.

I look forward to hearing from you and appreciate your consideration.
Best regards
SUMMARY
1.5 years of experience in software development with wide range of technologies (see SKILLS);
Experience as tutor of School and Higher Mathematics, Lineal Algebra, Algorithm and Programming (for 
pupils in high school and students) ;
Managed requirements quality and clarity. Confident problem-solving abilities to overcome issues with 
creative solutions;
Result and risk oriented approach. Support development teams and assist teammates to achieve goals 
successfully;
Excellent logical and analytical skills. High sense of intellectual curiosity – self ownership of learning and 
horizon broadening opportunities;
Experience in statistics, math and algorithm theory, particularly hands-on experience with machine 
learning;
Well organized, ability to work in team as well as be an individual contributor, self-motivated;
Good communication and interpersonal skills;
Intermediate English level;
SKILLS
Programming Languages:
Python, Matlab
Technologies and Libraries:
Pandas, scikit-learn, NumPy, Keras, Tensorflow Mat-
plotlib, Seaborn, openCV
Database Systems:
SQL (MySQL, PostgreSQL, Oracle), PL/SQL
Other Tools & Platforms:
AWS, Docker, Jenkins, Git, GCP, ETL, Flyway, Talend, 
Datadog, JIRA
"
data engineer,"
- Creating the reports for the tax and accounting industry using Sql,Pl Sql, Vbs, HTML technology.
- Download  information from various sources to the database using Sql Loader for Oracle, sql. 
- Advanced working SQL knowledge and experience working with relational databases, query authoring(SQL) .
Development of queries of any level of complexity( queries to transpose tables,aggregate functions, window functions,queries to tables with a hierarchical data structure, queries that contain JOIN instructions, queries that 
contain UNION instructions and other). 
- Experience reviewing execution plans and performance tuning.
- Structure design and creation of database objects.
- Program development(ASP, Pl SQl technology). 
- Technical support and maintenance of specialized software. 
- Providing advice to users on the work of the software.
- Good knowledge of mathematics.
- Good analytical skills
"
data engineer,"•	Participated in migrating data for the new Banking system
•	Participated in development of new ETL processes
•	Utilized Joins and sub-queries to simplify complex queries involving multiple tables while optimizing procedures and triggers
•	Performed SQL, PL/SQL tuning using multiple tools including explain plan, SQL Trace and PL/SQL Trace
•	Wrote 25+ SQL queries to obtain data from multiple tables to extract into CSV or Excel format for all company users and businesses
•	Created and maintained 50+ PL/SQL functions and procedures to maintain operation continuity
•	Identified and implemented programming enhancements that increased productivity by 30%
•	Translated business requirements into technical requirements delivering robust code that is fully tested, meets business requirements and complies with all company standards by 100%
•	Participated in utilizing Pentaho ETL tool for migrating daily data to Datawarehouse 
•	Utilized Python programming and Apache Airflow platform to develop ETL processes for Datawarehouse
•	Friendly team
•	Interesting Tasks
•	Professional growth"
data engineer,"
8+ years: MSSQL database development, designing of SQL databases, DWH systems, ETL processes, SSIS solutions. SQL/ETL performance optimization, analytics, reporting, SSRS. SSDT project support and build/deploy automation.

3+ years: .net C# / Python ETL development, client-server database applications, CRM systems, reporting systems.
"
data engineer,"
Over 8 years of experience in data warehousing and reporting systems including creating DWH from scratch , custom logics and migrating  to cloud. Huge experience with legacy systems and reverse engineering  of some ancient or not-so-well-documented processes.Good knowledge in domains of medical insurance, Revenue cycle analytics, supply chain and warehouse tracking. C1 English
"
data engineer,"
Developed, implemented, and optimized stored procedures and functions using SQL.
Create SQL queries for reporting.
Updating and optimizing SQL functions, and stored procedures;
Extracted and transformed multi-source data with Python.
Analyzed existing SQL queries to identify opportunities for improvements.
Seeking a position as SQL Developer to improve my technical skills and for interesting tasks and challenges."
data engineer,"It's better to check my LinkedIn profile, where information is presented
5+ years experience in BigData and strong Java development. Able to find, analyze, learn technologies required to a client and a project to build great working solutions, lead projects and teams, collect requirements, build infrastructure, implementation of technical tasks, writing user stories, troubleshooting, care about code quality, etc. Have experience with Agile/Scrum methodologies, Apache Airflow, Apache Spark, Apache Kafka, AWS cloud system, Java, Python, Scala, GCP cloud system. Last time, I mostly, working with environments to build BigData solutions in clouds using distributed systems and data formats.

I consider ONLY full relocation to the US, California. Just before COVID, I already had a business trip
"
data engineer,"• Optimized data and data pipelines architecture using Airflow, increased efficiency by 20%
• Designed, built, and maintained real-time event-based service for UA campaign optimization using predictive lifetime value, resulting in a 10% improvement in revenue
• Developed a data pipeline for creating product-based views, decreasing SQL execution time by 2x
• Designed and built services for payments validation with 95% accuracy
• Created effective BI dashboards in collaboration with the marketing department, improving UA campaigns' efficiency and shortening the creative/product test period while achieving KPIs on various product launches
• Built data infrastructure in two game publishing companies with the ability to scale horizontally.
Senior Data Engineer with over 5 years of experience in designing, building, and maintaining data pipelines and services. Adept in using Airflow, Ansible, S3, Python, Clickhouse, Docker, and Tableau. Managed data flow and quality by leading a team of data engineers.
"
data engineer,"- discovered json parsing related bug in Snowflake that was confirmed by their support;
- created custom ETL process based on Python for handling key-based replication deleted records and for cross-instance data quality checks (Postgres vs Redshift and Postgres vs Snowflake);
- created custom process based on panda | Python for various data quality checks;
- created data pipeline alert process based on Python for notifying users about various events in database, integrated with mailjet (Email service) and Slack;
- created basic frontend subsystem based on Python & Flask for notifications management;
- created databricks to hubspot API integration using Python
Last project description: 

Fintech company providing innovative insurance products and financial solutions for residential and commercial real estate professionals as well as their residents and tenants.
Data Science team.
Consolidating data from microservices into dwh; creating bi dashboards; tuning objects in dwh in order to make them cached;
dba stuff like creating users, setting access rights on different environments, aws rds db replication;
looking for missing indexes etc in order to optimize sql queries;
checking data quality and integrity, creating various concepts for future usage;
creating star-type dwh;
creating db functions for financial forecasting;
migration from Redshift/Periscope into Snowflake/PowerBI.
Postgres, Power BI, Stitch (etl tool), dwh, Python + sql coding, GNU/Linux, unix shell scripting;
Anaconda, Jupyter Notebook, databricks / PySpark basic usage;
various performance testing under high load;
parsing and joining complex json values in Postgres and Snowflake;
preparing documentation, presentations for business users, new team members onboarding (basic).
Additionally: I'm owl type of person and can work according to US (EST) timezone.
Desired technologies on project: Rest API integrations based on Flask/Python/AWS + Snowflake/Postgres. Casually considering openings, but not available until the end of May 2021. I would consider job descriptions written solely in English with ""100% remote"" mention; w/o any exUSSR management; US (EST) shift."
data engineer,"
As Software Developer (PROGRESS OpenEdge developer)
• Migrate the Procurement and eCommerce
departments functionality from console product 
version to browse version
• Provide accurate incident resolution and service 
request management, within established time 
frames, meeting or 
exceeding customer’s requirements and 
expectations.
• Taking ownership of incoming customer support 
requests pertaining to software and perform initial 
investigations and diagnosis through to incident 
resolution.
• Troubleshoot customer software incidents and identify 
root causes of software problems.
• Fix software configuration issues and test solutions prior 
to customers implementing solution.
• Support continuous improvements of the product and 
customers experience by proactively identifying and 
researching potential challenges and creating solutions.
When became to new position of eCommerce module 
owner from 2015
• Investigating and developed the eCommerce needs 
from the customers.
• eCommerce Implementation on the clients’ site.
• Defining the new features to be done in the project.
• New features design and documentation.
• Control testing process of the eCommerce
department. Test data preparation for QA team.
• Making integrations with third-party providers of 
eCommerce data
• Processing and analyzing big data

As SDET/QA Engineer
 Complex multilevel testing including DB results comparison 
 Creating corporate frameworks for app coverage with 
 autotests 
 Writing automation tests to check both UI and API tasks
 Making integrations with third-party systems for QA needs 
 Pre-test preparations: environment installation, build, 
 deploy and run applications. 
 Test case / Test Plans writing.
Classification: Confidential
 Troubleshooting customer software incidents and identifying
root causes of software problems.
I don't want to work in blockchain, gambling, crypto"
data engineer,"- Product MVP
- Grown multiple engineers in my team
- Production-ready solution
- Reduced tech debt
- Established SDLC
I have 5+ of experience in the development of software and data projects. 

I have strong experience in Data Solutions design and development (Data Lake, LakeHouse, Data Warehouse). including CI/CD for data pipelines and data cloud components. I have participated in the full life cycle of data project implementation and always followed references architectures from technology providers and best practices for SDLC.

Additionally, my background is not limited by data solutions. I have full-stack experience in .NET/Python and Angular, and I have an end-to-end vision of solution implementation focused on the achievement of a high-scalable, robust, and fault-tolerant solution.  
I'm familiar with the Infrastructure as a Code (Terraform) concept and used it in my previous projects, Terraform for automating environment setup and deployment of the new features. 

    I am always aiming to write clean and maintainable code. I pay attention to feedback from coworkers to make my code more efficient and do my job better. I am motivated to improve my programming skills and learn project-side technologies and tools. Also, I am interested in extending my knowledge of business processes related to the project.
I am self-organized and result-oriented, have good communication and analytical skills. Good in leading both technical and communication activities required to achieve customer satisfaction.
Looking for a switch to a Big Data engineering role with technical leadership and product responsibilities."
data engineer,"
I have been working for 10 years as a software engineer. Last couple of years occupied leading roles, where I developed the product architecture, set priorities and established work between teams.

I have good knowledge in the designing/implementing of distributed systems.

Also except coding, I'm eager to do DevOps tasks (e.g. writing terraform/cloudformation or CI/CD).

I'm result oriented, have strong attention to details with good communication and leadership skills.
No legacy"
data engineer,"- Regularly created scalable ETL frameworks and pipelines using Apache Beam, Dataflow, PySpark, and designed RESTful APIs for high-load environments.
- Contributed to high-load fintech projects, focusing on treasury software and banking compliance, handling millions of transactions per hour.
- As a Tech Lead, lead vaccination assessment project with complex vaccination schemas.
- Maintained high standards for code quality, utilized the latest Python versions and libraries, and actively contributed to open-source projects.
- Automated the characterization process of mixed-signal integrated circuits, working with both software and hardware.
- Engaged in brain performance measurement, heart rate visual measurement (by webcam), and other fun projects.
Experienced developer and data engineer with over 8 years of specialization in Python, adept at tackling complex tasks spanning various domains. Proficient in building scalable systems across web services, and big data processing. Skilled in SQL with a strong understanding of database design and optimization. Proficient in AWS, GCP infrastructures, and an array of Python frameworks. Committed to delivering robust and optimized code for high performance and scalable solutions.
I'm open to engaging in different projects that adhere to high-quality standards (such as comprehensive test coverage, type annotation, etc.), starting from scratch with Python as the primary tool, or joining a project to lead it to such standards."
data engineer,"I've completed several courses using MS SQL, and PostgreSQL. My recent project was Python pet-project using Flask framework.
I've completed several courses about SQL, Data analysis and Python . I have experience working with various RDBMS (mostly MSSQL and PostgreSQL). 
I'm interested to become  Data engineer.
I'm passionate about working with anything that is connected to data."
data engineer,"COURSES
Datacamp: Data Engineer with Python.
Udemy: Easy to Advanced Data Structure.
Prometheus: Basics of programming CS50 (EdX Harvard course), Basics of Web UI
development.
Coursera: Design thinking for innovations, Learn how to learn.
Codecademy: Introduction to HTML, Learn CSS, Introduction to JavaScript, Learn the
Command Line, Learn Git.
Lviv IT Cluster – Project manager 
February 2021 - November 2021
I worked in an event department, helping our company within the organization of the biggest tech conference in Eastern Europe also coordinated the first and the biggest vaccination centre in Lviv and managed the maintenance of internal events with the Cluster's CEO members. My key achievements are: - Coordination/Managing Tech communities, worked closely with the PR team and CEO 
- Coordinated United for health project
- Coordinated the work of the vaccination centre
- Established communication with clients
- Hunted May Muck as a keynote speaker for IT Arena
- Organized IT Arena from the very beginning (4000+ participants)
- Organize facilities and manage all events details such as decor, catering, entertainment, transportation, Location, invitee list, equipment, promotional material etc;
- Maintenance of CRM
- Technical support of two sites, worked closely with design and tech teams
- Was in charge of payments for services used by the company, worked closely with CAO
- Cooperation with sales and marketing teams to publicize and promote the event

AIESEC – Outgoing Global Volunteer manager
September 2018 - May 2018
- Contacting people who fill out internship applications
- Assistance with project selection
- Communication with the foreign side
- Concluding and signing contracts with applicants
- Been a speaker at AIESEC's PR events 

BlaBlaCar – Customer support 
March 2018 - August 2018
I provided remote audio and Text customer support for company clients.

Self-employed 
March 2017 - November 2017
Worked in the field of distribution.
"
data engineer,"
Trainee Data Engineer

• Finished the theoretical part of a Big Data education plan in a big outsourcing company with practical tasks for each module. 
• This education plan is designed to train new specialists in the most common Big Data tools and solutions(Hadoop, Spark, Spark Streaming, Hive, Kafka, Cloud Computing, Docker, Airflow, NoSQL, ELK, etc.).
• During this course qualified mentors teach you how and when to use these tools.
• To further consolidate our knowledge we implemented these techniques in practical exercises.
I want to develop my skills, gain more hands-on experience and learn something new from seasoned developers to become a high-class specialist."
data engineer,"SoftServe IT Academy, Internship.
Built our own online shop DataBase;
Work with Visual Studio SSIS packages (ETL/ELT);
Built our Data Warehouse and load all data from OLTP to OLAP;
Views, transactions, try/catch blocks, dynamic SQL, JSON.
SoftServe IT Academy, Internship.
Built our own online shop DataBase;
Work with Visual Studio SSIS packages (ETL/ELT);
Built our Data Warehouse and load all data from OLTP to OLAP;
Views, transactions, try/catch blocks, dynamic SQL, JSON.
"
data engineer,"Self-education using learning program from Senior Data Engineer
Courses finished:
SQL Interactive Course by SQL Academy
IT Fundamentals Self-Paced Program by EPAM
Learn Python 3 by CodeAcademy
Construction supervision engineer
OKKO Group/Lviv,Ukraine
Successfully passed the internship and started working, but as a result of the war, the department was closed.

Software used:
AutoCad
Revit
MS Office (Excel, Word)
"
