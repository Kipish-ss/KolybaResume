Category,Resume
data engineer,"–î–æ–ø–æ–º–∞–≥–∞—é –ø–µ—Ä–µ—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ –º–∞—Å–∏–≤–∏ –¥–∞–Ω–∏—Ö —É –∑—Ä–æ–∑—É–º—ñ–ª—ñ —Ç–∞ –¥—ñ—î–≤—ñ —ñ–Ω—Å–∞–π—Ç–∏. –ê —Å–∞–º–µ - c—Ç–≤–æ—Ä—é—é –∑–≤—ñ—Ç–∏ —Ç–∞ –¥–∞—à–±–æ—Ä–¥–∏, —è–∫—ñ —Å–ø—Ä–∏—è—é—Ç—å –ø—Ä–∏–π–Ω—è—Ç—Ç—é –æ–±“ë—Ä—É–Ω—Ç–æ–≤–∞–Ω–∏—Ö —Ä—ñ—à–µ–Ω—å, –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –±—ñ–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—ñ–≤ —Ç–∞ –∑–Ω–∏–∂–µ–Ω–Ω—é –≤–∏—Ç—Ä–∞—Ç.
–©–æ —è —Ä–æ–±–ª—é:
‚Ä¢ –ê–Ω–∞–ª—ñ–∑—É—é, —ñ–Ω—Ç–µ–≥—Ä—É—é –π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º—É—é –¥–∞–Ω—ñ –∑ —Ä—ñ–∑–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª (DWH, –¥–∞—Ç–∞ –º–∞—Ä—Ç–∏ —Ç–æ—â–æ)
‚Ä¢ –†–æ–∑—Ä–æ–±–ª—è—é –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ PL/SQL-–ø—Ä–æ—Ü–µ–¥—É—Ä–∏, —Ñ—É–Ω–∫—Ü—ñ—ó, –ø–∞–∫–µ—Ç–∏, CTEs
‚Ä¢ –°—Ç–≤–æ—Ä—é—é BI-–∑–≤—ñ—Ç–∏ —Ç–∞ —ñ–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ñ –¥–∞—à–±–æ—Ä–¥–∏ —É Power BI, Report Builder
‚Ä¢ –ì–µ–Ω–µ—Ä—É—é ad-hoc –≤–∏–±—ñ—Ä–∫–∏ —Ç–∞ —Ä—ñ—à–µ–Ω–Ω—è –¥–ª—è –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∞–≥—É–≤–∞–Ω–Ω—è –Ω–∞ –±—ñ–∑–Ω–µ—Å-–∑–∞–ø–∏—Ç–∏
‚Ä¢ –°–ø—ñ–≤–ø—Ä–∞—Ü—é—é –∑ –±—ñ–∑–Ω–µ—Å–æ–º ‚Äî —É—Ç–æ—á–Ω—é—é —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑—É—é –≤–∏–º–æ–≥–∏, –¥–æ–ø–æ–º–∞–≥–∞—é –∑ —Ñ–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—î—é –∑–∞–¥–∞—á
‚Ä¢ –ù–∞–¥–∞—é –ø—ñ–¥—Ç—Ä–∏–º–∫—É –¥–ª—è –∫—ñ–Ω—Ü–µ–≤–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏
üß∞ –ú—ñ–π —Ç–µ—Ö–Ω—ñ—á–Ω–∏–π —Å—Ç–µ–∫:
–ë–∞–∑–∏ –¥–∞–Ω–∏—Ö —Ç–∞ –º–æ–≤–∏: Oracle, SQL, PL/SQL, DAX
–Ü–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏: PL/SQL Developer, TOAD, SQL Developer
BI-–≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è: Power BI, Report Builder
–î–æ–¥–∞—Ç–∫–æ–≤–æ:
‚Ä¢ –î–æ—Å–≤—ñ–¥ —Ä–æ–±–æ—Ç–∏ –∑ –¥–∞—Ç–∞-–º–∞—Ä—Ç–∞–º–∏, –±—ñ–ª—ñ–Ω–≥–æ–≤–∏–º–∏ –¥–∂–µ—Ä–µ–ª–∞–º–∏, ETL-–ø—Ä–æ—Ü–µ—Å–∞–º–∏, –¥–æ—Å–≤—ñ–¥ –∑ –ø—Ä–æ–µ–∫—Ç—É–≤–∞–Ω–Ω—è –ë–î
‚Ä¢ Agile-–º–µ—Ç–æ–¥–æ–ª–æ–≥—ñ—ó, CI/CD, —Ä–æ–±–æ—Ç–∞ –∑ Jira
‚Ä¢ –ù–∞–≤–∏—á–∫–∏ —É Delphi (Object Pascal), HTML/CSS, JavaScript
–¶—ñ–∫–∞–≤–ª—è—Ç—å –¥–æ–≤–≥–æ—Å—Ç—Ä–æ–∫–æ–≤—ñ –ø—Ä–æ—î–∫—Ç–∏, –¥–µ –º–æ–∂–Ω–∞ —Ä–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –ø–æ–≤–Ω–∏–π —Ü–∏–∫–ª –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏ ‚Äî –≤—ñ–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –¥–æ –≤–ø–ª–∏–≤—É –Ω–∞ –±—ñ–∑–Ω–µ—Å-—Ä—ñ—à–µ–Ω–Ω—è.
------------------------------------------------------------------
I help telecom companies turn raw data into clear, actionable insights. Through precise analysis, scalable database solutions, and flexible BI reporting, I empower businesses to make data-driven decisions, optimize operations, and reduce costs.
What I do:
‚Ä¢ Analyze, integrate, and transform data from various sources (DWH, data marts, billing systems)
‚Ä¢ Design database structures (tables, views, CTEs);
‚Ä¢ Develop efficient PL/SQL procedures, functions, and packages;
‚Ä¢ Build BI reports and dashboards using Power BI, Report Builder
‚Ä¢ Create ad-hoc queries to quickly solve operational challenges
‚Ä¢ Collaborate with business teams to clarify and optimize requirements
‚Ä¢ Support and consult end-users on analytics tools and reports
üß∞ Tech Stack:
Databases & Languages: Oracle, SQL, PL/SQL, DAX
Tools: PL/SQL Developer, TOAD, SQL Developer
BI & Visualization: Power BI, Report Builder
Additionally:
‚Ä¢ Hands-on experience with data marts, billing systems, and ETL processes
‚Ä¢ Familiar with Agile methodology, CI/CD pipelines, and Jira
‚Ä¢ Skills in Object Pascal (Delphi), HTML/CSS, JavaScript
Looking for long-term, impactful projects where I can contribute across the full data lifecycle ‚Äî from source to strategy.
‚Ä¢	Managed and maintained 30+ recurring reports for a customer base of over 30 million users.
‚Ä¢	Implemented automated monthly reward calculations for company partners, optimizing financial workflows and reducing manual workload.
‚Ä¢ Opportunities for professional growth
‚Ä¢ Competitive financial conditions with a currency peg
‚Ä¢ Long-term projects
‚Ä¢ Transparent terms and regular Salary reviews"
data engineer,"Data Engineer/ Backend Engineer                                                                                                           Sep 2024 ‚Äì Jan 2025
Built the ETL-pipeline to onboard football clubs item-to-sell data from different sources to the unified magento server
Implemented Category matching microservice to handle different categories from different sources
Connected third-part API from sellers to the service
API documentation
Technologies: Luigi(Airflow), Python Redis, MongoDB, NodeJS, GraphQL
Data Engineer                                                                                                                                 Jun 2023 ‚Äì September 2024
Developed designed and maintaining ETL processes
Optimized data processing and storage for efficiency and scalability (over 7million rows of data) for fast update and access;
Developed and maintained ETL processes to extract, transform, and load data into various systems;
Covered with tests data processing part
Ensured fault tolerance for high load part of the system up to 97%
Optimized CI/CD pipeline capacity and throughput
1st place at Black Sea Science Blockchain conference in Information Technology. Developed(with team of 3) model and software foundament to blochain-based e-voting to choose the head of the student council. I was responsible for transactions
Looking for a long-term employment. Have experience with fully English-speaking international team. Would be appreciated to work on initiative-based team"
data engineer,"–ü—Ä—Ü—é–≤–∞–≤ —É –∫–æ–º–ø–∞–Ω—ñ—ó Jabil —è–∫ Trainee Programer Anslyst.
–£ –∫–æ–º–ø–∞–Ω—ñ—ó –∑–∞–π–º–∞–≤—Å—è:
- –°—Ç–≤–æ—Ä–µ–Ω–Ω—è–º windows service –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º C# –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª—É HR;
- –£–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–Ω—è –ø—Ä–æ–µ–∫—Ç—É –Ω–∞ ASP.NET (bag fixing, —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö —Ñ–æ—Ä–º);
- –ó–¥–æ–±—É–≤–∞–≤ –∑–∞–≥–∞–ª—å–Ω–∏–π –¥–æ—Å–≤—ñ–¥ —Ä–æ–±–æ—Ç–∏ —É —Å—Ñ–µ—Ä—ñ IT.
1. –ê–Ω–∞–ª—ñ–∑ –¥–∞–Ω–∏—Ö —Ç–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è:
–ú–∞—é –¥–æ—Å–≤—ñ–¥ —Ä–æ–±–æ—Ç–∏ –∑ –º–æ–≤–æ—é –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è R –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –¥–∞–Ω–∏—Ö —Ç–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤.
–í–æ–ª–æ–¥—ñ—é –Ω–∞–≤–∏—á–∫–∞–º–∏ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è –Ω–∞ Python —Ç–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –π–æ–≥–æ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –¥–∞–Ω–∏—Ö —Ç–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó.
–ó–¥–∞—Ç–Ω—ñ—Å—Ç—å –¥–æ —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Ç–∞ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –∫–ª–∞—Å–∏—á–Ω–∏—Ö –º–µ—Ç–æ–¥—ñ–≤ Deep Learning –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –¥–∞–Ω–∏—Ö —Ç–∞ —Ä–æ–∑–≤'—è–∑–∞–Ω–Ω—è –∑–∞–≤–¥–∞–Ω—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è.
2. –†–æ–±–æ—Ç–∞ –∑ –±–∞–∑–∞–º–∏ –¥–∞–Ω–∏—Ö:
–ú–∞—é –¥–æ—Å–≤—ñ–¥ —Ä–æ–±–æ—Ç–∏ –∑—ñ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è–º –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤ —Ç–∞ —Ä–æ–±–æ—Ç–æ—é –∑ –±–∞–∑–∞–º–∏ –¥–∞–Ω–∏—Ö, –≤–∫–ª—é—á–∞—é—á–∏ Big Data SQL —Ç–∞ —ñ–Ω—à—ñ.
–ó–¥–∞—Ç–Ω—ñ—Å—Ç—å –¥–æ —Ä–æ–±–æ—Ç–∏ –∑ –≤–µ–ª–∏–∫–∏–º–∏ –æ–±—Å—è–≥–∞–º–∏ –¥–∞–Ω–∏—Ö —Ç–∞ —ó—Ö –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏.
3. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π–Ω—ñ —Å–∏—Å—Ç–µ–º–∏:
–í–æ–ª–æ–¥—ñ—é –¥–æ—Å–≤—ñ–¥–æ–º —Ä–æ–±–æ—Ç–∏ –∑ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π–Ω–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏, –≤–∫–ª—é—á–∞—é—á–∏ Collaborative Filtering, Model-based —Ç–∞ Content-based –º–µ—Ç–æ–¥–∏.
–ó–¥–∞—Ç–Ω—ñ—Å—Ç—å –¥–æ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –ø–æ—Ç—Ä–µ–± –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ —Ç–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π.
4. –Ü–Ω—à—ñ –Ω–∞–≤–∏—á–∫–∏:
–î–æ—Å–≤—ñ–¥ —Ä–æ–±–æ—Ç–∏ –∑ Google Apps Script –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó –∑–∞–≤–¥–∞–Ω—å —Ç–∞ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—ó –∑ —ñ–Ω—à–∏–º–∏ Google —Å–µ—Ä–≤—ñ—Å–∞–º–∏.
–ï—Ñ–µ–∫—Ç–∏–≤–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –¥–∞–Ω–∏—Ö —Ç–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –∑ –º–µ—Ç–æ—é –Ω–∞–¥–∞–Ω–Ω—è —ñ–Ω—Å–∞–π—Ç—ñ–≤ —Ç–∞ –ø—Ä–∏–π–Ω—è—Ç—Ç—è —Ä—ñ—à–µ–Ω—å.
–ú–∞—é —Ä–æ–∑—É–º—ñ–Ω–Ω—è –≤ –û–û–ü –ø—ñ–¥—Ö–æ–¥—ñ —Ç–∞ –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—é –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö –º–æ–≤–∞—Ö.
1)–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–µ–±-–¥–æ–¥–∞—Ç–∫—É –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Node.js (—ñ–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ—ó –±–∞–∑–∏ –¥–∞–Ω–∏—Ö), —â–æ–± –ø–µ—Ä–µ–≤—ñ—Ä—è—Ç–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ñ —Å–≤—è—Ç–∞, –∞–±–æ –¥–Ω—ñ –Ω–∞—Ä–æ–¥–∂–µ–Ω–Ω—è –¥—Ä—É–∑—ñ–≤;
2)–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç –º–∞–≥–æ–∑–∏–Ω—É –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ PERN, –∑ –º–æ–∂–ª–∏–≤—ñ—Å—Ç—é —Ä–µ—î—Å—Ç—Ä–∞—Ü—ñ—ó —Ç–∞ —Ä–æ–ª—è–º–∏ (–ê–¥–º—ñ–Ω —ñ –ø–æ–¥—ñ–±–Ω–µ)
3)–ü—Ä–æ—Å—Ç—ñ –≤–µ–±-—Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º HTML, CSS, JavaScript;
4)–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¢–µ–ª–µ–≥—Ä–∞–º –∫–∞—Ç–∞–ª–æ–≥—É –¥–ª—è —Å–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∫–∞–Ω–∞–ª—ñ–≤ —Ç–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è –¥–ª—è –º–æ–¥–µ—Ä–∞—Ü—ñ—ó. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è Telegram API –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ –∫–∞–Ω–∞–ª.
–ù–∞–≤—á–∞–Ω–Ω—è —ñ –ø—ñ–¥—Ç—Ä–∏–º–∫–∞: –û—á—ñ–∫—É—é, —â–æ —è–∫ —Å—Ç–∞–∂–µ—Ä-–º–æ–ª–æ–¥—à–∏–π —Ä–æ–∑—Ä–æ–±–Ω–∏–∫, –æ—Ç—Ä–∏–º–∞—é –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –Ω–∞–≤—á–∞—Ç–∏—Å—è –≤—ñ–¥ –¥–æ—Å–≤—ñ–¥—á–µ–Ω–∏—Ö —Ä–æ–∑—Ä–æ–±–Ω–∏–∫—ñ–≤ —ñ –º–∞—Ç–∏–º—É –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–∞, —è–∫–∏–π –¥–æ–ø–æ–º–æ–∂–µ –º–µ–Ω—ñ —É —Ä–æ–∑—É–º—ñ–Ω–Ω—ñ –ø—Ä–æ—Ü–µ—Å—É —Ä–æ–∑—Ä–æ–±–∫–∏ —Ç–∞ —Ä–æ–∑–≤–∏—Ç–∫—É –º–æ—ó—Ö –Ω–∞–≤–∏—á–æ–∫. –í—ñ–¥–∫—Ä–∏—Ç—ñ—Å—Ç—å –∫–æ–º–∞–Ω–¥–∏ –¥–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è —ñ –Ω–∞–¥–∞–Ω–Ω—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–≥–æ —Ñ—ñ–¥–±–µ–∫—É —î –¥—É–∂–µ —Ü—ñ–Ω–Ω–æ—é –¥–ª—è –º–µ–Ω–µ.
–†–æ–∑—É–º—ñ–Ω–Ω—è –±—ñ–∑–Ω–µ—Å—É: –ë–∞–∂–∞–Ω–æ –º–∞—Ç–∏ –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –±–ª–∏–∂—á–µ –ø–æ–∑–Ω–∞–π–æ–º–∏—Ç–∏—Å—è –∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –±—ñ–∑–Ω–µ—Å—É —Ç–∞ –º–µ—Ç–æ—é –ø—Ä–æ–µ–∫—Ç—É. –†–æ–∑—É–º—ñ–Ω–Ω—è —Ç–æ–≥–æ, —è–∫ –º–æ—ó –∑–∞–≤–¥–∞–Ω–Ω—è —Å–ø—Ä–∏—è—é—Ç—å –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—é —Ü—ñ–ª–µ–π –æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—ó, –¥–æ–ø–æ–º–æ–∂–µ –º–µ–Ω—ñ –∫—Ä–∞—â–µ –æ—Ä—ñ—î–Ω—Ç—É–≤–∞—Ç–∏—Å—è –≤ —Ä–æ–∑—Ä–æ–±—Ü—ñ —ñ –ø—Ä–∏–π–º–∞—Ç–∏ –±—ñ–ª—å—à –æ–±“ë—Ä—É–Ω—Ç–æ–≤–∞–Ω—ñ —Ä—ñ—à–µ–Ω–Ω—è.
–†—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å –∑–∞–≤–¥–∞–Ω—å: –¶—ñ–∫–∞–≤–æ—é –¥–ª—è –º–µ–Ω–µ –±—É–¥–µ –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –Ω–∞–¥ —Ä—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω–∏–º–∏ –∑–∞–≤–¥–∞–Ω–Ω—è–º–∏, —è–∫—ñ –¥–æ–∑–≤–æ–ª—è—Ç—å –º–µ–Ω—ñ –≤–∏–≤—á–∞—Ç–∏ –Ω–æ–≤—ñ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó —Ç–∞ —Ä–æ–∑—à–∏—Ä—é–≤–∞—Ç–∏ —Å–≤–æ—ó –Ω–∞–≤–∏—á–∫–∏ —É —Ä–æ–∑—Ä–æ–±—Ü—ñ –ø—Ä–æ–≥—Ä–∞–º–Ω–æ–≥–æ –∑–∞–±–µ–∑–ø–µ—á–µ–Ω–Ω—è. –†—ñ–∑–Ω–æ–º–∞–Ω—ñ—Ç–Ω—ñ—Å—Ç—å –ø—Ä–æ–µ–∫—Ç—ñ–≤ –¥–æ–ø–æ–º–æ–∂–µ –º–µ–Ω—ñ –∑—Ä–æ–∑—É–º—ñ—Ç–∏, —è–∫–∞ –≥–∞–ª—É–∑—å —Ä–æ–∑—Ä–æ–±–∫–∏ –º–µ–Ω—ñ –±—ñ–ª—å—à–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å —ñ –¥–µ —è —Ö–æ—Ç—ñ–≤ –±–∏ —Ä–æ–∑–≤–∏–≤–∞—Ç–∏—Å—è –¥–∞–ª—ñ."
data engineer,"While studying at the university, I fundamentally studied the programming languages ‚Äã‚Äã‚Äã‚ÄãPython and C++, and also gained knowledge in the field of algorithms and computational methods. My educational program included the study of discrete mathematics, which is a key tool for developing effective algorithms and data structures. In addition, I gained experience in data analysis using the Python language, which allowed me to process and interpret information to make informed decisions. All this experience at the university formed a solid foundation for me in the field of programming, algorithms and data analysis, which I am ready to apply in my future work and i also had experience in passing Tensor Flow ML course.
Stack: python, c++, django, javascript(html, css), git, SQL(MySQL, MSSQL, Oracle, Firebird)"
data engineer,"RPS
Lead Engineer  (Oct 2022 ‚Äì now)
‚Ä¢	Optimization of existing processes and procedures
‚Ä¢	Creating new pipelines,modeles,views and drivers (Django + AWS)
‚Ä¢	Creating unit-tests for our drivers(PyTest+python)
‚Ä¢	Technical support storage
‚Ä¢	Writing and optimization drivers, functions
‚Ä¢	Kept project leaders regularly updated with progress, maintaining open, productive communication.
‚Ä¢	Managed multiple projects with differing technologies, including delegating tasks, assessing quality and sign off.
Lead Engineer  (June 2022 ‚Äì now)
‚Ä¢	Optimization of existing processes and procedures
‚Ä¢	Creating new pipelines and drivers (Java + AWS)
‚Ä¢	Creating unit-tests for our drivers(java spring+springBoot)
‚Ä¢	Technical support storage
‚Ä¢	Writing and optimization drivers, functions
‚Ä¢	Kept project leaders regularly updated with progress, maintaining open, productive communication.
Software Engineer  (April 2021 ‚Äì May 2022)
‚Ä¢	Optimization of existing processes and procedures
‚Ä¢	Creating new pipelines and drivers (python+Airflow+spark+AWS)
‚Ä¢	Creating unit-tests for our data-drivers(scala)
‚Ä¢	Creating a new data-notebooks(spark)
‚Ä¢	Technical support storage
‚Ä¢	Big Data migration process development
‚Ä¢	Writing and optimization views, stored procedures, and functions
Intetics
Software Engineer (September 2020 ‚Äì April 2021)
‚Ä¢	ETL process development
‚Ä¢	Preparation of data for building reports
‚Ä¢	Data migration process development
‚Ä¢	Writing and optimization views, stored procedures, and functions, construction and support of storage
EPAM Systems Inc
Software Engineer (August  2019 ‚Äì September  2020)
‚Ä¢	data collection from various sources: Source DB (Oracle, Postgres); different api (Beamery api,GoogleAnalytics api and other); flat files (exel, csv, json)
‚Ä¢	pre-processing of data before loading into the DWH (the storage is built in MS
SQL server)
‚Ä¢	creation and configuration of loading processes in the DWH (using SSIS packages and
services, python-scripts, PowerShell-scripts and .NET Application)
‚Ä¢	the creation of procedures, functions and scripts for data transformation for their preparation for
reports
‚Ä¢	development, support of storage performance
‚Ä¢	optimization of existing processes and procedures
GlowByte Consulting
Software Engineer (May  2018 ‚Äì July  2019)
‚Ä¢	ETL process development
‚Ä¢	Preparation of data for building reports
‚Ä¢	Data migration process development"
data engineer,"20+  years of experience as DBA and DB developer(Oracle, MSSQL, Postgres, ...)
T—Ähe last 5+ years worked as Big Data Engineer(GCP, AWS, Oracle Cloud)
Certified GCP Data Engineer+Cloud Developer, Databricks Data Engineer, Oracle DBA+Java
Languages: English, French
Location: Kyiv, Ukraine
Priority:
1) Projects with Databricks(the Salary isn‚Äôt the main factor here)
2) Big Data projects"
data engineer,"Versatile Python Developer and Data Engineer with 5+ years of experience designing and implementing
robust data pipelines, scalable backend systems, and data-driven solutions across multiple industries.
Strong expertise in Django, FastAPI, and data technologies including Apache Spark, Kafka, and Airflow.
Proven track record of optimizing performance, implementing best practices, and delivering high-quality
solutions while collaborating effectively with cross-functional teams. Fluent in English, Russian, and
Ukrainian with a commitment to continuous learning and staying current with emerging technologies.
My patience, ability to solve complex tasks and learn new techologies."
data engineer,"–ó–∞—Ä–∞–∑ –ø—Ä–∞—Ü—é—é —É –≤—ñ–¥–¥—ñ–ª—ñ —è–∫–∏–π —Ä–æ–∑–≤–∏–≤–∞—î DWH, –∑–∞–π–º–∞—é—Å—è —Ä–æ–∑—Ä–æ–±–∫–æ—é —ñ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é ETL/ELT –ø—Ä–æ—Ü–µ—Å—ñ–≤ –¥–ª—è –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–∏—Ö —ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ó—Ö –≤ –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–∏—Ö –∑–≤—ñ—Ç–∞—Ö. –ü–æ–≤—Å—è–∫–¥–µ–Ω–Ω—ñ –∑–∞–≤–¥–∞–Ω–Ω—è —Ü–µ –∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—è –∑ ""–¥–∂–µ—Ä–µ–ª–∞–º–∏"", —Ä–æ–∑—Ä–æ–±–∫–∞ –Ω–æ–≤–æ–≥–æ, –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ —ñ—Å–Ω—É—é—á–æ–≥–æ, –ø–æ—à—É–∫ –Ω–æ–≤–∏—Ö —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ —ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ —ó—Ö –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è –ø—ñ–¥ –Ω–∞—à—ñ –∑–∞–≤–¥–∞–Ω–Ω—è.
–ó —Ç–∏—Ö —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ–π —è–∫—ñ –ø—Ä–∏–Ω–µ—Å–ª–∏ –º–µ–Ω—ñ –º–∞–∫—Å–∏–º—É–º –∑–∞–¥–æ–≤–æ–ª–µ–Ω–Ω—è —Ü–µ:
- –ø–æ–≤–Ω–∏–π –ø–µ—Ä–µ—ó–∑–¥ —É—Å—ñ—Ö ETL –ø—Ä–æ—Ü–µ—Å—ñ–≤ –Ω–∞ —ñ–Ω—à–∏–π —Å—Ç–µ–∫.
- —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è ELT –∑ GA
- –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è RabbitMQ –¥–ª—è –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏—Ö –ø—Ä–æ—Ü–µ—Å—ñ–≤
- –≤–ø—Ä–æ–≤–∞–¥–∏–≤ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è NiFi
–í –∑–∞–≥–∞–ª—å–Ω–æ–º—É –≤—Å–µ –º–∞–±—É—Ç—å –¥–æ—Å–∏—Ç—å –∑–≤–∏—á–Ω–æ, –¥–æ–±—Ä–∏–π –∫–æ–ª–µ–∫—Ç–∏–≤, –ø—ñ–¥—Ç—Ä–∏–º–∫—É –∑ –±–æ–∫—É –∫–µ—Ä—ñ–≤–Ω–∏—Ü—Ç–≤–∞, –ø–æ—Ä—è–¥–Ω—ñ—Å—Ç—å –≤—ñ–¥ –ø—ñ–¥–ø—Ä–∏—î–º—Å—Ç–≤–∞, –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –±—ñ–ª—å—à—É —á–∞—Å—Ç–∏–Ω—É –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –≤ –æ—Ñ—ñ—Å—ñ —ñ –ø–µ—Ä—ñ–æ–¥–∏—á–Ω–æ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –≤—ñ–¥–¥–∞–ª–µ–Ω–æ."
data engineer,"DataArt Solutions, Inc., Data Engineer 2021 ‚Äì Present
-	Work with master data
-	Design table architecture and investigate data
-	Data migration;
-	Write, maintain and optimize queries and stored procedures
-	Redesign data structure and tables to ensure performance & optimization
Sep 2020 ‚Äì 2021: Data Analyst
‚àí Data monitoring and analysis;
‚àí Investigation of issues root causes;
‚àí Generating reports and analysis from single or multiple DWH;
‚àí Develop and analyze the effectiveness of methodology and scenarios of RnD, analyze their results;
‚àí Create PowerBI dashboards and other visualizations;
‚àí Ad-hoc analysis;
‚àí Analyse overall business data, draw insights, and prepare reports in a manner that can be understood by all stakeholders.
July 2019 ‚Äì Sep 2020: Business Analyst
‚àí Creation marketing Email campaigns;
‚àí Provide Marketing department with deep insights regarding all marketing activities;
‚àí Building forecasts and predictive customer behavioral resume_classifier;
‚àí Data analysis to detect the reasons for deviations in the main and secondary metrics;
‚àí Support in decision-making
‚àí Create visualizations and presenting them to the stakeholders.
May 2018 ‚Äì July 2019: Dispatch
‚àí Loads searching and booking;
‚àí Cargo orders monitoring, reports creation;
‚àí Solving issues with loads;
‚àí Communication with brokers and drivers, updates provide;
‚àí Construction logistics chain providing for each specific driver.
October 2016 ‚Äì May 2018 Reporting Specialist
‚àí Logistics reports creation and coordination (Service Level, Out Of Stock);
‚àí Automation of reporting projects;
‚àí Ad-hoc requests from other departments;
‚àí Development of tools for new reports to decrease Full-Time Equivalent;
‚àí VBA Macro tools support and modernization
I have learned Python, VBA, SQL on my own, improved the level of English in short period.
- I want to work in a friendly team,
- Learn something new,
- Always work on interesting and challenging projects,
- Develop new methods and approaches, to solve the task."
data engineer,"–¢–û–í ""–ù–æ–≤–∞–ü–µ–π"" (2024.04 ‚Äî —Ç–µ–ø–µ—Ä)
–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—è –∑–≤—ñ—Ç–Ω–æ—Å—Ç—ñ, –ø–æ—à—Ç–æ–≤–∏—Ö —Ä–æ–∑—Å–∏–ª–æ–∫ —Ç–∞ –æ–±—Ä–æ–±–∫–∏ –¥–∞–Ω–∏—Ö (Python + Gmail API, Google Sheets API, GitLab CI/CD). –ü–æ–±—É–¥–æ–≤–∞ –¥–∞—à–±–æ—Ä–¥—ñ–≤ —É Power BI, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—è –∫–ª—ñ—î–Ω—Ç—ñ–≤, –Ω–∞–ø–∏—Å–∞–Ω–Ω—è SQL-–∑–∞–ø–∏—Ç—ñ–≤ —ñ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—ó.
–ê–¢ –ö–ë ""–ü–†–ò–í–ê–¢–ë–ê–ù–ö"" (2021.03 ‚Äî 2024.03)
–ê–Ω–∞–ª—ñ–∑ –∫–ª—ñ—î–Ω—Ç—Å—å–∫–æ—ó –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ, –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –ø–æ–≤–µ–¥—ñ–Ω–∫–æ–≤–∏—Ö –ø–∞—Ç–µ—Ä–Ω—ñ–≤, –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –≤–∞–ª—é—Ç–Ω–∏—Ö –æ–ø–µ—Ä–∞—Ü—ñ–π, –æ—Ü—ñ–Ω–∫–∞ —Ä–∏–∑–∏–∫—ñ–≤ —Ç–∞ –≤–∏—è–≤–ª–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ–π —É —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ—è—Ö (SQL, –∞–Ω–∞–ª—ñ—Ç–∏–∫–∞, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è).
–¢–û–í ""–ê–¢–ë-–ú–∞—Ä–∫–µ—Ç"" (2020.08 ‚Äî 2021.02)
–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è, –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∏–π –∞–Ω–∞–ª—ñ–∑, –∞–Ω–∞–ª—ñ—Ç–∏–∫–∞ –ø–æ–ø–∏—Ç—É —Ç–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó —â–æ–¥–æ —Ü—ñ–Ω–æ—É—Ç–≤–æ—Ä–µ–Ω–Ω—è.
–û—Å–æ–±–∏—Å—Ç–∏–π –ø—Ä–æ—î–∫—Ç (2024.10 ‚Äî —Ç–µ–ø–µ—Ä)
–†–æ–∑—Ä–æ–±–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–æ–≤–∞–Ω–æ—ó —Å–∏—Å—Ç–µ–º–∏ –∑–±–æ—Ä—É –Ω–æ–≤–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É (web scraping), —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –∑ OpenAI API (–≥–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—ñ–≤), Telegram API (–ø—É–±–ª—ñ–∫–∞—Ü—ñ—è –ø–æ—Å—Ç—ñ–≤), –¥–µ–ø–ª–æ–π —É —Å–µ—Ä–µ–¥–æ–≤–∏—â—ñ GCP (Compute Engine + Airflow). –î–∞–Ω—ñ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è –≤ BigQuery/GC Storage.
–†–µ–∞–ª—ñ–∑—É–≤–∞–ª–∞ –ø–æ–≤–Ω—ñ—Å—Ç—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–æ–≤–∞–Ω–∏–π –Ω–æ–≤–∏–Ω–Ω–∏–π Telegram-–∫–∞–Ω–∞–ª —ñ–∑ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—î—é –∫–æ–Ω—Ç–µ–Ω—Ç—É —á–µ—Ä–µ–∑ –®–Ü (ChatGPT)
–ü–æ–±—É–¥—É–≤–∞–ª–∞ ETL-–ø–∞–π–ø–ª–∞–π–Ω –Ω–∞ GCP (Compute Engine) –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º Airflow, BigQuery, GC Storage
–û–ø—Ç–∏–º—ñ–∑—É–≤–∞–ª–∞ –ø—Ä–æ—Ü–µ—Å–∏ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏ —Ç–∞ –∑–≤—ñ—Ç–Ω–æ—Å—Ç—ñ –≤ –∫–æ–º–ø–∞–Ω—ñ—ó –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Python-–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó
–•–æ—á—É –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –∑ –ø—Ä–æ—î–∫—Ç–∞–º–∏, —è–∫—ñ –ø–µ—Ä–µ–¥–±–∞—á–∞—é—Ç—å –∞–Ω–∞–ª—ñ—Ç–∏–∫—É, data engineering, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—é –ø—Ä–æ—Ü–µ—Å—ñ–≤ –∞–±–æ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—é –∑ –®–Ü. –¶—ñ–∫–∞–≤–∏—Ç—å –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Å—É—á–∞—Å–Ω–æ–≥–æ —Å—Ç–µ–∫—É: Python, GCP, Airflow, Pandas, SQL, Machine Learning."
data engineer,"Freelance 2018-08-01 - 2021-08-30
Python and Web-developer
On freelancing, I took orders for front-end development (HTML/CSS/JS), the development of telegram bots (Python/PyTelegramBotAPI) and web-sites (Flask/Django).
Alnicko 2021-08-30 - 2022-05-21
Python Developer
I am writing an interface (PyQt5) and a backend part(Python, Mosquito, InfluxDB, SQLite, Flask API, etc..) for electronic devices, deploying them on boards (Raspberry PI, Docker, Bash). I also write unit tests for all of this.
Epam 2022-05-21 - now
Data Engineer (Python)
As a data engineer I am working on developing pipeline (ADF) and notebooks (Databricks/Spark), performance optimization and investigating new development approaches.
Mate Academy 2023-01-27 - now
Mentor (Python)
As a mentor I am helping students with questions regarding tasks on Python, checking homeworks and conducting teck-check (test interviews) for them.
I would like to get into a company with a good, friendly and responsive team, which gives you the opportunity to ""grow"" and develop in your favorite business."
data engineer,"I worked in such domains: telecommunications, big data, agro, data engineering, marketing and iGaming. I have experience in developing scalable services from zero and ongoing development, developing monolith, microservices and migration of monolith to microservices, implementing async code, working with integrations, debbuging and optimazing apps for performance, deploying and participation in code reviews.
‚Ä¢ Languages: Python, Javascript
‚Ä¢ Frameworks: FastAPI, Django, Flask, aiohttp
‚Ä¢ Cloud Providers: AWS, GCP
‚Ä¢ Databases: PostgreSQL(PostGIS), MongoDB, Apache Cassandra, Redis, ElasticSearch, QLDB
‚Ä¢ API Technologies: REST API, GraphQL
‚Ä¢ Infrastructure, Orchestration & DevOps: Docker, Kubernetes, Airflow, Terraform, AWS CloudFormation
‚Ä¢ CI/CD: GitLab CI, GitHub Actions, AWS Pipeline
‚Ä¢ Monitoring & Logging: Kibana, Grafana, New Relic, Datadog, Sentry
‚Ä¢ Testing: unittests, pytest, locust
‚Ä¢ Other: Git, Bash, Auth0, Celery, Swagger, Asyncio, Jira, Confluence
- Developed a high-load data parsing solution capable of handling data from 10 million TV boxes.
- Developed a visualization tool for connections between boxes that is utilized by a network of 5,000 customer support representatives.
- Designed a scalable big data solution using Apache Cassandra, optimizing data modeling and query performance to efficiently retrieve necessary information for critical business insights.
- Built a robust backend system for a mobile application that serves thousands of clients.
- I have received 20 different certifications from various well-known companies, showcasing my expertise and dedication to my field, like AWS Solution Architect Professional and GCP Cloud Architect."
data engineer,"I have experience in creating companies' Top and Middle management  BI Reporting and Data ecosystem from scratch.
Also I can help you setup regular reporting connected to you company valuation model or financial model. And then you and your management will be able to track performance, and act based on true actual information and company's goals.
At this point I'm also interested in combination of Data Engineer and AI Engineer roles to be able create more value for the companies.
Created BI solution to track companies' KPI's from scratch.
I may create value  for the company by helping investors, C-level and management have a clear view of company in digits.
Also I'm interested in combination of Data Engineer and AI Engineer roles to bring new value to the startups and companies."
data engineer,"I have worked with different technologies. Ranging from simple front end web pages, ending with infrastructure. But I can say most of my experience is related with backend development, mainly microservices
In case of technical challenge I have developed Ip blocking system(business logic based) . But most difficult situation I was ever in is about fixing a bug in production without knowing anything about the project and what the bug is about. (project hosted in kubernetes and no access to repo)
I expect an environment where I can have professional growth and not prone to unnecessary stress."
data engineer,"Data Analyst | Data Engineer with a background in Frontend Development.
Core Skills: SQL, Excel, Python & libraries, Power BI, Tableau, Machine Learning Basics
Relevant experience from Frontend Development:
- Optimized web performance, improving load speed and user experience.
- Implemented GSAP animations, reducing rendering time by ~20%.
- Collaborated with backend and design teams, reducing project delivery time by ~15%.
- Refactored and debugged projects, improving overall code efficiency.
- Completed IBM Data Science Specialization, applying SQL, Python, and statistical modeling for data analysis and process automation.
- Developed analytical SQL queries, enabling efficient job market analysis for Data Analytics roles (personal project).
- Optimized data processing by leveraging PostgreSQL and pipeline automation, improving analysis speed by ~30%.
I'm looking for a role in Data Analytics or Data Engineering where I can apply my technical background and problem-solving mindset."
data engineer,"* Advanced skills in MS Excel, particularly with Pivot Tables, Power Query, and various data visualization techniques.
* Extensive experience in SQL, including developing queries and performing data manipulation tasks such as inserting, updating, and deleting data rows.
* Deep understanding of task management software like Jira and Asana.
* Proficient in Business Intelligence tools such as MS Power BI, with a strong focus on DAX and Power Query for crafting insightful reports.
* Competent in using external Power BI tools like Tabular Editor 2, Tabular Editor 3, and DAX Studio for fine-tuning data resume_classifier and optimizing performance.
* Skilled in Power BI administration, including managing Workspaces, Gateway connections, update scheduling, and safeguarding data integrity.
* Extensive practice with Power BI Dataflows, MS Power Automate, and applying Row-Level Security (RLS) to ensure data control.
* Solid grounding in Python, with experience using PyCharm and Jupyter Notebook for automating workflows and conducting data analysis.
* Knowledgeable in MS Dynamics 365 for Finance and Operations.
* Created a new analytics system for the company, significantly improving the speed and accuracy of data reporting.
* Streamlined financial reporting through automation, cutting down on processing time and minimizing errors.
* Designed and implemented a forecasting tool that enhanced the quality of business decision-making."
data engineer,"Skills:
- Streamlining workflows and reporting in analytics tools like Google Analytics (GA4), Heap, Amplitude, and Mixpanel.
- Setting up and fine-tuning analytics for mobile apps using platforms such as AppsFlyer, AppMetrica, Firebase, and Facebook Analytics.
- Developing technical specifications for integrating analytical services, CRM systems, and third-party tools (K50, OWOX BI, Exponea, User.com, Hubspot, MailerLite, MailChimp, Zapier, etc.).
- Experience with Python for data extraction and processing via API for ETL pipelines, frequently using libraries such as JSON, requests, pandas, numpy, and client libraries (Google Cloud, Google Search Console, Twilio, Crisp, Meta, Bing).
- Conducting detailed analysis of advertising campaigns based on multichannel attribution data to drive strategic insights.
- Creating clear and effective data and report visualizations using tools like Google Looker Studio, Tableau, and Power BI.
- Experienced user of task management systems like Jira, Trello, Redmine, and YouTrack to manage projects efficiently.
- Planning, budgeting, and executing marketing activities with precision.
- Collaborating with external agencies, including advertising and analytics partners, to achieve project outcomes.
- Impartially evaluating the effectiveness of advertising campaigns to optimize performance.
- Customizing and optimizing contextual and targeted advertising strategies for better engagement.
- Experience working with databases like PostgreSQL and MariaDB, as well as data warehouses such as Google BigQuery.
- Proficiency in CSS, HTML, JavaScript, SQL, and Python for data manipulation and web analytics.
- Managing virtual machines on cloud platforms like Google Cloud and AWS (Debian, Ubuntu, Amazon Linux environments).
- Setting up cloud functions (Google Cloud Functions, AWS Lambda) for automating data tasks.
- Setting up and managing conversions and goals for site analytics using tools like Google Tag Manager, Google Analytics, Facebook Pixel, and Segment for precise tracking and reporting.
Big data analyzed with razor-sharp accuracy."
data engineer,"Hi there! I‚Äôm a data expert who creates complete data ecosystems and optimizes digital infrastructures. I have a proven track record of building reporting systems, implementing cutting-edge data platforms, and improving data flow efficiency to fuel growth and transformation. My skill set includes data analysis, modeling, and ETL processes, and I have expertise in T-SQL and Python, using tools like Pandas and PySpark. I‚Äôm experienced with Azure Synapse, Google BigQuery, and Snowflake, and I‚Äôm familiar with several databases and cloud environments. I‚Äôm passionate about using innovative technology to help organizations fully capitalize on their data!
* Designed and implemented comprehensive data ecosystems, enhancing digital infrastructure and optimizing data flows for improved business performance.
* Developed and deployed advanced reporting systems, providing actionable insights that supported business growth and transformation.
* Expertise in data analysis, modeling, and ETL processes, utilizing tools like T-SQL, Python, Pandas, and PySpark to ensure efficient data handling.
* Extensive experience working with cloud-based platforms such as Azure Synapse, Google BigQuery, and Snowflake, ensuring smooth integration across multiple environments.
* Passionate about leveraging cutting-edge technologies to help organizations maximize the value of their data and drive innovation.
I‚Äôm looking for a long-term project that will allow me to expand my skills as a Data Analyst and engineer and continue to develop professionally."
data engineer,"As a Senior Data Operations Manager and Data Analytics Team Lead, I have led teams to build data-driven frameworks and optimized data operations, aligning analytics capabilities with strategic business goals. My core focus has been on designing scalable data infrastructures that enable efficient, high-quality data management and insights delivery. I drive the establishment of critical business metrics and lead the team in creating robust ETL pipelines and data resume_classifier using Python, SQL, ClickHouse, and PostgreSQL.
I oversee predictive analytics initiatives, developing resume_classifier that leverage historical data for accurate forecasting and strategic planning. Additionally, I implemented A/B testing frameworks that allow the team to validate and optimize business processes, generating actionable recommendations for stakeholders. My approach ensures that analytics are both impactful and tightly integrated into business operations.
In this role, I collaborate closely with cross-functional leaders to align data initiatives with broader organizational objectives, facilitating a data-driven culture across departments. I also lead cloud migration projects, guiding the transition to Azure and AWS to enhance data accessibility, system scalability, and team productivity.
- Built a Centralized Data Infrastructure: Led the creation of a scalable data warehouse with efficient ETL pipelines, providing a solid foundation for enterprise-wide analytics.
- Automated and Standardized Key Metrics: Established interactive dashboards for continuous KPI tracking, enabling leadership to make real-time, data-driven decisions.
- Executed a Seamless Cloud Migration: Successfully transitioned data operations to Azure, improving reliability, performance, and scalability for future growth.
- Enhanced Data Efficiency and Accessibility: Automated routine data processing tasks, reducing time spent on manual reporting and increasing data accessibility for business users.
- Implemented High-Impact Forecast Models: Deployed predictive resume_classifier that deliver strategic insights, enabling the organization to anticipate trends and make proactive business decisions.
- Unified Retail Data Operations: Integrated sales, orders, and visits data, delivering a consolidated view of retail performance for operational oversight.
- Instituted Data-Driven A/B Testing: Developed structured A/B testing frameworks that empower the team to systematically improve key processes, delivering measurable business outcomes."
data engineer,"I am a senior data engineer experienced in design and implementation of various data related solutions (databases, ETLs, pipelines, data warehouses) with main focus on Azure services (Azure SQL, Synapse, Data Factory, Databricks)
General knowledge is related to database design, deployment, clean-up, performance tuning, maintenance, refactoring, migration, cloud, on-prem or hybrid environment.
Orchestration: Pipelines in Azure Data Factory.
Python and C#: Development of data integration tools, API connectors
Tableau, Power BI: Schema design, data preparation, load and visualization
CI/CD: Implementation with Azure DevOps and GitHub Action
Complex meta-data driven orchestration of data flows. Design and implementation of medallion data architecture (bronze, silver, gold layers).
Various API and data formats
Part-time remote job"
data engineer,"Projects and Tasks Completed:
I started as a Data Coordinator in clinical trials, where I worked on standardizing investigator and site data. After a few months, I moved into a quality control role and helped train new hires. Later, as a Senior Data Coordinator, I continued my initial tasks while taking on more responsibility for ensuring that our team‚Äôs data met quality standards.
When I became a Data Engineer, I took on projects that handled the complete data process. I wrote Python scripts to work with CSV files and prepare Excel sheets for manual annotation. Once the team filled in the data, I collected it, cleaned it up, and pushed it into Oracle SQL using Python. I also ran SAS scripts for data hygiene, updated and moved data between SQL schemas, created views for repeated tasks, and prepared reports to catch any errors or duplicates. I regularly report progress and issues in weekly management calls.
Technologies Used:
Python: For data manipulation, cleaning, and automation.
Oracle SQL: To store, update, and verify data.
SAS: For running established data hygiene scripts.
Excel: As an intermediate step for data annotation and review.
I also took a Data Science course that covered higher math, data engineering, and an introduction to machine learning.
Current Role in the Team:
I currently work as a Data Engineer. In this role, I manage the full cycle of data processing‚Äîfrom receiving and processing CSV files, cleaning data, moving it into SQL, to running necessary reports. I also take charge of making SQL updates and organizing data flow, which keeps our systems running smoothly.
What I Want to Improve:
I want to build my skills in creating data pipelines, especially using tools like Apache NiFi. I‚Äôm also looking to strengthen my expertise in data warehousing solutions, such as Snowflake or Google BigQuery, and further improve my SQL query optimization. Additionally, I aim to enhance my overall coding abilities and continue growing in effective communication with both my team and management."
data engineer,"A passionate and dedicated professional with a knack for innovation and problem-solving. Skilled in collaborating with diverse teams to achieve project goals and deliver outstanding results. Recognized for excellent communication skills and a proactive approach to challenges.
- My work on Embedded ML solutions was presented at the World Economic Forum's Annual Meeting 2023 in Davos, Switzerland.
- I built and integrated an expense management system into the existing project from scratch, which is used all around the world.
I am looking for a company that values critical thinking and freedom and, at the same time not give up on responsibility.
The project that drives innovation and makes people's lives better (including miltech). In other words, something that can make my work meaningful.
I am not considering projects, where the majority of a team does not like what they are doing, or any forms of toxic environment"
data engineer,"-Development, maintenance, and optimization of ETL processes in Informatica PowerCenter and Apache Airflow.
-Experience with Oracle PL/SQL: optimizing complex queries, developing stored procedures, and automating data processing workflows.
-Supporting real-time microservices with Apache Kafka to ensure seamless data streaming and processing.
Projects:
1. Customer Selection Process Migration from Oracle to Informatica
-Led the full migration of the customer segmentation process for the #kartakarta product, transitioning from PL/SQL-based Oracle procedures to Informatica PowerCenter.
-Optimized 10+ complex ETL procedures, improving data processing speed and reducing execution time.
2. Migration of 50+ ETL Pipelines from Informatica to Apache Airflow
-Successfully redesigned and migrated over 50 ETL workflows from Informatica PowerCenter to Apache Airflow, improving orchestration and monitoring capabilities.
3. Fraud Detection Microservice using Apache Kafka & AWS
-Designed and deployed a real-time fraud detection system for financial transactions using Apache Kafka and AWS services (S3, Lambda, and DynamoDB).
-Implemented streaming data processing pipelines that analyze incoming transaction data and detect anomalies and reduced fraudulent transaction processing time by 30%
4. Automated JSON File Parsing System
-Built a scalable ETL pipeline to ingest and process large volumes of JSON files from various sources, improving data integration efficiency.
-Developed custom Python scripts for parsing and transforming JSON data into structured formats for database storage."
data engineer,"–ö–æ–º–ø–∞–Ω—ñ—è Digitum (Factum Group Ukraine)
Head of Data Analytics Department
01.03.2023 ‚Äî –ø–æ —Å—å–æ–≥–æ–¥–Ω—ñ (–º–∞–π–∂–µ 2 —Ä–æ–∫–∏)
‚Ä¢ E2E –∞–Ω–∞–ª—ñ—Ç–∏–∫–∞.
‚Ä¢ –û–Ω–ª–∞–π–Ω-–≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö.
‚Ä¢ –ú–∞–∫—Ä–æ—Å–∏ —Ç–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—è.
‚Ä¢ –ü—Ä–æ–º—Ç —ñ–Ω–∂–µ–Ω–µ—Ä—ñ—è / AI-–∞–≥–µ–Ω—Ç–∏.
–ö–æ–º–ø–∞–Ω—ñ—è Digitum (Factum Group Ukraine)
IT Director
12.2019 ‚Äî 02.2023 (3 —Ä–æ–∫–∏)
‚Ä¢ –§–æ—Ä–º—É–≤–∞–Ω–Ω—è —Ç–∞ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –∫–æ–º–∞–Ω–¥–æ—é (Project Manager, –ø—Ä–æ–≥—Ä–∞–º—ñ—Å—Ç–∏, —Å–∏—Å—Ç–µ–º–Ω—ñ –∞–¥–º—ñ–Ω—ñ—Å—Ç—Ä–∞—Ç–æ—Ä–∏, –¥–∞—Ç–∞-—ñ–Ω–∂–µ–Ω–µ—Ä–∏).
‚Ä¢ –í–µ–¥–µ–Ω–Ω—è –ø—Ä–æ–µ–∫—Ç—ñ–≤ —É —Ä–æ–ª—ñ Project Owner, Product Manager, Project Manager.
‚Ä¢ –†–æ–∑—Ä–æ–±–∫–∞ —Ç–∞ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–∏—Ö —Å–æ—Ñ—Ç—ñ–≤, –º–æ–±—ñ–ª—å–Ω–∏—Ö –¥–æ–¥–∞—Ç–∫—ñ–≤, –ø—Ä–æ–≥—Ä–∞–º –ª–æ—è–ª—å–Ω–æ—Å—Ç—ñ, –≤–∞–π–±–µ—Ä- —ñ —Ç–µ–ª–µ–≥—Ä–∞–º-–±–æ—Ç—ñ–≤.
‚Ä¢ –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è E2E –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏ (—Å–∫—Ä—ñ–∑–Ω–∞ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∞).
‚Ä¢ –†–æ–∑—Ä–æ–±–∫–∞ –≥–µ–π–º—ñ—Ñ—ñ–∫–∞—Ü—ñ–π–Ω–∏—Ö –ø—Ä–æ—î–∫—Ç—ñ–≤, —Ä–µ–∫–ª–∞–º–Ω–∏—Ö –ª–µ–Ω–¥–∏–Ω–≥—ñ–≤, —Å–∏—Å—Ç–µ–º –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –¥—ñ–¥–∂–∏—Ç–∞–ª-–º–∞—Ä–∫–µ—Ç–∏–Ω–≥—É.
‚Ä¢ –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –±–æ—Ä–¥—ñ–≤ —É Looker Studio.
‚Ä¢ –ú–∞–Ω—É–∞–ª—å–Ω–µ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è.
–ö–æ–º–ø–∞–Ω—ñ—è Factum Group Ukraine
Head of IT Department and Media Department
01.2015 ‚Äî 12.2019 (5 —Ä–æ–∫—ñ–≤)
‚Ä¢ –ù–∞–±—ñ—Ä —ñ –∫–µ—Ä—É–≤–∞–Ω–Ω—è –∫–æ–º–∞–Ω–¥–æ—é –º–µ–¥—ñ–∞-–∞–Ω–∞–ª—ñ—Ç–∏–∫—ñ–≤ (2-3 –ª—é–¥–∏–Ω–∏) —Ç–∞ –º–µ–Ω–µ–¥–∂–µ—Ä—ñ–≤ (4-8 –ª—é–¥–µ–π).
‚Ä¢ –ù–∞–ø–∏—Å–∞–Ω–Ω—è SQL-–∑–∞–ø–∏—Ç—ñ–≤ —ñ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö —É –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω—ñ —Å–æ—Ñ—Ç–∏.
‚Ä¢ –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è —Ä–æ–±–æ—Ç–∏ –≤—ñ–¥–¥—ñ–ª—ñ–≤.
‚Ä¢ –ü—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è SPSS, Python, XML.
‚Ä¢ –í–µ–¥–µ–Ω–Ω—è –ø—Ä–æ—î–∫—Ç—ñ–≤.
–ö–æ–º–ø–∞–Ω—ñ—è Factum Group Ukraine
Manager of Data Processing
12.2012 ‚Äî 12.2016 (4 —Ä–æ–∫–∏)
‚Ä¢ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –º–∞—Å–∏–≤—ñ–≤ –¥–∞–Ω–∏—Ö –Ω–∞ –ª–æ–≥—ñ—á–Ω—ñ—Å—Ç—å —Ç–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö.
‚Ä¢ –ù–∞–ø–∏—Å–∞–Ω–Ω—è —Å–∏–Ω—Ç–∞–∫—Å–∏—Å—ñ–≤ —É SPSS.
‚Ä¢ –ù–∞–ø–∏—Å–∞–Ω–Ω—è –º–∞–∫—Ä–æ—Å—ñ–≤ –Ω–∞ VBA Excel.
‚Ä¢ –í–µ–¥–µ–Ω–Ω—è –ø—Ä–æ—î–∫—Ç—ñ–≤ —ñ–∑ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–∏—Ö –æ–ø–∏—Ç—É–≤–∞–Ω—å.
‚Ä¢ –ù–∞–ø–∏—Å–∞–Ω–Ω—è –∞–Ω–∫–µ—Ç –Ω–∞ XML.
‚Ä¢ –í–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è —Ç–∞ –≤–µ–¥–µ–Ω–Ω—è –≤–µ–ª–∏–∫–∏—Ö –ø—Ä–æ—î–∫—Ç—ñ–≤ –∑ E2E –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏ –¥–ª—è –±—ñ–∑–Ω–µ—Å—É.
‚Ä¢ –†–æ–∑—Ä–æ–±–∫–∞ —Ç–∞ –∑–∞–ø—É—Å–∫ –¥–∞—à–±–æ—Ä–¥—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –¥–∞–Ω–∏—Ö —É Power BI, Looker Studio.
‚Ä¢ –£–Ω—ñ–∫–∞–ª—å–Ω–∏–π –¥–æ—Å–≤—ñ–¥ —É –±—ñ–ª—å—à–æ—Å—Ç—ñ IT-—Å—Ñ–µ—Ä.
‚Ä¢ –ó–∞–ø—É—Å–∫ —ñ –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ –≤–µ–ª–∏–∫–∏—Ö –ø—Ä–æ—î–∫—Ç—ñ–≤.
‚Ä¢ –ü—ñ–¥–±—ñ—Ä —Ç–∞ —Ä–æ–∑–≤–∏—Ç–æ–∫ –∫–æ–º–∞–Ω–¥–∏, —è–∫–∞ –≤–∏—Ä–æ—Å–ª–∞ –≤—ñ–¥ –Ω–µ–¥–æ—Å–≤—ñ–¥—á–µ–Ω–∏—Ö —Å–ø—ñ–≤—Ä–æ–±—ñ—Ç–Ω–∏–∫—ñ–≤ –¥–æ –ø—Ä–æ—Ñ–µ—Å—ñ–æ–Ω–∞–ª—ñ–≤.
–†–æ–∑–≤–∏—Ç–æ–∫ —Å–µ–±–µ, –∫–æ–º–ø–∞–Ω—ñ—ó —Ç–∞ –∫–æ–º–∞–Ω–¥–∏, —è–∫—ñ –º–∞—é—Ç—å —Å—Ö–æ–∂—ñ —Ü—ñ–Ω–Ω–æ—Å—Ç—ñ: –±–∞–∂–∞–Ω–Ω—è —Ä–æ–∑–≤–∏—Ç–∫—É, –∑–∞–¥–æ–≤–æ–ª–µ–Ω–Ω—è –≤—ñ–¥ —Ä–æ–±–æ—Ç–∏, –ø–æ–∑–∏—Ç–∏–≤, —á–µ—Å–Ω—ñ—Å—Ç—å, –∫—Ä–µ–∞—Ç–∏–≤–Ω—ñ—Å—Ç—å —ñ –¥–æ—Å—è–≥–Ω–µ–Ω–Ω—è –≤–∏—Å–æ–∫–∏—Ö —Ü—ñ–ª–µ–π."
data engineer,"–Ø –∑–∞–≤–µ—Ä—à–∏–≤ —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—Å—å–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è, –∞ —Ç–∞–∫–æ–∂ –ø—Ä–æ–π—à–æ–≤ –∫—É—Ä—Å —ñ–∑ SQL, –¥–µ –æ—Å–≤–æ—ó–≤ —Ä–æ–±–æ—Ç—É –∑ –±–∞–∑–∞–º–∏ –¥–∞–Ω–∏—Ö, —Ç–∞–∫–∏–º–∏ —è–∫ MySQL —ñ PostgreSQL. –£ –ø—Ä–æ—Ü–µ—Å—ñ –Ω–∞–≤—á–∞–Ω–Ω—è –ø—Ä–∞—Ü—é–≤–∞–≤ —ñ–∑ —Ç–∞–∫–∏–º–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—è–º–∏, —è–∫ Python —Ç–∞ MongoDB, —Ä–µ–∞–ª—ñ–∑—É—é—á–∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø—Ä–æ—î–∫—Ç–∏ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –¥–∞–Ω–∏—Ö.
–ó–∞—Ä–∞–∑ —è –ø—Ä–∞–≥–Ω—É —Ä–æ–∑–≤–∏–≤–∞—Ç–∏—Å—è —É –Ω–∞–ø—Ä—è–º—ñ Big Data, –≤–¥–æ—Å–∫–æ–Ω–∞–ª—é–≤–∞—Ç–∏ —Å–≤–æ—ó –Ω–∞–≤–∏—á–∫–∏ —É Python —ñ –≤–∏–≤—á–∞—Ç–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –≤–µ–ª–∏–∫–∏–º–∏ –¥–∞–Ω–∏–º–∏, —Ç–∞–∫—ñ —è–∫ PySpark.
–£—Å–ø—ñ—à–Ω–æ –∑–∞–≤–µ—Ä—à–∏–≤ –∫—É—Ä—Å —ñ–∑ MySQL —É —à–∫–æ–ª—ñ Hillel, –∑–¥–æ–±—É–≤—à–∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ñ –Ω–∞–≤–∏—á–∫–∏ —Ä–æ–±–æ—Ç–∏ –∑ –±–∞–∑–∞–º–∏ –¥–∞–Ω–∏—Ö.
–û—Ç—Ä–∏–º–∞–≤ –¥–∏–ø–ª–æ–º –±–∞–∫–∞–ª–∞–≤—Ä–∞  —É –ù–∞—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ–º—É —É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç—ñ ¬´–û–¥–µ—Å—å–∫–∞ –ø–æ–ª—ñ—Ç–µ—Ö–Ω—ñ–∫–∞¬ª, —Å–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ—Å—Ç—å –∫–æ–º–ø'—é—Ç–µ—Ä–Ω—ñ –Ω–∞—É–∫–∏.
–†–µ–∞–ª—ñ–∑—É–≤–∞–≤ –Ω–∞–≤—á–∞–ª—å–Ω—ñ –ø—Ä–æ—î–∫—Ç–∏, —Å–ø—Ä—è–º–æ–≤–∞–Ω—ñ –Ω–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—é –±–∞–∑ –¥–∞–Ω–∏—Ö."
data engineer,"1. USA, AI Copilot for venture capitalists
2. Ukraine, Multiproduct freelance platform
3. Ukraine, Internal metrics tool
4. Prague, Czech Republic, Advanced Management tool
5. Ukraine, Internal tool for improving foreign language skills, testing vocabulary
Over 5 years of Python development, DevOps, and software architecture expertise. Proven track record in designing scalable backend systems using Django, FastAPI, and microservices architecture. Skilled in containerization (Docker), cloud technologies (Azure, GCP), and Big Data tools (Hadoop, Spark). Adept at optimizing deployment pipelines, and implementing secure, efficient solutions. Currently enhancing expertise in Big Data processing and cloud-based distributed systems. Strong problem-solving and troubleshooting abilities focused on delivering high-quality, maintainable code.
I'm looking for an opportunity to be useful and upgrade my skills.
I'll be glad to find a job with interesting tasks and projects."
data engineer,"Currently I am working as a Data Engineer, I build and automate ETL pipelines using Airflow, create APIs in Python, and integrate data from sources like Amazon SP API, Walmart API, and supplier websites. I support reporting and analytics through dashboards built with Looker Studio and Count, and manage databases in PostgreSQL. I also contribute to defining KPIs and improving internal data workflows. Previously, I worked as a Data Analyst, focusing on web scraping, product analytics, and report automation. I‚Äôm now looking to gain experience in cloud data platforms, CI/CD practices, and scalable data infrastructure. I'm especially interested in building automated solutions that improve efficiency, give value to the business, and support faster decision making."
data engineer,"I am a professional SQL Server and DWH/ETL/BI Developer, Data Engineer with over 20 years of experience in the field of data engineering, database development, and business intelligence development.
I have a proven track record of designing, developing, maintaining, troubleshooting, and optimizing data projects. I have demonstrated expertise in implementing efficient solutions and enhancing operational stability through innovative automation and effective troubleshooting. I am proficient in leading teams, mentoring members, and collaborating across the organization to boost efficiency and foster professional growth.
Skills:
‚Ä¢ DB: SQL, MS SQL Server, Snowflake, MySQL, PostgreSQL, Oracle
‚Ä¢ BI: DWH, ETL, SSRS, SSIS, SSAS, Looker, Azure Data Factory (ADF), Power BI, Monte Carlo
‚Ä¢ Code: SSDT, Git, TFS, TortoiseSVN, Mercurial
‚Ä¢ CI/CD: TeamCity, Jenkins, GoCD
‚Ä¢ Orchestration: ActiveBatch, SQL Server Agent
‚Ä¢ Cloud: Microsoft Azure
‚Ä¢ Project Management: Jira, TFS
I prioritize the integration of technical and soft skills to achieve goals and deliver value in the most effective manner, with a focus on breakthrough product development and continuous improvement.
‚Ä¢ Created complex reports, dashboards, KPIs, cubes, data warehouses, ETLs, and functional components for enterprise-level data solutions
‚Ä¢ Implemented intricate business logic for critical enterprise solutions
‚Ä¢ Diagnosed and resolved performance and locking issues of varying complexity in a high-load production environment
‚Ä¢ Automated enterprise-scale database development, deployment, maintenance, and business intelligence processes efficiently
‚Ä¢ Designed and implemented a continuous integration process for company-wide database development, improving system reliability, code quality, and development efficiency
‚Ä¢ Implemented effective workflow changes that streamlined collaboration and code management, enhancing system reliability, code quality, and development productivity
In my new role, I anticipate working on challenging projects, adhering to effective work and meeting times, and enjoying flexible working hours."
data engineer,"Senior Data Engineer with 5+ years of experience architecting and optimizing scalable data solutions. Proficient in Python, Java, SQL, Apache Spark, and Azure/AWS. Strong expertise
in ETL/ELT, data warehousing, and distributed systems. Passionate about building high-performance, fault-tolerant data solutions that enhance business intelligence and
operational efficiency. Currently working as a Senior Data Engineer in a Data Science & Engineering consulting company.
- Building fault-tolerant, scalable data analytics pipelines (ETL & ELT).
- Working with distributed processing systems & Apache Spark.
- Designing and implementing batch & stream processing solutions.
- Developing data warehouse solutions using Snowflake, Databricks, Azure, and AWS.
Previously worked as Software Engineer (Java, C/C++, and Python), designing and developing distributed systems. My freelance work as an Android Developer involved the development of native mobile apps using Java and Kotlin, releasing 40+ version of the product and supporting the entire software life cycle including determining application requirements.
Certifications:
- Databricks Certified Data Engineer Professional
- Microsoft Certified: Azure Data Engineer Associate
- Databricks Certified Associate Developer for Apache Spark 3.0
I want to join a team with different challenging projects. I opt to  educate and learn from team members - with various levels of experience. Startup ecosystem and mindset are preferred."
data engineer,"Data Engineer | Luxoft - Nordstrom | 11/2021 - Present
Python Developer | QuintaGroup, Kyiv | 04/2021 - 11/2021
Risk/Data Analyst | AsiaFinance Group, Kyiv | 05/2019 - 04/2021
Data Analyst | Avon Analytical Department, Kyiv | 06/2018 - 05/2019"
data engineer,"Certified Google Cloud Professional Data Engineer with a passion for building scalable, efficient, and cost-
effective data systems. Proven ability to reduce pipeline costs, enhance performance, and automate
infrastructure. Strong expertise in modern data stack: GCP, DBT, Airflow, Terraform, and Looker. Background in
analytics and engineering with a unique problem-solving mindset developed through research in medicine.
Successfully delivered 20+ end-to-end data projects across analytics and engineering functions."
data engineer,"Hi there! I have 9 years of experience (3 years as BI Analyst + 5 years as Software/Analytics/Data Engineer), currently working as Lead Product Data Engineer. I love my current job, but have some spare time for interesting projects on a part-time basis. Ideal projects scope right now: help setting up analytics framework from the scratch, consultations, ad-hoc analytics, mentoring analytics team.
Overall I worked with such data stack: AWS (s3, Athena, Glue, Redshift), Snowflake, Python, custom data ingests from various APIs, Airflow, Stitch, _dbt, Matillion, Hightouch.
I tend to be owner of the project regardless of what it takes to deliver. When setting up new data pipeline - it's not just technical steps for me. For me it's talking to all stakeholders to understand requirements, making sure that data is being generated in expected way (especially with manual input and automations), making sure data being collected in the most efficient way from data source, making sure that ELT transforms data per stakeholders needs, making sure visualisation of data clear and user-friendly.
I was mostly working with marketing data, some key results:
- Defined process flow and build data pipeline for automatic channel attribution in Salesforce
- Set up data pipelines for measuring full circle of Inbound Leads processing by Marketing team: incoming, qualified leads, tracking of the time lead was in work and communication activities
- Defined and set up process flow for automatic leads qualification in Salesforce
- Set up process flow and automatic leads attribution by Markets
I can't work full time at the moment."
data engineer,"–ù–∞—Ä–∞–∑—ñ –ø—Ä–æ—Ö–æ–¥–∂—É —Å–µ—Ä—ñ—é –∫—É—Ä—Å—ñ–≤ –≤ Coursera –Ω–∞ –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω—É —Å–µ—Ä—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—é 'Data Engineering'.
–†–æ–∑—É–º—ñ—é —ñ –º–æ–∂—É –æ–ø–∏—Å–∞—Ç–∏ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó –∂–∏—Ç—Ç—î–≤–æ–≥–æ —Ü–∏–∫–ª—É —Ä–æ–∑—Ä–æ–±–∫–∏ –¥–∞–Ω–∏—Ö, —Ç–∞–∫—ñ —è–∫ —Ä–µ–ª—è—Ü—ñ–π–Ω—ñ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö, —Å—Ö–æ–≤–∏—â–∞ –¥–∞–Ω–∏—Ö NoSQL —ñ –º–µ—Ö–∞–Ω—ñ–∑–º–∏ –≤–µ–ª–∏–∫–∏—Ö –¥–∞–Ω–∏—Ö.
–í —Ä–∞–º–∫–∞—Ö –Ω–∞–≤—á–∞–Ω–Ω—è –ø—Ä–∞—Ü—é—é –∑ SQL IBM db2, MySQL, PostgreSQL, SQLite3.
–†–æ–∑—Ä–æ–±–ª—è–≤ ERD —Å—Ö–µ–º—É —ñ SQL —Å–∫—Ä–∏–ø—Ç–∏.
–ú–∞—é –±–∞–∑–æ–≤—ñ –∑–Ω–∞–Ω–Ω—è Python —ñ –∑–∞—Å—Ç–æ—Å–æ–≤—É–≤–∞–≤ –ª–æ–≥—ñ–∫—É –ø—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è.
–ü—Ä–∞—Ü—é–≤–∞–≤ –∑ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞–º–∏, —Ç–∞–∫—ñ —è–∫ Pandas & Numpy. –í–ø—Ä–æ–≤–∞–¥–∂—É–≤–∞–≤ –≤–µ–±-—Å–∫—Ä–µ–ø—ñ–Ω–≥ –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é API —ñ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏, —è–∫-–æ—Ç Beautiful Soup.
–ú–∞—é –¥–µ–∫—ñ–ª—å–∫–∞ –ø—Ä–æ–µ–∫—Ç—ñ–≤ –Ω–∞ GitHub.
–ó–¥–æ–±—É–≤ —Ä–æ–∑—É–º—ñ–Ω–Ω—è –≤ Data Engineering —Ç–∞–∫—ñ —è–∫:
Data Mining, Extract-Transform-Load,
Network Security,
Databases, Data Warehousing, Big Data
SQL
Python Programming
–û—á—ñ–∫—É—é –Ω–∞ –ø—Ä–∞—Ü—é –≤ –æ—Ñ—ñ—Å—ñ –∑ –∫–æ–º–∞–Ω–¥–æ—é, –¥–µ –∑–¥–æ–±—É–¥—É —Ü—ñ–Ω–Ω–∏–π –¥–æ—Å–≤—ñ–¥ –≤ —Ü—ñ–π –≥–∞–ª—É–∑—ñ."
data engineer,"Data Engineer (2024 ‚Äì present)
- Implemented a web API for annotator data validation.
- Built robust data pipelines for the data warehouse.
- Developed integration methods with AWS S3, MongoDB, and PostgreSQL.
- Combined diverse data sources into unified formats for streamlined analysis.
- Developed pipelines for raw data collections, analysis and transfer to annotation from various sources.
- Implemented full support for new data type (transition from images to videos) in existing pipelines.
- Develop tools for QA and DE teams to analyze incidents and bug reports.
- Designed a DB and integrated it into the pipelines.
Tech Lead, Automation QA Team (2022 ‚Äì 2024)
- Reduced regression testing time from 14 days to 18 hours by optimizing the
framework and testing processes.
- Diagnosed and resolved a memory leak in Pytest.
- Designed and implemented a BDD framework for RTOS testing, basing on
custom mock-server.
- Led the development of a comprehensive mock server.
- Integrated radio devices into the framework using serial, telnet, jbus and emulators.
- Integrated IoT into existing framework.
- Migrated the framework to AWS.
- Implemented APN settings for RTOS.
Team Lead, Automation QA Team (2020 ‚Äì 2022):
- Accelerated test framework development by introducing clear code style and code review processes.
- Scaled, built and organized a team of 8, which later expanded to 25 specialists, introducing efficient onboarding and knowledge-sharing processes.
- Improved test stability by revising testing approaches and enhancing pipelines.
Full-Stack Python Developer, 2019-2020:
Developed microservices and key functionality for a Slack bot, designed as a product for corporate clients.
- Optimized operations with NoSQL databases, enhancing the product's performance and scalability.
- Improved existing testing processes to ensure reliability and seamless deployment.
Self-employed (2016 ‚Äì 2019):
Java/Python Developer
- Optimized a local mail service.
- Participated in developing trading platforms and Telegram bot functionality.
- Provided technical support for web studios.
- Developed the networking (net-code) for indie games.
Ensured uninterrupted framework delivery at the onset of the full-scale invasion, even with the team reduced by 8 members. The project stayed on schedule and continued to evolve.
Implemented a BDD framework for an RTOS system based on a mock server.
Reduced test execution time from 14 days to 18 hours.
Led the mock server development process for the RTOS system, providing full workflow simulation.
Managed a team of 25 developers and established effective communication processes."
data engineer,"I have extensive experience as a Data Engineer and SQL Developer.
Technical Stack:
Databases: MS SQL, PostgreSQL, Oracle, Teradata, MongoDB, Azure Data Lake
Programming: C#/.Net, ADO.NET, ASP.NET Core, Entity Framework Core, Blazor Server, MudBlazor, gRPC
BI Tools: Microstrategy BI, Power BI
Data Engineering: SSIS, Azure Data Factory, basic knowledge of Azure Databricks
Key Responsibilities:
Developed and optimized complex SQL queries, tables, views, and stored procedures for efficient data retrieval and storage
Built and maintained robust C# services for data synchronization across databases, services, and APIs
Designed and implemented data pipelines in Azure Data Factory to streamline ETL processes
Automated reporting solutions and integrated business intelligence tools for data visualization
Developed gRPC services using Protocol Buffers (protobuf) for efficient communication between microservices.
Engaged with MudBlazor for building dynamic user interfaces and components for administrative panels
Developed a comprehensive financial reporting system, encompassing the following components:
- Database Development: Created tables, views, and procedures
- Power BI Reporting: Developed interactive and insightful reports;
- Data Import: Imported Excel and CSV files into the admin panel;
- Data Export: Enabled data download to Excel from the admin panel;
- Payment System Integration: Integrated with various payment systems;
- Admin Panel Enhancements: Displayed data in the admin panel and added functionality to process payments directly from the admin panel;
At my future work,I would like to focus on professional growth, learn and working with new tools and technologies."
data engineer,"2023-Present: Data Officer, Research Company
+ Python ETL, cleaning and analysis,
+ Power BI, R Shiny, FastAPI online data collection tools,
+ Embedded Power BI reports,
+ PySpark ETL, ML
+ R language scripts for ETL, cleaning and analysis,
+ XLSForms data collection support and design
2022-2023: a back-end team engineer;
+ Node.js Express servers [OAuth2, auth, API, e-mailing, MongoDB ]
+ FastAPI micro services [SQLalchemy, pydantic]
+ Full-stack web scrappers for e-commerce
Node.JS, Cheerio, Websockets.
+ Prozorro government tenders tracker
Node.JS, Express, MongoDB, REST API, React, Redux
2016-2021: Head of R&D Department
+ Descriptive, diagnostic industrial market analysis, linear and logistics regressions,
+ Power Query, Power Pivot, MS Excel,
+ Power BI report with stock market prices visualisations,
+ Python scripts for data processing and regression predictions
I‚Äôm a Swiss Army knife for data work‚Äîfrom building a server and exchanging data with various sources, transforming and cleaning it, to visualizing it in a format that‚Äôs easy for stakeholders to understand. I can focus on any of these stages or switch between them depending on the client‚Äôs needs.
Everyone wants interesting projects, professional teams, and Salary growth‚ÄîI‚Äôm no exception. At the same time, I have a realistic view of things and extensive experience working in teams with people from different fields, tackling both challenging and routine tasks. I expect a fair Salary and opportunities for growth."
data engineer,"Designed data schemas and led optimization efforts in the Azure SQL database
Ensured stability of data orchestration processes by identifying and resolving
issues in Databricks and Azure Synapse, minimizing failures in production
pipelines
Reviewed infrastructure and code changes, deploying updates to production
using Azure DevOps
Collaborated with different focus groups to collect stakeholder feedback and
implemented enhancement in the team‚Äôs Toolkit PowerApps functionality
Performed compliance activities to ensure adherence to TCO standards and
policies
Provided technical onboarding and guiding to the newcomers
Automated monitoring of unused Data Lake folders using Python and
Databricks, reducing manual checks and optimizing storage management
Optimized notification services by enabling easy creation and scheduling of
alerts via a new table schema, eliminating the need to create new Power
Automate flows
Maintained Azure virtual machines for Self-Hosted Integration Runtime
Monitored ingestion processes in the production environment
Developed ingestion Azure Data Factory pipelines leveraging Logic Apps
Created Power Automate flows to send failure notifications via Teams and
Outlook
Built Power BI dashboards to monitor load statuses for ingestion and
orchestration
Developed PowerApps solution enabling data customers to subscribe to
notifications for ingestion and orchestration updates
Addressed ad-hoc data quality issues and provided timely resolutions
Wrote Azure Wiki documentation for data engineers and data customers"
data engineer,"As a Python developer, I have worked on the following projects:
1 Development of an Automation and Data Collection System:
- Created a system for automating data collection from various websites.
- Developed an API for integration with other services.
- Conducted testing using Python.
2 Development of a Parsing System for Amazon Dropshipping:
- Built an analytics system and scraping service.
- Automated processes using Selenium and involving Amazon and Google Sheets (scripts, integration).
- Implemented scraping and notification of new products, and generated reports.
3 API Development and Testing and Database Management:
- Created and managed Docker images.
- Developed and deployed APIs for integration with other services.
- Managed databases, including deployment and maintenance.
Currently, I am expanding my tech stack and focusing on developing my skills as a QA Automation Engineer (Python).
Understanding Client-server architecture and HTTP protocol.
Experience with Jira, Confluence.
Web UI tests: Python + PyTest + Selenium + Playwright.
API automation: Python + PyTest + Requests.
OTHER TOOLS: JMeter, Docker, Postman, Git, GitHub
As for me I easily find a common language with people and I would be happy to join a friendly team where we could work and grow together!
I am looking for Python AQA Engineer position. Expect to find a company with the possibility to grow up.
The most important expectations are:
- interesting tasks
- professional development
- opportunity to learn new
- competitive Salary"
data engineer,"WORK EXPERIENCE
Software Country(Senior Data engineer)
Software Development Company | Software Country
Work time: 2023-2024
The project goal maintain DWH, create data pipelines, support system
Tasks:
Database implementation, Write and optimize SQL queries for data retrieval, modification, and deletion, Develop data pipelines,  Develop and maintain stored procedures, functions, and triggers, Performance tuning of SQL queries and database procedures, Data Migration and Integration, Performance monitoring, Diagnose and troubleshoot database-related issues, Provide support for production systems, addressing incidents and problems;
Results:
Efficient Data processing, Scalable Data infrastructure, Optimized  database performance.
Technologies: Clickhouse, Databricks, Nats consumer, Airflow, OpenMetaData, R scripting,
DWH development (DWH, Snowflake, Snowpark ETL(Infromatica PC, Airflow), Python, BI(Tableau, Power BI, Superset), R, Kafka)
DataArt
DataArt | Enterprise Software Development Company
Work time: 2022-2023
The project‚Äôs goal was to build a data warehouse on Snowflake and create a BI reporting system to provide users with the ability to obtain the necessary indicators.
Tasks:
Building/supporting data pipelines;
Development and support of infrastructure for storing and processing data;
Designing and building a data warehouse on Snowflake;
Troubleshooting, profiling, performance tuning;
Python scripting for parsing/processing data;
R scripting for prepare unit tests;
Data visualization on Power BI, Superset.
Results:
Created DWH on Snowflake with curated data, Developed Data pipelines on Informatica PC, Monitoring and performance reports, Cost-effective Data management;
Technologies: Snowflake, AWS, Informatica cloud, Power BI, Python, R
Wealth Management Data Platform (DWH, Redshift, ETL,Python, BI, R)
Kanda Software(Senior Data engineer)
Kanda Software: Custom Software Development Company
Work time: 2020-2022
There are several projects.
One of the project's goal was to build a data warehouse on Redshift and create a BI reporting system to provide users with the ability to obtain the necessary indicators.
Tasks:
Data Pipeline Development;
Development and support of infrastructure for storing and processing data;
Designing and building a data warehouse on Redshift;
Troubleshooting, profiling, performance tuning;
Creating dashboards in a BI tool;
Python scripting for parsing/processing data;
Data Security and Compliance;
Data migration
- Database Design and Modeling
- SQL and PL/SQL Development
- Database implementation
- Data Migration and Integration
- Permonamce monitoring
- Diagnose and troubleshoot database-related issues.
- Provide support for production systems, addressing incidents and problems
ability to develop skills for data science;
learning new technologies;
from Kazakhstan looking for remote job."
data engineer,"Data Engineer with 5+ years of experience in production software development and data engineering. Proficient in Python with a background in Java, specializing in data engineering for 4+ years and leading a team of 3-6 developers for 1 year. Expertise in ETL processes, data migration, BI dashboards, RAG (Retrieval-Augmented Generation), and building data platforms. Skilled in a range of data processing, orchestration, and storage tools across cloud environments, including GCP, Azure, and cloud-agnostic solutions. Experienced in Agile methodologies, including Scrum and Kanban, with a proven ability to collaborate effectively with cross-functional teams on large-scale projects"
data engineer,"Certified Azure and Databricks  Big Data Engineer.
Developing  applications and data pipelines for integrating, transforming, and consolidating data from various
structured and unstructured data systems into a structure that is suitable for building analytics solutions and data products.
Cloud migration experience.
Researching new methods of obtaining valuable data and improving its quality.
Technologies & Tools
Databricks, Azure Synapse, Pyspark,  Python, SQL,  Spark, Spark Streaming, Delta Lake, Delta Live Tables, Kafka, DBT, Debezium, Airflow, Docker, Git, Linux, Scrum, Jira."
data engineer,"Completed DevOps school where had been studying basics of Linux, networking, basic DevOps tools, AWS and GCP
1,5 years experience as a member of L2 support team, working with GCP Monitoring and Logging
3 months experience as IT-assistant working with Terraform, Ansible, VMware
3 months experience as Data Engineer working with GCP (BigQuery, Cloud Storage, Cloud Composer).
- AWS Certified Cloud Practitioner
- GCP Associate Cloud Engineer"
data engineer,"13 years of commercial experience in IT, including:
- 10 years with Scala
- 5 years with Python
- 8 years with Big Data, Hadoop and Spark.
- 3 years as a Team Lead/Tech Lead.
Have experience of working with Cloud (AWS, GCP)
Strong advocate of the best practices in software development, such as Unit/BDD Testing, code reviews, pair programming etc. Also have experience in mentoring junior colleagues and conducting job interviews.
Have always been eager to create software which is well architected, tested and documented, easy to maintain and meeting all the performance requirements.
- Successful delivery of the project from Proof-Of-Concept stage to production
- Designing and implementing separate modules
- Performance tuning of Hadoop cluster and Spark jobs
- Mentoring junior colleagues
- Leading a small-size team of senior developers
- Java 8 and GCP certified
Looking for a job as a Lead developer in Python Big Data project.
Interested in opportunities for professional development and learning new technologies."
data engineer,"I am an experienced Data Engineer with a passion for building and optimizing ETL pipelines to ensure fast and efficient data processing. I have a broad skill set encompassing both Data Analyst and Data Engineer roles, enabling me to comprehensively understand various aspects of data work. My proactive and independent approach allows me to tackle complex tasks and solve challenging problems effectively. I continuously learn and implement new technologies, recently focusing on AWS, with hands-on experience in building ETL pipelines using S3, Athena, and AWS Glue."
data engineer,"Passionate Data Engineer with multiple years of experience in data acquisition, web scraping, and large-scale data processing. Adept at designing automated pipelines, integrating APIs, and optimizing workflows for accuracy and efficiency. Skilled at collaborating with cross-functional teams to deliver actionable insights and data-driven solutions. With a background and strong focus on applying data analytics to economics and research challenges.
EREI(Equity Real Estate Investment)
I was incharge of creating web scraping pipelines to collect large-scale data from real estate websites all over Europe.
OSINT Data Collection and Automation Tool:
I worked on a Social Media web scraper using Node.js, express, and advanced fortified Puppeteer for browser automation, I also utilized residential proxy pools to bypass antibot mechanisms facilitating seamless data collection without getting blocked in addition to this I have also conducted analytical and keyword analyses on the acquired data to generate actionable information.
Content Writer:
I have authored numerous content for various companies as a testament to my expertise in web scraping
technologies. I have delivered high quality blogposts and articles to companies like Zenrows, Scrapeops and AnyIP
My primary aspiration in a work setting is to continually expand my expertise and deepen my understanding of data collection, analysis, and presentation. I'm passionate about taking on roles that provide opportunities for learning and growth, allowing me to hone my skills further.
I thrive in environments that foster ongoing learning and encourage the exploration of innovative approaches to solve complex data-related challenges."
data engineer,"Data Engineer - Proxima Research International (June 2023 - Present):
-- develop new, maintain & improve existing ETL pipelines from different sources (migration to prefect)(Python, Prefect, PostgreSQL, AWS S3, 3rd party API,  FTP/SFTP, SSIS);
Data Engineer (Part time) - Metro Cash & Carry Ukraine (Aug 2023 - Oct 2023):
-- update existing and development new data flows (migration from Teradata to GCP BigQuery) ( Python, Teradata, GCP BigQuery,VBA Excel, VBScript );
Data Engineer (ETL) - DataMola Ukraine/Poland (March 2022 - Apr 2023):
-- develop new, maintain & improve existing ETL pipelines from different sources (Python, Prefect, IBM DB2, AWS S3, 3rd party API, SharePoint, SFTP, SSIS);
-- participate in build unify data flows (data pipelines) for all company projects (Prefect Dataflow Automation, Python, AWS S3, 3rd party API, SharePoint, SFTP, SSIS);
Senior Pricing Process Optimization Specialist/Reporting and Process Automatization Specialist - Metro Cash & Carry Ukraine (June 2019 -  Aug 2022):
-- create, maintain and improve data pipelines (ELT)/data processing for reporting and working tools in department (MS Office, Teradata, Python, VBA Excel, VBScript, Corporate Storage and Systems);
-- develop promo planning tool (work templates (MS Office), data processing (VBA Excel,VBScript, Python, Teradata, Corporate Systems);
-- pricing process automatization (develop tools for price analysis & price setting (MS Office, Teradata, VBA Excel, VBScript, Corporate Systems).
I am looking for a job where I can deepen my technical and professional skills in field of Data Engineering."
data engineer,"Web3 Full-stack pet projects using  React(Next)/HardHat/Solidity.
Intern in CGS as a React/Express/MongoDB Full-stack developer (1 month)
Trading strategies developer PineScript/Python (6 month)
Full-stack/data engineer Flask/PostgreSQL in quantitative hedge fund AlgoZeus (1 year 6 month)
Full-stack developer at web3 outsource TheBuidl (present)"
data engineer,"Data Engineer Jun 2023 - Present
U.S-based product software development company working on the innovative solution for
logistics market.
Bug fixing
Developing of new dashboards
Supporting of existing dashboards
Creating datasets
Writing sql queries for datasets
Tools & Technologies: Quicksight, Redshift, Postgresql, S3, GitHub, Jira.
Junior Data Engineer - January 2022 - January 2023
Web and mobile appication for managing security system
Project Description: Project is a Web and mobile application for managing, tracking, and supporting of security systems, maintaining user roles, and permissions for large businesses. App is being assed by external AKQA team, and based on that we have a brand new development stage using GCP.
Client is a US nationwide company that offers home security monitoring, equipment and installation services. It is recognized as the nation‚Äôs premier full service security provider, offering security services to residential, business, national account, and integrated system customers including professional installation, a high level of ongoing customer service and company owned monitoring centers to ensure a rapid response.
Customer: US company
Involvement Duration: 9 months
Project Role: Junior Data Engineer
Responsibilities:
Bug fixing
Migration data from SQL Server to BigQuery
SSRS report deployment
Perfomance/data usage refactoring
Project Team Size: 14 team member
Tools & Technologies: MSSQL Server, Visual Studio, GCP, SSRS, Scrum , Sourcetree.
SoftServe IT Academy ‚Äì Data Engineer Internship(LV-663 DB) ‚Äì December 2021-January 2022
Internet shop database
Project Description:    SQL Server Database for project. Development with more emphasis on database
design. Development of features including transactions, stored procedure, functions, triggers
for consistent product quantity, dynamic product rating based on reviews etc.
Customer:    SoftServe IT Academy
Involvement Duration:    2 months
Project Role:    Data Engineer
Responsibilities:    Requirements analysis and clarification;
Database design;
Database development and deployment script
Project Team Size:    5 team members
Tools & Technologies:    SQL Server, SQL Server Management Studio
SoftServe IT Academy ‚Äì Service Engineer (LV-637 SE) September-December 2021"
data engineer,"- skilled in data engineering and technical leadership
- participated in Software Requirements and Product Architecture creation
- participated in project stuffing
- played a tech lead role
- played a team lead role
- experience with major cloud providers: Google Cloud Platform, AWS, Azure
- have experience working with SQL and NoSQL databases
- have experience with CI tools and CI/CD process
- worked at both product and service companies
- worked with Agile methodologies
- worked with different teams from small local (4-5 people) to big distributed ones (80+ people)
- Professional Cloud Architect, certified by Google Cloud
- Professional Data Engineer, certified by Google Cloud
Open to positions that involve:
- one-time consultancy, advisory, pre-sales activity
- technical leadership & development"
data engineer,"I am a data engineer at an international humanitarian organisation. I develop data collection tools, automate tasks for data collection monitoring, scripts for cleaning & analysing the data. I create various ad-hoc automations and occasionally work as a cross-cutting specialist helping to bring other projects to fruition"
data engineer,"- Development of analytical systems;
- Integration of systems and data exchange: CRM, advertising systems, databases, web and app analytics systems, data visualization for reports;
- Google Cloud Platform(IAM & Admin,
Cloud Storage, BigQuery, Pub/Sub, Dataflow, Looker, Cloud Run, App Engine, Logging, Firebase, Artifact Registry, Cloud Scheduler);
- Developing Machine Learning Models to Predict Conversions, ROI;
- Working with API;
- Mobile App Analytics;
- Web Analytics;
- Building Attribution;
- Development of system description and documentation;
- Docker;
- Python;
- C#;
- ETL\ELT processes;
- dbt (Data Build Tool);
Databases:
Working with BigQuery and PostgreSQL: Storing and processing large amounts of data, creating complex queries and reports for deeper data analysis.
Creation of a comprehensive analytical platform (financial sector, medical) based on Google Cloud. Integration with external data sources, data transformation, logging system, backup, paplines, reports.
I want to work for a company that has its own product.
I am ready to pass the interview, but I refuse to take the tests."
data engineer,"Results-driven Data Engineer with 4+ years of experience in designing and optimizing scalable data pipelines, managing large-scale data systems, and leveraging cloud technologies (AWS, Azure). Skilled in Python, SQL, Apache Airflow, and big data tools (Spark, Hadoop). Adept at implementing ETL processes, automation, and real-time analytics, driving efficiency and data accessibility. Strong background in database development, data warehousing, and cloud-based data solutions. Passionate about mentoring and training aspiring data engineers.
Proficient in Python, SQL, and popular ETL tools, with hands-on experience in building automated data workflows.
I‚Äôm seeking a long-term remote position where I can collaborate with a dynamic team while enjoying flexibility in my schedule."
data engineer,"- Experience working in a development environment focused on managing purpose-built, highly available, distributed, and scalable cloud services.
- Strong problem-solving skills, with attention to detail and a commitment to quality.
- Deep understanding of the mathematical foundations of Machine Learning, including algorithms, with the ability to discuss them in detail.
- Solid knowledge of probability theory, random processes, statistics, and optimization techniques.
- Proficient in linear algebra.
- Previous experience in Information Retrieval (IR), Natural Language Processing (NLP), Computer Vision (CV), Text mining, Machine Learning, or Big Data mining is highly desirable.
- Proven track record of delivering enterprise-grade, scalable, secure, and reliable software systems.
- Extensive experience developing highly scalable machine learning/deep learning applications and services.
- Over 9 years of experience in the analysis, design, and development of client/server, web-based, and multi-tier applications, with expertise in developing Windows and web applications, services, and APIs.
- Ability to work on data mining and data science projects while collaborating with engineering teams, quality engineers, and product management.
Optimized Data Pipelines: Spearheaded the design and optimization of scalable ETL pipelines, reducing data processing time by 40%, enabling faster access to business-critical insights.
Implemented Data Lake Architecture: Led the implementation of a distributed data lake architecture, improving data availability and reducing storage costs by 30%, while enhancing data governance and accessibility.
Automated Data Quality Monitoring: Developed and deployed an automated data quality monitoring system that identified and resolved data inconsistencies, resulting in a 25% reduction in data errors and improving overall data reliability.
Streamlined Data Ingestion: Architected real-time data ingestion pipelines using Apache Kafka and Spark, enabling near real-time analytics and reducing batch processing windows from 24 hours to 2 hours.
Enabled Data-Driven Decision Making: Collaborated with business intelligence teams to integrate multiple data sources into a single platform, providing stakeholders with actionable insights and driving a 15% increase in operational efficiency.
Cloud Data Migration: Successfully led the migration of legacy data infrastructure to AWS, cutting operational costs by 20% while improving scalability and security.
Advanced Analytics Infrastructure: Designed and implemented a robust analytics infrastructure, enabling machine learning model deployment on large-scale datasets and reducing model training times by 50%.
Improved Data Security and Compliance: Implemented data encryption, role-based access controls, and other security measures, ensuring full compliance with GDPR and other regulatory requirements.
Data Warehousing Optimization: Reengineered the company's data warehouse architecture, improving query performance by 60% and supporting faster decision-making across the organization.
Cost Reduction through Data Efficiency: Identified inefficiencies in data storage and processing strategies, leading to a 15% cost reduction in cloud storage and compute resources."
data engineer,"Constantly improving Software Engineer with extensive knowledge of SQL and hands-on experience in development / optimization of DWHs, DB objects, ETL jobs, data modeling and analysis for reporting.
Worked for one of the Big Four accounting organizations, using SAP HANA, SAP Data Services (BODS), SAP Analisys for MS Office, SAP Web Intelligence (Business Objects), Tableau, Azure DevOps.
Professional geographer (Ph.D Graduate Student) with significant background in GIS and Geospatial data analysis.
Former mountain guide used to work with Ukrainian- and English-speaking groups in more than 12 countries on 4 continents, which helped me to develop my soft skills.
Independently implemented the sub-project from scratch, including the development of DWH, ETL procedure and job, client face view, report, security etc."
data engineer,"Skilled data specialist highly adaptable to new areas of domain knowledge and having a natural curiosity for what patterns and pain points connect the business world.
My skills include expertise in:
‚Ä¢ SQL: MySQL, PostgreSQL, BigQuery, Presto/Trino
‚Ä¢ Python: raw python, pandas, numpy, pyplot, scipy, sklearn, seaborn, some backend skills: unit tests, FastAPI, GenAI: langchain, llamaindex
‚Ä¢ Git and CI/CD
‚Ä¢ R: tidyverse, data.table, ggplot2, purrr
‚Ä¢ Visualizations using Tableau, QuickSight, Looker Studio
‚Ä¢ GA4, GTM, Docker, kubernetes, kubeflow, airflow
‚Ä¢ Jira, and Confluence for effective teamwork
‚Ä¢ Microsoft Excel and Google Sheets
Won't collaborate with any company providing services or products in the territory of russia. As well as in the company still having russians either leading any company‚Äôs business area or participating in teams. Save your time and respect mine please."
data engineer,"I've been working as a Data Engineer for over six years, specializing in building scalable ETL pipelines, automating data processes, and enhancing machine learning workflows. My expertise lies in Python, Databricks, and AWS, and I've successfully migrated large datasets to cloud-based warehouses while improving data accuracy and accessibility.
I focus on designing efficient data solution, from web scraping tools to interactive Tableau dashboards, and I've worked with technologies like Apache Kafka, PySpark, and Snowflake to optimize data processing and visualization. I'm passionate about improving decision-making through data-driven insights and enjoy collaborating with cross-functional teams to solve complex challenges.
What motivates me is streamlining data workflows and leveraging advanced analytics to deliver actionable insights. I aim to deepen my knowledge in real-time data streaming and further enhance machine learning resume_classifier to drive better business outcomes."
data engineer,"- Data engineer with 5 years' experience of implementing data solutions for customers from insurance, healthcare, credit scoring, and other realms.
- AWS Certified Solutions Architect.
- Able to write high-quality (maintainable and efficient) code in Scala, Python, and Java.
- Proficient in SQL.
- Experienced in implementing solutions using Spark, AWS (Glue, Step Functions, Lambdas, S3, Athena, DynamoDB, etc), GCP (BigQuery, Dataproc, Cloud Storage), data warehouse tools (worked with Hive, Impala, etc).
- Result-oriented with focus on business needs.
- Team player with good communication skills.
Key achievements:
- Migrated a Pentaho job to Spark, after which execution time decreased from 60-70 min to 10-15 min.
- Re-designed Spark jobs I/O module, significantly increasing maintainability of a project (e.g. instead of adding a log statement in 30+ different places it became possible to add it just in one).
- Re-designed a Python project (AWS Lambdas), introducing AWS SDK for Pandas, modularity, code reuse, and testing with pytest and moto.
Showed proven ability to:
clarify and decompose tasks, implement solutions with maintainability, performance and const-efficiency in mind, learn new technologies and tools on-the-fly,  document solutions and perform knowledge transfer.
I've been learning Computer Science and Maths (self-stadying using books, tutorials, online-courses, and articles; EPAM Java external training) for a few recent years. I learned the basics of algorithms and mathematical logic, core features of Python 3 and Java, base SQL commands (gained experience with MySQL and Postrgres), the main internals and usage of git, organization of Linux OS (Ubuntu) and a bunch of ways to interact with it via a terminal.
I'd like to learn by practice, gaining hands-on experience and receiving feedback about my current level of knowledge and skills, finding out ways of improving that level."
data engineer,"I am a  Senior Python Developer with data engineering specialization.
AWS Certified Solution Architect Associate.
For the last 6 years I've been taking part as a Python developer in a series of large SAAS projects/startups. Specifically
keelvar - logistics optimization AI platform
rebelmouse.com - social media platform
lifedojo.com - online learning
allyo.com - AI chatbot automation for recruiting
I like to deal with DATA, engineer it, develop ETL and data workflows, process it asynchronously and store it in the most optimal and safe way.
There were more projects, sentiment analysis apps, marketing apps, sports  betting apps, online auctions and web site generators.
Engineering data-intensive database applications for more than 10 years. Those include:
Allyo.com - designed from scratch, spearheaded and launched two analytics services, developed integrations with Tableau Analytics
Lifedojo.com - designed hundreds of apis for online learning provider, provided AWS integrations
Leadrouter - designed reporting for Fortune-500 company in real-estate, a db-heavy project for US.
Rebelmouse.com - social-network publishing platform.
Certified AWS Cloud Architect Associate.
Personal relations in a good team.
Preferring long-term projects."
data engineer,"Developing ETL process
Managing DWH
Maintaining data infrastructure
Communication with client
Developing systems for supply chain, fuel automotive domain, logistics
Collecting data for analytics and reporting
Data quality provisioning"
data engineer,"Big Data Engineer at TBC Bank(October 2023 - Present):
- Developed a scalable Data Lakehouse using Data Vault architecture on Azure Databricks to streamline data storage and processing for company-wide analytics capabilities.
- Developed and managed ETL processes using Azure Data Factory and Azure Databricks, automating data ingestion from various sources, including transactional databases, APIs, FTP servers.
- Collaborated with cross-functional teams and stakeholders to define data requirements, ensuring alignment with business goals and compliance standards in the banking industry and the internal platforms.
- Developed a reusable performance monitoring data platform for multiple teams to identify bottlenecks and necessary optimization strategies in over 300+ pipelines, resulting in a 30% reduction in processing time, and costs on average.
- Integrated Unit Tests and Data Quality checks in CI and Data Pipelines across different layers of the Medallion Architecture for the Lakehouse platform.
Data Engineer at Sweeft Digital(July 2021 - October 2023):
- Developed an analytical and visualization platform for automatically ingesting data to and from multiple marketing sources.
- Designed, developed, built, deployed and migrated existing numerous and reusable data pipelines using serverless and serverful technologies on Google Cloud Platform, resulting in a 50% reduction in data processing time and costs.
- Developed and maintained Data Lakes and Warehouses on Google Cloud Platform using Google BigQuery and Cloud Storage.
- Utilized best practices to support continuous process automation for data ingestion and data pipeline workflows.
- Developed reusable CI/CD workflows and IAC for data pipelines using Terraform and Cloud Build.
- Worked closely with stakeholders across departments to design, build and deploy various initiatives within the data platform.
- Mentored interns later becoming Data Engineers."
data engineer,"Currently I am working in healthcare domain at M42 Health. Our team is responsible for the largest in the world whole genome sequenced data. We facilitate fast and reliable access to data for analytical/research use cases. Currently I am responsible for developing distributed pipeline for preparing data for one of the largest research use cases in world (sample wise). In my previous roles, I have developed backend applications using FastApi and PostgreSQL. Developed large scale web scraping applications. Improved performance of existing ETL pipelines and developed new ones. I have deployed and maintained services on Kubernetes cluster using Helm software. I usually do data modeling and data ingestion."
data engineer,"As a seasoned Data Engineer, I have honed my skills across a wide range of technologies and platforms. My expertise spans Java, Groovy, Spring Boot, PostgreSQL, Hive, HBase, Phoenix, NiFi, Kafka, Apache Spark, Airflow, Python, Git, GitLab, Docker, and Kubernetes, among others.
Specializing in the design and management of efficient data infrastructure, I excel at building robust data pipelines that ensure seamless data processing and analysis. My deep understanding of these cutting-edge tools and frameworks allows me to architect and implement scalable, high-performing data solutions that drive business value.
Throughout my career, I have been instrumental in:
Designing and deploying data platforms that optimize data storage, processing, and accessibility
Developing and maintaining complex data pipelines using technologies like Apache NiFi, Kafka, and Airflow
Integrating disparate data sources and ensuring data quality, reliability, and security
Leveraging Big Data technologies such as Apache Spark, Hive, and HBase to power advanced analytics and reporting
Collaborating cross-functionally with stakeholders to understand business requirements and translate them into effective data solutions
Automating and streamlining data engineering workflows using Docker, Kubernetes, and other DevOps practices
Staying up-to-date with the latest industry trends and continuously expanding my technical expertise
With a keen eye for detail and a strong problem-solving mindset, I am adept at delivering high-impact data engineering solutions that drive business growth and insights. My versatile skill set and ability to adapt to evolving data challenges make me a valuable asset to any organization.
Accomplished Data Engineering Projects
Throughout my career as a Data Engineer, I have successfully executed a wide range of projects that leveraged my extensive technical expertise. Here are some key accomplishments:
1.	Designed and Implemented a Large-Scale Data Platform:
‚Ä¢	Architected a scalable data platform using a combination of PostgreSQL, Hive, HBase, and Phoenix to handle terabytes of structured and unstructured data
‚Ä¢	Developed sophisticated data pipelines with Apache NiFi and Kafka to ingest, transform, and load data from various sources
‚Ä¢	Optimized data storage and processing performance using Apache Spark and Airflow for efficient batch and real-time data processing
2.	Streamlined Data Ingestion and Transformation Processes:
‚Ä¢	Automated the ingestion of data from multiple sources, including databases, APIs, and files, using a combination of Python scripts and NiFi workflows
‚Ä¢	Implemented robust data transformation and enrichment processes using Spark Structured Streaming and Scala, ensuring data accuracy and consistency
‚Ä¢	Developed a self-service data mart powered by PostgreSQL , enabling business users to access and analyze data with ease
3.	Improved Data Monitoring and Incident Resolution:
‚Ä¢	Implemented a comprehensive monitoring and alerting system using tools like Prometheus, Grafana, and Kibana to proactively identify and address data pipeline issues
‚Ä¢	Developed a centralized logging and error handling framework using the ELK stack, enabling swift root cause analysis and incident resolution
‚Ä¢	Collaborated with cross-functional teams to enhance data quality, reliability, and security across the organization
4.	Facilitated DevOps Best Practices:
‚Ä¢	Automated the deployment and management of data engineering services using Docker and Kubernetes
‚Ä¢	Implemented a GitLab-based CI/CD pipeline to streamline the development, testing, and deployment of data engineering applications
‚Ä¢	Advocated for and implemented infrastructure as code (IaC) practices, enhancing the scalability and maintainability of the data platform
5.	Delivered Actionable Business Insights:
‚Ä¢	Leveraged Apache Spark, Hive, and HBase to build complex data processing pipelines that enabled real-time analytics and reporting
‚Ä¢	Collaborated with business stakeholders to understand their data requirements and translated them into effective data solutions
‚Ä¢	Provided data-driven recommendations and insights that informed strategic decision-making and drove business growth
Throughout these"
data engineer,"Over the past 6+ years, I have specialized in designing and maintaining scalable data pipelines that handle complex workflows and large volumes of data. I have developed efficient ETL processes, ensuring seamless extraction, transformation, and loading from diverse data sources while maintaining high standards for data quality and consistency.
Strategic Data Management: Developed and implemented strategic plans for IT Asset Management, focusing on data accuracy,
completeness, and reliability. For example, utilized Python and SQL to create automated scripts that identified and corrected data
inconsistencies, improving data accuracy by 30%.
‚Ä¢ ETL Processes: Built ETL pipelines using Apache Airflow to automate data extraction, transformation, and loading from multiple sources.
This reduced manual data handling efforts by 40% and streamlined data ingestion processes.
‚Ä¢ BI Tools and Dashboards: Developed and managed enterprise-level dashboards using PowerBI. Created interactive visualizations that
provided actionable insights into IT operations, helping reduce incident response time by 20%.
‚Ä¢ Cloud Integration: Integrated ServiceNow with AWS for cloud-based data storage and processing. This improved data accessibility and
operational efficiency by enabling seamless data flow between on-premises and cloud environments.
‚Ä¢ Statistical Analysis: Applied advanced statistical methods using Python's libraries like Pandas and NumPy to analyze large datasets. For
instance, conducted a root cause analysis on incident data, leading to a 15% reduction in recurring issues.
‚Ä¢ Collaboration and Communication: Worked closely with stakeholders to translate business objectives into data-driven solutions. For
example, collaborated with business analysts to design a predictive maintenance model that forecasted asset failures, resulting in a 25%
decrease in downtime.
‚Ä¢ Data Security and Compliance: Ensured compliance with data security policies by implementing AWS IAM roles and policies. Maintained
data confidentiality and integrity in accordance with organizational standards.
Built Scalable Data Pipelines: Designed and implemented an automated ETL pipeline that reduced data processing time by 40%, handling over 10 million data records daily across multiple sources.
Optimized Database Performance: Improved query performance in a PostgreSQL database by 60% by restructuring complex queries and indexing large tables, resulting in faster data retrieval and better system efficiency.
Cloud-Based Data Solutions: Migrated data workflows to AWS, utilizing S3 for storage and Lambda functions for processing, reducing infrastructure costs by 30% and increasing processing speed by 25%.
Automated Data Quality Monitoring: Developed Python scripts and automated monitoring tools that reduced data inconsistencies by 20% across a large dataset, ensuring reliable data for analytics teams.
Integrated New Data Sources: Integrated 5+ new data sources into an existing architecture, enabling the business to gain new insights and drive data-driven decision-making.
Enhanced Collaboration: Worked closely with cross-functional teams to deliver a data solution that improved reporting accuracy by 15%, increasing the trust in data for business stakeholders.
Led Performance Optimization Initiative: Spearheaded an initiative to optimize data pipeline performance, reducing data processing time from hours to minutes, improving efficiency for the entire data team."
data engineer,"Jul-2023 - Till now (Aug-2023) - Data Engineer, EPAM Systems
Customer Description: Life Sciences & Healthcare
Responsibilities:
‚Ä¢	Perform the migration from GCP to AWS for a key customer.
‚Ä¢	Design, develop, and maintain scalable data pipelines.
‚Ä¢	Develop the entire ETL process, including integrating new data sources and FHIR (CDR)
‚Ä¢	Participate in architecture design discussions and decision-making.
‚Ä¢	Develop and manage Airflow DAGs.
‚Ä¢	Optimize scripts to ensure compatibility with Redshift while improving performance and efficiency.
‚Ä¢	Optimize GCP costs to improve efficiency.
‚Ä¢	Build Looker reports to present product performance insights to stakeholders and customers
‚Ä¢	Schedule ETL processes (daily and monthly), ensuring smooth integration with existing ETL operations. Develop framework for Data Quality.
Tools and Technologies: BigQuery, Reshift, Lambda, Step Function, Composer ( Airflow), SQL, Looker,  Git, Jira, Confluence
Nov-2021 - Jul-2023 - Data Engineer, EPAM Systems,
Customer Description: FS - Insurance
Team Size: Dev Team: 3 Developers; Vendor Dev Team: 3 Developers; Business Analyst team: 4 Analysts
Responsibilities:
‚Ä¢	Developed and maintained Snowflake objects (Stored Procedures, Functions, Tables, Stages, Views) using JavaScript and Snowflake SQL
‚Ä¢	Developed Pipelines for ETL processes, Data Integration, Data Transformation, Data Archiving, and Process Automation
‚Ä¢	Developed PowerShell scripts to manipulate files on AWS S3
‚Ä¢	Designed complex automation processes using ActiveBatch (Integration between Snowflake, AWS S3, Voyager, and SFTP Servers)
‚Ä¢	Developed error logging, notifications, and alerts at different points of the data exchange process
‚Ä¢	Designed detailed documentation for the existing processes
‚Ä¢	Actively participated in the deployment of project changes to production (Bamboo pipelines, Flyway pipelines)
‚Ä¢	Performed data model synchronization between systems SimCorp and AWS Glue/Athena combo
‚Ä¢	Developed PowerBI data resume_classifier, reports, and dashboards
‚Ä¢	Developed and updated AWS policies
Tools and Technologies: SimCorp Dimension, Oracle, Snowflake, Snowflake, SimCorp DWH, ActiveBatch, AWS Glue, Git, CloudForge, CloudFormation, Bamboo, SFTP Client Applications, Jira, Confluence, Voyager, Oracle SQL,  Snowflake SQL,  Power BI, AWS Crawler, AWS Glue, SimCorp Dimension, ActiveBatch, Powershell
Feb-2021 - Sep-2021 - Database engineer, Innohub
Project Description: System for tracking and predicting health-check for cars, System for estimat"
data engineer,"**Senior Data Engineer**
**Latest Work Experience**
**RS Group (Sep 2023 - July 2024)**
- Spearheaded the integration and efficient streaming of data using Amazon Kinesis Data Firehose to enhance data flow to various analytics services.
- Developed and enhanced ETL processes utilizing Apache Spark and Kafka, significantly boosting real-time data processing capabilities and system scalability.
- Orchestrated complex data logging and analysis by leveraging AWS CloudWatch Logs with OpenSearch, akin to an ELK stack, to enable robust data visualization.
- Designed AWS Lambda functions for GDPR-compliant automated data management, enhancing data security and compliance.
- Initiated and managed the collection of unstructured data using AWS Matillion, effectively handling large data volumes across multiple formats.
**TATA Consultancy Services (Aug 2022 - Mar 2025)**
- Engineered data warehousing solutions with Amazon Redshift, using Data Vault modeling to ensure data integrity and scalability.
- Utilized Apache Flink for dynamic, real-time data analysis, facilitating rapid, data-driven business decisions.
- Deployed scalable web applications on AWS, incorporating Elastic Beanstalk, RDS, and OpenSearch for enhanced performance and user experience.
- Integrated Snowflake and PySpark within the ecosystem to centralize and process extensive data sources, streamlining analytics and reporting processes.
**Notable Achievements**
- Transitioned from a PHP/WordPress background to mastering Python and data engineering technologies within a week, highlighting rapid adaptability and commitment to growth.
- Pioneered a prototype of a sales-oriented chat bot in just one day, which evolved into a robust, fully developed product under my leadership.
- Developed a microservice for testing LLM responses, utilizing another LLM, enhancing the evaluation process and ensuring high-quality prompt adjustments.
- Created a sentiment analysis tool capable of parsing vast internet data, equipped with a web interface that supports both SQL queries and a graphical user interface.
**Looking For**
- Opportunities to make a substantial impact through innovative data engineering.
- A collaborative team environment where creativity and innovation are encouraged.
- A flexible work schedule that accommodates late starts, reflecting my nocturnal"
data engineer,"I have over 10 years of experience in Business Intelligence and data management across various domains, including finance, sales, and the gas & oil industry. I‚Äôve worked both as part of a team and as a solo BI developer on projects. My expertise includes building and developing large Data Warehouses (DWH), focusing on ETL processes and visualizing data through reports.
Technologies I‚Äôve worked with, ranked by experience, include:
SQL, BODS, Power BI, SSRS, Powershell, Databricks.
I am interested in Cloud solutions and would be excited to participate in projects that involve this technology. I'm looking forward to applying my expertise in Business Intelligence and data management to cloud-related initiatives."
data engineer,"Senior DWH/BI Developer | Pasha Life
09/2017 ‚Äì 11/2024
Developed end-to-end ETL processes in MS SQL Server for building and managing the enterprise Data Warehouse (DWH).
Created and automated internal reports using SSRS and Power BI, ensuring stakeholders had access to real-time and accurate insights.
Delivered training for new team members on SQL, Power BI, and business intelligence, fostering a data-driven company culture.
Spearheaded data integration efforts, ensuring seamless data flow across multiple platforms and systems.
Analyzed requirements from data analysts and designed appropriate data marts to support their reporting and analytical needs.
Ensured accurate and structured data provisioning within the Data Warehouse to facilitate future Artificial Intelligence (AI) implementations.
Delivered SQL training sessions to Data Governance teams to enhance their technical capabilities. Additionally, provided high-quality reports to support data quality management initiatives.
Software Developer | AXA Mbask
04/2015 ‚Äì 08/2017
Developed Call Center applications using C# WPF, enhancing customer support capabilities.
Built and managed HR ERP systems in ASP.NET MVC, streamlining HR operations and processes.
Conducted digital sales analysis by integrating ASP.NET MVC APIs and PL/SQL, providing actionable insights for sales optimization.
Designed and implemented a Face Detection system using C# (WinForms) to monitor and register employee attendance across regions.
Automated internal reporting systems using T-SQL, PL/SQL, and SSRS, reducing manual workloads and increasing accuracy.
Software Developer | Azerbaijan National Academy of Sciences
10/2014 ‚Äì 03/2021
Designed and implemented ""Optim ME,"" a software tool for automating mathematical calculations in chemistry processes using C# WPF.
Collaborated with researchers to customize algorithms for specific scientific experiments.
Accomplishments
Recognized multiple times as Employee of the Month for exceptional performance and dedication.
Awarded 2nd Best Employee of the Year, a significant achievement during the early stages of my career at the company.
Received several letters of appreciation from the CEO, acknowledging my valuable contributions to the organization.
Certifications
Successfully passed the T-SQL Querying Exam, demonstrating advanced SQL querying skills.
Earned a Python Programming Certificate, showcasing proficiency in Python for data analysis and development.
Completed IBM Data Analytics Certificates, gaining expertise in data analytics and visualization techniques.
Achieved Google Data Analytics Certification, focusing on data-driven decision-making and business insights.
What I want:
Opportunities to work on challenging and impactful projects that allow me to grow professionally and enhance my skills.
A collaborative and positive work environment where team members support and respect each other.
Fair compensation, recognition for accomplishments, and opportunities for continuous learning through training and certifications.
Clear communication of goals and expectations from leadership to ensure alignment and productivity.
What I don‚Äôt want:
A toxic or negative workplace culture that demotivates employees.
Lack of structure, unclear objectives, or insufficient feedback from management.
Micromanagement or excessive bureaucracy that hinders creativity and efficiency.
Limited opportunities for personal or professional growth."
data engineer,"–∫–æ–º–µ—Ä—Ü—ñ–π–Ω–∏–π –¥–æ—Å–≤—ñ–¥ –∑ python - 7 —Ä–æ–∫—ñ–≤, –≤—ñ–¥–∫—Ä–∏—Ç–∏–π —Ñ–æ–ø 3 –≥—Ä—É–ø–∞, —î –¥–æ—Å–≤—ñ–¥ Team Lead, –∫–æ–º–µ—Ä—Ü—ñ–π–Ω–∏–π –¥–æ—Å–≤—ñ–¥ –∑ Flask - 3 —Ä–æ–∫–∏, –∫–æ–º–µ—Ä—Ü—ñ–π–Ω–∏–π –¥–æ—Å–≤—ñ–¥ –∑ Django - 4 —Ä–æ–∫–∏, —î –¥–æ—Å–≤—ñ–¥ –º–µ–Ω—Ç–æ—Ä—Å—Ç–≤–∞ —Ç–∞ –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç—É –∫–æ–º–∞–Ω–¥–∏."
data engineer,"Data Engineer ‚Äì GlobalLogic
Jan 2025 ‚Äì  Present
- Built an automated pipeline to ingest PDF documents, structuring data efficiently using a Bronze-Silver-Gold architecture on Databricks, Azure, and Airflow.
- Created a real-time Google Drive crawler to keep the data lake updated with the latest documents, ensuring quick and reliable access.
- Optimized workflows to smoothly transform PDFs into usable structured data, enhancing them further with AI-generated metadata and smart classification.
- Ensured robust data pipelines with a strong focus on data integrity, clear lineage tracking, and advanced document classification processes.
- Technologies Used: Databricks, Azure, PySpark, Python, Airflow, Azure OpenAI.
Data Engineer ‚Äì EPAM Systems
Aug 2024 ‚Äì Dec 2024
- Improved notifications and assisted with DBX.
- Helped transition data encryption from PostgreSQL to Databricks.
- Technologies: Databricks, Azure, SQL, DBX, Apache Spark, Python.
Data Engineer ‚Äì EPAM Systems
Jun 2023 ‚Äì Jul 2024
- Migrated critical datasets to QuantHub with 100% data integrity.
- Automated data workflows using Power Automate, improving pipeline efficiency by 40%.
- Designed optimized data structures for seamless integration into Azure-based platforms.
- Tools: Python, Azure Databricks, Power Automate, Elasticsearch, Confluence.
Data Engineer ‚Äì EPAM Systems
Apr 2023 ‚Äì Jun 2023
- Created Proof of Concept (PoC) for Medallion Architecture in Databricks.
- Developed SQL queries to validate and optimize data migration processes.
- Reviewed and improved team SQL scripts for better performance.
- Tools: Databricks, Azure Data Lake, SQL, Python.
Data Engineer ‚Äì EPAM Systems
Apr 2021 ‚Äì Oct 2022
- Enhanced resource utilization in Druid, reducing costs by 25%.
- Optimized Kafka streams and resolved complex performance bottlenecks.
- Migrated and maintained distributed data systems with zero downtime.
- Tools: Apache Kafka, Druid, Google Cloud Platform, Kubernetes.
Software Engineer ‚Äì EPAM Systems
Aug 2019 ‚Äì Feb 2021
- Wrote data transformation scripts using SQL/Groovy
- Improved stability of AWS Aurora database during failover
- Wrote highly-tested Java backend code
- Profiled code with JProfiler
- Passed the first exam out of two for Java 11 certification
Software Engineer ‚Äì EPAM Systems
Jun 2019 ‚Äì Aug 2019
- Fixed problems in Java backend and JSP frontend
Experienced Data Engineer with over 6 years in data engineering and cloud computing. Expertise in Databricks, Apache Spark, and Azure, with proven success in implementing scalable data pipelines and optimizing data workflows. Certified in Azure Data Engineering (DP-203) and Databricks Data Engineer Associate. Skilled in Python, SQL, and Big Data technologies, with a strong focus on performance optimization and data reliability."
data engineer,"5th commercial project (>1.5 years, in progress) - data engineering (newly built B2B CRM) for a global enterprise client (a company from TOP-20 UK companies): AWS Step Functions, PySpark, Redshift, Lambda, Athena, SQL Server, Iceberg, etc.
4th commercial project (~6 months) - global privacy protection-based search engine (DuckDuckGo alternative). I defined architecture and built integrated and automated parsing of Wikipedia articles big data dumps within AWS Serverless infrastructure (AWS Step Functions, Lambda, Glue/PySpark, DynamoDB, AWS SAM) - to use it for displaying Wikipedia widget in a search engine results.
3rd commercial project (~3 months) - NFT-market based project in transition from Flask to Django. Writing basic Django replica (basic fullstack Django/Bootstrap) app from scratch with functionality of existing project in Flask (using docker & docker compose, consisting of Django web app, nginx, celery, Postgres, Redis, Rabbitmq), celery jobs of parsing of data from Opensea, endpoints and views, user dashboards & authentication etc.)
2nd commercial project (~14 months) - US e-commerce platform for managing products marketing on Amazon - new functionality, performance & speed improvements, debugging, Pytest tests from scratch, ad hoc scripts.
1st commercial project (~7 months) - US e-commerce platform for managing logistics & goods deliveries costs & efficiency for e-commerce traders - improvements and debugging.
pet-project (~6 months) - e-commerce platform for managing drop-shipping products sales in Ukraine e-commerce aggregators - myself full-stack building from scratch to prototype.
experience as team lead of a team of ~10 data engineers (~6 months)
experience as an architect of a separate data engineering feature for parsing various dumps of all Wikipedia articles/metadata in a startup (as part of a team of ~50 people) - 6 months
project to parse Amazon data using Tor network of free proxies as an alternative to paid services (client could decrease monthly costs by a few thousand US dollars)
background before switching to IT - economics & finance (passed 2 out of 3 levels of Chartered Financial Analyst [CFA] qualification), data analytics roles in various Ukraine-based western-capital top-tier corporate banks / venture capital & insurance companies.
Focus on data engineering in AWS environment.
Elements of data science/MLOps/ML/AI might be interesting, because I have some solid but a little bit outdated statistical background."
data engineer,"Data engineer, 2024-2025
Client - multi-platform audio and entertainment company.
The main idea of the project to support data users with expected, ready to use, up to date datasets based on their needs.
- ingest data from different sources
- implement ETL pipelines,
- moved Airflow pipelines from old account to the new one.
- develop new data pipelines using Airflow, Python, SQL, API.
- update existing data pipelines with the new features.
- fix production issues.
- use SQL queries in Snowflake to transform, clean, load the data.
- set data validation checks
Data analyst/engineer, 2021 - 2024
Client - US retail and CPG agency
- write, run SQL code in RDBMS YellowBricks, Netezza,
- support, monitor, modify bash jobs (Shell scripting), work in Autosys job scheduling system,
- data engineering task using Python and Spark in Azure Databricks,
- build data processing pipelines in Azure Data Factory,
- use Azure DevOps, Git for version control, create branches, Pull Requests, code review, code deployment
Senior Data analyst,  2019 - 2021
FinTech project. Client - one of the biggest US hedge fund.
Our team developed Data Factory - Customer Data Platform, where customer will collect all the data and use it for decision making process in stock markets.
Macroeconomic data tasks:
- extract macroeconomic data (gdp, cpi, ppi, flow of funds) releases from provider web site; COVID data from open sources,
- convert to custom XML format using Python scripts,
- check for errors, anomalies in data (outliers, seasonality, missed data points) on test environment, launch appropriate reports, Rundeck jobs,
- load data to platform, monitor progress in Kibana,
- save XML files with dataset releases on AWS S3 storage.
Company data tasks:
- analyze datasets in MS SQL database:  structure, types of data, relations, define mapping, metadata/time series fields, and measures of datasets (export/import, commodity data, ownership, equity),
Use MS SQL, Python (depends on source data).
- discuss with client results of analysis and provide it to business analysts and developers.
Senior Analyst/Consultant, 2 years
Experience in marketing research, FMCG, consumer insights, consulting. Create reports and dashboards in Power BI
‚Ä¢	Areas of experience include CPG and Retail.
‚Ä¢	Participated in:
o	Forecasting projects
o	Innovation strategy
o	Business Hierarchies
o	Price&Promo
market overview, Category and need states analysis, opportunity spaces prioritization, new hierarchies‚Äô creation"
data engineer,"I am a Python Developer with 3 years of experience and over 15 years in IT, including 10+ years as a system administrator. Currently a Team Lead at Ajax Systems, I focus on microservices architecture, SQL database management, and CI/CD automation. Proficient in Python Core, FastAPI, and tools like Apache Airflow. Detail-oriented and responsible, I am continuously enhancing my skills and seeking new opportunities as a Data Engineer to apply my expertise in data-driven solutions.
Under my leadership, the team became one of the best teams in terms of accomplishing the assigned tasks.
I am motivated by complex tasks that require teamwork and non-trivial solutions.
I'm interested in the data engineering and fintech."
data engineer,"As a DBA, I know how it should be, how it can be, and what the difference is. Yes, it's all about tradeoffs. And there are will be much more tradeoffs in case you want to scale your workload.
p.s.
Oh, there was a limit for the description's length. Ok, I fit it.
I may explain ""how it works"" to the dev team. Yes, with examples."
data engineer,"–ú—ñ–π –¥–æ—Å–≤—ñ–¥ –∑–æ—Å–µ—Ä–µ–¥–∂–µ–Ω–∏–π –Ω–∞ —Ä–æ–∑—Ä–æ–±—Ü—ñ –Ω–∞–¥—ñ–π–Ω–∏—Ö —ñ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–∏—Ö —Ä—ñ—à–µ–Ω—å –¥–ª—è –ø–æ–±—É–¥–æ–≤–∏ –ø—Ä–æ—Ü–µ—Å—ñ–≤ –æ–±—Ä–æ–±–∫–∏ –¥–∞–Ω–∏—Ö. –ú–∞—é –∑–Ω–∞—á–Ω–∏–π –¥–æ—Å–≤—ñ–¥ —Ä–æ–±–æ—Ç–∏ –∑ —Ö–º–∞—Ä–Ω–∏–º–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞–º–∏, —Ç–∞–∫–∏–º–∏ —è–∫ Google Cloud Platform (GCP), Microsoft Azure, Amazon Web Services (AWS), –∞ —Ç–∞–∫–æ–∂ —ñ–∑ —Å–∏—Å—Ç–µ–º–æ—é –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü—ñ—ó Kubernetes.
–¢–∞–∫–æ–∂ –º–∞—é –≤–µ–ª–∏–∫–∏–π –¥–æ—Å–≤—ñ–¥ —É —Ä–æ–±–æ—Ç—ñ –∑—ñ —Å—Ç—Ä–∏–º—ñ–Ω–≥–æ–≤–∏–º–∏ –¥–∞–Ω–∏–º–∏ —Ç–∞ –∞–Ω–∞–ª—ñ—Ç–∏–∫–æ—é –≤ —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ. –û—Å–Ω–æ–≤–Ω—ñ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó, –∑ —è–∫–∏–º–∏ —è –ø—Ä–∞—Ü—é–≤–∞–≤, ‚Äì PubSub Lite —Ç–∞ Azure Event Hub. –ú–µ–Ω—à –∑ Apache Kafka.
–ó–∞ —á–∞—Å —Å–≤–æ—î—ó —Ä–æ–±–æ—Ç–∏ —è —ñ–Ω—Ç–µ–≥—Ä—É–≤–∞–≤—Å—è –±—ñ–ª—å—à –Ω—ñ–∂ —ñ–∑ 40 –∑–æ–≤–Ω—ñ—à–Ω—ñ–º–∏ API. –ú–∞—é –≤–µ–ª–∏–∫–∏–π –¥–æ—Å–≤—ñ–¥ —É —Ä–æ–±–æ—Ç—ñ –∑ data warehouse —Å–∏—Å—Ç–µ–º–∞–º–∏, —Ç–∞–∫–∏–º–∏ —è–∫ Google BigQuery, ClickHouse —Ç–∞ Snowflake. –¢–∞–∫–æ–∂ –ø—Ä–∞—Ü—é–≤–∞–≤ –Ω–∞–¥ –ø–æ–±—É–¥–æ–≤–æ—é data lakehouse –Ω–∞ –±–∞–∑—ñ Amazon S3 —Ç–∞ Google Cloud Storage (GCS) —ñ–∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º Databricks —Ç–∞ Apache Spark.
–í–µ–ª–∏–∫–∏–π –¥–æ—Å–≤—ñ–¥ —É Python —Ä–æ–∑—Ä–æ–±—Ü—ñ. –ü–æ–±—É–¥–æ–≤–∞ API-–¥–æ–¥–∞—Ç–∫—ñ–≤ –¥–ª—è –º—ñ–∫—Ä–æ—Å–µ—Ä—Å–Ω–æ—ó –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ —Ç–∞ –∑–≤'—è–∑–∫—ñ–≤ —Ä—ñ–∑–Ω–∏—Ö –¥–∞—Ç–∞-–ø—Ä–æ—Ü–µ—Å—ñ–≤.
–ú–∞—é –∑–Ω–∞—á–Ω—É –µ–∫—Å–ø–µ—Ä—Ç–∏–∑—É —É —Å—Ñ–µ—Ä—ñ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–æ—ó —Ç–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ—ó –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏.
–ú–∞—é –¥–æ—Å–≤—ñ–¥ –º—ñ–≥—Ä–∞—Ü—ñ—ó –∑ Databricks –Ω–∞ Apache Spark —É Kubernetes, —â–æ –¥–æ–∑–≤–æ–ª–∏–ª–æ –∑–Ω–∏–∑–∏—Ç–∏ –≤–∏—Ç—Ä–∞—Ç–∏ –±—ñ–ª—å—à –Ω—ñ–∂ —É 6 —Ä–∞–∑—ñ–≤ (–µ–∫–æ–Ω–æ–º—ñ—è —Å–∫–ª–∞–¥–∞–ª–∞ –∫—ñ–ª—å–∫–∞ –º—ñ–ª—å–π–æ–Ω—ñ–≤ –¥–æ–ª–∞—Ä—ñ–≤ –Ω–∞ —Ä—ñ–∫).
–†–æ–∑–≥–ª—è–¥–∞—é —Ç—ñ–ª—å–∫–∏ –ø–∞—Ä—Ç-—Ç–∞–π–º –ø—Ä–æ–ø–æ–∑–∏—Ü—ñ—ó."
data engineer,"Developed Customer 360 platform to unify customer data from various sources into one view for sales and marketing teams while working as a data engineer.
Built a single customer platform to analyze full customer journeys across websites, apps, and stores to identify pain points.
Created ETL pipelines, database architecture and SQL queries to transform raw data into insights.
Stock Prediction Chatbot - Built an NLP chatbot in Python that provides stock price predictions using machine learning algorithms.
GigAi Chatbot - Worked on the development of a customer service chatbot handling FAQs for an e-commerce company.
Collaborated on an NLP project leveraging LLMs to analyze social media data for insights into global mental health and well-being.
Cleaning, processing and analyzing large datasets using Python and SQL to draw insights.
Building data visualizations and presenting findings to stakeholders across multiple countries collaborating on the project.
I have also worked as a Lead AI Engineer for a Stock trading company in the United States where I was tasked to oversee the back-tesking processes as well as developing AI resume_classifier that would help investors in real time.
Interesting projects"
data engineer,"Summary :  .
Over 20 years of IT experience
Primary skill : BI Analyst, Business analyst, BI team leader, BI SME, BI technical lead, ETL Technical Lead.
BI solution architect, Senior business intelligence developer, Senior DWH developer.
Industries : Financial, Telecommunication, Automotive, Airline
Organization skill : Software lead, Technical lead, Team leader
Primary DWH platform : Teradata, MS SQL Server
Primary BI platform : SSIS, SSRS, Power BI, MS Excell, SAP BO
Hands on programmer experience for the various databases. (procedures, triggers, ...)
."
data engineer,"I specialize as a Backend Developer with a primary focus on Python (Django). However, my expertise extends beyond these areas, covering a diverse range of technologies, including but not limited to Front-end development (React), Cloud
Service like Azure, as well as Machine Learning and Data Science.
Always eager to boost my engineering skills through exciting projects and contribute to a great community as an individual.
Fully skilled on:
- Python, Django, Rest Framework, Celery
- PostgreSQL, Microsoft SQL Server, MongoDB
- Redis, RabbitMQ
- Pandas, Numpy
- Azure, Sentry, Git, CI/CD, Docker/Docker-compose
- System Design
- LLM, LLama, OpenAI API, RAG, Microsoft Bot Framework
- Web scraping/crawling
Have general knowledge on:
- JavaScript, React, Ant Design, Redux-Toolkit
- Ngnix
- Machine Learnig/Data Science/Data Analyses
Roles and responsibilities I follow as Backend Developer:
- Writing, debugging and maintaining code
- Working under Agile Methodology
- Troubleshooting software issues
- Development of web applications from scratch
- Working closely with other developers to improve product‚Äôs functionality
- Developing innovative solutions and robust, scalable, and secure features
- Attending and contributing to company development meetings.
- Monitor the performance of internal systems
- Participating in quality assurance activities
- Participating in estimation discussions with the product team
- Continually improving coding skills by learning the codebase and improving my coding skills.
- Follow internal company coding conventions.
- Following a strict code of ethics and protecting any confidential information at all times
- Troubleshooting
Projects with stack of Python, Django, LLMs, Openai, RAG, LLama-index, Azure"
data engineer,"Data Engineer and Data Analyst with 6 years of experience in Data field. As a Data Professional, I possesss experience in different data platforms with focus on Data Engineering, Data warehousing, Data analytics and Cloud solutions (MS Azure, Snowflake and AWS). I have experience in building traditional as well as Cloud Data Warehouses (DWH) and Data Lakes. Solid experience in continuous delivery tools and technologies working with modern Agile development methodologies.
Technical skills and technologies:
- Programming languages: SQL (T-SQL, MySql, PostgreSQL), Python;
- BigData: Pyspark, Kafka, Hadoop, Yarn, HDFS
- RDBMS: SQL Server, MySQL, PostgreSQL;
- ELT/ELT and Orchestration Tools : SSIS, Azure Data Factory, Airflow, Azure Synapse, Azure Databricks, AWS Glue
- Cloud : MS Azure, Snowflake, AWS (EMR, S3, Redshift, EC2, Glue)
- Storages: HDFS, Azure Data Lake Gen2, Azure Blob, AWS S3
- Data Analysis: SQL, Pandas, Numpy
- Data Visualization: MS Power BI, SSRS, Tableau
- Data warehousing : Kimpball, Inmon, Data vault"
data engineer,"Data Engineer | Data migration | Retail
Designed and implemented scalable data migration pipelines from scratch using Apache Airflow .
Developed ETL workflows to extract, transform, and load large datasets from on-prem Teradata to
Cloud Storage and BigQuery.
Optimized data processing performance in batch workflows using Spark (Dataproc).
Collaborated with cross-functional teams to define migration strategies and troubleshoot data
quality issues.
Data Engineer | Data platform development | Travel industry
Participated in the design and development of dbt resume_classifier from scratch.
Took an active part in the development of Airflow data ingestion pipelines. Contributed in DAGs
design solution. Implemented automations that reduced the DAGs development process by 70% of time.
Certifications: Astronomer Certification for Apache Airflow Fundamentals, Databricks Academy
Accrediation - Apache Spark Programming, dbt Academy - dbt Fundamentals
I'm looking for engaging and meaningful tasks that challenge me and help me grow professionally. Just as importantly, I value a healthy and respectful work environment ‚Äî one with reasonable deadlines, mutual trust instead of micromanagement or constant tracking, and a generally positive atmosphere where people enjoy working together."
data engineer,"Results-oriented Data Engineer with over 10 years of experience in Databases, Cloud Data Warehousing, and Big Data solutions. Proficient in AWS (Redshift, Glue, Lambda, S3), GCP (BigQuery), Python, Spark, and SQL, with expertise in designing, implementing, and optimizing scalable data architectures. Skilled in orchestrating complex data pipelines using Apache Airflow, ensuring efficient and reliable workflows. Demonstrated ability to lead teams, collaborate with clients, and deliver autonomous, scalable solutions in complex and enterprise environments.
Buiilt and optimized Big Data pipelines.
Challenging tasks."
data engineer,"creating reports (stored procedure MS SQL Server + reporting form Stimulsoft)
optimization of own and previously written code
Wrote SQL statements and stored procedures.
Working with relational databases and SQL.
Developing findings and solutions to audiences, Technical support and administration.
Participated in back-end development(Java) in the company's Web-services.
Network administration, Kafka integration.
Creation and configuration of a workflow for processing and transforming incoming client data.
Cleansing and identifying duplicates.
Updating data through third-party services in a specific format.
Extracting subsets of data for the client.
Initiating a record scoring process for the client
Writing SQL queries.
Preparing Pandas,NumPy scripts to work with dataframes using JupyterLab.
Using Alteryx for some data preparations."
data engineer,"main responsibilities:
ensure quality of data exchange development from client`s or outsource developments between client`s ERP and our SaaS solution, or develop integrations internally. Also, responsible for developing\updating data integrations documentations
Currently working on BI development as Analytics engineer
Setup process of manual data integration as an easier integration option, currently used by multiple clients, setup of documentation of most common issues during data exchange processes.
Creating different data resume_classifier for further usage by other data team or for consumption by PowerBI developers, while also maintaining documentation.
Also familiar with dbt and elt processes (Djinni does not allow those as tags)
Cureently looking for data engineer\data analyst\analytics engineer jobs (as I am currently employed I do not seek to get hired
immediately)"
data engineer,"–ü–æ–¥—Ä–æ–±–Ω–æ –æ —Ç–æ–º —á–µ–º —è –∑–∞–Ω–∏–º–∞–ª—Å—è:
–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
–∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏: –≠—Ç–∞ —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª–∏–ª–∞ –Ω–∞–º –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ –≤—ã—è–≤–ª—è—Ç—å –∏ —É—Å—Ç—Ä–∞–Ω—è—Ç—å –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –∏—Ö –≤—ã—Å–æ–∫—É—é –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫–∏. –ú—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ Kafka –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ Elasticsearch –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞.
–°–æ–∑–¥–∞–Ω–∏–µ data lake –Ω–∞ –±–∞–∑–µ Hadoop: –Ø —É—á–∞—Å—Ç–≤–æ–≤–∞–ª –≤ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–∏ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–∏–∑–Ω–µ—Å-–ø–æ–¥—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–π. –ú—ã –≤–Ω–µ–¥—Ä–∏–ª–∏ Hive –∏ Impala –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±–µ—Å–ø–µ—á–∏–ª–∏ –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–æ—Å—Ç—É–ø–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö.
–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö API:  –†–∞–∑—Ä–∞–±–æ—Ç–∞–ª —Å–∫—Ä–∏–ø—Ç—ã –Ω–∞ Python –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π, –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º), –∏—Å–ø–æ–ª—å–∑—É—è Airflow –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–æ–≤–∫–∏ –∑–∞–¥–∞—á. –≠—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—Ç–∏–ª–æ –≤—Ä–µ–º—è, –∑–∞—Ç—Ä–∞—á–∏–≤–∞–µ–º–æ–µ –Ω–∞ —Ä—É—á–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö, –∏ –ø–æ–≤—ã—Å–∏–ª–æ —Ç–æ—á–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.""
–ü—Ä–∏–º–µ—Ä–Ω–æ —Ç–∞–∫ —ç—Ç–æ –≤—ã–≥–ª—è–¥–∏—Ç , —Å–µ–π—á–∞—Å –ø—Ä–æ–¥–æ–ª–∂–∞—é —Ä–∞–±–æ—Ç—É –Ω–∞–¥ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–º–∏ –ø—Ä–æ–µ–∫—Ç–∞–º–∏.
–ú–æ—è –ø–æ–∑–∏—Ü–∏—è –Ω–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç junior+ data engineer.
–ü—Ä–æ–¥–æ–ª–∂–∞—é –¥–∞–ª—å—à–µ —Ä–∞–∑–≤–∏–≤–∞—Ç—å—Å—è –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏ –∑–Ω–∞–Ω–∏–π –∏ –≤—ã—Ä–∞—Å—Ç–∏ –¥–æ —Å–∏–ª—å–Ω–æ–≥–æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞."
data engineer,"Data Engineer/Analyst/SRE with 7+ years of experience. With main background in various monitoring/alerting-related projects, data analytics, SRE and cybersecurity.
Designing monitoring strategy based on opensource and commercial solutions, APM and CI/CD. Striving to improve customer's monitoring infrastructure to operate effectively and meeting the needs of monitoring goals.
Have experience with SIEM solutions(Splunk, Elastic, Grafana), APM monitoring(NewRelic, DataDog), Experience working with languages: Python, Bash, PowerShell. Understading of of operating systems, databases, network, version control tools, participation in a software development life cycle.
Ready to work in a team, able to communicate clearly and concisely. Able to solve problems quickly, effectively, in a timely manner. Ready to deal with unexpected outages or performance issues."
data engineer,"I am a highly motivated and client-focused Senior Data Engineer with expertise in Microsoft Azure, Databricks, and Spark. My skills include advanced SQL, data modeling, developing scalable and high-performance ETL/ELT pipelines, and orchestrating data workflows using Azure Data Factory. I have a strong background in leading teams and working closely with clients on various projects, effectively handling both leadership and client-facing roles. I also have experience in designing scalable and robust data architectures and frameworks that meet and exceed customer needs, improving their data systems.
As a team leader, I am known for my excellent communication skills. I work well with team members and clients, ensuring clear communication and successful project completion. My leadership style is focused on achieving goals while also supporting my team's growth and success.
I am passionate about mentoring and sharing knowledge. I have mentored over 20 mentees in data engineering and contributed to the development of the data engineering mentoring program to support newcomers in the field. As a conference speaker, I often share my knowledge and experiences with the wider data community.
Additionally, as a Resource Manager at the company, I support my team's career advancement and skill development, manage their compensation, source suitable projects, and resolve any arising challenges.
As a Lead Data Engineer, I play a crucial client-facing role, guiding the project toward successful outcomes by designing and implementing new features based on client requirements:
1. Designed a scalable and robust architecture for the new version of the Metadata Driven Data Ingestion Framework, ensuring it meets client specifications.
2. Developing scalable and high-performance ETL/ELT pipelines utilizing Databricks and Apache Spark to enhance data processing capabilities.
3. Developed a key feature to analyze and cleanse emails and phone numbers, utilizing the third-party Melissa API to enhance data quality and reliability.
4. Orchestrating and monitoring data workflows using Azure Data Factory, ensuring efficient data management and integration across the platform.
5. Worked closely with DevOps to design deployment processes for SQL to Azure SQL Database, Databricks, and Azure Data Factory (ADF).
6. Leading the data engineering team, optimizing task distribution to maximize team efficiency and project success.
Company's Client Impact Award:
Recognized for making substantial contributions to the client's technology stack and methodologies in the data platform development. Played a pivotal role in innovating approaches and ensuring the timely completion of the MVP phase, demonstrating exceptional commitment and performance."
data engineer,"Development and maintenance ETL/ELT pipelines using Apache Airflow for seamless data integration and
processing, with outputs to HDFS, Hive, PostgreSQL, GreenPlum, and Oracle.
Sending data to FTP, SMB servers. Email distribution.
Development and maintenance real-time data processing pipelines using Spark Structured Streaming and
Kafka.
Working with the requests library to retrieve data via API.
Development and maintenance of services on Flask, FastAPI, deploying in Docker.
Web scraping with BeautifulSoup, Selenium.
Development and maintenance of the OpenMetadata data catalog."
data engineer,"Entry-level Data Analyst / Data Engineer with a strong interest in data automation and process optimization.
Contributed to a project that migrated data from Google Sheets to SQL using Python scripts.
I switched careers, and during my entire time in my previous field, only one client ended up dissatisfied‚Äîbut the issue wasn't with the work itself; it was my unwillingness to do part of it for free.
I expect to develop rapidly and continuously as a specialist, thereby accelerating and improving the company's performance. I look forward to long-term and productive working relationships."
data engineer,"Hi everyone - I am highly skilled Data Engineer with over 10 years of experience in Data Engineering, Analytics, Data Modeling, and Business Intelligence. Adept at designing and optimizing ETL pipelines while transforming complex datasets into actionable insights. Proficient in SQL and Python for data processing and automation, with extensive experience in developing interactive dashboards and visualizations using Tableau.
Known for bridging the gap between data and business, enabling strategic decision-making through data storytelling and BI solutions. Strong problem-solving abilities, attention to detail, and a results-driven mindset ensure accuracy and efficiency in data workflows. Excellent communicator with the ability to translate technical insights into business impact, ensuring stakeholders fully leverage data for informed decision-making.
Years of experience in designing and optimizing data
warehouse structures.
Expertise in developing ETL workflows using SSIS.
Skilled in automating processes with Python and
Databricks.
Extensive experience in managing database
migrations.
Proficient in leveraging Tableau, OBIEE, and similar
tools for reporting.
Proven ability to optimize SQL query performance.
Extensive background in data modeling and schema
design.
Skilled in designing AWS architectures for database
solutions.
Experienced in creating user dashboards with
Sisense and Tableau.
Proven track record of designing comprehensive
migration plans and strategies.
Expertise in developing and managing onboarding
processes.
Adept at translating business requirements into
technical specifications.
Skilled in leading and supporting ongoing data
reporting solutions."
data engineer,"1) Data integration pipelines from 3rd party services. Set up automated data integrations and build database design to store and operate with gigabytes of data to help business make data-driven decisions.
Stack: Airflow, Postgres, BigQuery, GCP
2) Migrate data that is being produced by IoT platform to a TimeScaleDB. Provide close to live big data aggregations for client dashboards.
Stack: Posgres, TimeScaleDB
3) Deploy ML model training cycle on a Databricks platform, including fetch of data, preprocessing, model training, comparing with previous version, serving better version as an API endpoint
Stack: scikit-learn, Postgres, MLFlow, AirFlow, Databricks
4) Educational Computer Vision projects
- Un-, semi-, self-supervised and metric learning approaches for image classification, representation and retrival. Utilizing transfer learning including ViT as feature extractors.
Stack: pytorch, clip, dino, timm
Image segmentation with U-net and SAM resume_classifier
Stack: pytorch, segmentation resume_classifier, SAM
5) Educational GenAI Computer Vision project
Implementing Autoencoders, VAE, GAN, Diffusions (pixel and latent), FlowModels
Completed Data Science Program at Epam University
Obtained a Deep learning certificate on Coursera
Courses certificates and completed projects on DataCamp"
data engineer,"Part-time Laravel developer (<1yr)
‚Ä¢ Built Web scraper with Beautifulsoup-like php library and expanded website‚Äôs database to store and use this
data, thus allowing to populate majority of a website with automatically scraped and processed data.
‚Ä¢ Managed and patched MySQL database for newly created services of multiple websites, ensuring data
integrity and anomaly free design which eased development process in the future.
‚Ä¢ Made custom console tool to communicate with OpenAI REST API to process data about various animals and
automatically assign generated ‚Äúproperties‚Äù tags to animals.
‚Ä¢ Maintained, expanded and integrated websites‚Äô admin pages and their associated databases.
‚Ä¢ Developed, patched and created websites‚Äô dynamically created pages, which often required creating admin
pages and such database schemas to enshure failproof
‚Ä¢ Used numerous JS, CSS libraries, like Jquery, Tailwind, Bootstrap, swiperjs etc. UI libraries.
Successfully launched containers with Apache Spark, Kafka, local Minio AWS compatible storage and
experimented building ETL pipelines with SparkSQL batch processing interface and writing data into storage.
Familiarized myself with the basics of cloud technologies (Google cloud) + terraform; launched remote
instances of virtual machines and connected to them via SSH protocol, setting them up
Worked with pandas and numpy python libraries as a part of university python data course"
data engineer,"More than 5 years of experience in data management, specializing in building and optimizing data management systems and digital solutions. Extensive expertise in setting up and maintaining reporting systems, integrating advanced data platforms, and streamlining data flow to drive business growth and innovation. Skilled in data analysis, dimensional modeling, and ETL processes, with strong proficiency in T-SQL and Python, utilizing libraries like Pandas and PySpark. Hands-on experience with platforms such as Azure Synapse, Google BigQuery, and Snowflake, along with managing various databases and cloud environments. Proficient in leveraging cutting-edge technologies to unlock the full potential of data for organizations.
-> Successfully built and optimized scalable data management systems, improving data flow efficiency by 30%.
-> Led the integration of advanced data platforms, enhancing reporting capabilities and supporting business growth.
-> Developed and maintained ETL processes, resulting in faster data processing and improved data quality.
-> Streamlined database operations across cloud platforms such as Azure Synapse, Google BigQuery, and Snowflake, ensuring seamless performance.
-> Utilized Python and T-SQL to implement complex data analysis and modeling, driving actionable insights for strategic decision-making.
I‚Äôm seeking a long-term project that will allow me to grow professionally and further solidify and enhance my skills as a Data Engineer."
data engineer,"Hi there! I‚Äôm a data enthusiast who builds comprehensive data ecosystems and robust digital infrastructures for top and middle management. I‚Äôve worked on projects where I‚Äôve set up regular reporting systems, integrated advanced data platforms, and optimized data flows across organizations to drive transformation, innovation, and growth.
I craft tailored business intelligence (BI) solutions from scratch, enabling companies to track and monitor key performance indicators (KPIs) crucial for their digital transformation journey. I utilize practical BI tools like PowerBI, QlikView, and Tableau to help businesses make informed decisions and stay agile in the digital landscape.
My skills in data analysis, modeling, and designing ETL processes are backed by a deep expertise in T-SQL and a proven track record in performance tuning and handling large data volumes. I‚Äôve written and optimized Python ETL data pipelines using libraries like Pandas, NumPy, and PySpark for data manipulation, analysis, and visualization. My technical toolkit includes dbt, R, Jupyter, Scala, Anaconda, GitHub, and Rust.
I‚Äôve developed reporting and analytics solutions using tools like Azure Synapse, Azure SQL Database, SSAS, Google BigQuery, and Snowflake. I‚Äôm familiar with CRM systems, data warehousing solutions, and technologies like PostgreSQL, MongoDB, MySQL, Oracle, and 1C8. Plus, I have extensive experience with cloud platforms like Azure, AWS, and GCP, including components like Data Lakes, Data Warehouses, Azure Data Factory, and BigQuery.
I also work with languages like KQL, DAX, MDX, HTML, PHP, CSS, and JavaScript, and I integrate tools like Jira into my projects. With a comprehensive understanding of the entire Software Development Lifecycle (SDLC), I‚Äôve successfully delivered projects from initial data architecture design to full-scale implementation and ongoing support.
I‚Äôm eager to continue growing in data engineering and analytics. I‚Äôm passionate about leveraging cutting-edge technologies to drive innovation and help organizations make the most of their data!
1. Successfully built comprehensive data ecosystems and digital infrastructures that improved decision-making and performance tracking for top and middle management.
2. Led the implementation of regular reporting systems and integrated advanced data platforms, streamlining data flows and driving organizational transformation and innovation.
3. Developed custom BI solutions from the ground up, enabling businesses to effectively monitor KPIs and accelerate their digital transformation journey using tools like PowerBI, QlikView, and Tableau.
4. Optimized and designed ETL pipelines using Python, Pandas, PySpark, and dbt, improving data manipulation, analysis, and reporting capabilities for large data volumes.
5. Delivered high-performance reporting and analytics solutions across platforms such as Azure Synapse, Azure SQL Database, SSAS, Google BigQuery, and Snowflake, ensuring scalability and reliability.
6. Enhanced data warehousing and CRM systems, working with databases like PostgreSQL, MongoDB, MySQL, Oracle, and 1C8, along with cloud services such as Azure, AWS, and GCP.
7. Proficiently used T-SQL for performance tuning and large-scale data handling, while employing languages like KQL, DAX, MDX, HTML, PHP, CSS, and JavaScript to integrate complex solutions.
8. Managed the entire SDLC, from data architecture to implementation and support, delivering end-to-end projects that leveraged cutting-edge technology to drive innovation and data-driven decision-making.
1. Opportunities to work on long-term projects that offer professional growth and allow me to expand my skills in data engineering and analytics.
2. Challenging and engaging work where I can apply cutting-edge technologies to solve business problems.
3. Involvement in projects that focus on building and optimizing data infrastructure, creating analytics systems, and reporting that help companies make informed decisions.
4. Flexibility in work arrangements, including remote work options and a flexible schedule.
5. Clear goals and transparent processes within the team to effectively complete tasks and achieve results."
data engineer,"Summary
I'm a data engineer with 5 years of experience (Fintech, banking, healthcare). I have experience in leading a team. I am very proactive, have management and organizational skills, willing to learn and have experience in mentoring. I am able to achieve goals and handle pressure, meet deadlines and flexibly adapt to changing project requirements. Participated in the development of a project from 0 with microprocessor architecture.
Skills
‚Ä¢   Maintaining regular communication with clients to gather requirements and provide updates.
‚Ä¢   Database and Query queryoptimization
‚Ä¢   Delivering sprint reports to clients, showcasing project progress and metrics.
‚Ä¢   Participated in the construction of architecture from 0
‚Ä¢   ETL automation and create sql tests
‚Ä¢   Tuning existing database objects, fixing bugs and errors. Create new stored procedures,functions,tables,views and other database objects;
‚Ä¢   Working with OLAP cubes: creating measures, dimensions and relations between it.Fixing bugs and cube processing errors
‚Ä¢   Creating Export/Import procedures by load data from local or ftp files
‚Ä¢   Checking the code in the git and following the code convention
‚Ä¢   Mentoring a group of people
‚Ä¢   Documenting business processes.
‚Ä¢   Supports environments and communicating with the customer
‚Ä¢   Evaluating tasks and checking scores
Studied at Lviv Physics and Mathematics Lyceum Graduated in 2015
Education Magister degree in Computer Science 2016-2022:
Ivan Franko National Iniversity of Lviv, Faculty of Applied mathematics, ""Computer Science ""
Speciality.
Course work - Development of a relational database for accounting Graduate work - Face recognition using Azure
Course work 2 - Analysis of relational database optimization
Master's work - Application of Olap technologies to solve FMCG market problems
For the past almost two years, I've been doing a lot of management and working with the customer service team, etc."
data engineer,"- Development of SQL queries of any level of complexity( queries to transpose tables,aggregate functions, window functions,queries to tables with a hierarchical data structure, queries that contain JOIN instructions, queries that contain UNION instructions and other).
- Development of SQL queries for large tables.
- Development of scripts for creating database objects, development of scripts for filling tables with data.
- Migration of database objects, data between different data stores using Oracle packages DBMS_METADATA, DBMS_DATAPUMP, utility  SQL Loader.
- Development of recommendations for the database administrator on the list of roles that must be provided to the user for correct script execution.
- Developing procedures and functions using pl/sql.
- Experience reviewing execution plans and performance tuning.
- Structure design and creation of database objects.
- Program development(SQL, Pl SQl , Sql*Plus technology).
- Developing scripts to automate tasks for the Windows operating system using the CMD command interpreter.
- Experience with the Jira project management system.
- Creating the reports using Sql,Pl Sql, Sql*Plus, Asp.
- Technical support and maintenance of specialized software.
- Providing advice to users on the work of the software.
- Good knowledge of mathematics.
- Good analytical skills"
data engineer,"I started learning programming in school. It was a competitive programming using C++. I also took part in an IT tournament as a member of a team. There I met Python for the first time, and I was working with image processing and analyzing them. The next stage came when I started studying at university. We started with intense learning of C#. I've created a bunch of applications using the WPF framework and SQL server during my first year. Before entering the second year I started taking courses to prepare for a data engineer position.
Then I started working as a data engineer in a British company (Aug 2022 - Mar 2023 ¬∑ 8 mos).
I was using Apache NiFi for data processing and GCP utilities as a data warehouse in particular GCP Storage buckets and BigQuery. Then the company delegated me some software engineer responsibilities. I was using Flutter to display the results of data processing we've done before. After the project with data processing, I had an opportunity to take part in designing an infostructure in GCP. We used  Cloud Run, and Cloud Function, Monitoring. During this project, I've learned Terraform, infrastructure-as-code software. Also, I got a Professional Cloud DevOps Engineer certificate. After that, I switched companies.
I started working as a software developer (Oct 2022 - Present). I got experience working with Java multithreading servers, more advanced NiFi data processing, configuring working servers, and a little bit more DevOps stuff. Meanwhile, I was using Flutter for front-end development. In a side project, I created a back-end for a report-creating application. I had experience working with large amounts of data, building SQL queries, and writing post-processing logic for data preparation.
I am looking for a promising company with interesting people and projects so that we can develop together."
data engineer,"Collecting Various Types of Data from Different Sources, Transforming, and Loading to Data Warehouses
building data pipelines to ingest, transform, and load the data into data warehouses using Azure Data Factory, Python, and Talend.
Visualizing data based on the nature of the data and the respective chart type to get insights easily for the customers.
creating and publishing dashboards and reports in various data visualization tools.
Tools that I have used so far:
Power BI, Azure, Azure Data Factory, Azure SQL, Snowflake,R, Python, SQL Server, SSRS, SSIS, SSAS, DBT, Excel and PostgreSQL
Successfully implemented three USA-based pure data engineering remote projects, showcasing expertise in ETL processes using Azure Data Factory, Python, Five Tran, Snowflake, DBT, and SQL. Collaborated closely with a skilled team, ensuring timely delivery. Specialized in data cleansing and transformation, leveraging a comprehensive tech stack, including Power BI, Azure, SQL Server, and more. Received positive client feedback for tailored data visualization strategies. Continuously updated skills to incorporate cutting-edge technologies and contribute to project success.
***What I Want:
Challenging projects and continuous learning
Collaborative and supportive work environment
Opportunities for professional development
Healthy work-life balance"
data engineer,"I worked as a data manager-data engineer. As a data manager (DM), I was responsible for database creation (the company I worked at had its own commercial web-based database), configuration, manual testing, coding automated calculations, data cleaning, and data validation. I also focused on delivering projects on time‚Äîfrom database creation to production release, along with all the necessary technical documentation‚Äîwhile coordinating cross-functional teams to align on project deadlines and requirements. Additionally, I provided training on databases to my colleagues and mentored junior staff.
Over time, I realized I wanted to take on more technical tasks, and I had the opportunity to work with some data engineering tools. Since I was doing a lot of data cleaning, I implemented Python scripts (great expectation (GX)) to speed up the process of finding duplicates and discrepancies, which I previously handled manually in Excel. The data was stored in CSV files within folders, making it inconvenient for medical experts to retrieve and analyze information. To improve this, I started developing ETL processes‚Äîextracting CSV files from cloud storage, loading the data into a PostgreSQL database, and transforming it using SQL (e.g., joining CSV files and aggregating them into data marts). I then sent the processed data to Superset for analysis and visualization by medical experts. I orchestrated the entire process using DAGs in Apache Airflow."
data engineer,"I‚Äôm a Data Engineer just starting out and already loving the world of data. I‚Äôve worked with Python, SQL, and some database management, contributing to real projects in both internships and my current role. So far, I‚Äôve built and optimized small data pipelines, cleaned datasets, and pulled insights from structured data. I‚Äôm excited to keep growing, learn more about databases, explore new tools, and get hands-on with cloud platforms.
I‚Äôve optimized data pipelines to run faster, built a SQL dashboard that helped my team make better decisions, and improved how we clean datasets. I also completed the Google Data Analytics Certificate, which leveled up my skills with databases and queries. These experiences have been super rewarding and keep fueling my passion for data.
I‚Äôd love a role where I can learn from others, work on exciting projects, and grow my skills with big data and cloud platforms. A workplace that supports learning and creativity would be amazing‚ÄîI‚Äôm all about collaboration and solving meaningful problems."
data engineer,"...
Developed a model to predict loan default probability, improving credit risk assessment and reducing non-performing clients.
Utilized Pandas for data preprocessing, handling missing values, and feature engineering to prepare datasets for classification tasks.
Applied Scikit-learn to develop and evaluate classification resume_classifier for predicting credit risk.
Developed a regression model to predict house prices.
Utilized pandas for data manipulation and Scikit-learn for model development."
data engineer,"DATA ANALYST INTERN (DRONE PROGRAMMING FOCUS) ‚Äì AZERCOSMOS ‚Äì Baku, Azerbaijan        January ‚Äì March 2021
Analyzed drone performance data through flight testing and assessments to improve reliability.
Developed Python-based automation scripts for integrating sensor data and monitoring drone operations.
Collaborated on the development of real-time reporting systems for autonomous drones, enhancing decision-making efficiency.
Utilized data visualization techniques to present insights on operational performance.
ROBOTICS COACH ‚Äì AZERBAIJAN ROBOTICS ENGINEERING ACADEMY (AREA) ‚Äì Baku, Azerbaijan 	August 2019 ‚Äì December 2020
Designed data-driven learning strategies to prepare student teams for the World Robot Olympiad.
Mentored students on analyzing and optimizing robotics projects using programming and sensor data.
Implemented performance-tracking tools to monitor progress, resulting in improved problem-solving and teamwork skills.
Utilized various data collection methods to assess student engagement and project success.
MASTER OF SCIENCE IN BUSINESS ANALYTICS AND DATA SCIENCE‚Äì Marie Curie-Sk≈Çodowska University ‚Äì Lublin, Poland (2024-2026)
BACHELOR OF SCIENCE IN ROBOTICS AND MECHATRONICS ENGINEERING ‚Äì Azerbaijan State Oil and Industry University ‚Äì Baku, Azerbaijan  (2018-2022)
What I Want from Work:
Opportunities for Growth and Learning: I am looking for a role where I can continue to develop my skills in data analysis, automation, and data visualization. I value environments that encourage learning new tools and technologies, such as Python, Power BI, SQL, and RPA tools, while also providing mentorship and guidance.
Collaborative and Innovative Environment: I thrive in teams where collaboration and innovation are encouraged. I enjoy working with diverse, cross-functional teams to solve complex problems and deliver data-driven solutions.
Impactful Work: I want to contribute to projects that have a meaningful impact, whether it‚Äôs improving business processes, optimizing decision-making, or driving innovation through data insights.
Work-Life Balance: While I am committed to delivering high-quality work, I value a healthy work-life balance, especially during academic commitments. Flexibility during exam periods or personal milestones is important to me.
Clear Goals and Structured Processes: I appreciate roles where expectations and goals are clearly defined, and processes are well-structured. This allows me to focus on delivering results and continuously improving efficiency.
What I Don‚Äôt Want from Work:
Micromanagement: I prefer roles where I am given the autonomy to manage my tasks and projects, with trust in my ability to deliver results.
Limited Opportunities for Skill Development: I want to avoid roles that do not offer opportunities to learn and grow, as continuous improvement is a key part of my professional journey.
Static or Repetitive Tasks: I am not looking for roles that involve repetitive tasks without room for creativity, problem-solving, or innovation.
Toxic Work Environment: I value a positive, inclusive, and supportive workplace culture and want to avoid environments that lack collaboration or respect."
data engineer,"Currently working with Azure Data Factory.
Microsoft Certified: Azure Data Engineer Associate.
Experience in refactoring / writing high performance T-SQL scripts / stored procedures for OLTP systems (available 24/7 without downtime for release deployment) including next approaches:
- design and development Micro Service DB part of architecture in new and existing projects;
- Reverse engineering NHibernate .Net code to MS SQL procedures (good .Net code reading skills);
- readable, commented SQL code;
- dynamic SQL code with parameterization (resistant to SQL-injections);
- index covering for certain cases;
- checking IO statistics and tuning;
- checking execution plans and tuning;
- profiler trace use;
- XEvents monitoring;
- creating fast working SSRS reports;
- altering SSIS packages / creating migration scripts.
Education:
University. Department of electronic and information technologies. Specialty: ""Computer systems and networks"". Master degree.
In OLTP solution changed type of wide spread column (near 20 tables) that were also used as composite primary key of few project tables. Done without downtime using triggers, migrations,  backward compatibility code in stored procedures.
Designed, tested and implemented high loaded message queue for Kafka.
Without downtime were moved DB objects of high loaded Micro Service to dedicated DB.
Interesting tasks, dynamic work, Scrum approach, effective work in team, getting new knowledge and experience.
I currently live abroad - in Turkey."
data engineer,"Under my belt is more than 13 years in backend and overall software system engineering, including (but not limited with) a wide spectrum of activities in SDLC itself (design and implementation, troubleshooting, supervision and audit) and personnel development (mentoring, leading, etc).
For last 7 or so years I work primarely with cloud-based solutions, targeting AWS and GCP, using modern (to date) practices of an operational automation (DevOps/GitOps).
I have experience in development and maintaining of highload large-scale platforms (MAU in millions) written in JVM-languages, namely Scala and Java.
Some of the mentioned systems were: 1 - adtech (US media corporation), 2 - fashion marketplace (UK/Europe), 3 - clinical CRM (US vettech/petcare). All of them allowed me to grasp domain knowledge of a respectful industry.
Conducted a technical initiative for a streaming processing at (1), using a state of the art (to date) opensource framework.
Led an outsorced engineering team at (2) with a goal of development a new subsystem of their platform to automate client to client disputes resolution.
Designed, implemented and maitained a significant portion of a data warehouse solehandedly due to the lack of team extention capabilities."
data engineer,"I'm Mid-Senior Azure Data Engineer and Power BI Data Analyst.
I specialize in designing scalable data architectures, enabling data-driven decision-making, and delivering actionable insights to address complex business challenges. With strong expertise in modern data engineering tools and cloud-based solutions, I excel in optimizing data workflows and empowering organizations to harness the full potential of their data.
Key Highlights:
Proficient in Azure Data Factory (ADF): Expertise in designing and orchestrating data pipelines for data ingestion, transformation, and migration.
Developed migration pipelines with ADF to migrate data from SQL Server to Azure SQL Database, utilizing Incremental Load (SCD Type 2) to ensure historical data accuracy and seamless updates.
Skilled in Azure Databricks (PySpark) for processing and transforming large-scale data from Azure Data Lake Storage Gen2 (TEXT and CSV files) into structured datasets using Delta Lake.
Designed and implemented multi-layered data pipelines, transitioning data through Bronze (iODS stage), Silver (iSTG stage), and Gold (ADW stage) Delta Tables to ensure high-quality, datasets for analytics.
Built optimized pipelines to load large datasets from Databricks into Azure SQL Pool, delivering superior performance for enterprise data warehousing and analytics.
Streamlined ETL processes using ADF and Databricks, ensuring seamless integration, data accuracy, and operational efficiency.
Scheduled and orchestrated Databricks notebooks with ADF, automating workflows to improve reliability and reduce manual effort.
Created and deployed over 10 Power BI reports across key business domains, including Sales and Stock, Financials, HR, Customer, and Marketing, providing actionable insights for stakeholders.
Proficient in using Jira for task management, sprint planning, and tracking progress across the Software Development Life Cycle (SDLC).
Collaborated with cross-functional teams to ensure alignment on project goals and delivery timelines.
Managed user stories, epics, and sprint cycles to maintain Agile workflows and enhance project delivery efficiency.
Utilized CI/CD pipelines to schedule and run data pipelines, ensuring consistency and reducing operational overhead.
Used Git for version control, pushing changes to the repository after updates to scripts, and configurations."
data engineer,"- Management of RDBMS and optimization SQL queries for performance;
- Monitoring and perfomance analysis;
- Data transformation and modeling;
- Implementation of solutions for offloading OLTP databases;
- Support for databases with terabytes of data using optimal tuning, partitioning and filegroup separation techniques;
- Developing, implement and maintain ETL/ELT workflows;
- Setting up batch and near-real-time data processing using CDC mechanisms;"
data engineer,"Data Engineer
January 2024 - present
‚Ä¢ Building new Data Platform based on Azure Databricks leveraging Apache Spark for large-scale data ingestion, transformation, and processing of healthcare-related datasets, ensuring high performance and reliability.
‚Ä¢ Collaborated with business stakeholders to design clear, explainable, and business-aligned data resume_classifier, enabling better decision-making and insights.
‚Ä¢ Using IaC approach to build Azure Databricks infrastructure to ensure scalability, consistency, and rapid provisioning.
‚Ä¢ Collaborated on the development of a cost-optimization strategy, achieving significant reductions in platform expenses.
‚Ä¢ Implemented reusable and modular PySpark-based data pipelines to enable advanced analytics and reporting, supporting downstream analytics teams with timely, clean, and accurate data.
‚Ä¢ Utilized SQL and Python to enhance data querying and transformation logic, integrating best practices for data integrity and pipeline orchestration within the Databricks environment
Data Scientist/Engineer
April 2023 - January 2024
‚Ä¢ Designed and implemented robust data pipelines to extract, transform, and integrate data from diverse sources, including APIs, websites, and CRM systems, ensuring seamless data ow, accuracy, and accessibility for analytics and reporting.
‚Ä¢ Developed interactive and insightful dashboards using Tableau, delivering clear and actionable data visualizations to enable stakeholders to make data-driven decisions effectively.
‚Ä¢ Utilized AWS infrastructure (including VPC, EC2, RDS, S3 and Lambda) to build scalable, reliable, and cost-efficient data solutions for processing and storing fitness-related data.
‚Ä¢ Collaborated on machine learning workflows, supporting analytics-driven features to enhance fitness application functionality and improve user experience.
‚Ä¢ Ensured data quality and integrity by implementing rigorous testing and validation processes across all stages of the data pipeline.
Trainee Data Engineer
October 2022 -April 2023
‚Ä¢ Developed and optimized data transfer scripts in Python to seamlessly move and process data across AWS services, including Amazon Athena, S3, and RDS, ensuring efficient and reliable data flow.
‚Ä¢ Contributed to building a robust Data Platform by integrating Python, Apache Airflow, FastAPI, and PostgreSQL.
‚Ä¢ Collaborated with cross-functional teams to enhance platform performance, optimize workflows, and ensure adherence to best practices in data engineering
and cloud infra.
Databricks Certified Data Engineer Professional"
data engineer,"have 9+ years of overall experience as a software developer (java/python)
have worked as JAVA developer for B2B web application for 7+ yeaars (Spring/Hibernate).
along with JAVE have worked as a python developer (majority of project I've worked as data engineer)
Having worked as Data engineer for several years, I have developed a deep understanding of data collection methodologies, scraping techniques, and API integration. My have strong experience wiht lib/packages like Scrapy, Beautiful Soup, and Requests to extract and process data from various websites and APIs. I have successfully created and deployed numerous spiders and scripts for different types of websites, including e-commerce platforms, online dictionaries, and news websites.
In addition to these projects, I have experience dealing with captcha-protected websites, including solving captchas using anti-captcha APIs. I am adept at working with a range of frameworks/packages, including Scrapy, inline_requests, Requests, Selenium, and various databases like Oracle, PostgreSQL, MySQL, SQLite, MongoDB, and DynamoDB.
I am always eager to learn and adapt to new technologies, making me well-equipped to contribute to your team's success.
""white Salary"". possibilities to work with cutting-edge technologies.
don't work with russians teams/clients!"
data engineer,"My experience consists of skills in Hadoop stack, BI (SQL databases), DWH (SSAS, SSIS, SSRS, PowerBI), Cloud Engineering (AWS, GCP, Palantir Foundry, Azure), ETL using Spark (Python, Scala), DevOps Engineering (Cluster Management, IAM, UseCase reporting dashboards development, ELK stack). Additionally, I have knowledge of Java Core, and participated in Java Software Engineering.
I describe myself as self-organized engineer, who is capable of responsibility to manage technical processes in team, problem solving and strong time-management. I have an understanding of business relations, analytics and SCRUM process organizing.
I managed entire development process starting from getting requirements, analyzing scope and estimating work to implementation, communicating status, testing and release management.
With no hesitation I‚Äôm open to new challenges and new opportunities."
data engineer,"Have worked in both startup and enterprise environments both in local and distributed across the globe teams. My experience is focused mostly on Data Engineering and Clouds. I used to work as a Data Scientist and had exposure to Web Development. Have led agile teams of up to 10 people with overall leading experience of 2 years.
Programming Languages: Python, SQL, Java, Shell
Data Engineering Tools: Apache Airflow, Apache Spark, Apache Beam on Google Dataflow, DBT
Data Warehouses: Google Bigquery, Snowflake
Databases: PostgreSQL/MySQL, MongoDB, GCP BigTable
Messaging Systems: Apache Kafka, GCP Pub/Sub
DevOps: GitLab, GitHub Actions, Terraform
Virtualization: Docker, Kubernetes
Technical Design: Attribute-Driven Design, Dimensional Modeling, Software Design Patterns
Leadership: leading up to 5 engineers, mentoring engineers up to senior level, conducting knowledge sharing sessions and demos
Projects:
- Luxury retail enerprise
Responsibilities:
- Designing and building data warehouse (BigQuery + Dataform, Cloud Run, Cloud Dataflow, Fivetran, Azure DevOps; from DB2, Redshift, Oracle, Informatica)
- Leading team of up to 10 people (data engineers, QA, BA)
- Data quality
Responsibilities:
- Establishing data quality framework and processes
- Designing integration of data quality framework into existing project architecture
- Leading the team of 3 (2 data engineers, 1 devops)
- Leading setup of CI/CD process
- Implementing data quality solution
- Snowflake DWH migration.
Responsibilities:
- Spinning up the GCP and project environment
- Consulting the customer's team
- Migrating from Snowflake to GCP stack
- Clinical Data Lake assessment
Responsibilities:
- Assessment of the Data Lake implementation state (based on GCP services)
- Preparation of the corrective actions report based on the assessment results (included data ingestion, working with sensitive data, orchestration)
- Application for mining engineers
Responsibilities:
- Leading the data engineering team
- Elicitation of requirements
- Data pipelines design and implementation (BigQuery, Dataflow, Composer, Storage)
- Trainee/junior engineers mentorship
- BigQuery Omni Alpha evaluation
Responsibilities:
- Implementing PoC with BigQuery Omni private alpha service
- Documenting results as an article on C2C global
Those are some of the projects that I was working on. We can go more in depth into my experience during the interview.
- Luxury retail enterprise: successful migration of data warehouse from AWS to GCP and reports retrofit (sourcing from BigQuery instead of Redshift).
- Data Quality project: successful establishment of data quality framework and processes for a startup that led to improved decision making and trust in data
- Snowflake DWH migration project: eliminated costs required to support Snowflake data warehouse and surrounding infrastructure by migrating to cost-effective serverless Google Cloud technologies (BigQuery, Dataflow, Cloud Run)
- Mining industry: successful development data processing pipelines and DWH (based on GCP) for multiple customers' plants - data pipelines supported the optimization model allowing to decrease the customer's facilities operational expenses by 14%
- Clinical data lake project: identified issues led to pipelines performance degradation and PII data processing violation
- Credit project: creation of the PoC product for detecting credit fraudsters that started to bring the company revenue (data science project)
Certifications:
GCP Data Engineer, GCP Cloud Architect
Databricks Data Engineer Professional"
data engineer,"As Data Engineer I:
- Optimized ETL pipelines and managed allocated resources, reducing time and cost by 15%.
- Implemented automation for critical primary data flow processing, increasing capacity and enabling daily handling of thousands of customer data requests.
- Designed and developed a comprehensive statistics dashboard for in-depth analysis of server loads, detailed user activity, and customer retention, improving our decision-making process needed for the company's sustained growth.
- Supervised the Data Collection team, focusing on optimizing performance and ensuring the consistent delivery of high-quality results.
As Python Backend Developer I:
- Designed and developed the crucial feature for a medical project, which enables doctors to record the patient‚Äôs medical notes.
- Proposed optimizations of the database architecture and reduced the main API response time by half.
- Initiated the creation of comprehensive onboarding materials for new trainees, thereby improving the developers' efficiency by reducing time spent on explaining technical project details.
- Developed a Telegram bot to help people in Chernivtsi, Ukraine. This bot aided citizens in monitoring power outages caused by missile attacks on the country's infrastructure."
data engineer,"I involved in creation of data platform which was built on Apache NiFi and enabled collection , integration, transformation, and cataloging of the all data.
Apache NiFi is Java based software, and extendable, therefore I worked on custom processors which extended its functionality, such as image tooling, security, different type of database implementation. It was deployed on k8s using helm, and monitored Grafana tools."
data engineer,"I am studying software engineering.
Acquired skills in: Snowflake, PostgreSQL, PySpark, MySQL, HTML/CSS, Technical Documentation, Rest API, Postman, CI, XML, JSON, MySQL, Git, VS Code, Jira, Python, UX/UI Design, Agile, SCRUM, Kanban, Waterfall
Completed online courses from Udemy in Software Testing,
University of Helsinki with Python, DataCamp with Data Engineer in Python.
Development in the field of Data Engineering."
data engineer,"Data Engineer at Rabitabank, Data Management Division
Responsibilities:
Managed and maintained the data warehouse, ensuring efficient data management and integrity.
Led the development of a dynamic customer rating model, based on transaction activity, for use in the bank's mobile app.
Migrated source data from Microsoft SQL Server to PostgreSQL using dbt, incorporating Change Data Capture (CDC) for incremental updates.
Automated SQL-based workflows on Oracle databases using Apache Airflow in a Dockerized environment."
data engineer,"Data Analyst / Data Engineer - Part-time project
October 2024 - Now
Project: Data Architecture and Analytics Automation
Role: Data Analyst and Azure Data Engineering Specialist
Key Contributions:
Designed and developed interactive dashboards in Power BI, enabling actionable insights and real-time data-driven decision-making for key stakeholders.
Built and optimized data pipelines using Azure Data Factory, automating ETL workflows and ensuring seamless data integration across diverse data sources.
Worked with SQL databases, performing data modeling, transformations, and advanced querying to support analytics needs.
Outcome: Enhanced the scalability and reliability of data architecture, supporting organizational growth and increasing operational efficiency.
Data Engineering Intern
Banking Sector
September 2024 ‚Äì  October 2024
Project: Data Pipeline Development on Azure for Financial Data Processing
Role: PySpark and Azure Data Engineering Intern
Key Contributions:
Designed and implemented scalable data pipelines using PySpark on Azure Databricks, facilitating ETL workflows for transaction and customer data.
Leveraged Azure Data Factory for orchestrating data movement and transformation, optimizing pipeline performance and reducing latency.
Collaborated with cross-functional teams to ensure data quality and integrity across various data processing stages.
Outcome: Successfully improved data processing efficiency and contributed to enhancing data pipeline reliability for downstream analytics, aiding decision-making processes in finance.
Junior researcher, Department of Information Technology, Kharkiv National University of Radio Electronics
September 2022 - December 2023
Project: Development of an IoT module for a smart farm
Role: Responsible for processing and visualizing data results from the IoT module
Key Contributions:
Developed and deployed a backend server using Python and Django
Designed and implemented a web interface for real-time data visualization and user interaction
Outcome: This work formed the basis of my master's thesis, focusing on architectural design of information systems for smart farming applications.
Python Programming: Engaged in self-directed learning to master advanced Python programming techniques, focusing on efficient coding practices and complex problem-solving.
Machine Learning Exploration: Currently diving into machine learning through online courses, focusing on foundational concepts and practical applications to integrate machine learning solutions into real-world projects.
Titanic Survival Prediction - Machine Learning Portfolio Project
Project Overview: Built a machine learning model to predict Titanic passenger survival using the Kaggle Titanic dataset, which includes features like age, gender, and socio-economic class. The project aimed to compare multiple machine learning algorithms, selecting the most accurate model for predicting survival outcomes.
Data Structures and Algorithms: Studied various data structures and algorithms through online platforms and textbooks, applying this knowledge to optimize code performance and solve computational problems."
data engineer,"Senior Analytics Engineer (Fin. domain)
September 2023 - present
‚Ä¢	Ad-hoc request processing;
‚Ä¢      Experience with ERD modeling and business process analysis for reporting services
‚Ä¢      Documented ETL processes
‚Ä¢	Combining, transforming and modeling data using dbt (ELT);
‚Ä¢	Optimize high-load querying;
‚Ä¢	Creation of data pipelines using dbt, Python, SQL;
‚Ä¢       Data visualisation;
Analytic Engineer
August 2022 - September 2023
‚Ä¢	Ad-hoc request processing;
‚Ä¢	Combining, transforming and modeling data using dbt;
‚Ä¢	Optimization of requests;
‚Ä¢	Creation of data pipelines using Python, SQL, Tableau;
Data Analyst
¬´‚Äî‚Äî‚Äî¬ª  March 2021 - July 2022
‚Ä¢	Providing analytical support and support of business processes;
‚Ä¢	Development of automated reporting in Power BI;
‚Ä¢	Data analysis to identify the causes of deviations of business metrics, as well as optimization of business processes;
‚Ä¢	Planning, conducting and analyzing experiments (A / B testing);
‚Ä¢	Preparation of infrastructure for use cases (ddl, procedures, job);
‚Ä¢	Conduct research and analysis of the effectiveness of shares;
‚Ä¢	Segmentation of loyal customers and forecasting
‚Ä¢	Support and refinement of pipeline (Python)
‚Ä¢	Calculation and analysis of marketing indicators  LTV, Retention.
Data Analyst
¬´‚Äî‚Äî‚Äî¬ª July 2019 ‚Äì February 2021
‚Ä¢	Processing of large arrays of statistical data;
‚Ä¢	Construction of distributions on the basis of data, analysis of trends, identification of trends, correlations with certain factors,
‚Ä¢	Ability to find different solutions to problems;
‚Ä¢	Identification of the correct root causes of defects;
‚Ä¢	Development of methodology for analysis;
‚Ä¢	Preparation of reports;
‚Ä¢	SQL"
data engineer,"With over two decades of progressive experience in software development, culminating in a role as a System Architect, I have honed a robust skill set that underpins my expertise in designing, developing, and optimizing complex systems. My career has spanned a variety of projects where I have successfully employed a blend of technical acumen and strategic insight to deliver innovative solutions that consistently exceed business objectives.
As a System Architect, I orchestrated the integration of scalable software solutions and led multidisciplinary teams in the agile development of high-performing systems.
Have Master's degree in Computer Science and currently pursuing a PhD in the same discipline, I am deepening my understanding of cutting-edge technologies and theoretical principles, which enhances my analytical skills and informs my practical work.
Seeking to leverage my background in software architecture, I would like to extend my skill-set in the role of a Data Engineer. My experience includes extensive work with large-scale databases and real-time data processing systems, where I applied best practices in data modelling, warehousing, and analytics to support data-driven decision making. I am proficient in a variety of programming languages and tools essential for high-level data engineering, including Java, Python, SQL, Spark.
I am eager to bring my strategic vision and technical expertise to a forward-thinking team where I can contribute to the development of advanced data architectures and drive the innovation of data solutions that enhance organizational performance.
Over the past eighteen months, I have digitalized a medical institution from scratch. I planned and successfully deployed a local network, laying the foundation for further transformations. I developed and implemented an information system that digitized approximately twenty key processes, significantly reducing both the volume of labor and the number of personnel involved. Additionally, I introduced several methodologies that enhanced the accuracy and reliability of the data. I also simplified and expedited the reporting system, making the institution's management more efficient.
I aspire to become a reliable component of a team that brings innovations to the world. With a flexible work schedule."
data engineer,"Results-driven software developer with 11 years of experience across diverse roles. Expertise in Scala, data engineering, and agile methodologies. Seeking opportunities to contribute to innovative projects.
Key Skills:
Languages: Scala, Haxe, Java, JavaScript, Python, Groovy/Grails
Frameworks: Spark, Databricks, Delta, OpenFL, Spring, Hibernate
Databases: AWS Aurora, Oracle, MySQL, InterBase, SQLite
Tools: Ant, Maven, Gradle, Jenkins, Concourse
Testing: Selenium, Cucumber, Spock, Mockatoo, mUnit
Version Control: Git, Github, Perforce
Servers: Tomcat, Jetty, Denver
Professional Experience:
Company A (2021-Present):
ETL pipeline development using Scala and Spark on Databricks clusters.
Enhanced data enrichment and transitioned pipelines to Delta Lake.
Automated AWS dashboards with Databricks notebooks.
Improved pipeline efficiency using auto-scaling Lambda functions.
Company B (2020-2021):
Specialized in logging development for streaming platforms.
Created Splunk dashboards for real-time data analysis.
Maintained unified codebase for various platforms, including Android TV, AppleTV, FireTV, and Linux-based devices.
Company C (2020):
Developed and supported data pipelines on Databricks.
Implemented and enhanced Concourse CI jobs.
Improved reporting data metrics and statistics.
Company D (2016-2020):
Maintained unified codebase for multiple platforms.
Collaborated on cable and IPTV software solutions.
Contributed to the development of VisulOn and Google Exo Player Library.
Provided training and mentoring to junior programmers."
data engineer,"CRM –ø—Ä–æ–µ–∫—Ç–∏, LMS —Å–∏—Å—Ç–µ–º–∏, —Ä–æ–∑—Ä–æ–±–∫–∞ –¥–æ–ø–æ–≤–Ω–µ–Ω—å, —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—É –∞–¥–º—ñ–Ω—ñ—Å—Ç—Ä—É–≤–∞–Ω–Ω—è —Ç–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—è –ø—Ä–æ—Ü—Å—Å—ñ–≤
–í–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–Ω—è –º–µ—Ä–µ–∂–µ–≤–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å—ñ–≤, —Ä–æ–∑—Ä–æ–±–∫–∞ —Å–∫—Ä–∏–ø—Ç—ñ–≤ —Å–ø—Ä–æ—â–µ–Ω–Ω—è —Ä–æ–±–æ—Ç–∏ –±—ñ–∑–Ω–µ—Å—Å—É, —Å–∏—Å—Ç–µ–º –ª–∏—Å—Ç—É–≤–∞–Ω–Ω—è.
–ü—Ä–æ—Å—ñ–¥–Ω–∏–π
Sometimes hyperfocusing and sometimes hyperfixation if you're in you're do it!
–î–æ—Å—Ç–∞—Ç–Ω—ñ—Å—Ç—å —á–∞—Å—É"
data engineer,"Hello there.
Am a seasoned data engineer. Started my career as a DBA/System Administrator in 2007. Here are some notes regarding noticeable activities:
1) DB2 migration 8.2 - 9
2) Informix migration 9.4 - 11.50
3) DB2 replication (from z/OS to AIX)
4) Oracle -> MS SQL data migration
5) building data pipelines in Hadoop with leveraging Hive/bash
6) migration sourcecode of stored procedures from Oracle to PostgreSQL
7) scraping data from a few platforms, and storing all the records in a DB
8) implementation of a delta lake and buliding up a dockerized big data environment (hadoop, spark, knox, livy) from scratch
Here are some results for the last 3-5 yerars
1) migration 200+ jobs from ControlM to Stonebranch enterprise scheduler;
2) developed a solution extracting data from pages. I used here a few technologies such as database development, scraping, restapi. Hence, such programming languages as sql, bash script, python were actively used during the implementation;
3) helped to migrate both data and stored functions/procedures from Oracle DB  to PostgreSQL. Besides that I developed a detailed guide how to do data migration in Windows environment,
4) was able to build up a big data environment in Docker Swarm taking into account a fact that I had never worked with Apache Livy, Apache Knox, Docker swarm tools before that moment
I:
1) avoid monkeyjob,
2) don't like working on boring tasks
3) am against of working in a huge office spaces.
My dream is a small development team (up to 10 employees)."
data engineer,"Technical Lead & Solution Architect:
Established CI/CD processes for the entire development team.
Released key applications for the Asset Integrity (10 PowerApps) and Plant Maintenance teams (.NET, PowerApps solutions), as well as for the Plant Turnaround team (PowerApps, Power BI dashboards).
Designed and implemented a large-scale Data Warehouse (Azure Data Factory, Azure SQL, Databricks, Python, Power BI) for management reporting.
Onboarded new team members and provided ongoing support, ensuring successful integration into projects.
In-House Application Development and Support:
Developed a .NET solution for the Geology Team at an Oil & Gas company, using Microsoft SQL as the database.
Led the cloud-based .NET solution for the Procurement Team, successfully replacing tool integrations and providing ongoing support for in-house .NET applications.
Vendor Application Support:
Improved a vendor drilling solution and enhanced its integration with another tool, leveraging Oracle DB and Microsoft SQL.
PowerApps Development:
Built tools in the PowerApps Model-Driven Platform to enhance the quality of data stored in the ERP system for business operations.
Throughout these roles, I overcame numerous challenges to deliver successful solutions, all while driving innovation and maintaining high standards in development and support processes."
data engineer,"Experienced Full Stack and Data Engineer with over 12 years in software development. I‚Äôve spent a decade working deeply with Python and Node.js, and five years focusing on modern data engineering and backend systems across fintech and enterprise-scale platforms. Strong experience with Django, FastAPI, Golang, data pipelines, Redis, and React. Cloud-native development using AWS, Azure, and GCP. I‚Äôm confident working with real-time data, ETL/ELT workflows, and backend APIs that scale.
In recent years, I‚Äôve focused more on data-heavy platforms in finance and AI-integrated applications. I like to keep things simple and productive. I follow how the software landscape is moving ‚Äì especially with the rise of cloud-native tools and how AI is reshaping backend architectures. Outside of work, I often explore new cloud services and try out open-source data tools. It helps me stay fresh and understand what‚Äôs next."
data engineer,"–ù–∞ –¥–∞–Ω–∏–π –º–æ–º–µ–Ω—Ç –ø—Ä–∞—Ü—é—é —è–∫
C–ø–µ—Ü—ñ–∞–ª—ñ—Å—Ç —Ç–µ—Ö–Ω—ñ—á–Ω–æ—ó –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ L3
–≥—Ä—É–¥–µ–Ω—å 2022 ‚Äì –∑–∞—Ä–∞–∑
–í –º–æ–á –æ–±–æ–≤‚Äô—è–∑–∫–∏ –≤—Ö–æ–¥–∏—Ç—å:
‚Ä¢	–û–±—Ä–æ–±–∫–∞ –∑–≤–µ—Ä–Ω–µ–Ω—å –≤—ñ–¥ L2 —Ç–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü—ñ—è —ñ–Ω—à–∏—Ö –≤—ñ–¥–¥—ñ–ª—ñ–≤ —Å—Ç–æ—Å–æ–≤–Ω–æ —Ñ—É–Ω–∫—Ü—ñ–æ–Ω—É–≤–∞–Ω–Ω—è –ü–ó.
‚Ä¢	–ö–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—è –∑ –∫–æ–º–∞–Ω–¥–∞–º–∏ (DEV, DEVOPS, QA).
‚Ä¢	–ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á —Ä–æ–∑—Ä–æ–±–Ω–∏–∫–∞–º, —Ç–µ—Å—Ç—É–≤–∞–ª—å–Ω–∏–∫–∞–º –ü–ó.
‚Ä¢	–¢–µ—Ö–Ω—ñ—á–Ω–µ —Ä–æ–∑—Å–ª—ñ–¥—É–≤–∞–Ω–Ω—è —ñ–Ω—Ü–∏–¥–µ–Ω—Ç—ñ–≤, —â–æ –≤–∫–ª—é—á–∞—î –≤ —Å–µ–±–µ —Ä–æ–±–æ—Ç—É –∑ –∫–æ–¥–æ–≤–æ—é –±–∞–∑–æ—é (Python).
‚Ä¢	–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–∞–ø–∏—Ç—ñ–≤ –¥–æ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö."
data engineer,"In my work experience as a cloud-certified data engineer with a focus on AWS, I have completed various projects and tasks that showcase my skills and expertise in data engineering and analytics. Here are some highlights:
1. **Projects and Tasks Completed**:
- Developed and implemented a standardized solution for Customer Lifetime Value (CLV) modeling.
- Designed and implemented data pipelines for real-time data processing using AWS services like Amazon Kinesis and AWS Lambda.
- Built scalable data lakes and data warehouses on AWS using services such as Amazon S3 and Amazon Redshift.
- Developed machine learning resume_classifier for predictive analytics and recommendation systems using Amazon SageMaker and other ML frameworks.
2. **Technologies Used**:
- AWS Services: Amazon Redshift, Amazon S3, AWS Glue, Amazon Kinesis, AWS Lambda, Amazon SageMaker.
- Data Engineering Tools: Apache Spark, Apache Airflow, SQL, Python.
- Machine Learning Libraries: TensorFlow, Scikit-learn, PyTorch.
3. **Current Role in the Team**:
As a data engineer in my current role, I am responsible for designing and implementing data pipelines, developing and deploying machine learning resume_classifier, optimizing data storage and processing solutions on AWS, and collaborating with data scientists and business stakeholders to deliver actionable insights from data.
4. **Areas for Improvement**:
- I aim further to enhance my expertise in cloud-native technologies and serverless computing to optimize data processing and reduce operational costs.
- I plan to deepen my knowledge of advanced machine learning techniques and algorithms to improve the accuracy and efficiency of predictive resume_classifier.
- I seek to improve my project management and collaboration skills to lead cross-functional teams and deliver high-impact data solutions effectively.
By continually honing my data engineering, cloud computing, and machine learning skills, I strive to stay at the forefront of technology trends and deliver innovative solutions that drive business value and growth."
data engineer,"I have started my professional work experience as a Data Engineering Intern at a local Bank. During my 6 monthes there I worked on banking data. My responsibilities were to gather this data from different branches of the bank, clean it, restructure it and save it in Spark to make processing easier for Machine Learning Algorithms.
After my intern period, I was hired by a US-based company on a contract to carry out web scraping and automation projects related to financial data. I set up ETL pipelines with AWS lambda and Overwatch to set the process to be carried out automatically.
My final experience was at Robert Bosch Kft. as an academic researcher. I was tasked with Computer Vision project related to autonomous driving. I used PyTorch to accomplish task."
data engineer,"I have practical experience (about 4 years) in the development and maintenance of large databases as a SQL developer in the field of auto parts sales and call center maintenance.
Daily work includes writing queries of varying complexity, views, common table expressions, functions, procedures, triggers, as well as analyzing the database for errors, inconsistencies and their elimination.
I have experience with MS SQL Server, MySQL, PostgreSQL, T-SQL, SSIS, ETL, SSRS.
Microsoft Office, Visual Studio Code, HeidiSQL, DBeaver, SQL Server Management Studio, Microsoft Visual Studio - Power User.
Familiar with HTML, CSS at a basic level.
I have completed courses and have theoretical knowledge of DWH, SSIS, ETL, SSRS.
I would like to do T-SQL, DWH, ETL, SSRS.
I don't want to administer the database"
data engineer,"I have a strong knowledge of Python and Java. I have worked with PySpark on a number of personal projects, some of which were developed on the Databricks platform. Moreover, I recently obtained the Databricks Data Engineer Associate certification, during the preparation for which I solidified my understanding of data processing stages.
I have gained practical experience with AWS while preparing for AWS Cloud Practitioner Certification and with Azure while developing data engineering projects.
Moreover, I have acquired knowledge of data
modeling while working on university projects with PostgreSQL and MySQL. In addition, I have a solid understanding of data warehousing and ETL/ELT
processes.
I also have experience with Docker, Kubernetes and Git, including complex operations such as branch management, rebasing, merging, and resolving merge conflicts.
I enjoy collaborating in team projects because I see team work as a great way to share knowledge and motivate one other. Furthermore, I love acquiring new
knowledge independently by reading articles and books, as well as watching videos and listening to podcasts."
data engineer,"‚Ä¢ 2yrs of expertise as dwh engineer, creating data sources, optimising ETL and in-house ETL tool, troubleshooting kubernetes, linux and data pipelines, setting up various scheduling tools.
Big focus on validation cases. Managing on premise dwh infra for b2c infra with up to 10 replicas ( each replica for separate b2b client ) with separate release schedules in different clusters.
Mentoring newcomer analytics engineers.
Specialized on BI-requests and troubleshooting data pipelines ( data mismatches and sentry-errors ) due to extensive analytics and linux background
‚Ä¢over 1.5 year of experience with data analytics stack:
SQL, Tableau, Python ( Pandas, numpy, sklearn, plotly ), Excel for data analysis. Creating datasources, dashboards, adhoc analytics, CPA fraud.
Employment achievements:
‚Ä¢ Improved data consistency and validation cases
‚Ä¢ Optimized multiple datasources loading logic
‚Ä¢ 100+ troubleshootings through 2024
‚Ä¢ Performed migrations of reports and datasources to new ETL infrastructure
‚Ä¢ implemented ELK stack for monitoring for dev server
‚Ä¢ Built and maintained affiliate marketing fraud tracking dashboard
‚Ä¢ Automated ETL data pipelines for marketing fraud detection
‚Ä¢ Performed operational and adhoc marketing analytics
certifications:
‚Ä¢ Swedbank Data academy
- Data warehousing training that included SQL, ETL, Data Modelling, Data Validation
‚Ä¢ Parimatch Tech Academy DevOps
- Course in cloud engineering utilizing AWS, infrastructure management and
DevOps practices
- Initial experience with troubleshooting Linux, Docker and Kubernetes
- Course project ‚Äì Kubernetes cluster with nodes and horizontal pods
autoscaling utilizing GitOps approach with Flux and Grafana for monitoring
‚Ä¢ Genesis IT School
- Completed comprehensive school of digital online bussiness where I was taught foundation of market research, product strategy, market positioning and marketing strategy, building sustainable internet projects
- Together with a team built and validated a prototype of mobile app
education:
‚Ä¢ University of Tartu ( Estonia ) - computer science masters
‚Ä¢ Kyiv Polytechnic Institute (IASA) - bachelor in system analysis"
data engineer,"Ph.D. holder, AWS and Snowflake certified Database Developer/Architect with over two decades of expertise. Specialized in AWS, AWS RDS, Amazon Athena, Amazon Redshift, Amazon DynamoDB, Azure, MS SQL Server, Snowflake, Trino (formerly Presto) Query Engine for Big Data, and proficient in database administration and performance improvement. Proven track record in architecting scalable solutions for large-scale enterprises, consistently achieving high client satisfaction. Known for creating robust and reliable databases, with extensive experience in modification, issue resolution, and adherence to industry standards. Adept at mentoring fellow developers, overseeing application enhancements, and collaborating effectively with teams for optimal database solutions. Committed to surpassing benchmarks and maintaining top-tier performance.
In a notable instance, I optimized a critical workflow by transitioning it from Python to high-performance Trino (formerly Presto) SQL Engine for Big Data. This transformation led to a remarkable reduction in execution time, plummeting from 2 hours and 40 minutes to just 6 minutes - this means that productivity increased by approximately 96.25%. This substantial enhancement was achieved through the Presto engine's superior parallel data processing capabilities. The outcome not only accelerated information processing but also resulted in significant cost savings for the client. This achievement underscores my proficiency in optimizing processes and leveraging cutting-edge technologies to deliver tangible benefits.
Advised a prominent German supply chain management company, managing vast data sets of 30 million daily records, in transitioning from Exasol to a more cost-effective database solution.
Conducted comprehensive evaluations and assessments, ultimately recommending the adoption of Snowflake database.
Identified critical performance issues with the Snowflake Query Optimizer for queries exceeding one minute, leading to the development and implementation of a targeted solution.
Demonstrated exceptional problem-solving skills by optimizing a typical query for updating 10 rows in a one-terabyte table from 6 minutes to an impressive 5 seconds, significantly enhancing operational efficiency.
Successfully resolved user interface delays, a critical concern, by implementing a caching strategy leveraging the MySQL database with MEMORY storage engine, ensuring near-instantaneous data retrieval even for big datasets. This approach achieved comparable performance to Redis, with the added advantage of a SQL interface.
----
Spearheaded workflow optimization initiatives for one of the largest tobacco corporations, focusing on Treasure Data's Enterprise Customer Data Platform.
Successfully transitioned tasks previously implemented in Python to leverage the power of Trino (formerly Presto), a high-performance SQL Engine for Big Data, resulting in remarkable performance enhancements ranging from 10 to 40 times.
Notably, a task with an initial runtime of 13-15 minutes was streamlined to achieve completion within an impressive 18-20 seconds, exemplifying a substantial boost in operational efficiency and productivity.
---
Led critical initiatives for one of the largest fintech companies specializing in investment management solutions.
Directed the development of robust monitoring and logging systems for Snowflake, ensuring optimal performance and reliability.
Formulated disaster recovery strategies for mission-critical databases including MS SQL Server, Confluent (cloud Kafka), Atlas MongoDB, and Snowflake, bolstering data integrity and security measures.
Engineered specialized SQL scripts for comprehensive Snowflake monitoring, providing real-time insights into system performance and stability.
Orchestrated seamless integration of Snowflake with Rapid7 for effective logging and incident response procedures.
Delivered strategic recommendations for modifying configurations to reduce RTO and RPO in the event of a disaster recovery scenario.
For The Brave Knight There Would Be Always A Kingdom To Serve ;)
Interested in working with big data. I love to learn new tools and apply them in work.
I am looking for a long-term, stable and reliable relationship.
I like to work with data, analyze it, look for patterns and relationships."
data engineer,"I have 2+ years of non-commercial experience in Python/ML.
Currently there are 3 projects worth mentioning:
1. Wallpaper app
Full stack web project with React and Django (Rest Framework) as core technologies. In essence it's an app to store, publish and rate wallpapers.
Frontend is just a regular React app with lots of (cumbersome) features, animations, etc.
Backend is a set of (currently) 2 micro services, which are connected by Celery, Redis and RabbitMQ.
All these are deployed by docker compose.
2. Wallpaper tagging
Machine learning (deep learning) project for automatically tagging images with multimodal input (image+noisy tags) for the above mentioned wallpeper hosting website, which is, by the way, my is my bachelor's thesis. Hence there is a detailed report on architecture, training process and model evaluation.
3. MPD Fronted
It's a Pyside6 project. MPD is Music Player Daemon, which is quite popular in desktop Linux. Basically it is a server with api to play music. The project is on early stages of development. It lacks code refactoring and accessibility windows (settings, initial configuration, etc)."
data engineer,"Throughout my professional journey, I have cultivated a diverse skill set and accumulated valuable experiences in data science, engineering, and programming. My commitment to excellence and meticulous attention to detail is evident in the projects I have undertaken.
As a Software Developer at SystemSpecs, I specialize in full-stack development with a focus on the backend. Proficient in Java, JavaScript, SpringBoot, ReactJs, and PostgresQL, I've created APIs, implemented microservices using Spring Boot, and collaborated closely with the operations team to deliver effective solutions.
During my tenure as a Software Engineer at QRA Corp, I engaged in tasks across the entire stack, contributing to new and existing features. I conducted investigations and provided valuable insights to enhance both the software and overall business operations.
In my role as a Senior Software Developer at Max.ng, I played a pivotal role in the company's transition to a microservices architecture, designing a normalized database, enhancing backend performance, and implementing an Event-Bus architecture using Kafka in the Confluent Cloud.
My skill set encompasses a wide range of technologies, including Python, TensorFlow, Spark, Airflow, SQL, NoSQL, TypeScript, JavaScript (Nest, React, Node, React Native, Electron frameworks), Java, Spring Boot, Tableau, Plotly, Dash, Kafka, RabbitMQ, AWS, GCP, Docker, Figma, Version Control (Git), and JIRA, CI/CD. I hold certifications in Professional Data Engineering on Google Cloud, Certified Cloud Practitioner (AWS), AWS Machine Learning Foundations, Google Analytics, CISCO CCNA1 ‚Äì Introduction to Networking, British Council Creative Enterprise Certificate, FCC ‚Äì Responsive Web Design Certification, and FCC ‚Äì JavaScript Algorithms and Data Structures Certification.
In summary, my professional journey reflects a commitment to excellence, a passion for learning and applying cutting-edge technologies, and a continuous drive for improvement. I am eager to bring these skills and experiences to new challenges and contribute to innovative projects in the future."
data engineer,"Architectures: Microservices, Monolith
Back-end:
Computer Science: Operating System, Algorithms, Data Structures
Programming Languages and Frameworks: Python, Pandas, Matplotlib, Scikit-learn, Numpy, JS/TS/NodeJs
Databases: MySql, PostgreSQL, MongoDB
Data Visualizations: MS Excel, PowerBI, Tableau, Matplotlib
Cloud Technologies: AWS, Azure, Docker
Professional Certifications:
AWS Certified Developer - Associate
DataCamp Certified Data Analyst - Associate
‚Ä¢ Developing and maintaining ETL pipelines using Apache Airflow, ensuring timely and accurate data
processing and migration from Oracle DWH to ClickHouse.
‚Ä¢ Creating and managing indexes and partitions in ClickHouse to enhance query performance and
optimize storage for large datasets.
‚Ä¢ Collaborating with cross-functional teams, including data analysts and clients, to understand data
requirements, ensuring the delivery of data-driven insights aligned with business objectives.
‚Ä¢ Monitor, test, and troubleshoot ETL workflows to ensure data integrity and consistent performance
in both staging and production environments.
‚Ä¢ Compare and validate data across DB systems, ensuring consistency by analyzing results using Polars
DataFrames.
‚Ä¢ Automating data processing tasks to improve operational efficiency, reduce manual intervention,
and increase process reliability.
‚Ä¢ Track and document data lineage using Open Metadata, ensuring transparency and traceability
throughout the data lifecycle
‚Ä¢ Led the development and management of RESTful APIs for smooth communication between
front-end and back-end systems.
‚Ä¢ Optimized application performance by implementing Redis caching, achieving a 20% reduction in
response times.
‚Ä¢ Leveraged MongoDB‚Äôs dynamic schema capabilities to design a data model that adapts to new data
types and fields without schema modifications.
‚Ä¢ Designed and deployed a queueing service using AWS SQS and AWS SNS, effectively decoupling
microservices and improving the scalability of the system by 25%.
‚Ä¢ Designed MongoDB queries with optimized indexes, resulting in a 4.5x increase in data retrieval
speed.
‚Ä¢ Held pair programming sessions to help other new-coming developers to understand the architecture of the project and fixing common bugs.
‚Ä¢ Supported the migration of legacy systems to modern architectures by providing expertise in NodeJS."
data engineer,"Cloud Developer | TATA CONSULTANCY SERVICES (2.5 years)
‚Ä¢ Implemented a cloud-native authentication framework using RBAC and ABAC for analytical data,
improving retail client domain knowledge.
‚Ä¢ Designed and improved ETL/ELT pipelines for better processing speeds and error reduction, and provided
L2/L3 support with improved automation and monitoring.
‚Ä¢ Volunteered as a mentor in a Data Engineering Internship, creating educational materials and leading
knowledge transfer sessions.
‚Ä¢ Developed ETL pipelines using Azure ADF and Databricks, ingesting data from SAP Hana, SQL Server,
Web Apps, and Salesforce, and implemented metadata resume_classifier for business-specific transformations.
‚Ä¢ Implemented data transformation tasks based on specific business use cases. focusing on pipeline
optimization for cost efficiency and increased performance.
‚Ä¢ Earned Azure DP-900 certification. Enhanced cloud computing expertise in DataOps, gaining
pharmaceutical domain knowledge with a major client.
SOFTWARE ENGINEER TRAINEE | NOKIA (1.5 years)
‚Ä¢ Experience has been gained in debugging and analyzing GSM network applications using C and C++.
‚Ä¢ The main focus was on developing monitoring, logging, and analyzing tools using Java Spring Boot, which
contributed to enhancing the system's capabilities.
‚Ä¢ Manual testing of GSM systems was conducted to ensure optimal functionality and system integrity.
Sprint tickets related to reported bugs were also addressed and closed.
‚Ä¢ Developed custom plugins for a JIRA application as part of a team project. The project used a
Microservices architecture using Java Spring MVC and was built on Maven.
‚Ä¢ Performed extensive unit and integration testing with JUnit and Cucumber. Detected and enhanced bugs
to improve the system's performance.
‚Ä¢ Worked with an international team in four locations using Agile methodologies to manage the project.
Programming Tutor (1.5 years)
‚Ä¢ Giving online/ in-person lessons for senior school pupils in C++/ Java programming languages.
‚Ä¢ Certified in Azure DP-900 Data Fundamentals
‚Ä¢ Completion of 100 Questions on AlgoExpert.io
‚Ä¢ Completion of skill path: Spring Boot Developer on Educative.io
‚Ä¢ Fundamentals of Deep Learning by Nvidia"
data engineer,"=> Developing streaming pipelines using Spark Streaming and Flink, Akka Streams (Alpakka) and Apache Beam
=> Developing batch pipelines using Spark(Scala), Python (Pandas/NumPy/PyArrow)
=> Iceberg integration with Spark and Flink
=> CDC using Debezium
=> Setup and configuring streaming pipelines using Kafka Connect
=> Writing and optimizing SQL complex queries in GCP BigQuery, ClickHouse, Hive and Athena/PrestoSQL
=> Apache Kafka and Airflow setup and support deployment on K8s
=> Orchestrating pipelines using Apache Airflow
=> Migrations from legacy components of the platform to the newest solutions
=> Researching, analyzing and building POC/MVP implementations
=> Preparing Architecture design documents and HL diagrams
- 10+ years experience in software and data domain development on various projects
- Leading CDC project in data streaming challenges (Debezium, Iceberg, Spark, Flink and Clickhouse)
- Streaming and in-batch data processing
- Data orchestration experience with Airflow/Composer, Docker/Kubernetes, Ansible/Terraform
- Expert in Python, experience in Scala/Java
- Some experience in ML (Image recognition using OpenCV)
- DevOps practical experience (Ansible, K8s)
- Participant in opensource projects (Airflow Spark Operator on top of K8s)
Leading an interesting project with real-time data processing, ideally with Flink, Kafka and Spark."
data engineer,"Projects:
1. Digital Transformation for Packaging and Additional Materials Production Tetra Pak June 2022 - Present
‚Ä¢Built data solutions for different areas of packaging and additional materials production, resulting in improved data visibility and analysis capabilities.
‚Ä¢Utilized tools like ADF, Synapse, Azure Functions to integrate various data sources such as SAP Hana, manual Excel files, MS SQL DBs, and REST APIs, enabling seamless data integration and analysis.
‚Ä¢Developed data resume_classifier using AAS and automated data refresh with Logic Apps.
‚Ä¢Acted as the lead data engineer, providing project architecture and performing the migration of on-premises data solutions to Azure, resulting in improved scalability and cost-efficiency.
‚Ä¢Created and executed Python and PySpark scripts to resolve historical data gaps in delta tables, ensuring data accuracy and completeness.
2.Loss Intelligence in Additional Material Production for Packaging Tetra Pak June 2022 - February 2023
‚Ä¢Project aimed for timely monitor Additional Material production and to take timely corrective action in case of any failures, resulting in improved production efficiency and cost savings.
‚Ä¢Developed and implemented a data parsing and flattening process using the Data Flow tool in ADF, effectively handling the complex structure
of 15k-20k daily JSON files and enabling efficient data processing and analysis.
‚Ä¢Transformed and cleansed the captured data by applying data quality checks using stored procedures, ensuring the accuracy and consistency of the data.
‚Ä¢Stored the transformed data in an Azure SQL database, facilitating further analysis and reporting.
‚Ä¢Designed a Tabular Data Model in AAS to optimize data retrieval and enhance reporting capabilities, enabling efficient data analysis and visualization.
‚Ä¢Utilized the Power BI tool to generate intelligent reports, enabling timely and accurate decision-making based on the analyzed sensor data.
‚Ä¢Empowered responsible individuals to promptly take corrective actions in response to any failures identified through the generated reports.
3.Microsoft Middle East Azure Migration Projects
‚Ä¢Recreated existing data pipelines and resume_classifier to seamlessly integrate client's data into the new Azure infrastructure.
‚Ä¢Migrated on-premises data solutions to Azure, including simulating existing ODI pipelines in Azure Synapse.
‚Ä¢Refactored Python scripts used in ODI and created it in Azure Synapse Notebook.
Microsoft Certified  Professional
Microsoft Certified Associate
Databricks Certified Data Engineer Associate
Microsoft Certified Azure Data Engineer Associate
Microsoft Certified Azure Data Fundamentals
Databricks Certified Data Engineering Fundamentals
Learning and growth opportunities. A chance to work on challenging and meaningful projects."
data engineer,"Hi, I'm a Data engineer. I thought I would just write my experience here, but if you are interested, I can send you my CV with detail information.
Had experience with AWS, Azure, Python, PySpark,  SQL, Airflow, Jenkins, and Scala. I am not against new technologies. I will gladly master them at 150% learning speed. Trying to always learn and develop myself in the Big Data domain.
Overall, I have experience in the Big Data domain, working in startups and outsourcing companies. Worked with integrating 60+ services and creating POC projects for customers.
‚Ä¢Databricks Certified Associate Developer for Apache Spark 3.0
I am expecting to work using Ukrainian FOP or any online payment service."
data engineer,"5 years of experience as a Data Scientist and Data Engineer.
Have various experiences:
- creating/researching resume_classifier and deploying them;
- creating an analytic platform from scratch;
- creating data pipelines and strategies for live trading.
Also, have some management experience.
Main language - Python.
Prefer AWS as a cloud service.
I'm looking for a friendly, life/work balanced environment focused more on interesting challenges rather than simple support of the existing applications."
data engineer,"ETL\ELT development
Python, Pandas, PySpark
Strong knowledge of software development processes In-depth knowledge of Oracle 10, 11 database, PostgreSQL, RDBMS concepts
DWH, OLTP
SQL, PL\SQL,PL\pgSQL Stored procedures, triggers, collections, partitioning, basic administration
Performance tuning, query optimization
AWS - Cloud Practitioner Certificate
C Builder, Delphi, C
XML, XSL ‚Äì transformation.
CVS, SVN, GIT
JIRA, ALM
English B2."
data engineer,"- Norwegian IT company, customer - international healthcare company, extension to customer BI Team, Jun'20 - till now
+ Senior Data Engineer supporting and improving existing integrations and cubes, performance tuning, troubleshooting, communicating with end-users and management. Using MS SQL stack + Automise (VSoft) + several tools originating from Nordic software companies.
- Norwegian IT company, customer - international healthcare company, extension to customer IT Team, Sep'18 - May'20
+ Level 1 support engineer. Supporting Microsoft applications and OSs, many 3rd party applications.
- Ukrainian entity of international FMCG company, IT Team, Aug'09 - Aug'18
Various roles in IT:
+ Started as HelpDesk / ServiceDesk specialist. Supporting and troubleshooting Microsoft applications and OSs, many 3rd party applications.
+ then moved to the position of SAP permissions coordinator, servicing ~ 5000 users of all company's subsidiaries in Ukraine. (SAP, custom developed apps)
+ then became IT Delivery Manager for Ukrainian Head Office. Leading IT team of 3-4 persons, ensuring we together with team are fulfilling all company IT needs, running global projects on our level, taking part in implementing and improving IT Security across our zones of responsibility, etc.
I strive for development in Data Engineering field in parallel with management career path.
I am looking for interesting, challenging and creative job."
data engineer,"I am a Junior SQL/PLSQL Developer with hands-on experience in writing and optimizing SQL queries and developing PL/SQL scripts. During my experience, I have worked on:
- Writing complex SQL queries to extract, manipulate, and analyze data from relational databases.
- Developing PL/SQL procedures, functions, and triggers to automate tasks and improve system performance.
- Creating and maintaining database objects such as tables, views, and indexes.
- Optimizing SQL queries for better performance by using techniques such as indexing, joins, and query refactoring.
- Collaborating with team members to troubleshoot and resolve database-related issues.
I am continually learning and improving my skills to contribute effectively to database management and development tasks. I am passionate about database systems and eager to grow in this field."
data engineer,"5 years of experience in Data Analytics / BI.
Programming: Python (Pandas, Numpy, SqlAlchemy, skicit-learn, BeautifulSoup)
Visualization: Power BI (PBI Desktop, PBI Service, Deployment Pipelines, Data Flows, Development Pipelines,Automated Refresh, ETL,DAX query Language, Power Query)
SQL: MS SQL (T-SQL), Oracle, PostgreSQL,SnowFlake
MS BI: SSMS
Jira (Agile Scrum)
Git"
data engineer,"Versatile data professional with a strong background (over 5 years) in analytics and data engineering, including 2 years in a non-profit organization. Proven track record in building efficient data workflows, automating processes, and driving data-driven decision making.
Spearheaded the transformation of a fragmented data infrastructure into a centralized system using AWS services at Sanku. This ensured a robust and scalable infrastructure for smooth and faster analytics processes
Developed robust data pipelines using Apache Airflow for workflow management and Python for data processing.
Designed an implemented an end-to-end streaming pipeline from over a 1000 Dosifier machines to the AWS platform using services like API Gateway, Lambda and SQS.
Boosted business analytics by integrating Oracle NetSuite ERP system activities into the data warehouse for quality reporting.
Implemented software development best practices within the data team, including the use of Git for version control, GitHub Actions for CI/CD pipelines, and Docker for containerizing applications."
data engineer,"Worked on data integration and automation projects with a focus on improving data quality and reducing manual effort. Built and optimized data pipelines using Airflow. Used Docker for containerization and Azure for cloud-based deployments. Developed web scraping tools with Selenium, XPath, and CSS selectors to collect data from dynamic websites. Built an API endpoint for user creation using Django ORM. Wrote unit and integration tests to ensure code reliability. Currently learning best practices in data modeling, distributed systems, and scalable ETL processes to grow as a Data Engineer. Have experience working with both Azure and AWS."
data engineer,"- Project #4
Company with marketing technology that helps auto dealers optimize the performance of their marketing campaigns, by consumer purchase journeys, marketplace tracking code,  anonymous attribution and much more to provide a single, complete view of how people buy cars and where to get the next sale.
Creating a robust data pipeline using Airflow, resulting in improved scheduling and monitoring of ETL processes. Implemented multiprocessing techniques to speed up data processing. Optimizeding critical SQL queries, reducing query execution time by several times. Implemented indexing strategies, MVs, partitioning to enhance performance for frequently accessed data. Working with API, make changes on server side. Scraping. Developing on JS t o create new view/controller for Dashboard. Creating API endpoints. ETL data through different services. Collaborating with cross-functional teams to design and implement new solutions.
tech: Python, PostgreSQL, Airflow, GCP (Compute Engine, Cloud Run, App Engine, SQL, Storage, Cloud Functions, Pub/Sub, CI/CD), MongoDB, Flask
- Project #3
Translating and implementing commercial requirements into queries to database, collection and clear the necessary data.
Implementing the logic of transforming data and forming the final result that are published to a visualization tool.
Optimizing the existing queries.
Writing the logic widgets on Javascript for visualization a received information.
tech: PostgreSQL, Google Cloud SQL, Amazon RDS, Javascript
- Project #2
Migrating the databases and managing the database.
Designing, building, maintaining data pipelines to migrate and transform data. Tracking pipelines stability and ETL testing. Creating indexes for faster retrieval of the information and enhance the database performance.
Building database schemas, tables, procedures and functions.
tech: MS SQL, Oracle, Azure Data Factory, Azure Storage/Data Lake, Azure SQL, Azure Function App (Python), Bicep (ARM templates)
- Project #1 (domain: pharmacy management system)
Building Data Warehouse for gather data from across the organization.
Creating ETL/ELT code to populate the Data Warehouse.
Creating Spark jobs for data transformation and aggregation.
Creating Airflow DAG to drive ETL/ETL code.
Creating merge and purge functions.
Working on teams operating under an Agile Scrum delivery methodology.
tech: PySpark, Spark Streaming, Kafka, Airflow, PostgreSQL, Microsoft Azure, ADLS, Docker, K8s, Grafana, Liquibase, Jaspersoft
Data Engineering tasks and tech. Focusing on the cloud products"
data engineer,"For over fifteen years, I  successfully led complex projects with small and big teams as a cross-team architect, tech lead, and engineering lead, managing various aspects of software development, processes, and methodologies.
I am a hands-on technology expert in different verticals and stacks:
Summary
15+ years of working experience in Data Engineering.
Robust analysis, problem-solving, mathematical, leading, and tutoring skills.
Ability to quickly understand the project, work on time and have a high level of organization.
Core skills
- Python (Django, FastApi, Flask, Dask, Airflow, Ray, PySpark)
- Scala/Java (Akka, Cats, Zio, Scio, Spark)
-‚Äã ‚ÄãSQL/NoSQL (PostgreSQL, MariaDB, Cassandra, Accumulo, HBase, Hive, Neo4j, ElasticSearch, Snowflake, DynamoDB, BigQuery, Presto, Milvus, Weaviate, Chroma)
-‚Äã ‚ÄãML (‚ÄãTensorFlow, PyTorch, Transformers, LangChain, LllamaIndex‚Äã, ollama)
-‚Äã ‚ÄãLanguages/Systems/Cloud: C, C++, Java, Go, Spark, Kafka, Redpanda, Iceberg, MinIO, Matlab, Zynq7 FPGA, SDR, IoT, Drone/PX4, Sigint,  AWS, GCP, Azure
-‚Äã ‚ÄãAgile/Development: SCRUM, Kanban, CI/CD, DevOps, DataOps, MLOps
Domains: big/quick data, hi-load, ML, distributed system architectural design implementation, drones, SigInt.
Verticals: finance and payment, realty, automatic control, mobile ads, corporate law, M&A, statistical process control, project management, sales.
I participated in dozens of successful projects, big and small, including the experience of selling startups and preparing for an IPO. Example of personal engineering (architectural) achievements (the last five years):
- has developed a new successful product with the use of NLP and ml approaches even before the advent of linguistic corpora in German;
- took part and made a recognized contribution as the team data architect to the startup, acquired by the Deca-unicorn company;
- took part as a data architect in designing a streaming ETL system to support domain-specific ML-based lead generation and forecasting system of a deca-unicorn company;
- took part as a cross-team dedicated tech lead in implementing the GAAP/management reporting system of a deca-unicorn company.
Preferable big data/geospatial/ml project (Scala/Python/Go stack)
Medium-sized or sizeable cross-functional team(s)
Interesting domain and product that addressed the >1B market"
data engineer,"Experienced Data Analyst/Engineer and BI/SQL Developer with over 20 years in data analytics, database development, and business intelligence. Skilled in Oracle SQL, PL/SQL, BI,  and data warehouse management. Expertise in report development, process optimization, and solution implementation."
data engineer,"Project Data Annotator
–∑ 01.2024 –ø–æ –Ω–∏–Ω—ñ (1 —Ä—ñ–∫ 3 –º—ñ—Å—è—Ü—ñ)
Label Your Data, Remoter.me (–≤—ñ–¥–¥–∞–ª–µ–Ω–Ω–æ, —á–∞—Å—Ç–∫–æ–≤–∞ –∑–∞–π–Ω—è—Ç—ñ—Å—Ç—å) IT
- –ú–∞—Ä–∫—É–≤–∞–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω—å —ñ –≤—ñ–¥–µ–æ (CVAT, Encord, Nexus, Annotation Editor)
- –ê–Ω–æ—Ç–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—ñ–≤ (Doccano)
- –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å —è–∫–æ—Å—Ç—ñ —Ä–æ–∑–º—ñ—á–µ–Ω–∏—Ö –¥–∞–Ω–∏—Ö
Web Scraping Specialist (Python)
–∑ 04.2023 –ø–æ 04.2024 (1 —Ä—ñ–∫)
–§—Ä—ñ–ª–∞–Ω—Å, Upwork (–≤—ñ–¥–¥–∞–ª–µ–Ω–Ω–æ, —á–∞—Å—Ç–∫–æ–≤–∞ –∑–∞–π–Ω—è—Ç—ñ—Å—Ç—å) IT
- –ü–∞—Ä—Å–∏–Ω–≥ —ñ –∑–±—ñ—Ä –¥–∞–Ω–∏—Ö (Requests, BeautifulSoup, lxml, Selenium, Xpath, RegExp)
- –û–±—Ä–æ–±–∫–∞ –∑–æ–±—Ä–∞–∂–µ–Ω—å, —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É (OpenCV, PIL, Tesseract)
- –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö (Matplotlib, Plotly)
- –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—è –∑–∞–¥–∞—á –≤ Excel/Google Sheets
–Ü–Ω–∂–µ–Ω–µ—Ä-–∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä/–Ü–Ω–∂–µ–Ω–µ—Ä-—Ç–µ—Ö–Ω–æ–ª–æ–≥
–∑ 07.2012 –ø–æ 08.2023 (11 —Ä–æ–∫—ñ–≤)
VENTS (Blauberg Group), –ü–µ—Ä—à–∞ –í–µ–Ω—Ç–∏–ª—è—Ü—ñ–π–Ω–∞ –ö–æ–º–ø–∞–Ω—ñ—è, Kromberg&Schubert,–ö–æ—Ç–ª–æ–∑–∞–≤–æ–¥ ""–ö—Ä—ñ–≥–µ—Ä""
–û—Å–æ–±–∏—Å—Ç—ñ —è–∫–æ—Å—Ç—ñ:
- –¥–∏—Å—Ü–∏–ø–ª—ñ–Ω–æ–≤–∞–Ω—ñ—Å—Ç—å, –ø–æ—Å–∏–¥—é—á—ñ—Å—Ç—å, —É–≤–∞–∂–Ω—ñ—Å—Ç—å –¥–æ –¥—Ä—ñ–±–Ω–∏—Ü—å;
- –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å —à–≤–∏–¥–∫–æ –Ω–∞–≤—á–∞—Ç–∏—Å—è, –ª—é–±–æ–≤ –¥–æ –æ—Å–≤–æ—î–Ω–Ω—è –Ω–æ–≤–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π, –±–∞–∂–∞–Ω–Ω—è —Ä–æ–∑–≤–∏–≤–∞—Ç–∏—Å—è —Ç–∞ –ø—ñ–¥–≤–∏—â—É–≤–∞—Ç–∏ —Å–≤—ñ–π –∫–≤–∞–ª—ñ—Ñ—ñ–∫–∞—Ü—ñ–π–Ω–∏–π —Ä—ñ–≤–µ–Ω—å;
- –∑–¥—ñ–±–Ω—ñ—Å—Ç—å –∑ —ñ–Ω—Ç–µ—Ä–µ—Å–æ–º –≤–∏–∫–æ–Ω—É–≤–∞—Ç–∏ —è–∫ —Ä—É—Ç–∏–Ω–Ω—ñ, —Ç–∞–∫ —ñ –∫—Ä–µ–∞—Ç–∏–≤–Ω—ñ –∑–∞–¥–∞—á—ñ;
- —Å—Ç—Ä–µ—Å–æ—Å—Ç—ñ–π–∫—ñ—Å—Ç—å, –∫–æ–º—É–Ω—ñ–∫–∞—Ç–∏–≤–Ω—ñ—Å—Ç—å, –Ω–µ–∫–æ–Ω—Ñ–ª—ñ–∫—Ç–Ω—ñ—Å—Ç—å;
- —É–º—ñ–Ω–Ω—è –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –≤ —Ä–µ–∂–∏–º—ñ –±–∞–≥–∞—Ç–æ–∑–∞–¥–∞—á–Ω–æ—Å—Ç—ñ —Ç–∞ —Ä–æ–±–æ—Ç–∞ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç."
data engineer,"Practical Database Engineer possessing in-depth knowledge of data manipulation techniques and
computer programming paired with expertise in integrating and implementing new software
packages and new products into system. Offering background managing various aspects of
development, design and delivery of database solutions. Tech-savvy and independent professional
bringing outstanding communication and organizational abilities."
data engineer,"Knowledge of¬†Software Testing Life Cycle;
Design and development of¬†technical documentation (test cases, checklists);
Processing tasks in Jira;
Analysis of the internal clients‚Äô requirements to the desire software tool;
Adapting and conducting the unit testing through creating SQL queries;
Development of the validation rules and requirements to the incoming data.
Development ETL process using Azure Synapse  (pipelines, DataFlows) , DataBricks
IT Team Management program
."
data engineer,"SQL-12 year
ETL-5 year
Data warehouse - 5 year
Microsoft SSRS - 2 year
Oracle Bi- 2 Year
Azure Synspace - 1 year
Python - 1 year
Data Geniue - 1 year
SAP- 5 year
Opentext - 5 year
OLTP - 10 year
Olap- 5 year
PLsql - 5 year
pyspark - 1 year
Earned PCAP (PCAP ‚Äì Certified Associate in Python Programming) -2023
Earned IELTS(Academic) Certificate - 2015
Earned OCA (Oracle Certified Associate) -2014
Earned Master Of Computer Sciense 2011-2013, Azerbaijan/Baku,
Azerbaijan Technical University
Earned Bachelor Of Applied Mathemaic  2007-2011, Azerbaijan/Baku, Azerbaijan Technical University
It would be great to work with professionals as we exchange our knowledge."
data engineer,"AI & Data Science Projects
CTU in Prague ‚Äì 2023‚ÄìPresent
Worked on machine learning tasks using Python, Pandas, NumPy, and Scikit-learn for data analysis and model building.
Built a search engine using TF-IDF, SVD, Flask, and MongoDB; frontend in React.
Created visualizations with Matplotlib and Seaborn for data insights.
Currently studying deep learning and building resume_classifier with PyTorch."
data engineer,"- Design and Develop BI enterprise analytics and reporting solution (Azure Cloud, Power
BI Embedded, Python, Power BI Custom Visuals)
- Develop Power BI reports, datasets, and dashboards.
- Support and develop data ingestion and ETL pipelines (Azure DataFactory, Azure Synapse, Azure Databricks)
- Support CI/CD development (GIT, Azure DevOps)"
data engineer,"8 years work experience
Proficient in Apache Spark Scala for distributed data processing and analysis
Experience with Apache Airflow for workflow management and scheduling
Knowledge of PostgreSQL for secure data access and management
Proficient Apache Hive and Oracle Databases for data storage and management
Strong skills in advanced data analytics techniques for distributed system
Led the design and development of a real-time data processing platform using Spark Streaming, resulting in a 50% reduction in data processing time.
Built and maintained data pipelines using Airflow, ensuring timely and accurate data delivery to downstream applications.
conducted extensive data analysis using advanced statistical techniques, resulting in actionable insights that drove business growth.
I am passionate about leveraging my expertise in big data technologies to drive business growth and streamline data-driven decision-making processes. With a proven track record of delivering high-quality data solutions on-time and within budget, I am confident in my ability to make a significant contribution to any organization
Passion and Interest:  driven by my  passion for a in data engineering field or industry. I actively seek opportunities that align with my interests and allow me to work in areas they are genuinely passionate about."
data engineer,"I have a lot of experience in MS SQL. Last few years work closely with cloud (Azure DB, Logic Apps, Key Vault) and Kafka Streams (ksql part). Would like to continue improving my skills in cloud technologies."
data engineer,"December 2020 ‚Äì February 2021, Data Analyst at PWC:
‚Ä¢	Completed an intensive training program, followed by hands-on experience in financial data processing.
‚Ä¢	Received raw source files (Excel, CSV), analyzed their structure, and identified debit and credit columns.
‚Ä¢	Performed data cleaning and transformation to ensure accurate reconciliation of debit and credit balances.
‚Ä¢	Validated financial data integrity by ensuring that the total debit equaled the total credit, using manual adjustments in Excel or SQL queries in MS SQL Server.
‚Ä¢	Loaded the processed data into Qlik for reporting, analysis, and visualization.
March 2021 ‚Äì November 2021, BI developer at VEO:
‚Ä¢	Completed training in IBM Cognos Analytics, focusing on report development and data visualization.
‚Ä¢	Worked with test datasets to design and create custom reports, gaining hands-on experience in BI reporting.
‚Ä¢	Explored and configured data packages in IBM Cognos ETL, enhancing data processing and transformation skills.
November 2021 ‚Äì till now, Data Engineer at Symfa:
Insurance-related project for a large American corporation (Fortune 500).
‚Ä¢	Designed and implemented ETL solutions using Astera Centerprise to efficiently process diverse data sources.
‚Ä¢	Developed and optimized ETL workflows and database objects in MSSQL, enhancing data processing performance.
‚Ä¢	Managed version control and collaborated on development using Git, ensuring streamlined code deployment.
‚Ä¢	Conducted thorough testing and validating of ETL processes, ensuring data accuracy, integrity and completeness.
‚Ä¢	Monitored and maintained ETL jobs, proactively troubleshooting issues to optimize system performance.
‚Ä¢	Performed comprehensive code reviews, improving code quality and adherence to best practices.
PwC (Data Analyst)
Optimized financial data processing, reducing analysis and file preparation time by 20%.
Automated the detection of debit and credit discrepancies, minimizing manual adjustments.
Utilized SQL for financial data validation, improving reconciliation speed and accuracy.
VEO (BI Developer)
Developed over 100 BI reports in IBM Cognos, providing clearer analytical insights for business users.
Optimized report structures, improving execution speed by 30%.
Explored and configured IBM Cognos ETL, gaining deeper expertise in data transformation processes.
Symfa (Data Engineer)
Designed and optimized over 150 ETL implementations in Astera Centerprise and MS SQL, reducing data processing time by 40%.
Implemented data quality controls, decreasing reporting errors by 25%.
Enhanced SQL query performance, reducing execution time of key database operations by 50%.
Automated ETL job monitoring, lowering incident rates and improving process stability.
Introduced code review standards in the team, improving code quality and reducing technical debt.
I want to grow a lot and work on intereesting projects."
data engineer,"I specialize in end-to-end analytics, extracting data from various sources and automating data workflows. I build ETL processes using Python, Node.js, and SQL, ensuring efficient data management. I also create visualizations in Looker Studio and Power BI to provide actionable insights.
Currently, I focus on data integration, automation, and business intelligence. I aim to grow in data engineering, enhancing my expertise in cloud solutions, scalable data pipelines, and advanced analytics."
data engineer,"-----------
‚Äî Built and managed ETL pipelines using Airflow, defining efficient DAG workflows
for automating data processes
‚Äî Developed data cleaning and transformation scripts using Python and Pandas for
structured and semi-structured data
‚Äî Designed and optimized PostgreSQL schemas and SQL queries for data
aggregation and business insights
‚Äî Processed large-scale data with Apache Spark, focusing on performance
optimization for batch workflows
‚Äî Deployed and maintained data solutions on AWS (S3), integrating cloud-based
storage with local systems
-------------------
‚Äî Designed and implemented data resume_classifier in DBT, enabling efficient data
transformations and documentation
‚Äî Leveraged Python to develop reusable scripts for automating data integration
and error handling
‚Äî Worked with both SQL (PostgreSQL) and NoSQL (MongoDB) databases for
efficient data storage and retrieval
‚Äî Migrated critical data processing workloads to AWS (S3), improving pipeline
reliability and scalability
‚Äî Streamlined data delivery processes, enabling faster access to clean datasets for
machine learning pipelines.
----------------------------
‚Äî Design and develop decentralized applications using Next.js
‚Äî Connect applications with various blockchain networks (e.g., Ethereum, Binance
Smart Chain, Polygon)
‚Äî Explore and integrate emerging technologies, such as Web3.js, Ethers.js, and
other decentralized protocols, to enhance the functionality and performance
‚Äî Ensure robust and efficient API endpoints for data transactions and
communication with the blockchain."
data engineer,"I have over 7+ years of IT experience in Analysis, Design, Modeling, Development, Testing and Maintenance of Data Warehouse Business Applications. I have worked in different domains like Finance, Commerce, Legal and Retail.
I have extensive experience in creating PL/SQL Packages, Procedures, Functions, Cursors, Event Handlers, and Database Triggers. I have excellent SQL skills, written complex queries and correlated sub queries.
In all my projects, I have created database objects like Tables, Views, Sequences and Synonyms. I am proficient in logical and physical Data Modeling. In my recent project with Georgia Pacific I was involved in both physical and logical Data Modeling, Forward and Reverse Engineering. In all my projects, I was involved in Performance Tuning, Partitioning tables, Optimizing Indexes and Queries. I have worked on both UNIX and Windows environments. I am proficient in migrating data and worked with SQL Loader and DTS packages. I have experience in UNIX Shell scripting.  I have an excellent understanding of Iterative, waterfall and Agile Methodologies and was involved in all stages of Software Development Life cycles during my projects."
data engineer,"I am transitioning into a data engineering role, focusing on Python programming, advanced SQL and deepening my expertise in PostgreSQL. Currently, I am building skills in data engineering concepts such as OLAP/OLTP, ETL processes, and cloud-based solutions.
In my previous role as a 1C developer and team leader, I managed diverse projects, including:
- System Upgrades and High-Load Optimization: Configured 1C and MS SQL Server systems to support version changes and high-load operations.
- Sales System Transition: Led the migration to new sales systems across ~150 stores, implementing inventory control and warehouse management solutions.
- Report Automation: Developed and automated financial, sales, HR, and inventory reports, optimizing generation and distribution processes.
- API Integrations: Integrated third-party applications (e.g., Rozetka, Nova Poshta) and hardware (sales and warehouse equipment) through API development.
- Data Migration: Designed and implemented robust data migration workflows for system transitions.
- Team Leadership: Onboarded and trained new team members, estimated tasks, managed workloads, conducted code reviews, and ensured timely delivery of solutions.
- Technical Documentation: Authored comprehensive technical documentation, analyzed requirements, and communicated effectively with internal stakeholders.
One of the core achievements in my career story I consider developing ability to manage projects with unspesialized business requirements. Its demands not only high level hard skills to propose solution, but also soft skills to understanding business side.
I am looking for a role in a company that values stability and fosters a transparent workflow. I thrive in environments where clear communication, structured processes, and collaborative efforts are prioritized, enabling both personal and organizational growth."
data engineer,"Highly skilled and certified Data Engineer with advanced
expertise in Azure tools. Demonstrated expertise in data
integration, ETL processes, and cloud-based data storage.
Strong understanding of Azure services including Azure Data
Factory, Microsoft Fabric, Azure Logic Apps, Azure SQL
Database, and Azure Analysis Service etc."
data engineer,"With a background in data analysis, forecasting resume_classifier and visualization, I've developed into a highly skilled data engineer with experience designing and implementing modern data stack (DBT, AirByte, Terraform, Airflow and GCP). My ability to think strategically about how data can drive business value, while also being detail-oriented and capable of handling complex technical tasks, makes me an effective data engineering leader (led a team of 5 data engineers)."
data engineer,"I have 10+ years of commercial development using Python, Projects I was working on relate to the different areas:  network/security, ERP, ETL, and Data processing. I prefer using the standard library instead of using modern external packages.
Resolving the issue with high quality in predefined terms means more than the desire to try modern technologies.
Developed Android client for Python based application.
Developed several integrations using SOAP, XML-RPC.
Implemented and configured task tracking process.
Automated development process with internal tools including Slack bot.
Developer REST API from scratch.
Developed scraping scheduling and data post-processing from scratch.
Switched LDAP client libraries with integrations.
Migrated an old REST API to the newer with FastAPI.
Developed several integrations with external APIs (Domo, Marketo, Hubspot, etc)
Developed ETL pipeline on AWS from scratch with terabytes of data in Redshift. Managed Redshift cluster.
No Django development."
data engineer,"- –í–ø—Ä–æ–¥–æ–≤–∂ –æ—Å—Ç–∞–Ω–Ω—ñ—Ö 3 —Ä–æ–∫—ñ–≤ –∑–∞–π–º–∞—é—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—î—é —Å–µ—Ä–≤—ñ—Å–Ω–æ–≥–æ –æ–±—Å–ª—É–≥–æ–≤—É–≤–∞–Ω–Ω—è —Ç–∞ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–∏—Ö –∑–∞—Ö–æ–¥—ñ–≤ –Ω–∞ –æ—Å–Ω–æ–≤—ñ CRM —Å–∏—Å—Ç–µ–º–∏ Creatio (bpmn, C#, js), —Ç—ñ–º–ª—ñ–¥ –∫–æ–º–∞–Ω–¥–∏ —Å—É–ø—Ä–æ–≤–æ–¥—É –≤—ñ–¥–¥—ñ–ª—É –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ –∫–ª—ñ—î–Ω—Ç—ñ–≤ –≤ –∫–æ–º–ø–∞–Ω—ñ—ó Meest Shopping;
- –ü—Ä–æ—Ç—è–≥–æ–º –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö 8 —Ä–æ–∫—ñ–≤ —Ä–æ–∑—Ä–æ–±–ª—è–≤ —Ç–∞ –≤–ø—Ä–æ–≤–∞–¥–∂—É–≤–∞–≤ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ–π–Ω—ñ —Å–∏—Å—Ç–µ–º–∏ –¥–ª—è –≤–∏—Ä–æ–±–Ω–∏—Ü—Ç–≤–∞, –º–∞–≥–∞–∑–∏–Ω—ñ–≤, –º–æ–±—ñ–ª—å–Ω—ñ —Ä—ñ—à–µ–Ω–Ω—è –¥–ª—è —Ç–æ—Ä–≥–æ–≤–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–Ω–∏–∫—ñ–≤, —Å–∫–ª–∞–¥—ñ–≤, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–æ–≤—É–≤–∞–≤ —Ç–∞ –ø—Ä–∏—à–≤–∏–¥—à—É–≤–∞–≤ —Ä–æ–±–æ—Ç—É –≤—ñ–¥–¥—ñ–ª—ñ–≤ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø—Ä–æ–≥—Ä–∞–º–Ω–∏—Ö –∑–∞—Å–æ–±—ñ–≤ –≤—ñ–¥ 1–° –≤ –∫–æ–º–ø–∞–Ω—ñ—ó Storm group.
–£—Å–ø—ñ—à–Ω–æ –ø—Ä–æ–π—à–æ–≤ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –∫—É—Ä—Å—ñ Data Engineering –≤—ñ–¥ robot dreams, –Ω–∞–ø–∏—Å–∞–≤ –∫—É—Ä—Å–æ–≤—É —Ä–æ–±–æ—Ç—É —ñ –æ—Ç—Ä–∏–º–∞–≤ —Å–µ—Ä—Ç–∏—Ñ—ñ–∫–∞—Ç. –ó–∞ –±–∞–∂–∞–Ω–Ω—è–º –Ω–∞–¥–∞—é –ø—Ä–∞–≤–∞ —Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç–∞ –Ω–∞
PR –≤ Git.
–®—É–∫–∞—é —Ä–æ–±–æ—Ç—É –∑ —á–∞—Å—Ç–∫–æ–≤–æ—é –∑–∞–π–Ω—è—Ç—ñ—Å—Ç—é –¥–µ –∑–º–æ–∂—É –≤–∏—Ä—ñ—à—É–≤–∞—Ç–∏ –∑–∞–≤–¥–∞–Ω–Ω—è —Ç–∞ –Ω–∞–±–∏—Ä–∞—Ç–∏—Å—è –¥–æ—Å–≤—ñ–¥—É —è–∫ —ñ–Ω–∂–µ–Ω–µ—Ä –¥–∞–Ω–∏—Ö, –∞ —Ç–∞–∫–æ–∂ –±–∞–∂–∞—é –Ω–∞–¥–∞–ª—ñ –ø–æ–≥–ª–∏–±–ª—é–≤–∞—Ç–∏ —Å–≤–æ—é –µ–∫—Å–ø–µ—Ä—Ç–∏–∑—É –≤ —Å—Ç–æ—Ä–æ–Ω—É –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏ –¥–∞–Ω–∏—Ö —á–∏ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö. –Ø –æ—á—ñ–∫—É—é, —â–æ –º–æ—ó –∫–æ–º–ø–µ—Ç–µ–Ω—Ü—ñ—ó –∑ –º–∏–Ω—É–ª–∏—Ö –ø—Ä–æ—î–∫—Ç—ñ–≤ –∑–≥–æ–¥—è—Ç—å—Å—è, –∞ –±—Ä–∞–∫ –¥–æ—Å–≤—ñ–¥—É –∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—é —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—î—é –Ω–µ —Å—Ç–∞–Ω–µ –Ω–∞ –∑–∞–≤–∞–¥—ñ.
–í—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω–∏–π, –≤–º–æ—Ç–∏–≤–æ–≤–∞–Ω–∏–π, –Ω–∞—Ü—ñ–ª–µ–Ω–∏–π –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –Ω–∞–º–∞–≥–∞—é—Å—è —Ä–æ–∑—ñ–±—Ä–∞—Ç–∏—Å—è –≤ —Ç–æ–º—É, —â–æ —Ä–æ–±–ª—é —ñ –æ—Ç—Ä–∏–º–∞—Ç–∏ –∑–∞–¥–æ–≤–æ–ª–µ–Ω–Ω—è –≤—ñ–¥ —Ä–æ–±–æ—Ç–∏"
data engineer,"Apr 2021 ‚Äì Present
Company: ApexHealth Inc.
Project Role: Senior Data Engineer
Developed the database schema, including tables, relationships (foreign keys), indexes, and constraints to ensure that the design is normalized to reduce redundancy and improve data integrity.
Wrote complex stored procedures and functions to meet business logic within the database, including input parameters, output parameters, and error handling mechanisms.
Developed and maintained robust ETL (Extract, Transform, Load) pipelines to ingest data from various sources into Azure Databricks.
Created custom user-defined functions in Python and used in Spark SQL queries to apply complex transformations in Spark SQL.
Designed, developed, deployed, and maintained Power BI report solutions, selecting appropriate visualization types such as bar and column charts, line charts, pie charts, KPI visuals, table matrix, tailored to display data effectively and provide actionable insights.
Jan 2018 ‚Äì Mar 2021
Company: Al Abdulghani Motors Co.
Project Role: Senior Data Engineer
Designed and implemented a robust data warehousing solution in Azure Synapse Analytics, leveraging distributed tables, partitioning schemes, and indexing strategies for optimal performance.
Employed Azure Databricks (including structured streaming) to develop real-time analytics with windowed aggregations and temporal joins, and performed SQL query and Spark job tuning via execution plan analysis. Worked closely with data analysts and customers to gather project requirements and ensure timely delivery.
Built complex DAX expressions for sophisticated business calculations‚Äîlike dynamic segmentation, time intelligence, and multi-dimensional analysis‚Äîwhile creating calculated columns for more efficient reporting. Used DAX Studio and Power BI Performance Analyzer to optimize slow queries.
Jun 2016 ‚Äì Nov 2017
Company: Acronis
Project Role: Data Engineer
Improved query performance in T-SQL by examining execution plans, optimizing joins/sub-queries, and creating stored procedures/functions for better modularity and flexibility.
Created and scheduled SSIS packages to transform data from Oracle/SAP and SQL Server into a centralized Data Warehouse
Implemented complex MDX expressions for time intelligence, calculated members, and KPIs in SSAS.
Deployed data-driven subscriptions in SSRS alongside caching strategies, reducing server load and delivering timely updates to stakeholders.
Implemented the migration of an on-premise data warehouse to Azure Synapse for a mid-sized financial services client that was experiencing slow, unreliable analytics.
Created a new data model in Azure Synapse using star schema design to significantly enhance query performance.
Built ETL pipelines in Azure Data Factory to consolidate data from various sources (CRM, ERP, and third-party APIs) into a unified data lake.
Applied row-level security and Always Encrypted in Synapse to meet GDPR requirements.
Established a fully automated CI/CD process in Azure DevOps, substantially minimizing the chances of deployment errors.
What I‚Äôm Looking For
I work best in environments where knowledge-sharing, transparent communication, and a supportive atmosphere are top priorities. So I am looking for opportunities to grow.
I perform best within teams that emphasize shared knowledge, transparent dialogue, and mutual support, meaning that cooperative environment is another priority.
What I try to avoid:
Unproductive conflict, poor communication, or practices that undermine team morale and well-being."
data engineer,"Highly-skilled professional with a background in migrating (lift & shift or lift & optimize), designing, building, and maintaining large databases, warehouses in a team environment, as well as playing an impactful role on the BI & data visualization front.
During my work experience, I have used different tools and technologies to deliver business value, such as:
Technologies - ADF, dbt, Terraform, Snowflake, MS Fabric, Azure DevOps, git;
Languages - Python, SQL, DAX, R.
Azure and Power BI certifications"
data engineer,"(February 2024 - Present)
Role: Data Engineer (Contract)
Current Role in Team: Data Consultant at Nordstrom (Fashion Retailer in the US)
Projects and Tasks:
Data Loading: Loaded data into Teradata landing and fact tables based on business requirements.
Pipeline Development: Built pipelines on Airflow to onboard Kafka streaming events to the Teradata service layer, using both internal framework and custom DAGs.
Data Management: Consumed messages from multi-schema Kafka topics in Avro format, saved raw data as ORC files in S3 buckets, and defined external Hive tables on top of the ORC files.
CI/CD: Managed all data deployments through CI/CD pipelines in GitLab.
Monitoring: Monitored logs using Splunk, New Relic, and PagerDuty.
Technologies Used:
Teradata
Apache Airflow
Apache Kafka
Amazon S3
Hive
GitLab CI/CD
Splunk, New Relic, PagerDuty
Python, SQL
What I Want to Improve:
Enhance skills in advanced Kafka management and real-time data processing.
Explore more optimization techniques on Spark
Gain experience with Google Cloud Platform (GCP) services.
Gain deeper knowledge in data security and compliance measures.
-More cloud experience, especially GCP.
-Satisfying tech stack
-Advanced Spark
-Remote opportunity"
data engineer,"- For Kazakhstan's largest telecom operator, conducted stakeholder interviews to gather data requirements, assessed current data governance maturity, designed data architecture blueprint, and implemented new data pipelines, resulting in 20% boost in operational efficiency. Analyzed current data flows, documented data quality issues, designed new data quality framework, and implemented automated data quality checks
- For major Central Asian bank, analyzed existing credit scoring resume_classifier, gathered requirements from risk department, developed new statistical resume_classifier using Python, and implemented enhanced scoring system, improving accuracy by 20%
- In a large Central Asia company for SAP S/4 HANA implementation, mapped data sources, designed ETL architecture, developed PowerBI dashboards, and automated operational reporting, reducing report creation time by 50%
- During PwC Digital Hackathon, led team discovery sessions, designed database schema in MongoDB, developed React JS frontend, and deployed performance management application, increasing process efficiency by 300%
- For State Authority project funded by UNDP, conducted data analysis workshops, mapped taxpayer data sources, developed scoring algorithms, implemented Big Data platform, and automated taxpayer assessment, boosting processing efficiency by 50%
- Led a team of 10 consultants to deliver the IT Strategy and long-term roadmap for Kazakhstan's largest private equity company. Conducted in-depth analysis of business resume_classifier (value chains) across 10 portfolio companies, resulting in the development of a target model of IT and Data architecture. This initiative led to a 20% increase in operational efficiency and a 25% reduction in IT costs
- For national atomic company, performed quality assurance for the SAP system implementation, ensured alignment across business and IT, conducted data quality assessment, developed testing scenarios, and monitored system integration, improving data integration efficiency by 20%
- For 50+ financial audits, extracted transaction data using SQL, performed anomaly detection using Python and SPSS, developed automated testing scripts, and generated fraud detection reports, improving detection efficiency by 90%
- For large bank project, analyzed current database structure, identified optimization opportunities, implemented new data resume_classifier, and automated query processes, reducing data redundancy by 30% and improving query speed by 40%"
data engineer,"6 years of working experience in the IT industry as a Software Engineer, 3 of them as a Data Engineer. 2.5 years experience with Spark+Java+AWS, about a year experience with Spark+Scala+Hive. Knowledge of Java, Scala and Python. Hands-on experience with SQL (PostgreSQL, MySQL) and NoSQL (HBase, Redis, DynamoDB).
Looking for challenging projects for professional growth as a Data Engineer, extending my experience to tech stack Spark+Scala/Python. Working in a team to be able to gain new experience and share the existing one."
data engineer,"Current experience includes building and optimising ETL pipelines, developing and maintaining APIs (Flask, FastAPI), and implementing automation solutions. Extensive experience with Apache Airflow (including integration with AWS S3 and Lambda), and various APIs for data extraction and processing. Developed complex algorithms for duplicate detection, missing opportunity identification, and structuring large-scale data workflows for analytics and reporting. Work has also involved automation using Selenium, streamlining repetitive processes, and integrating reporting solutions in Google Sheets and Tableau.
- Optimised ETL Pipelines: Improved data processing speed and efficiency by designing and optimising ETL workflows in Apache Airflow.
- Developed APIs: Built and maintained APIs.
- Automated Data Processing: Reduced manual workload by implementing automation with Selenium and Airflow.
- Integrated Cloud Solutions: Worked with AWS S3 and Lambda for scalable data storage and processing.
- Enhanced Reporting: Optimized reporting processes with Google Sheets (Google API) and Tableau.
- Applied Machine Learning: Used ML techniques for data clustering, image analysis, data vectorization, etc.
Particularly interested in cloud technologies, and scalable data solutions, data science. Looking for a company where I can contribute, grow, and work on challenging projects."
data engineer,"Argus Media - Technical Lead (December 2024 - Present)
Real-Time Data Integration:
Role: Technical Lead.
Tech: AWS, Spark, DBT, Fivetran, Timbr.ai, OpenMetadata, GCP stack.
Details:
1. Leading and managing data pipelines on AWS to ensure efficient data flow and processing.
2. Designing and implementing data resume_classifier in DBT to align with evolving business requirements.
3. Overseeing and maintaining the GCP data infrastructure.
4. Utilizing Timbr.ai for semantic data modeling and knowledge graph implementation.
5. Integrating tools like Fivetran and OpenMetadata to enhance data lineage and observability across platforms.
Argus Media - Senior Big Data Engineer (Feb 2024 - December 2024)
Real-Time Data Integration:
Role: Senior Data Engineer.
Tech: AWS, Spark, DBT, Fivetran.
Details: Built and managed diverse AWS data pipelines, modeling data for DBT to support business needs.
IdeaSoft.io - Big Data Engineer (Nov 2023 ‚Äì Feb 2024)
Real-Time Data Integration:
Role: Data Engineer.
Tech: AWS Glue, Snowflake, DBT.
Details: Architect and Initiated Real-time Data Streams using AWS Glue & Lambda & Terraform. Optimize customer data presentation in Snowflake using DBT
AM-BITS LLC - Big Data Engineer (Apr 2021 - Oct 2023)
Real-Time Data Integration:
Role: Lead Data Engineer.
Tech: PySpark, Kafka, HBase.
Details: Developed a real-time data system using PySpark & Kafka; enabled scalable storage with HBase.
Database Migration:
Role: Data Engineer.
Tech: PostgreSQL, Spark, Kafka, MongoDB.
Details: Migrated data from Oracle to PostgreSQL using PySpark, Kafka, and MongoDB.
Cloud Migration:
Role: Data Engineer.
Tech: HDP, Apache Nifi, Hive, AWS S3, Snowflake.
Details: Transitioned data to AWS S3 and Snowflake; aided a telecom client's transition from Oracle to HDP.
MDM-Group - Big Data Engineer (Nov 2019 - Jan 2021)
ETL Enhancement:
Role: Data Engineer.
Tech: Airflow, HDFS.
Details: Optimized ETL workflows using Apache Airflow.
BI & Visualization:
Role: BI Specialist.
Tech: Superset, HDP.
Details: Integrated Superset with HDP for real-time insights.
Kyiv Polytechnic Institute StartUp - QA Engineer (Aug 2018 - Feb 2019)
Academic Platforms QA:
Role: QA Specialist.
Tech: JMeter, Python.
Details: Quality assurance for academic platforms using manual and automated testing.
Accomplishments:
IdeaSoft.io:
Seamless Cloud Migration: Migrated over 2TB of structured data from MS SQL Server to Snowflake using AWS Glue and Terraform.
Achieved a 18% reduction in infrastructure overhead by replacing legacy ETL processes with serverless Glue jobs.
Implemented Terraform for provisioning Glue jobs and associated resources, ensuring reproducibility and minimizing configuration drift.
AM-BITS LLC:
Real-time Data Processing Leadership: Spearheaded the development of a real-time data integration system, leading to a 35% increase in data processing speed and achieving 99.5% accuracy in data integrations.
Database Efficiency: Successfully migrated over 3TB of data from Oracle to PostgreSQL, resulting in a 25% improvement in query performance and a 15% reduction in infrastructure costs.
Cloud Scalability: Migrated over 5TB of crucial datasets to cloud platforms (AWS S3 & Snowflake), realizing a 20% cost saving in storage expenses.
Client Satisfaction: Guided a telecom client in their transition from Oracle to HDP, leading to a 30% cost reduction and a 90% satisfaction rate from the client.
MDM-Group:
ETL Performance: Revamped and optimized ETL workflows with Apache Airflow, resulting in a 20% decrease in data latency and ensuring 24/7 data availability for analytics.
Stakeholder Engagement: Integrated Apache Superset with the company's HDP stack, giving stakeholders real-time business insights which boosted data-driven decisions by 40%.
Kyiv Polytechnic Institute StartUp:
QA Excellence: Identified and resolved over 150 platform discrepancies, ensuring optimal user experience for academic platforms.
Automation Mastery: Implemented automated testing procedures with JMeter, reducing manual testing time by 45% and expediting the QA cycle by 30%."
data engineer,"As Data Engineer
(October 2021- November 2024)
IBM Integration: developing REST APIs, Integration solutions and apps in banking domain.
Tools: IIB Toolkit, MQ, ESQL, DB2.
AWS RDS for Postgres DB ‚Äì Salesforce integration: designing and developing a solution for DB ‚Äì
Salesforce integration.
Tools: AWS AppFlow, AWS Lambda, Python, SQL.
Migration of enterprise reports from SAP Crystal Reports to MS SSRS:
- developing reports
(creating configs, layouts, data formatting, testing, deploying).
Tools: SAP Crystal Reports, MS SSRS, MS Report Builder, Oracle.
Informatica Data Integration:
-building data flows with transformations and mappings according to business requirements.
Tools: Informatica Intelligent Cloud Services.
Oracle DB maintaining, PLSQL development: developing and testing PLSQL functions and
procedures.
Tools: ORACLE RDBMS 19, PLSQL Developer, SQL Developer.
Commercial experience as  Python Developer: (September 2020 - August 2021)
I was engaged in working with:
- AWS serverless application development with API, Lambda, Step Function, DynamoDB, Rekognition, Athena, Glue etc;
- ETL pipeline implementation;
- developing REST APIs with Python, OpenAPI (Swagger), YAML.
Data Engineer Associate Certification
Certificant: Dmytro Asliaiev
Date Obtained: May 17, 2024
Expiration Date: May 17, 2026
Key Skills and Competencies:
Data Management: Proficient in ensuring data accuracy and quality for analysis, with strong SQL skills validated through timed exams and practical exercises.
Exploratory Analysis: Knowledgeable in data interpretation and collaboration with analysts and scientists, facilitating insightful data-driven decisions.
Assessment Methodology:
Timed Exams: Completed exams covering a wide range of data engineering topics to establish foundational knowledge.
Practical Exam Submission: Demonstrated real-world problem-solving abilities by cleaning and preparing data through hands-on tasks.
I`m open for new opportunities due to the end of current project. I would like to consider the company where I can closely work with highly qualified professionals on inspiring technological  projects. If the potential employer is looking for long-time cooperation, I would love to be part of that company."
data engineer,"Highly skilled and versatile Software and Big Data Engineer with 8 years of extensive experience in designing, developing, and optimizing large-scale data solutions. Proficient in a broad array of programming languages including Java, Scala, Python, SQL, Bash, and PowerShell. Adept at leveraging cutting-edge big data technologies such as Hadoop, Hive, Spark, Dask, Jupyter, JupyterHub, Pig, Sqoop, HBase, Phoenix, Cassandra, Kafka, Oozie, NiFi, and Airflow. Experienced with leading platforms like Hortonworks, Cloudera, Azure, AWS, Databricks, and Kubernetes. Skilled in handling various data formats including CSV, JSON, XML, AVRO, Parquet, ORC, Delta, and Iceberg. Proficient in web technologies and frameworks such as REST, Spring, Hibernate, and Akka, ensuring robust and scalable software solutions."
data engineer,"Self-directed and driven Data Engineer with a background in Data Science, having good problem-solving skills.
- Years of experience: 5
- Domain: banking, fintech, healthcare.
- Technologies stack: Python (Spark, Trino, SQLalchemy, Airflow, FastAPI, MinIO, Boto3), Postgres, Kubernetes, Docker, AWS (S3, Athena, Lambda, Glue, QuickSight, LakeFormation), Dask.
- BI Tools: SAP, Alteryx, PowerBI, Tableau
- Version control: Git, BitBucket
- Documentation: GitLab, Confluence
- Pipeline: ArgoCD
- Monitoring: Grafana/Loki
- Education: Carnegie Mellon University (2019)
- Major: Data Analytics/Data Science
- Rated TOP 20% in McKinsey x Tinkoff hackathon
- Rated TOP 35% in McKinsey x Alibaba hackathon
- Certified Alteryx designer (ETL tool)"
data engineer,"Experienced Data Engineer with a proven track record in building modern data warehouses, optimizing ETL processes, and architecting cloud-based infrastructure on Azure and AWS.
Proficient in Python, Scala, Java, SQL, and Bash for code development and review. Recently led the development of a Data Lakehouse using Azure Databricks and Delta Live tables for a Single Source of Truth (SSOT) project.
Seeking to enhance architectural skills in Data Engineering. Aspire to explore advanced data engineering design patterns, scalability solutions, and cloud-native technologies. Additionally, interested in mentoring and leadership roles to contribute to the growth of the data engineering community.
Reviewed and optimized the existing project architecture, resulting in a remarkable cost reduction of 4x. Achieved a significant enhancement in data transformation efficiency, reducing execution time from 1 day to just 1 hour.
As a typical engineer, I thrive on engaging and intricate challenges. I'm passionate about creating solutions from scratch or enhancing existing code to elevate its quality and performance.
In a leadership role, I value a proficient and cohesive team. I anticipate each team member to possess a clear understanding of their role, the ability to estimate and manage their tasks effectively, and a commitment to meeting deadlines.
Additionally, I anticipate the product owner to have a well-defined vision and objectives for the products we work on, ensuring a sense of purpose and direction in our development efforts."
data engineer,"Work Experience
Senior Software Engineer 2024-Peresent
Current Role:
Front-end developer on the SaaS-based HST Payroll Management System.
Specializing in building scalable web applications with React.js, Tailwind CSS, and TypeScript.
Enhancing user experiences with intuitive UI designs and responsive layouts.
Collaborating with cross-functional teams to ensure system security and performance.
Key Projects:
Developed and optimized the Oromia Transport Management System as a full-stack developer.
Integrated APIs and implemented dynamic data visualizations to improve usability and real-time reporting.
Technologies Used: React.js, Node.js, Tailwind CSS, TypeScript, MongoDB, MySQL, REST APIs.
What to Improve:
Enhance skills in testing frameworks like Jest and Cypress for robust unit and end-to-end testing.
Strengthen understanding of microservices and advanced DevOps practices for scalable solutions.
Software Engineer & System Administrator Jun 2022-Dec-2024
Built and managed custom web solutions with Vue.js and React.js.
Administered IT systems to ensure smooth operations and minimal downtime.
Played a vital role in integrating security protocols for safeguarding sensitive user data.
System Security Specialist July 2022 - December 2023
Monitored systems and networks using specialized tools to ensure 24/7 security.
Reviewed logs and performance metrics to identify and mitigate potential threats.
Notable Certifications
Full-Stack Development Certification (ALX Africa).
Advanced Web Development Certification.
Key Strengths
Deep expertise in front-end frameworks like React and AngularJS.
Proven ability to deliver projects with tight deadlines and challenging requirements.
Strong problem-solving and debugging skills, ensuring high-quality deliverables.
**System Reliability and Performance**
- Developed and maintained advanced security systems at Koppa Smart Security, resulting in a 25% increase in system reliability and a 20% improvement in overall performance.
- Implemented robust security measures that reduced system vulnerabilities by 30%.
**Operational Efficiency**
- Led the development of ERP systems at HST Consulting PLC, tailored to meet the unique needs of various business sectors, resulting in a 40% enhancement in operational efficiency.
- Improved user experience by integrating intuitive user interfaces and streamlining workflow processes, reducing user task completion time by 35%.
**Improved Transportation Management**
- Spearheaded the development of the Oromia Transport Management System at DAFTech Social ICT Solutions, which optimized transportation routes and schedules, leading to a 50% increase in overall transportation efficiency.
- Enhanced management capabilities by integrating real-time tracking and reporting features, reducing manual management efforts by 45%.
**Accurate Billing Processes**
- Maintained and upgraded the water billing system for Addis Ababa city at DAFTech Social ICT Solutions, ensuring 98% accuracy in billing processes.
- Implemented automated billing verification processes that reduced billing discrepancies by 25% and improved billing cycle time by 30%.
SaaS-Based Payroll Management System
Impact: Streamlined payroll processing for HST Consulting by designing a scalable front-end architecture.
Details:
Developed features like payroll period selection, payslip Email automation, and department-wise Salary visualizations using React.js and TypeScript.
Reduced user interaction time by 30% through optimized UI/UX.
Tools/Technologies: React.js, Tailwind CSS, REST APIs.
2. Oromia Transport Management System
Impact: Enhanced transportation operations by building a centralized management platform.
Details:
Designed and implemented full-stack features, including real-time tracking and reporting modules.
Integrated secure payment gateways and notification systems for user transactions.
Delivered the project within a tight deadline, resulting in successful deployment.
Tools/Technologies: Node.js, React.js, MongoDB, MySQL.
3. IT Systems Security and Monitoring
Impact: Improved system uptime and security for Koppa Smart Security Solutions.
Details:
Monitored critical infrastructure 24/7, reducing downtime by 20%.
Implemented security protocols that safeguarded sensitive client data from breaches.
4. Certification & Continuous Learning
Impact: Boosted technical expertise through advanced training and certifications.
Details:
Graduated from the ALX Africa Career Essentials Program with a Full-Stack Development Certification.
Improved project delivery quality by applying new skills in frameworks like Vue.js and AngularJS.
5. Collaboration & Teamwork
Impact: Improved team productivity and cross-functional collaboration.
Details:
Played a key role in DevOps discussions, introducing CI/CD practices to reduce deployment time by 50%.
Mentored junior developers, improving team coding standards and knowledge-sharing culture."
data engineer,"As a Data Engineer, I have worked on projects in the Ad-Tech industry, handling high query volumes and managing terabytes of data for both SSP and DSP. I possess expertise in programmatic ads.
Tech: Scala, Python, Apache Spark, AWS, GCP.
Both batch and stream data processing
Building Analytical Data Platforms from scratch for eCommerce companies. Model of a Data Warehouse.
Snowflake, DBT, Python, Go
Worked in Telecom as a Data Engineer. Building of Data platform to analyze the mobile network telemetry and predict the possible failures.
I have experience in migrating on-premise Data Warehouse and ETL into Cloud.
Having predominantly worked with startups, I deeply understand the importance of ownership, delivering fast results, and maintaining a problem-solving attitude.
In addition to my data engineering expertise, I possess strong backend development skills in Python, Scala, and Go. I am also a beginner in Rust, continuously expanding my technical toolkit to adapt to evolving technologies and industry demands.
Overall, my diverse experience showcases my ability to deliver effective data engineering solutions using various technologies.
I'm looking for a job as a Senior/Staff Data Engineer or Backend engineer.
Led the creation of comprehensive Data Platforms from scratch, effectively handling all aspects from data ingestion to Data Warehouse, Data modeling, and building both forward and reverse ETL pipelines.
Successfully deployed complex data-centric systems to production environments.
Handled and processed large-scale data sets, working with volumes in the range of hundreds of terabytes.
Successfully established and grew effective data engineering teams.
For one of the clients, I reduced Snowflake costs by up to 35% and AWS costs by up to 30%.
Initiated and implemented the use of DBT within the organization, enabling analysts to write their own data transformation pipelines."
data engineer,"Mykyta takes programming and engineering with a passion. He is a big data middle engineer working in the field for the past 2 years specializing in distributed solutions and cloud deployments. His main tools are Spark for developing efficient data lakes and Kubernetes for deploying solutions in the cloud. He uses Kafka for microservice communication, Cassandra as a distributed database, and Redis as distributed cache. His language of choice is Scala. He uses Akka to leverage concurrency and ZIO as an effect system for writing functionally pure applications. Mykyta is open to challenges and eager to deliver solutions that fit customers‚Äô needs."
data engineer,"‚Ä¢ 20+ years of professional experience in the Information Technology industry;
‚Ä¢ Rich experience in Database/ETL development and Data Engineering;
‚Ä¢ Solid experience in Business Intelligence;
‚Ä¢ Domains of experience: financial, transportation and electronics manufacturing.
In the last decade, I have built several enterprise-level data warehouses from scratch. Currently, I am helping Fortune 500 company with cloud migration from on-premises reporting and analytics solutions to AWS.
I am looking for a long-term assignment at a firm with a strong data-driven culture and enthusiastic data engineering and data analytics teams. I'm a firm believer in the importance of data and would like to join a company where it is treated with respect and dignity."
data engineer,"I am a motivated and fast-learning individual eager to dive into the fields of Data Science.  Although I‚Äôm early in my career, I am constantly developing my skills through online courses and hands-on practice. My main interests lie in harnessing data to drive decision-making, automating processes through software development, and building solutions that have a tangible impact.
Key Skills & Technologies:
Data Science: Exploratory Data Analysis (EDA), Data Preprocessing, Feature Engineering, Machine Learning (Logistic Regression, KNN, etc.)
Programming: Python, SQL, NoSQL, Git
Software Engineering: Object-Oriented Programming, Version Control, Basic Web Development (HTML, CSS, JavaScript)
Tools: Pandas, NumPy, Scikit-learn, Matplotlib, Jupyter, Visual Studio Code
I‚Äôm always excited to collaborate on new projects, expand my technical knowledge, and contribute to real-world challenges. If you‚Äôre looking for someone who is passionate about learning and capable of adapting to new challenges, feel free to connect!"
data engineer,"I have 7+ years of experience in the development of software and data projects.
I have strong experience in Data Solutions design and development (Data Lake, LakeHouse, Data Warehouse). including CI/CD for data pipelines and data cloud components. I have participated in the full life cycle of data project implementation and always followed references architectures from technology providers and best practices for SDLC.
Additionally, my background is not limited by data solutions. I have full-stack experience in .NET/Python, and I have an end-to-end vision of solution implementation focused on the achievement of a high-scalable, robust, and fault-tolerant solution.
I'm well familiar with the Infrastructure as a Code (Terraform) concept and used it in my previous projects, Terraform for automating environment setup and deployment of the new features.
I am always aiming to write clean and maintainable code. I pay attention to feedback from coworkers to make my code more efficient and do my job better. I am motivated to improve my programming skills and learn project-side technologies and tools. Also, I am interested in extending my knowledge of business processes related to the project.
I am self-organized and result-oriented, have good communication and analytical skills. Good in leading both technical and communication activities required to achieve customer satisfaction.
- Product MVP
- Grown multiple engineers in my team
- Production-ready solution
- Reduced tech debt
- Established SDLC
Looking for a Data Engineering role with technical leadership and product responsibilities."
data engineer,"At Kifiya Financial Technology, I work as a Data Engineer, primarily focusing on:
- Managing data pipelines for large-scale financial applications.
- Automating data cleaning and transformation processes to improve operational efficiency.
- Collaborating with cross-functional teams to enhance system performance and optimize code.
- Building and deploying machine learning resume_classifier for financial forecasting.
Areas for Improvement:
I am continuously seeking to improve in the following areas:
- Advanced Big Data Techniques: Expanding my expertise in handling even larger datasets and optimizing data processes for efficiency.
- Deep Learning: I plan to dive deeper into advanced machine learning and deep learning techniques, especially for unstructured data.
- Cloud-based DevOps: I aim to master cloud services like AWS to handle the scaling of machine learning resume_classifier in production environments.
I have successfully completed several key projects at Kifiya Financial Technology PLC and through my training at 10 Academy:
1. Data Engineering for Fintech:
- Developed data pipelines for financial data analysis, handling large datasets and ensuring data accuracy.
- Worked extensively with dbt for data transformation and DVC for data version control, which helped automate tech stacks and improve project scalability.
- Technologies: Python, SQL, dbt, DVC, Docker, GitHub, CI/CD, MySQL, PostgreSQL.
2. Machine Learning Engineering:
- Built predictive resume_classifier, specifically in the Fintech domain, focusing on time series forecasting and statistical modeling.
- Deployed machine learning resume_classifier using MLOps practices with MLFlow for tracking experiments and model management.
- Technologies: Python, TensorFlow, Scikit-learn, MLFlow, Streamlit for dashboards.
3. Web Development:
- Designed and developed web applications using React for frontend development and FastAPI for backend services, integrating them seamlessly for user-centric experiences.
- Deployed applications using Vercel and created interactive dashboards via Streamlit.
- Technologies: React, FastAPI, JavaScript, HTML/CSS, Streamlit, Vercel.
4. Leadership and Community Engagement:
- As the lead of the Google Developer Student Clubs (GDSC) at Wolaita Sodo University, I organized workshops on Python, JavaScript, and AI, and promoted collaborative tech learning on campus.
- Gained strong communication and leadership skills by mentoring peers and managing community events.
I seek a workplace where innovation is encouraged, and teamwork and collaboration are central to problem-solving. I thrive in environments that challenge my skills and foster professional growth, particularly in the domains of data engineering, data analysis machine learning, and full-stack development. However, I aim to avoid roles that involve repetitive tasks without opportunities for skill advancement or a clear direction for the team.
I‚Äôm excited to continue pushing the boundaries of my knowledge, contributing to impactful projects, and working with a passionate team."
data engineer,"Results-driven software and data engineer with extensive years of experience in the AI industry, Healthcare, E-commerce and Financial Services. Proven track record of delivering high-quality solutions and driving business outcomes. Skilled in developing cutting-edge AI applications, leveraging advanced data analytics techniques, and optimizing software development processes. Experienced in cloud technologies (AWS, GCP), DevOps practices (CI/CD, automation), and containerisation (Docker, Kubernetes). Strong collaborator with excellent problem-solving skills and a passion for staying current with emerging technologies and seeking opportunities to contribute to a dynamic team and drive innovation in a forward-thinking organisation."
data engineer,"Experience
Senior Data Warehouse Engineer | Zapier
(Nov 2021 ‚Äì Present)
- Own the data architecture for Growth, Revenue, and Finance domains, overseeing ELT pipelines.
- Led migration from Matillion to dbt, introducing version control and streamlined transformations.
- Drove the shift from Redshift to Databricks, boosting performance and cutting SQL Warehouse costs by 50%.
- Implemented robust alerting, testing, and documentation to ensure reliable, scalable analytics.
Software Engineer (Data Engineer) & Team Lead | DataRobot
(Dec 2018 ‚Äì Nov 2021)
- Led a team of 7 engineers delivering data pipelines in Customer Success and Sales domains.
- Migrated the data warehouse from Postgres to Snowflake, improving performance and scalability.
- Introduced dbt for pipeline versioning/modeling and deployed Stitch for self-service data ingestion.
- Automated workflows (churn modeling, OKR tracking, user management), driving data-informed decisions.
Data Scientist | Talkable
(Nov 2017 ‚Äì Dec 2018)
- Built an analytics environment from scratch, including data warehouse, ETL pipelines, and reporting.
- Architected data ingestion from multiple systems to power churn prediction, A/B testing, and fraud detection.
- Ensured data reliability, scalability, and governance for advanced analytics and BI dashboards.
Database Developer & Team Lead | Luxoft
(Oct 2013 ‚Äì Nov 2017)
- Owned ETL design, report development, and data analysis, driving key client deliverables.
- Oversaw end-to-end data pipeline architecture, stakeholder communication, and team mentorship.
- Promoted from Database Developer to Team Lead, managing a team of 4.
Skills & Tools
- Programming & Scripting: Python, SQL, R, Shell (Unix), PL/SQL
- Data Warehousing & Databases: Snowflake, Databricks, Redshift
- Transformation & Orchestration: dbt, Airflow, Matillion
- BI & Visualization: Looker, ChartIO, Tableau, Spotfire
–®—É–∫–∞—é –º—ñ—Å—Ü–µ —Ä–æ–±–æ—Ç–∏ –∑ –±—Ä–æ–Ω—é–≤–∞–Ω–Ω—è–º"
data engineer,"Big Data Engineer
‚Ä¢	DBT modelling for analytics workflow.
Creating DBT resume_classifier from clients on-prem Oracle stored procedures. The source of data was a AWS Redshift. After DBT creation, I implemented data visualization using AWS QuickSight.
Customer - Multinational Saas company.
Responsibilities:
-	Investigate business logic implemented by stored procedures in clients on-prem Oracle database
-	Recreate those procedures on AWS Redshift
-	Create a DBT resume_classifier for tables
-	Create a visualization in AWS QuickSight based on client request
Environment and Tools: Helm, Kustomize, Openshift Container Platform 4.x, Gitlab, Jira, Confluence, Argo CD, Private Container Registry, Red Hat SSO, WMvare vSphere Center,  Red Hat supporOracle, AWS Redshift, AWS DMS, AWS Quicksight, DBT.
Big Data Engineer
‚Ä¢	ETL processing for multiple sources. Creating ETL pipelines for multiple sources using medallion architecture(bronze, silver, gold) gesign pattern.
Customer - International education organization.
Responsibilities:
-	Investigate client data processing
-	Create and build data lake
-	Implement data pipelines using Databricks clusters
-	Orchestrate ETL using Airflow
-	Convert and optimize sql procedures to Spark code
-	Validation and testing data ingestions
Environment and Tools: Apache Spark, Airflow, Databricks, AWS Glue, AWS S3, Redshift, SQL Server, DinamoDB, Terraform, Azure DevOps.
Search, Cache Redis, RDS MySQL, SNS, SES, IAM, ECR, Route53, Certificate manager.
Big Data Engineer
‚Ä¢	Database migration from Teradata to Redshift. Conversin and applying specific Teradata sql to Redshift with further data migration.
Customer - Sweden based company.
Responsibilities:
-	Convert several Teradata scemas using AWS SCT
-	Manual convertion of scripts that was not converted with SCT
-	Applying converted scripts to Redshift, test functions
Environment and Tools: Teradata, Redshift, SCT, Apache Airflow, FlyWay.
Big Data Engineer
‚Ä¢	Database migration with performance testing. Migration of Oracle database to Snowflake with data transformation for better performance.
Responsibilities:
-	Migration big amount of data
-	Data transformation
-	Moving Snowflake cost and performance metrics to CloudWatch with further analytics
Environment and Tools: Python, Oracle, S3, Snowflake, Lambda, CloudWatch, Kinesis.
Big Data Engineer
‚Ä¢	Database replication with further data transformation. Replication of big amount of data (near 15 TB) from on-prime MS SQL DB to Snowflake.
‚Ä¢	More than 7 years of experience in IT
‚Ä¢	Skilled in web and mobile applications development using Python , Java and Scala
‚Ä¢	Skilled in data processing projects mostly doing data preparation and ETL with Python in AWS cloud
‚Ä¢	Experience as full-stack data engineer, implementing end-to-end pipelines from input data scrapping to data consumption layer
‚Ä¢	Fast leaner, good team player, have excellent communication skills, love to explore new technologies and business domains
‚Ä¢	Design and Implementation: Designed and implemented scalable data pipelines using AWS services such as Glue, Redshift, S3, and Lambda.
‚Ä¢	Data Transformation: Performed data transformation and ETL processes using AWS Glue and PySpark to ensure data quality and consistency.
‚Ä¢	Data Storage: Managed and optimized data storage solutions using AWS S3, Redshift, and RDS for efficient data retrieval and analysis.
‚Ä¢	Data Migration: Conducted data migration projects from on-premises to AWS cloud environments, ensuring minimal downtime and data integrity.
‚Ä¢	Streaming Data Processing: Implemented real-time data processing solutions using AWS Kinesis and Apache Kafka to handle high-velocity data streams.
‚Ä¢	Data Lake Architecture: Designed and built data lake architectures on AWS to support large-scale data ingestion, storage, and analytics.
‚Ä¢	Security and Compliance: Ensured data security and compliance with industry standards by implementing AWS IAM, KMS, and encryption techniques.
‚Ä¢	Performance Optimization: Optimized query performance and reduced data processing times using AWS Redshift Spectrum and partitioning strategies.
‚Ä¢	Monitoring and Logging: Implemented monitoring and logging solutions using AWS CloudWatch and CloudTrail to track data pipeline performance and troubleshoot issues.
‚Ä¢	Collaboration and Documentation: Collaborated with data scientists, analysts, and stakeholders to gather requirements and document data engineering processes."
data engineer,"Business Intelligence Team Lead
Creation of BI infrastructure from scratch
DWH creation,
OLAP cubes and Reporting
Analysis and Management
(DWH, ETL, SSIS, SSAS Tabular and Multidimensional, Power BI Report Server/Services)
DWH creation:
‚Ä¢ development DWH;
‚Ä¢ development ETL processes strategy (SSIS);
‚Ä¢ administration of DWH;
‚Ä¢ writing queries to get data from DWH. Their optimization
OLAP & Reporting:
‚Ä¢ design and development of SSAS Tabular and Multidimensional OLAP cubes;
‚Ä¢ administration of cube:
‚Ä¢ writing and optimizing queries to get data from the cube (DAX queries);
‚Ä¢ reporting development using Power BI (Power BI RS, Power BI services)
Skils & Tools:
T-SQL, PL/SQL;
DAX language (inc DAX queries);
Power Query (M language);
MDX;
SSMS, PL/SQL Developer, SSAS, SSIS;
Report Builder, Power BI services / Power BI RS
Data Engineer
BI Developer
Data Analyst"
data engineer,"I have experience implementing and maintaining Data Warehouse, designing various data pipelines (batch and streams), integrations and ETL`s, transforming and preparing data for reports. Was involved in data modelling tasks using  different patterns. Mostly experienced in Azure Cloud services. Worked with many kind of Data related cloud solutions like DataFactory, Databricks, Azure Storages.
Experienced with SQL and SQL-databases, Python, Spark.
Was involved in different kind of Data Integrations tasks and sometimes in data analysis - dealing with Power BI and reports generating.
- Integrated different data sources with Data Warehouse - from Databases and simple FIles to complex CRM`s like Salesforce and NetSuite.
- Designed and implemented streaming solutions for DWH to migrate to from batching.
- Created and maintained data quality solution for detection anomalies in Warehouse.
Looking for a place where I can develop and sharpen my data engineering skills, improve my technology expertise to deliver high-quality solutions."
data engineer,"- Have 10 months experience as a Data Analyst/Data engineer in startup
- Have 1.2 y experience as a Data analyst in ecommerce project
- Know SQL, can build ad-hoc queries
- Work with Python on a high level
- Know pandas, pySpark
- Work with Databricks
- Created more than 5 custom report for Google spreadsheets and Google Merchant
- PowerBI/Looker Studio report creation
- 7 years in PPC field optimizing Google ads/Facebook/TikTok ads campaigns, know how to launch profitable campaigns through all these platforms. I have understanding how to optimize campaigns
- Freely can speak on english language daily bases
- GTM - I know how to setup goals and ecommerce via it
- GA4 understand about goal creation, attribution resume_classifier etc
- Measurement protocol offline analytics setup
- A/B Testing"
data engineer,"JAVA DEVELOPER
Bookmap
03.02.2023 - 23.04.2023
Developing an addon for the Bookmap application
Working with Git
Daily meetings with the senior developer.
Lead Generator
Hallwil
Oct 2023 - Jan 2024
Communication and English
DATA ENGINEER
EPAM
Feb 2024 - Present
My role involves data streaming with Kafka, Spark, and Structured Streaming. I'm proficient in cloud platforms like GCP and Azure, focusing on Python, SQL, and Scala within Azure and Databricks. I conduct code reviews, fix bugs, and collaborate with teams to meet project goals, maintaining clear communication with stakeholders. I've worked on diverse projects across various domains, employing technologies such as SQL, Python, Scala, Java, Apache Spark, Kafka, Hive, Hadoop, Kubernetes, Docker, and Git, with additional experience in GitHub CI/CD and Amazon Athena.
Certificates:
- DP-203 + DP-900
- Databricks Lakehouse Fundamentals
- dbt Fundamentals
- Apache Spark Programming with Databricks
____________________________________________________________
I have experience as a Data Software Engineer specializing in Big Data, with skills in Python, SQL, PySpark, and Databricks. I have developed solutions for data analytics and commercial applications, efficiently handling data transformations and pipelines using Azure and Snowflake. I am proficient in optimizing dbt resume_classifier and implementing Python scripts. My practical experience includes working with cloud platforms such as GCP and Azure, and tools like Spark, Kafka, and GitHub CI/CD. My software development experience also includes roles as a Junior Developer, where I focused on creating dbt macros and optimizing SQL queries. I am known for my problem-solving abilities, effective communication, and teamwork, ensuring project goals are met efficiently."
data engineer,"Senior Data Engineer with experience in digital transformation, migration, and data analytics within the financial and IT consulting sectors. Expert in collaborating with business teams and stakeholders to identify and implement operational improvements through data and IT solutions.
Technical Skills:
- Cloud Platforms: Google Cloud Platform (Certified Professional Data Engineer), Microsoft Azure (Certified Data Engineer).
- Data / Software Engineering: Data warehousing, ETL/ELT, data pipelines, data modeling, SQL, Python, Spark, Pyspark, Kafka and software development lifecycle.
- Data Analytics: Data exploration, statistical analysis, data visualization.
- Project Management: Agile methodologies.
Led development teams on data projects, providing support in programming (Python, SQL, and PySpark) and business topics to foster continuous improvement for both the company and team members.
- Designed and implemented data monitoring and observability processes that decreased failures and reduced the time between issue detection and resolution in data products.
- Designed and implemented ETL processes to streamline data integration and enhance data accessibility.
I want to work on impactful projects that drive operational improvements, optimize processes, and support decision-making. I enjoy collaborating with teams to develop data solutions that meet business needs and mentoring others to foster growth and excellence.
I prefer not to work on Java development projects, except for occasional or  tasks related to Spark with Scala. My focus is on utilizing tools and technologies that align with my strengths and the project‚Äôs data-centric goals."
data engineer,"As a Data Engineer with a strong background as a Python Developer, I have built and optimized ETL data pipelines using tools like Airflow and AWS Glue, as well as processed big data with Apache Spark and Amazon Athena. I have extensive experience working with databases such as PostgreSQL, Redshift, and MySQL. My expertise in back-end development includes building robust APIs with Flask, automating workflows, and developing chatbot systems using Aiogram. Additionally, I‚Äôve written extensive unit tests to ensure the reliability of my solutions. I focus on delivering efficient and scalable data solutions.
As a skilled Python developer, I have a diverse range of expertise that can be leveraged in different types of projects.
Opportunities for career growth and self-development;
- A lot of learning in the proficient team;
- Competitive compensation package;
- Supportive and friendly team of professional"
data engineer,"Proven ability to gather and translate complex business requirements into efficient, scalable, and
high-performance data resume_classifier;
Strong collaboration with BI teams, Data Analysts, Data Platform Engineers, and business users to align
data solutions with business needs;
Successfully managed tight deadlines and customer escalations, leading teams of data engineers and
platform engineers to resolve critical issues;
Ability to handle end-to-end data engineering workflows, from data ingestion, cleansing, transformation,
and loading to ad hoc analytics, reporting, and dashboard creation;
Strong focus on data quality, pipeline reliability, efficiency, monitoring;
Experience working with teams of various sizes, ranging from 3 to 20 members, across different roles and
responsibilities;
Expertise in Data Warehousing, Data Lakehouses, and Medallion Architecture;"
data engineer,"Big Data Project with (Apache beam,Apache Airflow, Apache Spark).
Good knowledge Google Cloud Platform (Dataflow, Bigquery, Dataproc, Bigtable etc..).
Good knowledge PySpark
‚Ä¢	Exceptional background in analysis, design, development, customization, and implementation and testing of software applications and products.
‚Ä¢	Demonstrated expertise utilizing ETL tools, including SQL Server Integration Services (SSIS), SQL Server Annalise Services (SSAS), SQL Server Reporting Services (SSRS), Data Transformation Services (DTS), and DataStage and ETL package design, and RDBM systems like SQL Servers, Oracle.
‚Ä¢	Strong understanding of data warehousing concepts, OLTP, OLAP, Normalization, Star and Snow Flake data resume_classifier.
‚Ä¢	Create OLAP Cubs.
‚Ä¢	Strong leader with experience training developers and advising technical groups on ETL best practices.
‚Ä¢	Excellent technical and analytical skills with clear understanding of desig
Worked on all phases of data warehouse development life-cycle, from gathering requirements to testing, implementation, and support.
I would like to Relocate and live in the EU."
data engineer,"‚Ä¢ Development and Optimization of ETL Processes: Designing and implementing ETL processes for the risk
management department using Apache Airflow 2, SSIS, and Apache NiFi. Improving the quality and timeliness of
data for scoring model calculations, minimizing data loss, and implementing ETL process monitoring.
‚Ä¢ Microservices Development with FastAPI: Participating in the design and development of microservices for the
credit pipeline using FastAPI. Setting up data flows for credit applications via REST API.
‚Ä¢ Working with Relational Databases: Managing databases for efficient data storage and processing, including
schema, table, and index design. Migrating all existing data from MSSQL to PostgreSQL, ensuring data transfer
without loss. Optimizing database structure (tables, indexes) and SQL queries, which improved system
performance and reduced response times.
‚Ä¢ Building Data Warehouses: Developing data warehouses for analysts and risk modelers, ensuring easy access to
data to support decision-making.
‚Ä¢ Technical Stack: Python, FastAPI, Apache Airflow, PostgreSQL, MSSQL, Grafana, Apache NiFi,
SSIS, Kafka
‚Ä¢ Development, optimization, and monitoring of ETL/ELT processes: Managing ETL processes on the Airflow
platform (versions 1 and 2). Migrating old processes from Airflow 1 to Airflow 2 while optimizing DAGs, which
significantly reduced data processing time. Developing new pipelines.
‚Ä¢ Development and implementation of microservices: Creating and integrating microservices using FastAPI to
improve internal data exchange between various company services.
‚Ä¢ Working with SQL and NoSQL databases: Designing and supporting solutions based on PostgreSQL, MySQL, and
MongoDB for efficient data storage and processing.
‚Ä¢ Cloud technologies: Managing cloud infrastructure on AWS (S3, Athena) and GCP (BigQuery) for storing and
processing large volumes of data.
‚Ä¢ Integration of third-party APIs and Data Parsing: Developing integrations with external APIs (Google APIs,
Yandex APIs) to automate data exchange with external services. Developing scripts to collect data from various
web resources and processing it for business needs.
‚Ä¢ Business process automation: Automating repetitive tasks and processes to improve the company‚Äôs operational
efficiency. Creating a service for managing billboard ad placements, which improved the efficiency o"
data engineer,"Develop tailored Power BI dashboards, empowering stakeholders with insightful analytics to enhance data-driven decision-making for optimized financial strategies.
Analyze customer feedback data, identifying trends and areas for improvement in operational processes and service delivery, ultimately enhancing overall customer satisfaction and loyalty.
Implement comprehensive data quality analytics to maintain accuracy, consistency, and adherence to predefined standards, ensuring reliable and trustworthy data for decision-making across the organization.
Create self-service reports using SSRS, empowering departments to generate ad-hoc reports
independently.
Establish a comprehensive Data Cleansing and Data Enrichment program using Python and SQL stored procedures, ensuring data accuracy and completeness.
Implemented seamless data migration from a legacy system to a newly developed system, utilizing
SSIS for efficient ELT/ETL processes, resulting in minimal disruption to operations and successful
consolidation of information across systems.
Conduct data literacy training sessions, promoting data-driven decision-making across the organization.
Perform data reconciliation processes to ensure data accuracy and consistency for reliable decision-making.
Collaborated with the development team to enforce data governance and standardization.
Automate key operational processes using Excel, python and SSRS, significantly reducing turnaround time for customer requests."
data engineer,"Health Care, Wellness, Accounting
Database Design , ETL, Data Visualization, Tableau,Postgres"
data engineer,"I have worked on several important projects, where I was involved in designing databases, automating data import and export processes using Python and PostgreSQL, as well as cleaning and transforming raw data. I collaborated with system analysts to ensure database alignment with architectural standards. Later, while working with Azure cloud technologies, I configured resources (Azure SQL Database, Azure Data Lake Storage Gen2, Azure Key Vault, Azure Data Factory) to automate data loading and processing, while also ensuring the security of integrations. Additionally, I optimized databases, automated ETL processes, and created analytical reports in Power BI for business analysis. My role included monitoring performance and ensuring high availability of databases. In the future, I plan to develop my skills in big data, machine learning, and deepen my knowledge of cloud platforms such as Azure and AWS.
Designed and implemented database schemas to optimize data storage and processing, improving productivity and reducing processing costs.
Automated data import/export processes using Python and PostgreSQL, significantly reducing processing time and errors.
Automated ETL processes, increasing accuracy and speed of data processing for business analysis.
Created interactive reports and dashboards in Power BI, providing real-time insights for decision-making.
Earned Apache Airflow certification, demonstrating expertise in automating data workflows.
Successfully balanced work and studies, gaining additional theoretical knowledge and practical skills without impacting work productivity.
I am eager to develop my skills in big data, machine learning, and cloud platforms like Azure and AWS. I aim to work on more complex projects involving real-time data processing and advanced analytics, while deepening my expertise in tools like Apache Airflow. My goal is to take on more leadership responsibilities and contribute to innovative data-driven solutions"
data engineer,"I'm Data Engineer with experience indeveloping and optimizing data processing processes. Istrive to create effective systems for collecting, storing andanalyzing large amounts of data in order to providebusiness with valuable information for decision making
Optimization of data processing
Improving the performance of the ETL process
Creating dashboards to track performance
Optimization of SQL queries
It is important for me to avoid such jobs where there is no opportunity to use advanced technologies and develop new solutions to optimize data processing. My goal is to develop and apply best practices in the field of data processing and analysis, and I am looking for opportunities where this goal will be achieved."
data engineer,"1) Junior Data Engineer at FUIB
04/2024 - 04/2024
2) Junior Data Analyst at Philip Morris Ukraine
09/2023 - 11/2023
3) Python programming assistant at Ukrainian Catholic University
09/2022 - 06/2023
4) Cooperation with the company LaStrava, regarding the self-service store
( Created an autonomous store for the LaStrava company, which produces restaurant-grade ready meals. Gathered data. Defined the target audience. –°reated hypotheses and working with them. Financial model: cost analysis for two formats, Profit&Loss, Break- even analysis )
02/2023 - 06/2023
Projects:
1) LaStrava (02/2023 - 06/2023)
Created an autonomous store for the LaStrava company, which produces restaurant-grade ready meals. Gathered data. Defined the target audience. –°reated hypotheses and working with them. Financial model: cost analysis for two formats, Profit&Loss, Break- even analysis.
2) Price Predictor For Used Cars (03/2023 - 06/2023)
Conducted an analysis of the used car market, created a multiple linear regression model. Then, after
identifying the main factors affecting the price, created a model that can predict the price of a car
3) Face Recognition (03/2023 - 05/2023)
We developed a program that identifies a person's face based on the provided dataset
4) Smart Float For Fishing (01/2020 - 05/2021)
Created a fishing device that help people with visual and hearing impairments to fish, by making remote notifications with vibration and color change during biting with a significant signal transmission distance.
Implemented a float that assesses the current condition and a pocket device that will receive these signals.
5) Check The Integrity Of The Field Of View (01/2021 - 05/2021)
Implemented a game with an interesting script for checking the integrity of the visual field using Pygame. During the game, gradation is performed and the field of vision is divided into good visibility and dead
zones.
Achievements:
1) Took second place at the All-Ukrainian Competition J.A.S, section Multimedia systems, and Software (2021)
2) Took second place at the All-Ukrainian Competition J.A.S, section Electronics, and
Instrument-making (2021)
As a data enthusiast student, I possess a strong foundation in statistical analysis, programming, forecasting and
data visualization. Eager to expand my knowledge and contribute to the team's success in
accomplishing company objectives."
data engineer,"Worked as a PHP developer on freelance.
1 year of experience in web scraping, in office, using Web Query Language. Experience in Jira and SVN.
Last 5 years work with python, 2 of them as a GCP Data engineer in data lake.
Certified Google data engineer.
Upgraded project from python 2.* to python 3.*
Experience in scraping and parsing with selenium, beautifulSoup, lxml
Created GUI with pyqt
Created new and updated existing data pipelines with dataflow, orchestrate a jobs with airflow and other data lake related job."
data engineer,"In my role as Head of Data Engineering at a leading fintech firm, I have driven several transformative projects that underpin our data-driven strategies and innovation. I oversee a dynamic team responsible for designing, developing, and maintaining our scalable data infrastructure, ensuring that our systems efficiently handle gigabytes of transactional data daily.
Key Projects and Initiatives:
Real-Time Data Pipelines & Streaming Analytics:
Spearheaded the design and implementation of robust, real-time data pipelines utilizing Apache Kafka and Apache Spark. This project enabled seamless ingestion and processing of streaming data from various sources, crucial for real-time fraud detection and risk management.
Cloud Migration and Data Warehousing:
Led the migration of our legacy systems to a cloud-native architecture on AWS. By leveraging services such as Amazon S3, Redshift, and EMR, I improved system scalability, reliability, and performance while reducing operational costs. This transition laid the foundation for a modern data lake that supports both structured and unstructured data.
Machine Learning Infrastructure:
Collaborated closely with data science and ML engineering teams to build end-to-end machine learning pipelines. I ensured that resume_classifier‚Äîfrom training to production deployment‚Äîare seamlessly integrated into our data ecosystem. Technologies such as TensorFlow, Scikit-learn, and PyTorch have been pivotal in enhancing our predictive analytics capabilities.
Data Quality and Governance:
Implemented rigorous data quality checks and governance frameworks to maintain data integrity, ensure regulatory compliance, and support business intelligence initiatives. This includes deploying automated data validation and anomaly detection systems."
data engineer,"More than 6 years of experience in providing reliable and meaningful data solutions for companies from different sectors, including government agencies. It helped to reduce the time to get the expected product by development and automation. My experience spans IT transformation, data integration, distributed storage systems, DevOps practices, cloud solutions, and agile practices."
data engineer,"About 25 years of experience in software development and systems design.
Typical responsibilities: system analysis, coding, troubleshooting, and team management. I have assisted in implementing and sustaining 4 long-term software projects for clients in different businesses, including Healthcare, Energy, and Manufacturing.
Results-driven, logical, and systematic approach to achieving tasks and objectives; determined and decisive; uses initiative to develop effective solutions to problems; responsible and dependable - high personal standards and attention to detail.
The main experience (22 years) I obtained during work on a big software project - Medical clinical and financial system for Hospice / Home Health agencies for the US Company. This feature-rich product addresses the business and clinical needs of home health, hospice & private duty providers in one comprehensive, integrated software solution. Developed for a Windows/Web-based and Mobile/Tablet environment using scalable three-tier client-server architecture, Internet-enabled, and offers encrypted data transmissions and advanced system security. Other features include a customizable desktop, drill-down reporting, audit trails, visual assessment & billing format template design, generation, parsing, EDI, HL7, interoperability with external systems, and a wide range of reporting capabilities.
My different positions/roles include Software Engineer, Tech Lead, Sustaining Analyst, and Sustaining Team Lead.
Completed multiple tasks in the following areas:
‚Ä¢ business/requirements analysis;
‚Ä¢ creating functional/technical specs;
‚Ä¢ system design, coding, and code review;
‚Ä¢ configuration management, DB administration;
‚Ä¢ clients‚Äô environments sustaining analysis and troubleshooting (L3);
‚Ä¢ developing, training, coaching, and mentoring others.
The main used technologies/tools: Microsoft SQL Server, PostgreSQL, DBeaver, MS Visual Studio/Code, PyCharm, GodeGear/Embarcadero Delphi, TurboPower Flash Filer 2, InstallShield, EDI (HL7, ASC X12 837/835/277/997/999 4010/5010).
As a technical leader and software engineer (Delphi, C#, InstallShield):
‚Ä¢ designed and implemented several big modules within the medical application including the database part, e.g. claim printing, billing format designer, electronic claims, electronic remittance advice, interactive/silent installation program for the server and client parts of the product including DB upgrade by SQL scripts for a bunch of releases with automatic target DB version processing;
‚Ä¢ worked on the releases and patches preparation;
‚Ä¢ collected and reviewed DB upgrade scripts in SQL from developers to incorporate them into releases.
As a data engineer (MS SQL, PostgreSQL, Oracle, Python):
‚Ä¢ designed and implemented DB schema for the OLTP/OLAP relational databases;
‚Ä¢ created various SQL stored procedures, functions, views, triggers, indexes, external stored procedures, and data update scripts for MS SQL Server, PostgreSQL, and Oracle;
‚Ä¢ completed tasks of DB performance optimization based on the SQL Profiler tool and query execution plan analysis;
‚Ä¢ investigated and fixed many different data and schema issues, referential integrity, duplicated/missing records, and erroneous data in the client databases using own elaborated custom and universal parametric SQL scripts.
As a sustaining analyst (Delphi, C#, ASP.NET, MS SQL, SalesForce, SecureLink):
‚Ä¢ investigated and resolved problems directly inside the clients‚Äô environments and databases;
‚Ä¢ performed clients‚Äô environment build upgrades, detected, documented, and fixed various user issues as well as optimized performance in SQL queries and application modules implemented in different software technologies;
‚Ä¢ supervised the team of 3-4 sustaining analysts including metrics collection and status reports.
- Ability.
- Agility.
- Advantage."
data engineer,"As AWS Certified Data Engineer, I was responsible for the following tasks:
- Managed and optimized MySQL (on-prem) and SingleStore (AWS) databases, including schema changes via Liquibase and query tuning.
- Migrated SingleStore databases and tools to AWS EC2.
- Maintained ETL pipelines to ensure smooth data ingestion and transformation (Kafka, Airflow).
- Upgraded MySQL version in a complex monolithic architecture.
- Developed tools for db management using Python and Scala.
AWS Certified Data Engineer - Associate
An innovative company with a friendly team and opportunities for continuous professional growth."
data engineer,"Currently I am working as a senior data engineer to create an analytical data warehouse based on Snowflake using dbt as ETL tool.
Responsibilities: creating resume_classifier in dbt, query optimization, code review, performance monitoring, mentoring.
In my portfolio I have such projects/experience as:
I.
Data Architect/Team Lead at [%COMPANYNAME%]
I had experience as Data Architect of analytical system
and as lead in the DB team.
The project was based on Vertica platform to provide
reports and analytics for our customers. My
responsibilities are data model creation, creation of the
SQL modules with business logic, code review, performance tuning and communications with users and other teams in the project.
II.
Senior Data Warehouse Developer at [%COMPANYNAME%]
Development of the analytical and reporting system for
mobile operators. Database design, queries
performance tuning, implementation of the life cycle of
the Data Warehouse, development of the dimensions
and facts refreshing, development of the business
reports. Development of ETL processes with help of
internal project tools. Resolving Vertica performance
issues, conguration and adjustments in terms of
performance and concurrency.
III.
Data Warehouse Architect at [%COMPANYNAME%]
Design, development, implementation and maintenance
of Data Warehouse and BI system for retail company
based on SQL Server 2008-2014 (over then 300
business users and over 5 TB size of database).
Development ETL based on SSIS from different data
sources. Development and maintenance
multidimensional cubes with SSAS. Development
analytical reports with SSRS. Optimization and
performance tuning DW processes, integration with
other systems
IV.
Sql Server expert at freelance:
Development ETL processes, stored procedures, user
dened functions, CLR functions by C#, etc. Analyzing
and tuning stored procedures/queries, resolve
lock/deadlock issues for OLTP/DataWarehouse systems.
Administration and maintenance SQL Servers.
I am a certified Snowflake specialist:
SnowPro Advanced: Data Engineer
SnowPro Advanced: Architect
AWS Certified Cloud Practitioner
It would be interesting to work on projects as a Data Engineer/Data Architect. Preferably working with Snowflake and dbt but I am always open to new opportunities and challenges."
data engineer,"I've worked as a data engineer for 1.5 years.
Developed ETLs using Azure Data Factory and worked with  DWH.
Have experience in PySpark and a little bit of experience with AWS Glue.
Also have experience with training ML resume_classifier and using them for prediction.
- Azure DP-203 certified
- C1 Cambridge English Certificate"
data engineer,"Power BI and Data Engineer Intern
Reenbit (3 months)
Designed and maintained data pipelines in Azure Data Factory (ADF), ensuring reliable data flow for business operations.
Performed data transformation and integration using SQL, streamlining data processes and improving data accessibility.
Created Power BI dashboards to visualize key metrics, enabling stakeholders to make data-driven decisions.
Conducted data analysis to extract insights and presented actionable recommendations to the team.
To contribute to a data-driven organization by applying my skills in data engineering and analytics.
To work on complex data pipelines and improve ETL processes, helping businesses make informed decisions.
To continuously develop my expertise in cloud-based data solutions like Azure and expand my knowledge in big data technologies.
To join a supportive and innovative team where I can grow professionally and contribute meaningfully to projects."
data engineer,"Data Analyst with over 9 years of experience in IT industry, worked for 4 years in software development, 2 of which in support. Strong soft skills, team player, Data Jedi. As a developer with projects mainly in Drop Wizard/Micronaut, with proper CI/CD through Spinnaker and Jenkins. Company was focused on data processing, so mostly different kinds of parsers, ETL's, streaming apps (Flink + Kafka), Hibernate. Very Descent SQL skill, strong knowledge of Tableau.
- Volunteered to learn data visualization tool (Tableau) as data engineer and developed whole bunch of workbooks which simplifies all department work and were used for years and years.
- As a first Java project developed in-house solution for task management and internal tools work analyzer. Data from those two areas combined in half a year allowed to refine business processes and automate most time-consuming tasks without spending much resources which lead to product delivery in one week instead of 3 in two departments.
- During work in support development team played a key role in creating documentation, refactoring and automation of legacy components which led to decreased needs in support by 70-80%, which led to support team elimination and all support team members become core developing team members."
data engineer,"As a Data Analyst with experience in data analysis ,I recently completed a Data Engineering course to pivot into this field. I‚Äôm eager to apply my skills in building data pipelines, optimizing workflows, and supporting data-driven decision-making.I am familiar with payment systems.PSP,FinTech."
data engineer,"- Development and maintenance of analytical solutions using Power BI, Consulting business units on the use of BI tools, Creation of simple pipelines in Microsoft Fabric, Searching for and fixing issues in connections of various connectors, optimizing queries, and refining DAX formulas.
- Reports for Marketing, Customer support, Customer Success departments (Marketing funnels, Sales funnels, different conversion rates, retention, LTV etc.) Product researching for different hypothesis.
- Python Scripts for automatic creation report in google sheets, Google Data Studio, Power BI.  Import and export data to/from different sources (MySQL, Google Big Query etc). ETL data from different API (Google Analytics, Google Ads,  Amplitude, Stripe, Slack, Helpscout, Canny).
- Using Selenium for scraping data from WEB pages
- Familiar with Machine Learning ( Regression, Random Forest, Decision trees, SVM, k-mean,  Neural networks)
-  SQL queries to DWH,  SQL Server Integration Services (SSIS)
- Dashboards and other visualizations in Power BI
- Orchestration in AirFlow
- Finished It-school with final project - Text multiclass classificator by Neural network. (Definition of Text genre)
- Took part in 4 competitions  on the kaggle
- Basic of programing on R - Certificate with honors
- Data analysis with R - Certificate with honors.
- Basic of statistic
Difficult and therefore interesting tasks, Work-life balance"
data engineer,"Data Engineer with hands-on experience in data processing, analytics, and backend development. Passionate about growing in Data Engineering, currently exploring ETL pipelines, data processing, and cloud technologies.
What I‚Äôm Looking For:
Opportunities to learn and grow: I want hands-on experience with data technologies, mentorship from senior engineers, and room to sharpen my skills.
Collaborative environment: I thrive in teams that value open communication, knowledge sharing, and mutual support.
Impactful work: I appreciate projects that have a tangible effect on business outcomes or customer experience.
Continuous challenges: I enjoy tackling new problems, optimizing workflows, and constantly refining processes."
data engineer,"**Big Data Software Engineer** with 5 years of experience in developing and optimizing scalable Big Data solutions. Proficient in leveraging technologies such as HDFS, PySpark, Hive, Apache Airflow, and cloud platforms like AWS and GCP. Skilled in backend development, specializing in microservice architectures, with a proven track record of enhancing the performance of APIs and databases.
Experienced in designing and deploying Big Data architectures, primarily on AWS, while also having hands-on expertise with GCP. Adept at producing comprehensive technical documentation, including software requirements specifications, technical designs, and implementation notes to support project features and deliverables.
- Designed architecture for the Big Data project that was successfully deployed and working on production, that helps clients to explore telemetric data and they can see possible ways on how to improve performance of some specific part of work to spend less money and at the same time earn more
- Improved performance of SQL queries execution on 30%
- Implemented a new async service to enrich information about entities that were added to the production
- Implemented a script to transfer specific data from one environment to another to avoid errors that can occur due to constraints or triggers on DB. This allows us to quickly start working with the system in a new environment, with the data that was set up in the old environment"
data engineer,"Web scraping Specialist (Python)
Competitoor
12/2024 ‚Äì present
Remote
‚Ä¢ Developing advanced web crawlers using Python (Scrapy, Playwright, Beautiful
Soup).
‚Ä¢ Bypassing anti-bot protections (e.g., CAPTCHA, IP blocking, fingerprinting
techniques).
‚Ä¢ Optimizing large-scale web scraping infrastructure for competitive price analysis
in e-commerce.
‚Ä¢ Managing and improving distributed crawling architectures to handle millions of
requests daily.
‚Ä¢ Implementing headless browsing techniques for dynamic content extraction.
Software Developer
HST Consultant PLC
12/2023 ‚Äì 12/2024
Addis Ababa, Ethiopia
‚Ä¢Integral member of a dynamic team, spearheading innovative software solutions.
‚Ä¢Expertise in Django, FastApi, React, backend development, advanced database solutions (PostgreSQL, MySQL), and API integration.
‚Ä¢Utilized Docker to containerize applications, ensuring consistent development environments and scalable deployment.
‚Ä¢Improved CI/CD pipelines using Docker, optimizing workflow and deployment time.
‚Ä¢Developed and deployed a comprehensive payroll management system (HST-Payroll) as a Software-as-a-Service (SaaS) solution.
Automation with AI and Software Developer
Freelancer,
11/2021 ‚Äì present
‚Ä¢Successfully completed diverse projects on Upwork, including complex Django applications, web scraping for popular e-commerce sites, and generative AI solutions using the OpenAI API.
‚Ä¢Experienced in database management and APIbased development.
‚Ä¢Known for crafting engaging chatbots and optimizing user experiences.
‚Ä¢Fostered long-term client relationships, collaborating directly with clients outside Upwork for ongoing projects.
‚Ä¢Scraped many e-commerce sites with antiscraping
‚Ä¢Delivered Dockerized solutions for freelance clients to streamline deployment and improve scalability
AI Mastery Trainee (With Distinction)
Kifiya AI Mastery Training Program
08/2024 ‚Äì 11/2024
‚Ä¢Successfully completed 12 weeks of project-based training in Machine Learning Engineering, Data Engineering, Data Analysis and Financial Analysis.
Addis Ababa, Ethiopia
‚Ä¢Designed fintech solutions tailored for the Ethiopian market, focusing on scalability and efficiency.
‚Ä¢Gained advanced knowledge in Python-based AI workflows, model optimization, and fintech data pipelines.
Accomplishments
- Built Scalable SaaS Solutions: Developed HST-Payroll, a multi-tenant payroll management system with subdomain-based logins.
- Web Scraping Expertise: Successfully scraped data from major e-commerce sites (e.g., Armani, Prada, Tiffany) with anti-scraping measures.
- Generative AI Projects: Created engaging chatbots and AI-powered solutions using OpenAI APIs, including Telegram integrations.
- Video Engagement Platform: Developed a rewards platform with a Telegram bot, enabling video-based points and prize draws.
- CI/CD Optimization: Improved deployment pipelines with Docker and GitHub Actions, ensuring faster and reliable workflows.
- Fintech Development: Designed scalable fintech solutions during the Kifiya AI Mastery Program, earning a distinction.
What I Want:
- Collaborative environment fostering creativity and innovation.
- Opportunities for continuous learning and growth.
- Clear communication and supportive teamwork.
- Challenging projects driving excellence and recognition.
What I Don't Want:
- Micromanagement or rigid hierarchies.
- Stagnant or toxic work culture.
- Projects lacking clear direction.
- Limited opportunities for advancement."
data engineer,"Data Engineer with expertise in SQL, DBT, ETL/ELT, AWS Athena,
Airbyte, Azure Data Factory, Azure
Synapse, Azure blob storage, Office
365, Python, Power BI
- Design, develop, and maintain scalable ETL/ELT pipelines
- Collect, clean, and integrate data from multiple sources
- Update and maintain databases
- Perform data cleaning and preprocessing
- Implement data transformations for analytics and reporting
I have an upper-intermediate level of English."
data engineer,"Worked on data warehouse in fairly large company, mostly in hadoop ecosystem. Built ETL pipelines using spark and airflow. Used python programing language."
data engineer,"Data Engineer with expertise in SQL, Python, dbt, Airflow, GCP, Databricks, Snowflake. Built ETL pipelines, optimized DWH, automated CI/CD. Seeking to deepen ML integration and enhance real-time data processing.
Optimized ETL Pipelines: Reduced data processing time by 40% by refactoring SQL and Python jobs in Databricks and GCP (Dataproc, Dataflow).
Data Warehouse Performance Tuning: Improved query performance by 60% through indexing, partitioning, and materialized views.
Automated CI/CD for Data Pipelines: Implemented CI/CD with Terraform and GitHub Actions, reducing deployment time by 50%.
Implemented dbt for Analytics: Standardized transformation logic, improving maintainability and reducing ad-hoc queries.
Secure SFTP to S3 Migration: Automated data ingestion from SFTP to S3, integrating AWS Secrets Manager for credentials.
Cross-Platform SQL Compatibility: Developed macros to unify Snowflake and Databricks syntax in dbt.
Real-time Data Processing: Designed a streaming pipeline with Pub/Sub and BigQuery, cutting report latency from hours to minutes.
Infrastructure Cost Reduction: Optimized cloud resources, cutting GCP costs by 30% without sacrificing performance.
Automated Report Generation: Built Power BI and Looker dashboards with scheduled data refresh and alerting."
data engineer,"2016 ‚Äì 2017                 Mindy supports
Graphical assistant
Responsible for editing and labeling various objects on photos
Checking the correctness of tasks performed by other assistant and correcting mistakes
Providing feedback to team leads to eliminate similar mistakes in the future
2018 - 2021 		      Ring Ukraine
Data Operator
Responsible for labeling different types of objects on videos and setting their parameters
Checking markup quality of other operators
Mentoring new operators
2021 - 2023 		      Wix
Data Labeling Specialist
Labeling site content to make better layout for mobile version
Labeling Text from customer requests to optimize feedback from CC team
Making classification of site types and categories"
data engineer,"‚Ä¢ Project: Data Provisioning Layer
Participation in projects for the implementation of Medallion DWH using Databricks on Azure Cloud.
Building ETL pipelines between Bronze, Silver and Gold layers using PySpark scripts and Apache Spark.
Create and maintain jobs and workflows in Databricks.
Design and development of data for analysis and modeling in Delta Tables;
Github CI/CD actions, Terraform scripts for automation;
Creating Unit Tests for new ETL
Refactore existing Python code
Design and implementation of DataLake\Delta Lake
* Project: Build DWH for Retail company
Analysis and assessment of client requirements, followed by compliance with the work plan, building DWH Architecture based on the client's needs.
Design and development of data for analysis and modeling;
Design and implementation of enterprise data warehouses (DWH - Azure Synapse, Azure DB, Azure Services)
Performance analysis of databases, data warehouses and their performance tuning
Design of implementation, configuration, customization and implementation of ETL\ELT processes (Azure Data Factory, Azure Functions, Spark)
Design and implementation of DataLake\Delta Lake
Monitoring and optimization of display assembly processes;
Loading and processing data from various sources(1C ERP/CRM systems, Dynamics, on-premise databases etc)
* Project: Puma - Prism project moving DWH from on-premise SQL Server to Azure Cloud
Building Data lakes, Data warehouses architecture, and ETL solutions;
Used Slowly Changed Dimensions technique to build customer profile;
Experience in building ETL processes, understanding how to test them and check data integrity;
Experience with Azure Synapse Analytics, building ETL Pipelines, orchestration, DataMarts and Data Layers;
SQL query optimization;
Azure SQL DWH, Azure SQL managed Instance;
* Project: Teradata to AWS S3 migration
Building Data Pipelines with SQL, Python(PySpark);
Using DBT and Airflow to automate data quality testings;
Configuring Spark Cluster based on the needs;
Validation of data processing processes;
Analysis of the quality of data prepared for ETL and BI systems;
Development of complex SQL queries to various databases to check the quality of data and the results of their processing;
Databricks, Spark"
data engineer,"Data Engineer and BI Developer with over 4 years of experience and technical knowledge in T-SQL, Python (Pandas, Numpy), Power BI, ClickHouse, dbt, PySpark, Snowflake, Microsoft Azure (ADF, ADL gen2, Synapse), AWS (Athena, LakeFormation, Glue), Airflow, Kafka, CI/CD tools, Git,  GCP(Bigquery, dataflow, GA). Proficient in data extraction, transformation, and loading (ETL) processes, ensuring data accuracy in the e-commerce domain.
Adept in crafting interactive Power BI dashboards for real-time insights. Demonstrated skills in data warehousing, ETL optimization, and data pipeline management, contributing to data-driven strategies and actionable insights."
data engineer,"Last 3 years:
Several project using Microsoft Azure tech stack.
–ü—Ä–æ—Å—å–±–∞ –ø—Ä–µ–¥–ª–∞–≥–∞—Ç—å –≤–∞–∫–∞–Ω—Å–∏–∏ —Ç–æ–ª—å–∫–æ –ö–ª–∞—É–¥ –∏ —Ç–æ–ª—å–∫–æ Azure stack"
data engineer,"Data Engineer/ Data Scientist with a a math background and 8+ years of experience building data-driven solutions (10+ years in software development). Passionate about solving different problems alongside learning something new.
Databricks Certified Data Engineer Professional
Azure Data Scientist Associate
Coursera specialization:
Machine Learning & Data Analysis Specialization
Deep Learning Specialization
Algorithms specialization by Stanford University
2023 Databricks Data Team Awards: Data Team Transformation Winner"
data engineer,"I am a Data Engineer with more than 6 years of experience in the full data delivery pipeline,
which includes database design, database maintenance, data collection, ETL
processing, data validation, data cleansing, report delivery, and data analysis.
Proven success in leadership, operational excellence and organizational
development with keen understanding of elements of different areas such as
healthcare, finance, consulting,retail."
data engineer,"QUALIFICATIONS:
Over 10 years of experience working with website analytics and over 2 years of experience with mobile app analytics
8 years of experience leading an internet marketing department with 8 employees
Proficient in Python, JavaScript, SQL, HTML, CSS
Experience working with APIs of various services, including Google
Experience writing scripts for automating work processes, writing parsers for search engines and websites, and working with Selenium
Excellent knowledge of web technologies, allowing effective communication with developers, accurate task assignment, and result monitoring
ANALYTICS:
Accurate setup of Google Analytics to collect data for informed decision-making, including the setup of GA4 with data streaming in BigQuery.
Professional implementation and setup of Google Tag Manager, from setting up event tracking on websites to content replacement on specific pages based on predefined conditions, and setting up Enhanced Ecommerce, including for GA4. Proficient in JavaScript, HTML, and CSS, allowing most data collection tasks to be handled without involving developers.
Working with raw data in BigQuery and writing analytical SQL queries.
Identification of the most effective conversion channels for optimization, monitoring of traffic acquisition channels based on analytical data, redirecting resources to more effective advertising channels.
Analysis of search engine optimization efficiency.
Building funnels and optimizing them, working with conversions.
A/B testing, setting up server-side experiments for Google Optimize.
Obtaining large, unsampled data volumes from Google Analytics using Python/R and working with them.
Working with Search Console API.
User segmentation, RFM analysis.
Building end-to-end analytics, enabling conversion tracking not only for leads but also for paying users and making more accurate decisions on traffic channel optimization based on revenue.
Creating reports in Power BI and GDS.
Building a DWH on BigQuery and Google Cloud Platform for marketing, product and cross-channel analytics.
Sources of data for the DWH include:
- GA4
- Firebase
- AppsFlyer
- Google Ads
- Facebook Ads
- Microsoft Azure
Creating data marts for building analytical reports in DataStudio (Looker Studio) and Power BI."
data engineer,"Tech. knowledge:
Database Development (MS SQL Server, T-SQL)
Databricks for Real-time data using Spark Structured Streaming
Python for data analysis
Machine Learning
Other knowledge:
Git, AWS, Jira, PostgreSQL, Snowflake, Databricks
Domain knowledge:
- genomics
- finance"
data engineer,"In my previous roles, I have successfully undertaken projects such as building a company's social media presence from scratch, leveraging analytics to guide strategy. I‚Äôve conducted data analysis for organizations, creating user-friendly dashboards and slides to support profitable decision-making. Additionally, I implemented a network security update for a company with over 800 staff, transitioning to a zero-trust policy and conducting data protection courses to ensure compliance.
I have also developed websites for various businesses, combining functionality with user-centric design. Currently, I am seeking opportunities to grow my expertise in data engineering while continuing to contribute in areas such as course administration, web development, IT support, data protection, IT policy creation, and systems optimization. My goal is to continually expand my technical skills while delivering impactful solutions."
data engineer,"For 2.5 years I have worked in IT startups as Data Scientist (NLP field) with trade and crime data.
For the last year i've been working as data engineer in medical field.
- NER model with Prodigy package (data preparation, labeling, modeling, error analysis, iterate)
- Geocoder (Google APIs, Pelias)
- Supervised resume_classifier (xgboost, bi-lstm, transformers) for free Text comparation
- Unsupervised resume_classifier / custom systems for free Text clustering
- A lot of tabular data (SQL)
- Cloud services (AWS, Databricks)
- Airflow
- Spark
- ETL
- OpenAI
- Successfully developed from end-to-end NER pipeline with help of Prodigy tool for annotations with human level performance, resulting in enhanced company data and user experience and increased customer satisfaction significantly.
- Implemented geocoding pipeline from scratch using Google‚Äôs places and addresses api, created voting system as part of pipeline for selection best of multiple geocoding api outputs.
- Combined the two above projects to create ML driven system that takes news (media articles) as input and outputs Location (lat-lon pair), time and actors of crime event.
- Spent significant amount of time developing tools for automatic data insight and statistics generation to make user experience more pleasant and insightful.
- Been building ETL pipelines that utilizes chatgpt."
data engineer,"Self-motivated and results-oriented Data Engineer with strong problem-solving skills and ability to deliver reliable, scalable, and efficient data solutions.
Experience: 4.5 years
Domains: Banking, Healthcare, Insurance/Reinsurance, Mobility
Technology Stack: Azure, AWS, Databricks, Python, DBT, Snowflake, RDBMS (MsSQL, PL/SQL)
Version Control: Git
Documentation: Confluence (creating and maintaining clear, structured technical documentation)
- Building and maintaining ETL pipelines using Azure Data Factory to integrate and transform data from various sources.
- Designing and implementing data resume_classifier and transformations using dbt.
- Utilizing a multi-layer Snowflake DWH architecture for storing, transforming, and managing clean transactional data for analytics and reporting.
- Developing and optimizing Databricks Notebook Activities to handle data processing and advanced transformations.
- Working extensively with RDBMS systems like MsSQL and PL/SQL, ensuring efficient management and querying of relational data.
- Using AWS Lambda to create serverless data processing solutions for real-time and batch workflows.
- Using Kafka for managing data streaming pipelines.
- Collaborating with cross-functional teams to improve data quality, performance, and architecture."
data engineer,"UK - London, Cogniflare (6 months)
Fullstack Engineer / DevOps
Smart City project.
Aggregation of 2 000 cameras. Recording video streams in GCP. Analysis
with OpenCV and ML. Rich reporting.
Work with RTSP protocol. ONVIF integration.
Recording of streaming video with FFmpeg.
Apache Nifi for data flows. GC buckets for storing videos.
Custom high-load web server with API written in Java.
Web interface and mobile apps with Flutter.
Working with GCP(Google Cloud Platform).
Working with DataBases: FireBase, PostgreSQL, SQL Server.
WPF Applications
Kyiv, product company - 2 years
Back-End Developer (partially front) / DevOps.
Development of an industrial product. Writing a Java socket server to communicate with physical devices, sensors, and Modbus devices. The use of Apache Nifi technology is designed to organize ETL processes within the Hadoop ecosystem. Use of the Groovy, Dart languages, and the PostgreSQL database. Flutter acts as a visualization. Also writing bot telegrams to quickly inform customers at industrial facilities. Wrote Flask Python API server for taking data, analytic, and excel reports. Provided Telegram Bot written on Python/telebot for quick messages and alerts.
Worked as DevOps with Debian to control server productivity.
Google Cloud Certified - Professional Cloud DevOps Engineer.
Volunteer development for the armed forces of Ukraine."
data engineer,"I am a highly motivated and client-focused Data Engineer with extensive experience in designing and implementing scalable and efficient data solutions using Azure and AWS. Throughout my career, I have worked on a variety of projects that have allowed me to hone my skills in big data technologies, cloud platforms, and database management.
Projects and Key Responsibilities:
Large-scale Data Storage System:
Developed a system for storing large volumes of data on Azure.
Implemented backend services with Python and Flask.
Managed data storage using SQL Server and Snowflake, and orchestrated ETL processes with Azure Data Factory.
Technologies: Azure, Python, Flask, SQL Server, Snowflake, Azure Data Factory
Oracle to Azure Migration Tool Development:
Developed a tool for migrating data from Oracle to Azure using Azure Data Factory and Microsoft SQL Server.
Managed architecture design, task creation, pipeline scripting, and documentation.
Technologies: Azure Data Factory, Microsoft SQL Server, Oracle
AWS Data Pipeline Implementation:
Implemented data pipelines on AWS using AWS Glue and Apache Airflow.
Configured data storage with AWS S3 and Redshift.
Technologies: Scala, AWS EMR Serverless, Apache Airflow, AWS S3
Currently, I work as a Senior Data Engineer, focusing on maintaining and enhancing a data platform while leading a team of data engineers and backend developers. I am passionate about continuing my growth in data engineering, with a particular interest in improving my skills in DevOps, architecture, and leadership.
I look forward to further developing my expertise in these areas and contributing to innovative and impactful data projects.
1. Cost Optimization:
Successfully optimized cloud costs for Azure and AWS, reducing consumption by over 50% and 30% respectively. This resulted in significant savings for the company while maintaining performance and efficiency.
2. Project Success:
Led the development of a large-scale data storage system on Azure, implementing backend services with Python and Flask. Ensured robust data storage using SQL Server and Snowflake, and streamlined ETL processes with Azure Data Factory.
Developed a comprehensive migration tool, utilizing Azure Data Factory and Microsoft SQL Server. This tool facilitated seamless data migration, ensuring data integrity and security.
Implemented efficient data pipelines on AWS using AWS EMR and Apache Airflow, successfully configuring data storage with AWS S3. This enabled real-time data processing and improved data accessibility.
3. Mentorship and Knowledge Sharing:
Actively involved in mentoring junior data engineers, providing guidance and support to enhance their skills and career growth.
Created and delivered educational courses on data engineering topics, helping professionals understand and navigate the complexities of the data world.
Authored numerous articles on data engineering, sharing insights and best practices with the broader community.
These achievements highlight my ability to deliver impactful results, optimize resources, and contribute to the professional development of others in the field of data engineering.
I am eager to continue developing my skills and expertise in data engineering, particularly focusing on cloud platforms like Azure and AWS, big data technologies, and advanced SQL optimization. I thrive in collaborative team environments where I can contribute to innovative projects and help businesses maximize their data value.
What I Want:
1. Opportunities to work on challenging data engineering projects that leverage my skills in Azure, AWS, Spark, Airflow, and SQL.
2. A collaborative team setting where knowledge sharing and continuous learning are encouraged.
3. The ability to contribute to business growth by providing scalable and efficient data solutions.
What I Don‚Äôt Want:
1. Environments that do not value teamwork or continuous learning.
I am committed to delivering high-quality work and helping businesses achieve their goals through effective data management and innovative solutions."
data engineer,"‚Ä¢ AWS Certified Data Analytics ‚Äì Specialty
‚Ä¢ Experience in working with AWS (S3, Lambda, StepFunctions, SNS, SQS, KMS, SSN, DynamoDB, CloudFormation, CloudWatch, Kinesis, Glue, Athena, Redshift)
‚Ä¢ Experience in working with Qubole (Hive, Presto, Spark)
‚Ä¢ Development of ETL: Prefect 2.0, Airflow (MWAA 2.0), dbt (Data Build Tool), IBM DatatStage, Informatica, MS SSIS;
‚Ä¢ Database design and implementation: Oracle, MS SQL, Teradata, DB2, Hadoop Hive, Snowflake;
‚Ä¢ SQL-tuning;
‚Ä¢ ETL performance tuning;
‚Ä¢ Development of Oracle PL/SQL, Microsoft T-SQL, Teradata SQL PL, Snowflake PLSQL program units;
‚Ä¢ Experience developing on Python;
‚Ä¢ Fivetran, Rivery data integration tools;
‚Ä¢ Experience using bash & PowerShell scripting;
‚Ä¢ CI/CD tools: Jenkins, GitHub Actions, Terraform
‚Ä¢ Debugging & Unit Testing;
‚Ä¢ Experience with Git, SVN, Perforce; TFS;
‚Ä¢ Troubleshooting, problem solving.
===========================================
Senior Big Data Developer
EPAM Systems ¬∑ Contract
Jun 2021 - Present
Customer: EF Tours
AWS services used for data lake hydration using DMS, S3, Glue, Lambdas, Step Functions etc.
Data loading into Snowflake (Cloud Data Warehouse)
Projects pipeline:
- Implement tooling for task orchestration and data transformations using Prefect, dbt (Data Build Tool), Fivetran, AWS.
- Ingest and process additional data flows into Snowflake
- Near real-time replication to hydrate data lake and Snowflake DWH.
Tech stack:
- Prefect Cloud 2.0;
- AWS: ECS, Lambda, StepFunctions, S3, SQS, SNS, DMS, KMS, SSM, Athena, CloudWatch;
- Snowflake (DB replication & sharing, PLSQL, administration and maintenance, data integration setup)
- DBT (Data Build Tool);
- Fivetran;
- Python;
- Terraform;
- GitHub Actions;
- Docker.
NRT data processing
Event Driven data processing in AWS
AWS
Prefect
Snowflake
dbt
Python
FiveTran / Rivery"
data engineer,"I have experience in DW development from scratch, leading projects.
Implemented data warehouses using dimensional modeling best practices, delivering a reusable data model that is well-structured for consumption by data analysts and highly performant.
Interested in projects that heavily focus on data modeling."
data engineer,"I have been in the IT industry for more than 7 years. I worked as Software Developer and Data Engineer, I took part in several projects. I have been working Data engineer my major experience is in python and sql technologies. I have got experience with Big Data projects, creating ETL/ETL data pipelines, web application development and worked with Database & DWH Architecture, Design & Development."
data engineer,"I have practical experience in the development and maintenance of large databases as a SQL developer in the Financiel field.
Daily work includes writing queries of varying complexity, views, common table expressions, functions, procedures, triggers, as well as analyzing the database for errors, inconsistencies and their elimination.
I have experience with MS SQL Server, MySQL, T-SQL, SSIS, ETL, SSRS, C#, Python,
Microsoft Office, Visual Studio Code, SQL Server Management Studio, Microsoft Visual Studio - Power User.
Familiar with HTML, CSS, JS, C++, C#, Python at a basic level.
I have completed courses and have theoretical knowledge of DWH, SSIS, ETL, SSRS.
I would like to do T-SQL, DWH, ETL, SSRS.
Financial Project which calculates best rates for client loan (Mordgade and ect) including his/her rating and individual fees
Hi there, i have great experience working as  a DB Developer"
data engineer,"Experienced data engineer with Python, SQL, GCP, DataWarehousing, Snowflake, GCP Dataflow, Apache Beam/Spark. Certified associate Google cloud engineer.
Specializing in full life-cycle of data management and reporting with latest technologies like GoogleCloudPlatform, Airflow, Snowflake, Looker
Extensive knowledge in data modelling, data pipelines, ETL processes, storing and analysis.
Certified associate Google cloud platform engineer."
data engineer,"As a data engineer in a pharmaceutical company ""Optima-Pharm, LTD"" with 2 years of experience, I have focused on designing, building, and maintaining ETL pipelines to collect, transform, and load data from diverse sources into data warehouses and data lakes. My responsibilities include integrating data from multiple internal and external sources, ensuring data consistency and reliability. I write and implement code to clean, normalize, and transform data into analysis-ready formats, addressing issues like missing values and duplicates to uphold data quality standards.
I am skilled in optimizing data pipelines and queries to improve processing speed and minimize storage costs. Additionally, I work closely with data scientists and analysts to understand their data requirements, ensuring data is delivered in the right format and quality. I have experience in managing and scaling databases or data lakes, automating routine data tasks, and setting up monitoring systems to proactively detect and resolve pipeline issues. My work is well-documented to maintain transparency and ensure smooth maintenance by my team.
During my two years at the company, I successfully developed and optimized SQL queries and procedures to automate the processing of large amounts of data, which significantly reduced the time for generating reports and increased their accuracy. This helped reduce the time for processing reports by 40%.
I also implemented automated ETL processes to integrate data from various sources, such as internal databases, CRM systems, and external suppliers. This significantly improved the availability of data for analytics and ensured its integrity and accuracy.
Thanks to the developed procedures for monitoring and validating data, I was able to ensure a high level of data quality used for making business decisions. This reduced the number of errors at the data processing stage by 25%.
As part of my work, I also actively collaborated with a team of analysts and developers to create new functional modules in the company's software platform, which significantly improved the processes of data management and order history storage. and also to improve the quality of use of the main website of ""Optima-Pharm LTD"", through which thousands of orders from the pharmacological sector are processed daily.
I am looking for an opportunity to develop my technical skills by working with new tools and technologies. I would like to participate in large and complex projects that allow me to apply my knowledge and solve new challenges facing the company."
data engineer,"I am Junior Data Engineer with 1.5 years of experience.
‚Ä¢ Building and optimizing internal DWH
‚Ä¢ Development and maintaining ELT processes in internal DWH
‚Ä¢ Collaboration with different departments and providing Data Marts
‚Ä¢ Data extraction via API using Python
‚Ä¢ Using cron to schedule data pipelines"
data engineer,"I'm a data engineering and data analysis expert, with around 4 years of commercial experience - 2.5 of which my role focused on Data Engineering and BI Engineering and the last 2 - solely Data Engineering.
I focus on development in AWS cloud, using such tools like Glue, EMR, Lambda, MWAA, Kinesis, ECS, EvenBridge.
Other tools and technologies:
Python, SQL
PostgreSQL, MySQL, DynamoDB
AWS (Glue, EC2, EMR, Lambda, ECS, Kinesis, MWAA, Athena, S3), Airflow, Dagster, dbt, Snowflake, Docker, Git
QuickSight, PowerBI, Tableau, Looker
Apart from that I'm a quick learner, adapting to different working environments, methodologies and tool sets."
data engineer,"- Knowledge of Hadoop ecosystem and different frameworks inside it ‚Äì Hive, HDFS, YARN, MapReduce; - SQL-based technologies (e.g. MySQL/ PostgreSQL/MSSQL); - Assembling large, complex data sets that meet business requirements; - Developing ETL processes using Talend; - Troubleshooting performance issues. - Developing of DWBI solutions; - Developing SSIS ETL packages; - Building reports in SSRS, Tableau, PowerBI, TIBCO Spotfire; - Designing and developing of Data warehouse and Integration services; - Troubleshooting performance issues; - Developing of dimensions and cubes within the Analysis Services project; - Writing relational and multidimensional database queries; - Creating complex functions, scripts, stored procedures, and triggers to support application development; - Optimizing database systems for performance efficiency.
Want to work with new technologies. I don't like boring tasks and am ready to have challenges."
data engineer,"Oracle PL/SQL developer, database developer.
Studying Python development, snowflake, databricks, data engineering.
Developed and optimized PL/SQL scripts for business processes
Implemented customizations and workflows in SAP R/3 and Oracle EBS systems
Created and consumed REST APIs using JSON and Java
Managed databases including Oracle, PostgreSQL, MySQL, and Firebase
Collaborated with teams using Redmine, Jira, and Github for project management"
data engineer,"DE with experience in cloud DataOps and deep understanding of DQ/DA/BI/DS practices.
tech stack: AWS, GCP, Python, Pandas, Pyspark, SQL/NoSQL, pipeline orchestrations, data lake-house, architecture design.
- solving complex and challenging tech issues
- providing architecture design suggestions
- promoting open and constructive team communication
I am looking for dynamic projects where I can further develop my expertise and make a significant contribution to business growth."
data engineer,"Work Experience
I have experience designing and optimizing data infrastructure, building ETL pipelines, and implementing end-to-end analytics systems. My projects have focused on improving data processing efficiency, reducing costs, and enabling better decision-making for marketing, sales, and client management teams.
Key Projects & Responsibilities:
Built real-time web analytics processing with micro-batching, enabling faster ad campaign decisions.
Developed an LTV prediction model with 73% accuracy, improving traffic analysis and budget allocation.
Migrated a data lake, reducing storage costs by 31% and improving access speed by 70%.
Created data pipelines for Marketing and Scouting teams, reducing data delivery time from days to hours.
Designed infrastructure for data storage and processing from scratch, ensuring scalability.
Technologies Used:
Data Processing: Python, SQL, Apache Spark, Airflow
Databases & Storage: PostgreSQL, Oracle, ClickHouse, S3
Infrastructure: Docker, Proxmox, CI/CD pipelines
Analytics & Monitoring: Loki, Prometheus, PowerBI
Current Role & Future Goals:
Currently, I work as a Data Engineer, focusing on ETL pipelines, data modeling, and infrastructure optimization. I enjoy solving complex data challenges and want to deepen my expertise in big data processing and cloud-based data architectures."
data engineer,"1) Remote Contractor - Senior Software & Data Engineer
1.Development of microservices
2.Development of data pipelines(batch, streaming)
3.Integration and testing of different 3rd party APIs
Technical stack:
Programming: Java 17, Python 3, Kotlin, Spring Boot stack
Data engineering: advanced Oracle SQL, PL/SQL, optimizing complex SQL queries, Postgres, MySQL, ETL, BI reporting, Kafka, Apache Spark, Spark Streaming, Solr
Cloud: AWS RDS, Aurora, EC2, ECR, Lambda, S3, IAM, SQS, SNS, Athena, CDK
DevOps: Docker, Kubernetes, CI/CD, Jenkins
2) Senior Java and Big Data Engineer
I was one of the core team members in European Bank's GDPR related Big Data project.
1.Developed near real time big data processing pipeline with Kafka, Apache Spark, Spark Streaming, Solr and HBase
2.Developed microservices with Spring Boot
3.Developed artifact and configuration deployment scripts for DevOps engineers.
Technical stack:
Programming: Java 8, microservices with Spring Cloud
Big Data: Cloudera Hadoop cluster on AWS, Apache Spark, Spark Streaming, Kafka, HBase, Solr
DevOps: CI/CD pipeline with Jenkins, Docker, Ansible.
Atlassian stack (JIRA, Confluence, BitBucket)
3) Telecom Company-Head of Datawarehouse Unit
My role was 50% technical and 50% managerial. I was responsible for Oracle Exadata based 51 TB corporate data warehouse system. DWH was highly critical reporting source for company‚Äôs daily operations and financial reporting.
Responsibilities:
1) Collaborating with business units to understand, analyze their reporting & analytics needs
2) ETL development, data modeling(dimensions, fact tables, data marts), BI reporting with SAP BO tools
3) Supervising team members, driving prioritization and delivery
We maintained Cloudera Hadoop cluster, implemented different telecom specific Big data use cases.
1.Telecom billing database, tables with millions of rows. Optimized complex Oracle SQL query from 3 minutes(cost 13000) to 2 ms(cost 50), rewrote with advanced analytical functions, applied correct indexes with hints.
I‚Äôm interested in Java microservices, Oracle, Postgres, data and cloud engineering projects"
data engineer,"Experience in collecting, processing, analyzing and visualizing data. In my work I use Python, SQL, Big Query, Google Sheets, Excel, Tableau,  Amplitude, Google Looker Studio, A/B testing and basic statistics.
Have experience working on multiple projects, e.g.
Project_1, selection of films by types and settings of TOP categories.
The goal of the project is to prepare data for analytical processing. The program is written in Python in PyCharm and is available on GitHub.The program generates files by film type with the ability to select the TOP level (from 0.1 to 99.9) by average rating, in .csv format for further analytical processing. The source files for processing are pulled from the network, unpacked, the archive is deleted. It is also possible to update the source files and delete previously generated user files
Cash Flow Analysis, Sales
The goal of the project is to analyze the dynamics of cash flow changes by product metrics.
Using the PostgreSQL DBMS in DBeaver, the initial preparation of data was made, prepared for processing and visualization in the Tableau source, based on two tables. The final information is conveniently visualized on a dynamic monitoring panel, where you can analyze the activity of different users and cash flow over time, as well as by age and language of users
Adaptation funnel. Sales
The goal of the project is to conduct an initial analysis of the sales funnel and visualize the primary information
Using the tools provided by Tableau, the sales funnel is visualized. Information on the distribution of user activity in the selected event and the connection with registered users is presented graphically.
Ads analysis, Retail
The goal of the project is to calculate and visualize marketing metrics for an advertising campaign in retail.
Using SQL and Google Looker Studio, this project extracts and analyzes data to evaluate the effectiveness and efficiency of advertising on key marketing metrics. Segmentation methods are used to evaluate different advertising channels, campaign types, and target audience segments.
Cohort analysis, GameDev
The goal of the project is to optimize the design, acquisition, and monetization of games based on data obtained from interactive analysis in Google Sheets.
The project helps analyze player behavior and track key activity indicators. The project measures user engagement and retention using basic statistics and analysis.
Successful use of Python, SQL, Big Query, Google Sheets for data processing and transformation
Experience using Tableau, Google Looker Studio for data visualization and processing
Use A/B testing and basic statistics to analyze and evaluate performance.
I am looking for a full-time position in a company that's aimed at making both everyday life and business easier, more productive and user-friendly.
I am a fast learner, responsible and ready for chalenging tasks"
data engineer,"Experienced Data Engineer with a strong background in SQL, Azure Cloud, ETL, MySQL, and Python. Skilled in data analysis, Power BI, Git, and machine learning. Passionate about leveraging the power of Microsoft Azure to design and develop robust data workflows and pipelines for optimal data management and analysis. Currently working as a Medior Data Engineer at Tata Consultancy Services, collaborating with cross-functional teams to deliver innovative solutions to the clients. Open to exciting opportunities and continuous learning in the dynamic field of data engineering."
data engineer,"Experience as a data engineer
Strong experience in SQL, Python,
Spark
Extensive experience in developing ETL
processes
Extensive experience in AWS services
Extensive experience in Spark and
Azure Databricks
Extensive experience in writing shell
scripts and python code
Extensive knowledge of mathematics
and math analysis
Experience in Azure cloud
Experience in working Database:
Oracle, MSSQL server, Postgresql,
MySQL etc
Giti//c
Data Engineer
ETL processes development(SSIS + SQL+ Python)
Working with AWS(S3, Secret Manager, RDS)
Process optimization
Working with MSSQL server, Python
StartUp
Data Engineer
ETL processes development
Process optimization
Working with Postgresql, Python
Working with MSSQL server and T-SQL
Creating pipelines(AWS+Azure Databricks)
Systems Technology
Data Engineer
Creating etl processes
Support for existing infrastructure
Working with Oracle database and pl/SQL
Writing functions, stored procedures, packages,
triggers, various SQL queries (from simple to
complex).
Process optimization
RPS
Intern programmer
Working with MSSQL database
Writing functions, stored procedures, packages,
triggers, various SQL queries (from simple to
complex).
Europost
Manager
She supervised several departments, was engaged in
the introduction of new employees into the company's
processes, helped with the study of technologies
necessary for work.
Helped with the installation and configuration of
equipment in new branches.
I was engaged in writing queries to the database,
creating tables / views."
data engineer,"Unibank 	Data (Big data) Engineer November 2024‚ÄìPresent
Tech stack: Python,  SQL,  Apache Airflow, Pyspark, Apache Kafka,Pandas, GitLab CI/CD, Google sheets, ETL
Bank Respublika - Python/Data Engineer 2023 august - 2024 november
- Ensuring the robustness of data pipelines and ETL‚Äôs by optimizing their structure and code
- Analyzing data using Google Sheets and SQL to identify and address issues, enhancing system
performance
- Designing and implementing Data Warehousing solutions while ensuring their ongoing maintenance and
optimization
Tech stack: Python, SQL, Docker, GitLab CI/CD, ETL, Google sheets
ArtTech - Backend developer     April 2022‚ÄìJuly 2023
- Developed REST APIs for new mobile app functionalities and also crafted intuitive RESTful APIs to
empower our management web app
- Worked on developing insightful analytical and statistical dashboards for a management application,
empowering data-driven decision-making and facilitating in-depth data analysis
- Built a socket app and implemented data pipelines to support real-time data on a dynamic dashboard
- Developed new tools from scratch to support our current product and clients so that we can increase the
development and product delivery efficiency
Tech Stack: Python, SQL, Excel, Fastapi, Flask, Github, Linux
i graduated from bachelor of device engineering from Azerbaijan State Oil and Industrial University , then i studied masters .
i would like to work mostly data based apps"
data engineer,"2 Years of DBA experience at EY, have used VBA, MSSQL, as well as RPA(Corezoid) and PowerBI.
7 month at SPS Commerce as Data Engineer. My responsibility was to maintain current pipeline using Snowflake, C#, SQL, we have used AWS as cloud provider.
8 month at SoftServe as Data Engineer. With team we have developed new pipeline for educational company using AWS, Databricks, Apache Airflow
9 month so far at 3Shape as Data Engineer, currently we are working on brand new pipeline using Azure Databricks, Kafka, Delta Lake
1 year at Pin-Up as Data Engineer, have mastered Clickhouse and Airflow
Currently at Raiffeissen bank, working with aws and on-prem servers"
data engineer,"In my previous roles, I've completed diverse data engineering and data science projects. At EPAM Systems, I built a robust AWS S3 data lake, designed ETL processes, optimized pipelines with Spark, and managed databases. I used Python, AWS services, Airflow, SQL, and more.
At Philip Morris International, I gathered and processed data from various sources, managed pipelines, configured BI systems, and developed a real-time calculation website. I employed Cloud Computing, Python, SQL, AWS, and Django.
As a Data Science Engineer at SCDM LLC, I excelled in data retrieval, processing, and application development with Python, OCR, and ML. I maintained data pipelines and ensured data quality.
In EPAM I focused on Python development, ETL processes, AWS services like AWS Glue and AWS Redshift, systems architecture, automation, and security. I aim to enhance my cloud computing and serverless architecture skills while staying updated with industry trends for optimal system performance."
data engineer,"Data Scientist with hands-on experience in developing and deploying machine learning resume_classifier, coupled with a strong foundation in classical data analysis techniques. As a Junior Data Scientist, I have honed my skills in building and refining predictive resume_classifier, conducting data- driven analyses, and deploying solutions to production. Prior to this, I successfully completed multiple freelance projects across diverse domains, delivering high-quality analytical solutions.
My expertise spans across Natural Language Processing (NLP), deep learning, and Large Language Models (LLMs), with a proven ability to implement these technologies to solve complex business problems. I am proficient in generating interactive dashboards to communicate insights effectively and deploying scalable solutions on cloud platforms. Eager to leverage my skills in machine learning and data science to drive business impact in a fast- paced, collaborative environment.
My projects can be found on my Github repositories
Skills :
Speak german and english fluently
Python (packages :pandas, numpy, sklearn, seaborn, matplotlib, plotly )
SQL
Machine Learning
Deep Learning
Data cleaning, preprocessing, data wrangling of Text data and metadata
Clustering -Regression- Classification algorithms
unsupervised- supervised learning
logistic regression, SVM, Random Forest, XGBoost, Naive Bayes, k-Means, Hierarchical clustering etc)
NLP (Word2vec, Text mining, tokenization, sentiment analysis, lemmatization, sPacy, vectorization etc)
WORD EMBEDDINGS
Statistical analysis
Data Visualisation
Github
Time Series analysis
Image analysis with CNN
Streamlit, Dash
Webscraping
Gradient Boosting
OpenCV , Computer Vision
CNN, Deep learning
Linux, Bash
R programming language
Docker
Airflow
Rest API
Flask FastAPI
Databases, NoSQL, MongoDB, Neo4j, Elastichsearch
LLM, Transformers, BERT
I completed several  projects in which I applied a variety of supervised and unsupervised machine learning algorithms, as well as deep learning and NLP techniques. The project involved scraping data, generating a custom dataset, performing data cleaning and preprocessing for both Text and metadata, and modeling. I extensively utilized data visualization and statistical data analysis for this project. II used methods like K-means, Xgboost, Logsitic regression, Naive Bayes, SVM, PCA, Dense Neural Networks, tokenisation, POS Tagging, Doc2vec, Word2vec resume_classifier, semantic analysis of customer reviews and data visualisations, statistical data analysis and interactive plots."
data engineer,"I studied management, worked in marketing and technical writing but my passion always was IT. And all my activities were related to different types of IT. Data become one of the most important assets for companies and I would like to power it for you.
I studied management, worked in marketing and technical writing but my passion always was IT. And all my activities were related to different types of IT. Data become one of the most important assets for companies and I would like to power it for you."
data engineer,"Business Intelligence Developer with 4+ years of experience in the Transportation and Hospitality is seeking to obtain a position as a Developer to utilize my knowledge, previous work experience, and my technical skills in Business Intelligence tools and system analysis. To gain the knowledge and understanding of Business Analysis process more deeply and the requirements for projects planning. Experienced with receiving and monitoring data from multiple data sources, including MSSQL, SSIS, SSRS, Power BI, Azure. Also, experience with Java programming language and its frameworks Spring boot, Hibernate, REST API, Maven, Spring Security, Spring Data JPA, Microservice, Git, Junit, OpenAPI, Docker. Profound knowledge in industry-specific data such as above mentioned. Experienced in database design, development, and business intelligence of Microsoft SQL Server in development, test, and production environments."
data engineer,"-Monitoring of key product indicators, and quickly respond to deviations
-Creating a financial reporting system
-Data visualisation in Redash
-Creating an alerting and notification system
-Evaluation of AB tests
-Working with API (Google Analytics, Amazon S3, Google Ad Manager, Slack, etc.)
-Building streaming data pipelines with bunny CDN, Amazon Kinesis
-Writing project documentation
-Mentoring of other analysts in a team
I use Python and SQL mostly in my work. First of all I want to continue improve my Python skills, I also want to advance in the evaluation of AB tests (now I have a basic knowledge in this field). One day I would like to manage a team of analysts so that we can reach new heights and learn new technologies together.
Built a financial reporting system, where I had to combine multiple data sources using API and write the basic logic of calculation, specify the reliability requirements and implement quality control levers. And I'm still working on the code to make this system even better
Implemented integration with Amazon Kinesis to pull, process, and store in a database for further analysis of this data. The main challenge for me was to implement fast data reading and processing (because there is a lot of data and Kinesis has its own specifics of writing data to stream), so I had to implement parallel processing
I prefer not to work on boring reports, especially in Excel"
data engineer,"I‚Äôm a passionate developer specializing in Web Scraping , Front-end
development, Data Science and App development with a proven ability to
extract complex data at scale and build intuitive interfaces. My GitHub
showcases 7 Production-grade Scraping Projects (Zillow, Reddit, Indeed,
Glassdoor, etc.), where I overcame anti-bot measures (Cloudflare , ReCAPTCHA
V3, Imperva, etc) dynamic content, and pagination using Python/Playwright. I
also develop responsive web interfaces with Flutter/Dart, and three of my live
sites (still in development) are publicly accessible for review.
I‚Äôve honed my skills through hands-on projects and building sites for clients"
data engineer,"I am a Data Engineer with over 4 years of experience in the fields of e-commerce, healthcare, and security. I am proficient in Python and Java, and I have built various data pipelines for cross-system integrations. My work includes providing security analysis and incident response support, as well as ensuring data consistency across data sources. Gain GCP Cloud Developer Certification.
1. Built comprehensive server log analysis pipelines for system monitoring, reducing data storage costs by approximately 5% in the BigQuery Data Warehouse.
2. Created efficient and secure data connections on the Google Cloud Platform, enabling external queries and fine-grained access control.
3. Provided consulting support for transitioning from custom to standardized authentication and authorization frameworks, including OAuth2, OpenID, and Workload Identity Federation setup.
4. Developed company-wide documentation and code analysis practices (Arch-as-Code, Doc-as-Code). Integrated SonarQube for code quality scanning and established repository inclusion processes. Implemented branch protection techniques within a GitFlow workflow using Terraform.
5. Established a reporting process for cross-service traffic analysis, spike detection, and user Location identification using Redash, Looker, and Jupyter Notebooks.
6. Implemented data integrations for a large-scale e-commerce project."
data engineer,"Chief Technology Officer (CTO) - Oct 2024-Jan 2025
‚Ä¢ Managed a team of developers to create and maintain the whole infrastructure from the client dashboard to internal metrics collection and data warehousing
‚Ä¢ Built most of the internal systems (admin pages, invoicing system, metrics collection and analysis, etc.) from scratch, ensuring scalability and streamlining internal operations
‚Ä¢ Negotiated strategic deals with enterprise partners and suppliers to streamline external operations, reduce overhead, and boost efficiency
‚Ä¢ Closely worked with teams on all levels, from the CEO to individual developers, ensuring the whole staff is aligned on our goals and means to achieve them
Data Engineer - Mar 2024-Jan 2025
‚Ä¢ Managed the processing of over 10 billion data points daily, ensuring high efficiency
‚Ä¢ Designed and built a robust invoicing system from scratch, streamlining billing operations
‚Ä¢ Developed a tool to track, forecast and optimize AWS spending across more than 100 accounts
‚Ä¢ Managed over $1,000,000 in AWS monthly spending, helping the clients save over $100,000 on AWS costs
Project Manager - Dec 2023-Mar 2024
‚Ä¢ Led a small team of developers working in the iGaming industry
‚Ä¢ Achieved a remarkable time-to-market in under 3 months from scratch
‚Ä¢ Worked closely both with the client and the team to ensure clear vision of the project and it's goals
‚Ä¢ Discovered and implemented innovative approaches to various aspects of iGaming
Python Developer - Apr 2022-Dec 2023
‚Ä¢ Completed 20+ successful projects in Back-End Development, Web-Scraping, Data Engineering, Chatbots, and iGaming industry.
‚Ä¢ Optimized clients' infrastructure to boost efficiency and enhance performance.
‚Ä¢ Collaborated with clients to set up solutions to meet their specific needs.
At Umbrelly, a startup optimizing AWS costs through Reserved Instances and Savings Plans, I built a scalable analytics and billing automation platform from scratch. Initially hired to develop a script recommending optimal AWS savings strategies, I quickly identified a critical bottleneck: manual client billing and invoice generation, which was time-consuming and error-prone.
I proactively took ownership of both engineering and strategic aspects, negotiating data access agreements with AWS distributors, ensuring compliance with multi-country data privacy regulations, and securing client buy-in. On the technical side, I evolved a simple Python script into a sophisticated ETL pipeline leveraging Apache Airflow, PostgreSQL, and MongoDB.
The final product handled over 10 billion data points daily, fully automating invoicing for 100+ clients and providing real-time spending insights. This solution dramatically reduced accounting workload, significantly enhanced accuracy, and provided critical analytics enabling customers to better understand their AWS expenditures.
I would prefer working as a Data Engineer, however Data Analyst/Scientist is suitable.
In a small-medium team that values commitment to the project and growth.
Preferably in a Python + SQL environment using Spark, Airflow, DBT, Snowflake/RedShift.
For the cloud I would want AWS, but have experience with GCP and self-hosted too.
Can work up to 100 hours/week."
data engineer,"I was committed to effect change in billing, parental monitoring, fitness & wellness, content, and social networking industries.
Work with hybrid, cloud-based and on-prem data environments, such as AWS, GCP, Kubernetes, and micro-services architectures. Specifically, deploy usage of Airflow, Spark, PHP & Python, and Kafka & RabbitMQ.
Currently perform in Head of Data Engineering and Data Consultant roles.
Having combined diverse technical and leadership credentials and backed by discipline and outgoing attitude, I thrive in a mature and collaborative environment.
- Launched DWH as a primary source of truth to improve data accuracy and reporting efficiency.
- Led initiatives to transition from GA3 to GA4 within the organisation.
- Simplified integration of data vendors which expedited data provisioning to Analytics Team.
- Modernized analytics ecosystem by eliminating legacy analytics services and obsolete databases.
Remote, mature and friendly environment."
data engineer,"Data Engineer, Luxoft
Jul 2023 - PRESENT
Remote, Kyiv, Ukraine
‚Ä¢	Designed and developed ETL/ELT pipelines using ADF and Databricks.
‚Ä¢	Improved the existing data pipeline by splitting the job into multiple tasks and refactoring the code, resulting in a 32% improvement in running time.
‚Ä¢	Based on medallion architecture, prepared data marts for the end users and maintained data consistency in the existing ones.
‚Ä¢	Maintained a robust Kubernetes cluster, ensuring optimal performance and reliability, and managed a Starburst Enterprise instance for efficient and scalable data processing and analytics.
‚Ä¢	Connected data products to LLM model used to gain valuable insights from internal data.
Database Administrator, Uniqa Insurance Group
Mar 2023 - Jul 2023
Remote, Kyiv, Ukraine
‚Ä¢	Communicated with stakeholders and prepared data marts based on their requests, using SQL, in the insurance domain.
‚Ä¢	Maintained data quality so that there would be no conflicts and data remained accurate and up to date.
‚Ä¢	Developed documentation system for tables and storage containers to exclude misunderstandings inside the team.
Teacher Assistant (CS 431), University of Waterloo
Jan 2023 - Apr 2023
Hybrid, Waterloo, ON, Canada
‚Ä¢	Actively supported the professor in assignment marking, maintaining high standards of academic evaluation. Excelled in communicating with students, providing insightful and constructive feedback on assignments, fostering an engaging and supportive learning environment.
Research Trainee, Waterloo AI
May 2022 - Sep 2022
Hybrid, Waterloo, ON, Canada
‚Ä¢	Conducted extensive research on social graphs and propaganda spreading techniques under the supervision of a distinguished professor, gaining valuable experience in data analysis, network analysis, and research methodology."
data engineer,"I have been working at Alfa-Bank (Sense Bank) since December 2020, currently holding the position of Lead Data Engineer.
Key Responsibilities and Achievements:
Leading the Data Engineering team, including technical guidance, code reviews, and architectural decision-making.
Designing and developing decision-making systems for banking operations using Spark (PySpark), Kafka, and Hadoop.
Managing and optimizing ETL pipelines across multiple platforms including MSSQL, Postgresql,  Oracle and BigQuery.
Orchestrating data workflows with Apache Airflow and Google Cloud Dataflow to ensure efficient and reliable data processing.
Developing interactive dashboards using Django and implementing various data visualizations with libraries such as Seaborn, Matplotlib, and others.
Administering both Windows-based big data servers and Linux environments, ensuring system stability and performance.
Successfully optimized large-scale datasets, including the restructuring of a table containing over 6 billion records, reducing its physical size from over 2 TB and significantly improving performan"
data engineer,"Creating ETL processes with different data sources (from other databases, by API requests, from files on the disk etc). Developing and supporting of data warehouse. Visualizing the data using BI platforms."
data engineer,"Self-motivated Python developer with almost 2 years of commercial experience in using Numpy, Pandas, EXCEL, SQL in a financial market analysis company.
Experienced in optimizing routine tasks, working with various APIs, porting R scripts to Python, and working with visualization tools such as Grafana and Looker (Google Data Studio) in combination with BigQuery, Tableu.
As a self-taught freelancer, I have gained skills in web scraping, data analysis, automation, and data visualization.
I am passionate about continuous learning and taking on new challenges.
–ù–µ—Ç —Ç–∞–∫–æ–≥–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è."
data engineer,"I have some experience in developing projects in which I worked with Python/Django, MySQL, serving as a back-end developer. The project was organized using Git and Docker. In addition, I am quite familiar with .NET and DBMSs such as Postgre and MSSQL.
Obtained bachelor degree of Software Engineering from the Igor Sikorsky Kyiv Polytechnic Insitute, Faculty of Informatics and Computer Science. During the study, basic topics and more advanced areas of programming were studied, using various programming languages. Some interesting projects can be seen on the GitHub."
data engineer,"Autodoc, Python data engineer, Ukraine.
Utilize Pandas, NumPy, Matplotlib, Streamlit, and Power BI to perform comprehensive data analysis and create insightful visualizations.
Design, develop, and manage complex data pipelines using Kubeflow Pipelines (KFP), Apache Airflow, Cloud storage, Artifact registry and Google Vertex AI.
Implement data solutions on Google Cloud Platform (GCP) and Azure, ensuring seamless integration and scalability.
Deploy machine learning resume_classifier using Vertex AI, with a focus on continuous training and automated hyperparameter tuning.
Create comprehensive documentation for new tools like Vertex AI to facilitate adoption and effective use across the entire team.
Build and maintain web applications using Django and Flask, integrating with data sources such as Snowflake and Clickhouse.
Utilize Docker for containerizing applications, ensuring consistency and portability across different environments.
Collaborate with cross-functional teams to automate data workflows using Gspread and other scripting tools.
Experienced Python developer and data engineer with 7+ years in IT across banking, healthcare, retail, and telecommunications. Recently managed end-to-end data pipelines for a multinational automotive retailer, delivering scalable, cloud-based solutions. Designed an automated reporting system that improved staffing decisions and marketing strategies, driving increased sales. Known for analytical expertise, technical proficiency, and delivering impactful solutions that drive business growth. Keen to bring my skills and experience to this exciting role and contribute to your company‚Äôs success.
I'm looking for a company where I can acquire good experience as a Python developer data engineer, I'm ready to do my best to be a good and professional developer."
data engineer,"I am an ambitious Data Engineer with a focus on developing scalable analytical solutions for telecom products serving over 10M users. My expertise includes advanced data modeling, modern data ingestion, and transformation methodologies, resulting in a 90%+ improvement in marketing campaign execution and a 10%+ optimization of ETL processes.
Proficient in Python (Pandas, SciPy, Matplotlib, Plotly), SQL, PL/SQL, ODI, PowerBI, Windows and Linux OS, Bash scripting, PowerShell scripting, and Airflow. I have also successfully managed ML projects, improving data processing accuracy by over 93%, and have a strong background in team leadership and project management.
Currently pursuing a Ph.D. in Applied Computer Science with a focus on stabilizing platforms on moving surfaces using digital solutions, contributing to innovative approaches in data science and machine learning.
Developed a robust CVM Oracle Database recovery system: Ensured 730+ days of consistent data availability, leading to a 90%+ improvement in the timely execution of marketing campaigns for a telecom product serving over 10M users.
Architected a secure ETL pipeline: Utilizing ODI and Linux cryptography, I safeguarded 25% of core AdTech data, critical to the primary revenue stream, and optimized ETL processes by over 10%.
Led Python training program: Successfully trained the Data Engineering team through a 30+ hour program, enhancing their proficiency in Python, resulting in significant improvements in project efficiency.
Machine Learning Project: Designed a custom AI model with 93%+ accuracy for detecting and processing car number plates, significantly optimizing data extraction for a car dealership.
Looking for: A challenging remote data engineering role where I can apply my expertise in developing scalable data solutions, optimizing ETL pipelines, and leveraging cloud technologies. I value an innovative and agile work environment that encourages continuous learning and offers opportunities to work on impactful projects.
Not Interested in: Roles with limited scope for growth or innovation, or those that do not fully utilize my technical skills and experience in data engineering and machine learning."
data engineer,"Role: Senior Data Engineer / Team Lead
Summary of experience:
Experience in the implementation of business management software and solutions in DWH/BI and RDBMS
Solid understanding of designing and building data warehouses based on MS SQL stack (SQL Server, SSIS, SSAS, SSRS), Azure (Synapse Analytics, Data Factory, Data Lake, SQL Database, Databricks, Analysis Services, Power BI), Snowflake, Databricks
Strong experience in database development, implementation, administration, and maintenance
Experience in ETL development using different tools and approaches
Proven knowledge of architecture design patterns for data solutions
Solid background in creating and performance tuning of SQL queries, views, stored procedures, functions
Strong experience in data analysis and data transformation
Excellent analytical skills and logical thinking, good time-management and always focus on the result
Good communication skills, experience in negotiation and cooperation with clients
Responsibilities: data warehousing, ETL, integration, data modeling, CI/CD, version control
Project domains: e-commerce, supply chain, loyalty system, gambling, ERP, insurance, real estate"
data engineer,"I have worked on various projects, primarily focusing on ETL processes and data engineering tasks. In my first project, I was responsible for designing and implementing ETL workflows, utilizing asynchronous functions, Spark, and Pandas. I also performed statistical analysis on the processed data and developed machine learning resume_classifier based on these insights.
In my second project, I worked in the healthcare sector as a Big Data Engineer. My role involved working with technologies such as Spark, Pandas, AWS (EC2, S3, EMR, Athena), Airflow, and DBeaver, PostgreSQL. I also focused on data validation and testing using Pipelines and Deequ to ensure data quality.
Currently, I‚Äôm interested in further developing my skills in cloud-based big data solutions and machine learning, with a focus on scalable and efficient data processing workflows."
data engineer,"I work as a data engineer - ETL and DWH on Azure stack - (Azure data factory + MSSQL + Azure functions on Python).
Have experience with Snowflake, Azure Databricks\pyspark.
I've passed an internship in C# + Angular stack.
Had a couple of pet projects based on ASP.NET MVC + SQL and ASP.NET Core + SQL and Angular.
I have improved the internal organization of the project‚Äîreorganized the sprint release process and backups for project essentials. I have developed a new ETL process and implemented a new pattern for data cleaning. In addition, I have implemented a pattern of interacting with the source in close to real-time to improve the project in the future.
Microsoft Certified: Fabric Analytics Engineer Associate
Microsoft Certified: Azure Data Engineer Associate
GCP certified - Associate Cloud Engineer
Databricks Certified Data Engineer Associate
Looking for new knowledge, a friendly team,  and a field for improvement."
data engineer,"I'm Senior Data Engineer using Clojure/AWS right now, also working as Backend Developer, designing and implementing APIs used by frontend or end-users, rolling out and maintaining Apache Druid database. I'd love to have another Clojure project and getting more experience with something new at same time.
I've rolled out two Apache Druid clusters and maintained them myself, while they were becoming a backbone for every new project of a company. That alone made possible to reduce time of dinamic user-requested reports from hours to seconds. I've also was creating the batch jobs that generated datasources and fed the Druid clusters (spark-based calculations with several GB data a day) and API that was using this historical data for building reports on the fly.
I'd love to work with remote-first companies, products preferably. I love to work with people, asking the questions and getting the answers. I prefer not communicating in russian."
data engineer,"7+ years experience as SQL/BI developer / Data Engineer
(Healthcare/Banking/OpenERP/Advertising/Sales/Finance)
Skills: MS SQL, Snowflake, Azure, PostgreSQL, Pentaho Report Designer, Pentaho Integration Server, Google Cloud Platform, NoSQL, Hive, Power BI, Tableau, Git, ETL/ELT, Visual Studio, SSRS, Stimulsoft, Inform√°tica, Power Center.
Solid understanding of BI.
Basic knowledge of C#, C++, JavaScript, HTML, CSS.
Business Intelligence  - Epam Academy"
data engineer,"–£ —Ä–∞–º–∫–∞—Ö –≥—Ä—É–ø–æ–≤–æ–≥–æ –ø—Ä–æ–µ–∫—Ç—É —è —Ä–æ–∑—Ä–æ–±–ª—è–ª–∞ ETL-pipeline, —â–æ —Å–∫–ª–∞–¥–∞–ª–∞—Å—è –∑ AWS Lambda —Ñ—É–Ω–∫—Ü—ñ–π, —è–∫—ñ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏—Ç—è–≥—É–≤–∞–ª–∏, –æ–±—Ä–æ–±–ª—è–ª–∏ —Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞–ª–∏ –¥–∞–Ω—ñ –∑ –æ–ø–µ—Ä–∞—Ü—ñ–π–Ω–æ—ó –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –≤ —Å—Ö–æ–≤–∏—â–µ –¥–∞–Ω–∏—Ö. –Ø –Ω–∞–ª–∞—à—Ç–æ–≤—É–≤–∞–ª–∞ –¥–≤–∞ AWS S3 buckets –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è ""ingested"" —Ç–∞ ""processed"" –¥–∞–Ω–∏—Ö, –∑–∞–±–µ–∑–ø–µ—á—É—é—á–∏ —ó—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—ñ—Å—Ç—å —Ç–∞ —ñ–º—É—Ç–∞–±–µ–ª—å–Ω—ñ—Å—Ç—å. –ó–∞–π–º–∞–ª–∞—Å—è —Ä–µ–ª—è—Ü—ñ–π–Ω–∏–º –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è–º –¥–∞–Ω–∏—Ö –¥–ª—è data warehouse, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ star schema, —Ç–∞ —Ä–æ–∑—Ä–æ–±–∫–æ—é ETL –ø—Ä–æ—Ü–µ—Å—ñ–≤ –¥–ª—è –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è —Ç–∞–±–ª–∏—Ü—å —Ñ–∞–∫—Ç—ñ–≤ —Ç–∞ –≤–∏–º—ñ—Ä—ñ–≤. –ê–≤—Ç–æ–º–∞—Ç–∏–∑—É–≤–∞–ª–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –ø—Ä–æ—Ü–µ—Å—ñ–≤ –∑–∞ —Ä–æ–∑–∫–ª–∞–¥–æ–º —Ç–∞ —Ä–µ–∞–≥—É–≤–∞–Ω–Ω—è –Ω–∞ –ø–æ–¥—ñ—ó, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ AWS Cloudwatch –¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º—É–≤–∞–ª–∞ –¥–∞–Ω—ñ —É —Ñ–æ—Ä–º–∞—Ç Parquet –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è –≤ AWS S3 —Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤ data warehouse. –ó–∞–±–µ–∑–ø–µ—á–∏–ª–∞ —è–∫—ñ—Å—Ç—å –∫–æ–¥—É, –Ω–∞–ø–∏—Å–∞–≤—à–∏ –º–æ–¥—É–ª—å–Ω—ñ —Ç–∞ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ–π–Ω—ñ —Ç–µ—Å—Ç–∏, –¥–æ—Ç—Ä–∏–º—É—é—á–∏—Å—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ñ–≤ PEP8, —Ç–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ pip-audit —Ç–∞ bandit –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –±–µ–∑–ø–µ–∫–∏. –ê–≤—Ç–æ–º–∞—Ç–∏–∑—É–≤–∞–ª–∞ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è —ñ–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º Terraform —Ç–∞ CI/CD. –°—Ç–≤–æ—Ä–∏–ª–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –¥–∞–Ω–∏—Ö –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –∑ data warehouse –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞–º —Ç–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞–ª–∞ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ—Ü–µ—Å—ñ–≤, –ª–æ–≥—É–≤–∞–Ω–Ω—è —Ç–∞ –æ–ø–æ–≤—ñ—â–µ–Ω–Ω—è —á–µ—Ä–µ–∑ Email —É –≤–∏–ø–∞–¥–∫—É –ø–æ–º–∏–ª–æ–∫.
–ï—Ñ–µ–∫—Ç–∏–≤–Ω–∞ —Ä–æ–±–æ—Ç–∞ –≤ –∫–æ–º–∞–Ω–¥—ñ;"
data engineer,"- Big Data stack - Apache Spark, Kafka, Hadoop, Cassandra, HBase, Flink, AWS EMR, Airflow, Luigi
- Web with Scala - Play Framework, Akka-Streams, Akka-Http, Spray, Slick, Quill
- Scala FP tools - Cats, Algebird and slightly Scalaz and Eff
- Enterprise with Java - Java EE - Servlets API, JSP, JPA - Hibernate, and Spring stack - Boot, Security, MVC, WS, Data;
- General knowledge of Go and Erlang
- Cloud technologies - AWS;
- DevOps stuff usage - Docker, Jenkins, ELK, Ansible, Grafana/ Graphite
- Hands-on leveraging of DBMS: PostgreSQL, MongoDB, Cassandra, Redis, MySql
I'm an enthusiastic Scala developer with an entrepreneurial mindset and always a source of new and innovative ideas. Academically well qualified, with the scope and ambition to continue improving on an intellectual and commercial level.
I have a desire to make a top-notch software and persevere to work on creating high-quality software with leveraging cutting on the edge stuffs like Scala. I hate tedious tasks and work with a bad code base. I would like to collaborate with a team of easy-going and talented software engineers."
data engineer,"I have extensive experience as a data engineer, data scientist, and co-founder. My most recent role as Co-founder & Chief Product Officer (CPO) at KenesAI involved leading product development and leveraging virtual environments to simulate human behavior for faster hypothesis testing. I also secured a $200,000 investment through the 500 Global Flagship Accelerator and built strategic partnerships with major brands like Yamaha, Chanel, and Coach.
Prior to this, I worked as a Senior Data Engineer at Fairo App, where I reduced data errors by 40%, developed scalable data pipelines, and improved data processing times by 40%. My role also involved implementing GDPR-compliant policies and streamlining data operations, reducing manual workload by 60%.
As a Data Scientist at Andersen Lab, I developed computer vision resume_classifier for rapid antigen testing and created customer segmentation solutions for e-commerce. My earlier roles at Werush Co and Sergek Development Ltd focused on customer segmentation, time-series forecasting, and environmental data analysis, achieving a 90% accuracy rate in emission detection.
Secured a $200,000 investment through the 500 Global Flagship Accelerator, one of the most competitive programs worldwide, with only 16 out of 7,000 startups selected.
Successfully launched new product lines at KenesAI, optimized pricing strategies, and entered new markets at scale, partnering with renowned brands like Yamaha, Chanel, and Coach.
Reduced data errors by 40% and enhanced data processing times by 40% through the development of scalable data pipelines and automated quality checks at Fairo App.
Achieved 90% accuracy in emission detection using XGBoost while working at Sergek Development Ltd, contributing to environmental monitoring and urban planning improvements.
Developed customer segmentation resume_classifier that boosted e-commerce engagement and conversion rates at Andersen Lab.
Built a dynamic data warehouse on Google Cloud Platform, enhancing data accessibility and enabling organization-wide analytics.
Established strategic partnerships and advisory board agreements with industry leaders, including executives from Esprit and behavioral science experts from Stanford University.
I am looking for a company to grow with and achieve goals with"
data engineer,"In my experience, I have participated in data processing projects using Python and libraries. My responsibilities include developing machine learning resume_classifier and analyzing large volumes of data to identify correlations and dependencies. In my team, I fulfill the role of a data scientist and analyst, focusing on the development and implementation of complex machine learning resume_classifier."
data engineer,"Data Engineer (Intern-level)
AgroHolding ‚Äì January 2022
Developed data ingestion and transformation pipelines using Python and SQL
Consolidated scattered agricultural data (crop yield, logistics, inventory) into PostgreSQL
Automated daily data updates and supported reporting for business teams
Improved data quality and consistency across departments
Quickly self-learned ETL tools and built working data pipelines without direct mentorship
Took ownership of complex tasks and delivered them independently from start to finish
Known for being highly reliable: always meet deadlines, communicate clearly, and follow through
Gained trust from analysts and managers ‚Äî my data was used in key business reports
Strong attention to detail and commitment to clean, accurate, and maintainable data work
I‚Äôm looking for a team with a healthy, friendly atmosphere where people take their work seriously and strive for the best results. I value collaboration, mutual respect, and a strong focus on delivering high-quality solutions."
data engineer,"Dataforest
June 2022 - now
Position: Data Engineer at dropship.io
Responsibilities:
Creating an architecture for scraping data, its development and support of a highly loaded scraping/data processing system
Detailed competitor analysis to improve the data provided
Management of databases with a large amount of data for efficient operation with its:  ElasticSearch cluster (~1.5TB), Postgres (~3.1TB), Kafka (~2TB)
Optimization of the existing code base to reduce the cost of server capacity
May 2021 - June 2022
Position: Scraping expert/Data Engineer
Responsibilities:
development of scrapers, integration systems, dashboards, ETL/ELT systems
Evo Company
July 2019 - March 2020
Position: Backend Developer
Responsibilities:
development and support of localizations/translations application for internal usage.
ensuring stable operation of web application (kabanchik.ua)
development and support of web application (kabanchik.ua)
Hard tasks and opportunity to develop myself."
data engineer,"Design, develop, and maintain scalable data pipelines using Python, Spark, Kafka, and Apache Flink.
Implement ETL processes using Airflow to automate data workflows.
Optimize and manage databases, including Greenplum, Impala, MS SQL, and Postgres SQL.
Develop and optimize complex SQL queries using PL/SQL, T-SQL, and other SQL variants.
Collaborate with data scientists and analysts to understand data requirements and deliver solutions that meet their needs.
Monitor and troubleshoot data pipelines and systems using Grafana and other monitoring tools.
Implement data governance and ensure data quality and integrity across all systems.
Manage version control and collaborate on code using Git.
Use containerization and orchestration tools like Docker and Kubernetes to build and manage scalable data solutions.
Develop and execute unit tests to ensure the reliability and accuracy of data processing and transformation logic.
Stay current with industry trends and best practices in data engineering and related technologies.
Develop and maintain data resume_classifier and database schemas to support application development and data analysis.
Utilize LogiX to design and create data visualizations, reports, and dashboards.
Collaborate with cross-functional teams to gather and analyze requirements, ensuring that database solutions align with business needs.
Troubleshoot and resolve database issues, including performance problems and data inconsistencies.
Develop and execute unit tests to verify database functionality and performance.
Document database designs, processes, and best practices."
data engineer,"Python Data Engineer | 4+ Years of Experience
I specialize in designing scalable data architectures, ETL processes, data warehouses (DWH), and data transformation pipelines while ensuring high data quality.
Extensive experience with cloud-based systems (GCP, AWS).
Proficient in machine learning techniques and tools (Scikit-learn, clustering, classification).
Expertise in web scraping and automation."
data engineer,"Data Engineer / QA (DW/BI Testing)
Worked on a Business Intelligence (BI) project with a focus on Data Warehouse (DW) and BI testing:
- Analyzed data, wrote SQL scripts, validated BI reports, and performed issue tracking.
- Demonstrated strong analytical thinking and proficiency in key skills including data cleansing, data analysis, and data visualization (Power BI).
- Gained experience with Jira and Confluence, with a solid understanding of Agile methodologies."
data engineer,"I am 4th year student at Artificial Intelligence. I worked as a 1L Technical Support (SQL) in leading medicine distributor company.
Technical Support (SQL). Optimapharm, LTD. October 2024 - April 2025 (6 months).
- Solve company employees problems with warehouse management system programs (5 programs and one customer site).
- Handling tasks from different databases and servers.
Overall 3500 servicedesk tasks completed.
- Using scripts; tracing problems; looking for them in procedures; communication with different departments.
- Monitoring sql jobs on servers at night time, approximately 7 days in a month.
- Work in an understaffed team, 4 employees out of 8 needed.
Also, I have theoretical experience in Data Science gaining through self-studying and university. I want to make this experience practical, and work, grow in this field."
data engineer,"Deep Learning Data Engineer
at Continental Autonomous Mobility KFT
- develop and execute Extract-Transform-Load processes on image and label data with Python
- develop and execute Quality Check for the data to be loaded.
- Work with SQL and NOSQL dbms (Mongo, PostgreSQL)
- Get familiar with AWS( work with AmazonS3 data storage)
- Get familiar and work with GPFS
- Use Airflow for day-to-day batch data workflow.
Junior Python Developer
at Kapital Bank
Gained general knowledge about machine learning and AI
Learned about MVC(Model-View-Controller) pattern
Became familiar with Web frameworks of Python, such as Django and Flask
Used to write services for Face Recognition App in Django and Flask"
data engineer,"Dear Hiring Manager,
I am developer with 6+ years of experience
Result oriented and qualified data engineer with hands-on experience in:
* Developing and maintaining ETL processes.
* Designing and building BI platform and DWH.
* Designing and implementing microservices and solutions for DataOps
* Have experience with AI/ML
* Good knowledge and experience in software development, including such additional
skills as resolving product or service problems, determining the cause of the
problem, diagnosis of non-obvious problems and finding solutions for solving them
* With excellent problem solving skills, multitasking, team player with strong coaching and
mentoring skills, ability to develop creative solutions for complex problem"
data engineer,"* Data modelling, Designing DB schemes
* Python, PostgreSQL, Linux, Git, Apache Airflow
* Amazon Web Services (AWS: EC2, S3, Lambda, Redshift)
* Apache Kafka, Spark, PySpark, SparkSQL
* Google Cloud Platform (GCP: BigQuery)
* Data pipelines, ETL, dbt (data build tool),
* DWH, Data Lake, BI (Tableau), ERP, CRM
* OOP, API, JSON, Backend, HTML, CSS
* SQL, MySQL, Oracle PL/SQL, T-SQL
15+ years of development experience; Over 25 successful projects were in areas: software R&D, Internet-portal, bank, stock exchange, wholesale / retail trade, logistics, warehouse;
Ability to learn complex technical material and troubleshooting complex problems; Ability to work independently; Willingness to learn new skills; Interest in technical progression and growth; 10-finger method skill;
Adequate management. Professional growth"
data engineer,"I have work experiences with creating procedures, functions, views, mviews, jobs, tables, indexes, packages.  Working with REST API,  SOAP-API. ETL processes,  XML, JSON.
Creating sites using Oracle APEX  (apex collections, autentification, autorization, multi languages, dynamic SQL)  .
I have also  work experience with  Python/Django,  Celery, RabbitMQ
–ú–∞—é –±–∞–≥–∞—Ç–∏–π –¥–æ—Å–≤—ñ–¥ —Ä–æ–±–æ—Ç–∏ —è–∫ PL/SQL, Apex —Ä–æ–∑—Ä–æ–±–Ω–∏–∫, –∞ —Ç–∞–∫–æ–∂ –æ–∫—Ä–µ–º–∏–π –¥–æ—Å–≤—ñ–¥ —Ä–æ–±–æ—Ç–∏ –∑—ñ —Å—Ç–µ–∫–æ–º —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ–π Python/Django —è–∫ —Ñ—É–ª—Å—Ç–µ–∫ —Ä–æ–∑—Ä–æ–±–Ω–∏–∫. –ü–æ–∑–∏—Ü—ñ—è Data Engineer –¥–æ–∑–≤–æ–ª–∏—Ç—å –º–µ–Ω—ñ –ø–æ—î–¥–Ω–∞—Ç–∏ —Ä–æ–±–æ—Ç—É –∑ –¥–∞–Ω–∏–º–∏,  Python —Ç–∞ –º–æ–∂–ª–∏–≤–æ –Ω–∞–≤—ñ—Ç—å –≤ —è–∫–∏—Ö–æ—Å—å –∞—Å–ø–µ–∫—Ç–∞—Ö –≤–µ–±-—Ä–æ–∑—Ä–æ–±–∫–∏, —á–æ–≥–æ —è —ñ –ø—Ä–∞–≥–Ω—É..."
data engineer,"At TripMyDream, I worked on building data-driven solutions to optimize user experience and support business decision-making. My responsibilities included:
- Designing and developing machine learning resume_classifier to predict flight prices and optimize travel packages.
- Creating crawlers for data collection and integrating external data sources into internal databases.
- Developing interactive chatbots for platforms like Facebook, Telegram and Slack.
- Conducting data analysis for marketing and SEO optimization.
- Working with BigQuery, Clickhouse, and Tableau for processing and visualizing large datasets.
In the team, I acted as a key Data Scientist, collaborating with developers, analysts, and designers to deliver high-quality solutions. My role involved the entire project lifecycle‚Äî from requirement gathering to deployment and support.
- Developed a flight price prediction model with 84% accuracy, enhancing user decision-making.
- Created a dynamic travel package optimization model with 91% accuracy, increasing booking efficiency.
- Improved SEO and marketing performance through data-driven strategies, leading to higher traffic and conversions.
- Designed and implemented chatbots for enhanced user engagement on platforms like Facebook, Telegram and Slack.
- Successfully utilized BigQuery and Clickhouse for querying and analyzing large datasets, and Tableau for actionable data visualizations.
- Played a critical role in implementing machine learning solutions for complex datasets, helping the business achieve smarter operational decisions.
- I want to work on large-scale projects with opportunities to design and implement analytical platforms and architectures for Big Data.
- I expect to utilize modern technologies and cloud solutions to create high-performance and scalable systems.
- I am eager to join a team that values transparent communication, innovation, and results-driven approaches.
- I am interested in projects where I can have a tangible impact on the final outcome, leveraging machine learning and analytics to enhance business processes.
- I look forward to expanding my technical expertise and sharing knowledge within a collaborative team environment.
- I am not interested in monotonous data analysis tasks without the opportunity to apply or implement new technologies and tools.
- I prefer to avoid environments lacking a clear strategy for project or team development.
- I am not drawn to tasks with no potential impact on the final product or without visible results.
- I do not want to work in settings where team values do not align with professional ethics or an innovative approach."
data engineer,"I worked on Risk Domain where I supported Risk Financial Regular Reporting Execution, Reporting Automation as well as ad-hoc data preparation according to stakeholder management and business requirements. This role not only cover SQL scripting but also covers effective communication, clear understanding and collaboration with stakeholder management. I also contributed to Data Quality improvement to maintain reporting accurately and timely delivered. Later, I started to work as Data Engineer where I supported Data warehouse Unit under AnaCredit Regulatory Reporting Domain. This role requires SQL, datawarehousing and business requirements understanding. I have also experience in fintech environment as a DWH Analyst where I contributed in system requirements specification documentation explaining source-target mapping from Oracle Data Integrator for DWH developments."
data engineer,"Currently, I work in the field of Data Engineering at Kaspi Bank. Developer DWH. We are consolidating data using big data tools like Pyspark, Apache Kafka, Hadoop. My task is to stream data from various sources and send it to DWH or another database.
We also work with cloud services like Google Cloud and AWS.
I want to develop further in this area and learn new technologies.
Graduated Bachelor of Suleiman Demirel University (2020)
I am studying for a master's degree in Computer Science.
Successfully completed the ""Big Data Engineer"" training from Alfa Bank."
data engineer,"Data Engineer January 2024 ‚Äì February 2025
Kodemade Montrouge, France
‚Ä¢ Developed and optimized data pipelines using Spark Scala and Python (ingestion, transformation, loading).
‚Ä¢ Optimized SQL queries and Big Data architectures to reduce processing time.
‚Ä¢ Administered and operated Snowflake, Databricks, and AWS/Azure environments.
‚Ä¢ Orchestrated and monitored workflows using Airflow and Jenkins to ensure pipeline reliability.
‚Ä¢ Implemented monitoring and log analysis for rapid detection and resolution of anomalies.
‚Ä¢ Managed and optimized Dockerfiles for deploying Spark and Airflow jobs.
‚Ä¢ Built real-time data pipelines with Kafka for event ingestion and distribution.
Technologies: Hadoop, Spark, Python, SQL, Snowflake, Databricks, AWS, Azure, Airflow, DBT, Jenkins, Docker, Git, Kafka
BI Consultant March 2023 ‚Äì September 2023
Business Decision | Mission Sanofi Lyon, France
‚Ä¢ Designed and deployed a framework to load data into Snowflake using Informatica Cloud.
‚Ä¢ Built a star schema model for business intelligence analytics.
‚Ä¢ Developed Python scripts to automate data loading into Snowflake via Snowpipe from AWS S3.
‚Ä¢ Created and managed data warehouses and data marts in Snowflake.
‚Ä¢ Transformed data with IICS to clean, enrich, aggregate, and load it into Snowflake data marts.
‚Ä¢ Optimized SQL queries in Snowflake (ELT, caching, parameter tuning).
‚Ä¢ Analyzed tickets, prioritized fixes, and continuously improved pipelines to meet business requirements.
‚Ä¢ Migrated data from development to UAT environments while ensuring compliance with quality standards.
Technologies: Snowflake, Snowpipe, Snowpark, Informatica Cloud, Jira, SQL Server, PostgreSQL, Excel, Git
Software Developer April 2015 ‚Äì September 2022
SinaTechnologie Port-au-Prince, Haiti
‚Ä¢ Developed backend applications in Java with Spring Boot for complex business systems, improving performance and reliability.
‚Ä¢ Designed a FinTech platform integrating web services with Wildfly and EJB, enabling interoperability with external systems.
‚Ä¢ Authored and validated technical architecture documents aligned with business needs.
‚Ä¢ Automated internal workflows, reducing operational turnaround times.
‚Ä¢ Resolved critical issues and implemented automated tests to enhance application robustness.
‚Ä¢ Conducted active technology watch on Java and Spring frameworks to ensure modern and efficient architectures.
Technologies: Java, Microservices, EJB, Wildfly, MongoDB, Spring Boot, JavaScript, MySQL, Bootstrap, Git"
data engineer,"I have studied in GOIT as a data scientist\ML engineer.
I made several projects such as neural networks for clasification, ML algorithms for defining images, PostgreSQL related projects. The following technologies were used and studied: python, pandas, numpy, seaborn, mathoplotlib, scikit-learn, tensorflow, keras, SQL and so on.
Was a team leader in the project ""Forecasting customer churn in a telecommunications company‚Äù. I have several own pet-projects with different neural resume_classifier such as RNN, CNN, NLP. Also i did projects with ML and data analysis, and some of my personal projects. One of them was used by the university. Here is my github nickname: eughappy
I want to have prospects in work that will provide greater opportunities for development, learning new skills and career growth. Also i want to bring succsess and prosperity to the company"
data engineer,"–ø—Ä–∞—Ü—é—é Data Engineer. –ø—Ä–æ–µ–∫—Ç–∏ –ø–æ –º—ñ–≥—Ä–∞—Ü—ñ—ó, –ø–æ–±—É–¥–æ–≤–∞ DWH, –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è ETL –ø—Ä–æ—Ü–µ—Å—ñ–≤. —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—ñ—Ç—Ä–∏–Ω –ø—Ä–æ–¥—É–º—É–≤–∞–Ω–Ω—è –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ —ñ –Ω–∞–ø–∏—Å–∞–Ω–Ω—è –±—ñ–∑–Ω–µ—Å –ª–æ–≥—ñ–∫–∏. —Ä–æ–∑—Ä–æ–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–Ω–æ—ó —á–∞—Å—Ç–∏–Ω–∏ —Å–∞–π—Ç—É.
–æ–ø—Ç–∏–º—ñ–∑—É–≤–∞–≤ —Ä–∞–Ω—ñ—à–µ –Ω–∞–ø–∏—Å–∞–Ω–∏–π –∫–æ–¥ –∑ 10 –≥–æ–¥–∏–Ω –¥–æ 15 —Ö–≤–∏–ª–∏–Ω
–Ω–µ —Ö–æ—á—É –ø–æ–≥–∞–Ω–∏–π –∫–æ–ª–µ–∫—Ç–∏–≤"
data engineer,"SimplyBook.me ‚Äì Customer Representative Agent
March 2023 - present
As a dedicated Customer Representative Agent at SimplyBook.me, I had the privilege of working with diverse clients from around the world, ensuring their utmost satisfaction by addressing their needs and requests promptly and effectively. SimplyBook.me has allowed me to cultivate a unique blend of customer-centricity, technical sharpness, and effective collaboration. I am dedicated to continually enhancing user experiences and driving the evolution of cutting-edge product.
Lviv IT Cluster ‚Äì Project manager
February 2021 - November 2021
I worked in an event department, helping our company within the organization of the biggest tech conference in Eastern Europe also coordinated the first and the biggest vaccination centre in Lviv and managed the maintenance of internal events with the Cluster's CEO members. My key achievements are: - Coordination/Managing Tech communities, worked closely with the PR team and CEO
- Coordinated United for health project
- Coordinated the work of the vaccination centre
- Established communication with clients
- Hunted May Muck as a keynote speaker for IT Arena
- Organized IT Arena from the very beginning (4000+ participants)
- Organize facilities and manage all events details such as decor, catering, entertainment, transportation, Location, invitee list, equipment, promotional material etc;
- Maintenance of CRM
- Technical support of two sites, worked closely with design and tech teams
- Was in charge of payments for services used by the company, worked closely with CAO
- Cooperation with sales and marketing teams to publicize and promote the event
AIESEC ‚Äì Outgoing Global Volunteer manager
September 2018 - May 2018
- Contacting people who fill out internship applications
- Assistance with project selection
- Communication with the foreign side
- Concluding and signing contracts with applicants
- Been a speaker at AIESEC's PR events
COURSES
Datacamp: Data Engineer with Python.
Udemy: Easy to Advanced Data Structure.
Prometheus: Basics of programming CS50 (EdX Harvard course), Basics of Web UI
development.
Coursera: Design thinking for innovations, Learn how to learn.
Codecademy: Introduction to HTML, Learn CSS, Introduction to JavaScript, Learn the
Command Line, Learn Git."
data engineer,"BICS (over INFOPULSE)                                                                 2022 - 2025
BICS carries around 50% of the world's data roaming traffic and partners with more than 500 mobile operators. Network includes over 700 direct connections in 180+ countries, capacity on 75 submarine cables.
In 2021 BICS carried over 20.8 billion minutes of international traffic.
Main responsibilities: Resolving technical issue of international telecom operators (BICS‚Äôs clients and supplier) on  Fraud, Voice, Messaging, Signaling.
Key competencies:
Fraud prevention. Detection, analysis, blocking (FAS, IRSF, Wangiri, PBX hack, CLI Spoofing, GT Spoofing, Smishing, Malware, IPX Security, etc.)
Voice.  Investigation client's complaint, call flows analysis (roaming, MNP), supplier testing, rerouting, blocking,  QoS analysis (ALOC, ASR(%), NER(%), PGD).    Protocols SIP, ISUP, MAP, MNP, INAP, RTP. Numbering plans E.164, E.212/214. Scenarios: International call, Roaming, MNP
Messaging.  Investigation client's complaint, SMS flows analysis, supplier testing, rerouting, blocking,  Sender ID registration, QoS, etc.  Protocols SMPP, SS7.  P2P/A2P rerouting management.
Signaling. (SS7/SCCP, LTE/Diameter/DRA, Sigtran,  signaling call flows and roaming scenarios).
Big Data Analysis, Business intelligence.  Analysis of large data volumes (up to 1 million records), search for patterns and abnormalities. Analysis of frequency, sequence, overlaps. Creation of unique dashboards. (Excel, Power BI, Splunk, QlikSense)
Network and DB fundamentals. SQL, TCP/IP, UDP, SCTP, SIGTRAN (M2PA/M3UA),  TDM
Platforms: SMS HUB (Openmind), Dialogic/Veraz, Ribbon/Sonus, Titan (NetNumber), Empirix (InfoVista), FraudGuard (Amdocs)
Professional trainings:
Linux Red Hat. System Administration.
Amazon Cloud (AWS) Core.  (VPC, EC2, IAM, RDS,  S3, Route 53)
DevOps Intro  (AWS, Git, Terraform, Jenkins, Apache Tomcat, Docker)
Power BI Advanced.
Reference:
Andrii  Ivanov,  DM,  andrii.ivanov_infopulse.com  +44-7867-109724
Alexandre Ameloot, PM,  alexandre.ameloot_bics.com  +32-473-196863"
data engineer,"Aspen Technology Labs, Inc.
08/2024 - Present
Achievements/Tasks:
- I am second (replacement person) after the person
responsible for the largest account for the date collection.
- I process big data by scraping, normalizing, and posting it.
- I support about 1500 web scraping configurations.
- I am responsible for some of the most difficult ATS for
web scraping in our direction. Made the most optimal configuration for scraping these ATS for now.
Basic tools:
Python, RegExp, API Requests, Postman, Java Velocity, XML/XSLT.
FREELANCE TEAM
03/2024 - 08/2024
Achievements/Tasks:
- We worked on orders from users of freelance platforms.
- I wrote various scripts for web scraping, browser automation, and basic data analysis.
Basic tools:
Python, API Requests, BeautifulSoup, lxml, Selenium Web Driver, NumPy, RegExp, XPath, SQL
I'm a student at National Aviation University, currently in my third year of studying in Computer Engineering. My studies are in full-time mode. I diligently study English and aim to achieve a high level of proficiency.
I want to participate in really interesting projects with data scraping and data processing.
I also expect the employer to provide opportunities for further career growth both as a Data Scientist. I currently learn this profession and aim to further develop in it."
data engineer,"Junior data scientist
* Comparing advantages of usage preventive maintenance for high loaded vehicles, other than while equipment failure occurs.
* Project using Bayesian networks (health field)
Technologies: Python(pandas, numpy, matplotlib), openAiGym, Reinforcement learning.
Big Data Analyst / Engineer
* Exploratory data analysis
* A/B tests
* Visualization
* AWS data services
* Building tools for data quality and monitoring
* HTTP
* Monitoring data completeness, reporting and fixing gaps
* Setting up Monitoring dashboards
Big Data Engineer
* Building ELT/ETL pipelines
* Creating Airflow dags
* Dockerizing solutions
* Spark
* AWS (EMR, S3, Athena, Redshift, ECR, ECS)
* Gitlab CI
Technologies:
Python(pandas, matplotlib, numpy), Grafana, Airflow, Spark, AWS ( Athena, Redshift, EMR, ECS, S3), PostgreSQL, MySQL, Gitlab, Jira, Confluence.
Pet projects:
* weather service (Django, postgresql, docker),
* Analysis car prices (SQL, bigquery, beautifulsoup, pandas, bumpy, matplotlib, sklearn).
Automatisation regular tasks, which are part of production. Implementing algorithm for outliers detection, good performance, high roc auc score."
data engineer,"Senior Analyst with a strong foundation in model validation, statistical analysis, data pipelines and AI-driven solutions, backed by over 4 years of experience in marketing analytics.
Proficient in Python, SQL, Power BI, and Azure ML, with hands-on experience developing AI agents using LLMs,
RAG pipelines, and LangChain. Adept at building end-to-end data solutions, from data ingestion and ETL pipelines to model deployment using Azure and Docker. Proven track record in leveraging data to drive business growth, optimize marketing strategies, and deliver scalable AI solutions
Languages and Libraries:
Python, SQL, PySpark, Pandas, NumPy, SciPy, Statsmodels, Matplotlib, Seaborn, XGBoost, LangChain, Langgraph
Data engineering:
PostgreSQL, MySQL, MongoDB, Snowflake, BigQuery, FAISS DB, Vector Databases, Airflow, ML flow, Azure, Aws
Visualization:
Power BI, GA4, Tableau, Looker"
data engineer,"I have worked on multiple data engineering and machine learning projects:
Milan Telecom: Developed a scalable data pipeline for processing telecom data using Apache Airflow, PySpark, and SQL. The pipeline automates data ingestion, transformation, and aggregation to generate meaningful insights, visualized through analytical charts.
Salary Prediction System: Built a machine learning model to predict salaries based on experience, education, and job type. Used Random Forest, Gradient Boosting, Python, Docker, and Dask to develop and deploy the system.
Gained hands-on experience with Apache Airflow, PySpark, SQL, Docker, and cloud technologies in data engineering projects. Designed and implemented scalable data pipelines, improving data processing efficiency."
data engineer,"I study Data Engineering and have experience working with such tools as SQL, Python, Apache Airflow, PostgreSQL, Git, ETL/ELT, Pandas, AWS, GCP and other data engineering tools.
I have 6 month experience in building an ETL data pipeline for the automatic processing of prices from suppliers.
My Projects
1. Price Update
ELT process of updating price lists from suppliers. At my current workplace, I save up to 1.5 hours a day on updating product stocks on the website. (Python, PostgreSQL, S3, IAM, Pandas, Airflow, VPS server, Git)
2. Employee Data
ETL pipeline for processing and visualizing employee data. (GCP, SQL, Cloud Storage, BigQuery, Looker, Data Fusion, Cloud Composer, Python, Airflow)
3. Global Health Data
ELT data pipeline to process 1 million records. (GCP, Computer Engine, Python, Airflow, SQL, Cloud Storage, BigQuery, Looker )
4.Housing Market Analysis
ETL data pipeline for processing and analyzing the housing market. (AWS, S3, VPC, EMR, Jupyter, Pyspark)
For the last 5 years, I have been the founder and owner of the online travel equipment store outfitter.in.ua, where I currently work.
Before that, for 10 years, I held the position of sales manager for spare parts for trucks and special equipment.
For my current place of work (outfitter.in.ua), I created an ELT data pipeline to update product stock on the website, which saves 1.5 hours daily.
I am looking for an interesting project where I can work for a long time, have professional growth, a good Salary and a friendly team."
data engineer,"my experiences are the following
‚Ä¢ Implemented cost-effective data pipelines that enabled efficient data ingestion into our centralized data store. This streamlined the development process for the credit scoring model.
‚Ä¢ Used Kedro as a framework to document data workflows and display lineage for our product, ensuring smooth maintenance and scalability in the future.
‚Ä¢ Revamped a legacy ETL pipeline by leveraging Python, AWS Glue, and AWS Athena. Optimized queries and adopted the Parquet columnar format, resulting in a 40% reduction in data loading time.
‚Ä¢ Maintained various Airflow-automated ETL scripts, including an ingester script that triangulated 20GB of log data every 3 minutes for downstream analytics and reporting.
‚Ä¢ Automated biweekly revenue and team value reports given during management meetings.Validated various sources, cutting report generating time by 30% and minimizing manual work.
‚Ä¢ Constructed an AWS S3 data store to act as the company‚Äôs single source of truth. Reduced data discrepancies, obsolete pipelines, and enabled accurate decision-making."
data engineer,"In my current role as a Senior Data Engineer, I have led several impactful projects focused on building and optimizing data pipelines and cloud-based infrastructures. One of the key projects I recently completed involved deploying Mage orchestrator on an AWS EKS cluster to schedule and manage diverse pipelines. These pipelines ingest data from APIs and databases, perform ETL processes on sensor-based data, and store the processed results in an AWS S3-based data lake. Additionally, I implemented schema management with AWS Glue, making data queryable through Athena, which supports analytical dashboards used for decision-making across the organization.
Technologies I regularly work with include Python, Kubernetes (EKS), AWS Glue, Athena, Kafka, and Terraform. I‚Äôve also built real-time streaming pipelines with Apache Kafka and enabled horizontal autoscaling to handle varying workloads efficiently. Additionally, I designed a data warehouse solution on AWS and deployed microservices supporting thousands of daily loan applications.
Currently, I play a pivotal role in the team, bridging data engineering with DevOps practices. I ensure the scalability, performance, and quality of pipelines, mentor colleagues on best practices, and collaborate across teams to align technical solutions with business needs.
Looking ahead, I aim to deepen my expertise in data modeling for AI-driven applications and further optimize cross-team collaboration to deliver even more impactful, human-centered data solutions.
Here‚Äôs a summary of your accomplishments:
1. Loan Product Development:
Successfully led the design, implementation, and launch of two loan products‚ÄîDevice Financing for women and Inventory Financing‚Äîby defining data requirements, ensuring accuracy, and building scoring infrastructure.
2. Scalable API Development:
Built a high-performance API using FastAPI, MongoDB, and Feast, supporting over 1 million customers and processing 700,000 monthly loan applications, contributing to a loan portfolio of 11 billion Birr in disbursements.
3. Real-Time Data Streaming:
Deployed a real-time data streaming pipeline on AWS using Apache Kafka, Kafka Connect, and Debezium, efficiently capturing and streaming MongoDB changes to S3.
4. Big Data Processing:
Designed a Spark pipeline to process large utility datasets and store transformed results in Feast, enhancing model accuracy and enabling data-driven decision-making.
5. Pipeline Orchestration:
Transitioned from Airflow to Mage.ai, optimizing pipeline orchestration and introducing data quality checks, significantly improving execution efficiency.
6. Monitoring and Alerting:
Established centralized monitoring and alerting systems using Grafana and Prometheus, creating dashboards for microservices‚Äô performance and ensuring operational resilience.
7. Data Integration and Governance:
Partnered with banks to facilitate data integration, establish governance protocols, and define critical data points for credit scoring resume_classifier.
8. Real-Time Reporting:
Automated data aggregation pipelines across banks, delivering hourly-updated dashboards to monitor loan disbursements and support decisions on delinquency management, compliance, and marketing.
9. Data Lakehouse Architecture:
Implemented a data lakehouse architecture using AWS S3, Athena, EKS, and Glue, enhancing data storage, retrieval, and processing.
10. Data Quality Tooling:
Integrated Great Expectations into pipelines for comprehensive data validation and profiling.
11. Custom Python Package:
Developed a Python package to manage data securely within AWS Glue, S3, and EC2, empowering the data science team with better data governance.
I'm also certified in AWS cloud practitioner."
data engineer,"Work Experience:
I have worked on a variety of projects ranging from developing mobile casino games to creating and optimizing big data analytical pipelines. Initially, I started as a Java developer working on unique mobile casino games with challenging tasks. Later, I transitioned to a Big Data role, developing ETL pipelines and data back-end services using technologies such as Scala, Spark, Kafka, Akka, and Cassandra, all hosted on AWS cloud.
In my current role, I enhance and migrate ETL pipelines to new technologies, optimize Spark jobs for performance and stability, and actively participate in architectural discussions and code reviews. The technologies I primarily use include Scala, Spark, Cats, HDFS, AWS, kubernetes, Airflow
My Role in the Team:
I play a key role in developing and optimizing data solutions, contributing to both technical development and strategic discussions. I ensure the reliability and scalability of our data processes, and work closely with my team to implement best solutions.
I would be happy to find and join a project related to Big Data analytics. Feel enthusiastic and passionate to work with Spark-NLP, LLMs and machine learning.
Seeking a challenging role that leverages my expertise in Apache Spark while allowing me to contribute to and expand my skills in Spark-NLP, Large Language Models, and machine learning."
data engineer,"‚Ä¢ Experience in working with AWS (S3, Lambda, CloudWatch, Glue, Athena, Redshift, Quicksight, ERM, DynamoDb)
‚Ä¢ Experience using GCP (Bigquery, pub/sub, composer, Cloud Storage, Cloud Functions, Data Proc, Looker)
‚Ä¢ Experience in working with Databricks
‚Ä¢ Experience with Hadoop, pyspark
‚Ä¢ Development of ETL: Airflow, GCP scheduler
‚Ä¢ Database design and implementation: Oracle, Bigquery
‚Ä¢ SQL-tuning;
‚Ä¢ ETL performance tuning;
‚Ä¢ Development of Oracle PL/SQL procedures;
‚Ä¢ Experience developing on Python(functional programming, OOP)
‚Ä¢ NoSql(MongoDb, DynamoDb)
‚Ä¢ Web page parsing using Selenium
‚Ä¢ Experience using bash & PowerShell
‚Ä¢ Experience using Linux
‚Ä¢ Debugging
‚Ä¢ Experience with Git
‚Ä¢ Experience using Power Bi, Power Automate
‚Ä¢ Troubleshooting, problem solving.
Currently working as Senior Data Engineer at Globallogic
Help company to save half a million annually through restructuring the infrastructure."
data engineer,"Data Engineer based in Warsaw with 3 years of previous experience in working with Data. Building upon my previous
experience in Data Analytics and Data Engineering, I am now focused on further growth and development in this field.
Proficient in Python, SQL, Spark/Kafka, and AWS/Azure, I possess hands-on experience in constructing data pipelines
and developing data warehouses. I am eager to leverage these skills to contribute to the success of data driven projects
and expand my expertise in Data Engineering."
data engineer,"AM-BITS
Data Engineer
As a Data Engineer at AM-BITS, I design and implement data processing solutions using technologies such as PostgreSQL, DBT, Athena (AWS), MySQL, SQL Server, Apache NiFi (ETL tool), Apache Flink (SQL), and Python (BeautifulSoup). My role involves managing data and ensuring data quality, preprocessing data, performing business intelligence tasks, and creating SQL procedures.
Raiffeisen Bank
Data Analyst
At Raiffeisen Bank, I was responsible for creating data logic, mapping, and SQL code for the developer team. My duties included testing new data logic and tables, ensuring data quality, working with data warehouses and Customer Data Platform (CDP), and interacting with business units to resolve issues. I used SQL Oracle, Oracle Data Integration, Git, Jira, Confluence, and Excel.
FavBet (part-time)
Accounting Analyst
At FavBet, I created solutions for fast calculations and analysis of event data, including esports events. My responsibilities included finding insights and patterns, optimizing processes, working with various departments, solving event-related issues, and training new employees. I used tools like Outlook, Excel, Word, Microsoft Teams, and ChatGPT.
SoftServe
Junior Data Engineer (full day)
At SoftServe, I worked on designing and creating databases, manipulating data, and ensuring data quality. My experience included working with various data formats (JSON, XML, CSV, XLSX), designing and testing new pipelines, creating views and stored procedures, and using tools such as Git, Jira, and Confluence. I worked with SQL Server, Snowflake, Hadoop, PySpark, ETL, and ELT Integration Services (SSIS) packages, and was involved in business intelligence tasks.
Responsibilities:
Designing and developing logical data resume_classifier and SQL code for the development team, ensuring alignment with business requirements.
Conducting thorough testing of new data resume_classifier and tables to guarantee accuracy and functionality.
Managing data quality and performing data manipulation and preprocessing to support analytical needs.
Collaborating with business stakeholders to resolve issues and provide data-driven insights.
Utilizing Oracle SQL and Oracle Data Integration tools for effective data management and integration.
Documenting data mapping and transformation processes to support ongoing data analysis and reporting.
Achievements:
Improved data accuracy and reporting efficiency by developing and implementing new logical data resume_classifier.
Streamlined communication between business and technical teams, facilitating quicker issue resolution and project delivery.
I am looking for a role where I can further develop my skills as a Data Engineer and work on challenging and innovative projects. I am particularly interested in opportunities that involve:
Cloud Technologies: Gaining hands-on experience with cloud platforms such as AWS, Azure, or Google Cloud to enhance my expertise in cloud-based data solutions.
Advanced Data Engineering: Engaging in complex data engineering tasks, including designing and optimizing data pipelines, and leveraging modern ETL tools and techniques.
Cutting-Edge Tools and Technologies: Working with state-of-the-art technologies and tools to stay at the forefront of the industry and contribute to impactful projects.
Collaborative Environment: Being part of a team that values collaboration, knowledge sharing, and continuous learning, where I can contribute to and learn from others."
data engineer,"Data Engineer with 10+ years of commercial experience.
Worked in outstaff, outsource and product companies.
Details of experience:
DB: Amazon Redshift, MS SQL Server, Aurora SQL, Oracle, MySQL, Dynamo DB
Technologies: SQL, PL/SQL, T-SQL, AWS services, Python, DBT, Airflow, Databriks, Fivetran, ETLeap, LookML, DAX, ETL, SSIS, Talend
Reporting: Looker, Power BI, SSRS, Apex
Tracking: Asana, Jira, Confluence, Redmine, SVN, TFS
Stable and long-term project"
data engineer,"1648 Factory ‚Äì Coastal Tourism
Senior Data Engineer | Data Scientist | Business Analyst (April 2024 ‚Äì Present)
‚Ä¢	Collaborated with a German company and three clients to gather requirements, set deadlines, and align tasks.
‚Ä¢	Automated monitoring of regional parameters via Marine Traffic API, increasing update frequency from 1x/day to 12x/day.
‚Ä¢	Streamlined SharePoint file processing, reducing manual effort by 64%.
‚Ä¢	Designed backup architecture, boosting data transfer speed by 10x and enabling daily updates.
‚Ä¢	Managed FTP access permissions, ensuring secure and efficient user management.
‚Ä¢	Provided data insights for reports to support client decision-making.
Technologies: Python, PySpark, MS SQL, SharePoint, Power BI, Bash, SSIS, pandas, pyodbc
‚Ä¢ Developing and supporting REST API
‚Ä¢ Optimization and refactoring of code to improve performance and lead to uniform style
‚Ä¢  Changing the structure of the database
‚Ä¢ Creating reports on the work done and monitoring the timely updating of product documentation
‚Ä¢ Conducting staff training, documentation maintenance
I prefer a long-term contract"
data engineer,"Currently, I am involved in an internal project focused on developing a resources accounting platform. Within this project, I hold the positions of Data Engineer and Data Analyst. My responsibilities primarily involve integrating monthly data into MS SQL and creating Power BI reports. This includes extracting raw data from our OLTP system, performing data cleaning, creating data resume_classifier, and finally visualizing the data. To accomplish these tasks, I utilize various technologies such as Power BI Dataflow, M query, DAX, and T-SQL."
data engineer,"Main responsibilities:
- build new and support existing ETL pipelines (ingest data from different
sources - DBs, GCS, API, files);
- optimize existing bottlenecks (query performance, Airflow DAGs)
- data validation and alerting
- collaborate with data analysts to define data requirements and ensure
data accuracy and quality
- support of teammates"
data engineer,"With a primary focus on the backend, I've excelled in FullStack projects, particularly in the domains of Medical Material Management, E-commerce and Trade. My expertise gravitates towards NodeJS and NestJS, where I've played a pivotal role in architecting and implementing seamless backend solutions using modern frameworks.  This, coupled with my proficiency in ReactJS/NextJS on the frontend, makes me a strong candidate.
I bring a wealth of experience in spearheading Data Collection, Analytics, and Visualization initiatives. My forte lies in warehousing data, harnessing Big Query in Google Cloud for robust analysis, and presenting insights via platforms such as Google Data Studio."
data engineer,"Big Data Engineer with an extensive experience in end-to-end design of reliable, maintainable, scalable Big Data batch processing and ETL pipeline design. Have 5 years of experience in relevant positions.
Implemented overall 4 various Big Data Products from scratch in different architecture patterns (Spark Batch Processing, Event-Driven Architecture et cetera). This always resulted in bringing the single source of truth for the data i provided with my work, dramatic increase of Data Quality transparency and measurements and decrease of AWS bill.
Also I sucessfully mentored several colleagues, both interns and non-technical guys.
Interesting and challenging projects focused on Big Data intensive processing."
data engineer,"I am a Data Engineer with 2.5 years of experience working in a media organization, specializing in managing both structured and unstructured data. My primary responsibilities include reverse engineering websites, developing ETL processes, and ensuring the efficient storage and management of vast datasets in the Cloud. I have extensive experience working with large data leaks and storing them securely, as well as performing network analysis to uncover insights from complex datasets.
In addition to my current role, I have prior experience as a data specialist at an NGO, where I was responsible for web scraping, data analysis, and data visualization.
My diverse background has improved my expertise in handling complex data pipelines using advanced tools and technologies, from extraction to analysis and visualization.
Key Skills:
* Python: Advanced proficiency with 3 years of experience in data processing, automation, and building scalable solutions.
* SQL: Strong command with over 3 years of experience, especially in querying large datasets and optimizing database performance.
* Cloud Platforms: Proficient in Google Cloud for data storage, management, and analytics.
* Version Control: Skilled in Git for collaborative project management.
* API Integration: Experience working with REST APIs to integrate data from various sources.
* Containerization & Orchestration: Competent in Docker and Kubernetes for deploying and managing scalable applications.
* Workflow Automation: Experience with Argo Workflows to streamline data processing tasks.
* Tools: Comfortable using tmux for terminal multiplexing, enhancing productivity in server-based environments.
* Data Visualization: Experience with R for data visualization and statistical analysis.
Languages:
* English (fluent)
* Ukrainian (native)
Speaker at the conferences Dataharvest 2024 (API for Telegram Channels) and Dataharvest 2023 (Command Line)."
data engineer,"Results-driven Data Engineer with 2 years of specialized experience in building and optimizing data pipelines, combining hands-on engineering expertise with a strong analytical background. Demonstrated ability to design and maintain data pipelines while bringing valuable analytical perspective to technical solutions. Fast learner and responsible, I strive for development and improvement of professional skills.
Qualifications:
- Implemented and maintained data pipelines using Python and SQL (AlloyDB, MariaDB)
- Experience with Google Cloud Platform services, particularly GCS, Cloud Run and Cloud Functions, enabling scalable and serverless data solutions
- Working knowledge of Apache Airflow for workflow orchestration and automation
- Leveraged previous data analyst experience to better understand business requirements and data quality needs
- Strong time management skills, with the ability to prioritize and multitask.
- Good communication and creative problem solving skills."
data engineer,"Programming & Tools: Python, Pandas, Jupiter
IDE: PyCharm, Visual Studio Code
Databases: SQL, PostgreSQL, MySQL, Microsoft SQL Server, SQLite, MongoDB
WEB: HTML, CSS, JavaScript, JSON, XML, REST, API
Software Development: Flask, Django
Version Control: Git, Azure DevOps
Testing Tools: Selenium, Pytest, Postman, Swagger
Test management system: TestLink, TestRail
WEB: HTML, CSS, JavaScript, Java, PHP, JSON, XML, REST, API
Network: TCP, IP, DNS, SSL, HTTP
WCMS: WordPress, –úoto3CMS, Joomla
Computer graphics: Photoshop, CorelDRAW (advanced level); Figma, Flash, 3ds Max, AutoCAD, MindCAD, Sony Vegas Pro
Testing Methodologies:
Testing Pyramid; Fundamental testing process; Classification of testing; Project documentation; Basics of Software Quality Management; Documentation and static testing; Test design techniques; Defect reports; Website promotion and SEO optimization; Software Development Life Cycle (SDLC); Software development resume_classifier; Clients-server communication; Basic programming skills.
EXPERIENCE
1. Automation QA, Mentee
Name: SoftServe, mentorship
Duration:  12.2024 ‚Äì 02.2024
Python, Pytest, PostgreSQL, Selenium Webdriver, Page Object Model
2. Python Developer,  Intern
Name: Palianytsia Foundation
Duration:  06.2024 ‚Äì 10.2024
Web scraping. Selenium WebDriver, BeautifulSoup4. Collect and preprocess data.
3. Freelance, Manual QA Engineer
Name: Upwork, Test io, uTest; Freelance, Crowd Testing;
Duration: 06.2022 ‚Äî 12.2023
Responsibility:
1.	Conduct test cycles.
2.	Record bug reports.
3.	Set up test environments and proxy settings.
4.	Exploratory testing, User Story for mobile and web applications.
5.	Perform regression testing.
6.	Find functional, content, visual defects.
4. Junior QA Engineer
Name: AWWCOR, Inc, Junior QA Engineer. Recruiting, Integration & HR Marketplace
Duration: 05.2021 ‚Äî 05.2022
Responsibility:
1.	Web application testing.
2.	Team communication.
3.	Search, record bugs.
4.	Design and execute QA documentation: test cases, checklists, use cases.
5.	API testing.
6.	Functional, non-functional testing.
7.	Regression and re-tests.
Stack: Postman, Swagger, DevTools, Azure DevOps, Visual Studio, Figma, .NET, Angular, MS SQL Server Management Studio"
data engineer,"Experience:
Company Name: Capgemini
DevOps Engineer | 10.2023 - present
Working as a DevOps Engineer, focusing on cloud infrastructure automation, resource optimization, and security in Azure.
Scope of work:
-Designing and maintaining automated infrastructure management processes using Terraform, Packer, and Ansible
-Optimizing resource usage and costs in Azure
-Writing and optimizing Python scripts for automation, data processing, and infrastructure management
-Integrating Azure with DevOps tools
-Configuring monitoring solutions and optimizing VM performance
-Conducting security audits and analyzing logs and configuration
-Utilizing Python for data analysis, log parsing, and reporting
-Creating technical documentation on automation processes and best practices
Technology Stack: Azure | Python | SQL | Git | Terraform | Ansible | Linux | Agile
Company Name: Fly On The Cloud
Cloud DevOps Engineer | 02.2023 - 09.2023
Working as a DevOps Engineer at a company specializing in the sales and distribution of Google products.
Scope of work:
-Resource management in GCP
-Customer bill automation in cloud services
-Ticket management
-Customer support during and after cloud implementation
-Installing and configuring software on a Linux server
-Providing feedback with logs and metrics
-Building and maintaining CI/CD pipelines
Technology stack: GCP | Linux | Git | CI/CD | Bash | Terraform | SQL | Agile | Zendesk
Company Name: Capgemini
Application Consultant | 03.2021 - 01.2023
Working with SAP and OpenText environment.
Scope of work:
Systems administration
-Monitoring and management of pipelines and containers
-Developing test scripts and conducting testing procedures
-Resource management in Azure
-Working with data, creating dashboards and graphs
-Performing transports in SAP and backup of systems
-SAP user authorization
Technology stack: Microsoft Azure | SQL | Power BI | SAP | Git | Linux | Windows | Agile | ServiceNow
Company Name: Bank of New York Mellon
IT Support Analyst | 03.2020 - 10.2020
Working as 2nd line Support Engineer for Wroclaw and Mainland Europe employees.
Scope of work:
Monitoring, reporting, and bug fixing
-Troubleshooting of software, applications, and hardware
-Ticket management
-Technology stack: Windows Server | SQL | Python | ServiceNow | PowerShell | Active Directory"
data engineer,"Help Desk Specialist - Expert Solution Servio Software
April 2019 - June 2021;
Application Software Analyst - GMS Service
June 2021 ‚Äì Nov 2024;
Data officer - International Medical Corps (IMC)
Nov 2024 ‚Äì Now"
data engineer,"Over 12 years of Expert level Oracle, Big Data experience, Batch and Streaming system.
Over 12 years of Data Engineering with Oracle Databases, Data Warehouse, Big Data, and Batch/Real time streaming systems. Working on Cloud (Azure/AWS) and on-Premises Big Data/Hadoop Ecosystem/Data Warehouse, ETL, CI/CD and Visualization (Power BI, Tableau).
Personally love playing/watching football and swimming. DWH Methodologies: Inmon, Kimbal, Data Vault 2.0
Oracle Big Data Appliance, Cloudera, HBase, HDFS, YARN, Zookeeper, HUE, Hive, Spark, MapReduce, Impala, Apache Kafka, Confluent Kafka, Airflow, NIFI, Databricks, Delta Lake, Data Lake, Azure  Data Factory, Azure HDInsight
Cloud infrastructure setup/configure/support OCI, Azure, AWS, and GCP related to the DBA/BDA and Data engineering.
CI/CD - Jenkins, Jira, Confluence, HP ALM, Docker
Confluent Certified Administrator for Apache Kafka
Microsoft Azure Fundamentals certification
DP-200: Implementing an Azure Data Solution (Data Engineer certificate)
DP-201: Designing an Azure Data Solution (Data Engineer certificate)
Microsoft Azure Data Engineer Associate
AWS Certified Cloud Practitioner
AWS Certified Data Analytics Specialty
Oracle Certified Associate DBA for Oracle 11g
Oracle Certified Professional DBA for Oracle 11g
Oracle Certified Professional DBA for Oracle 12C
Oracle E-Business Suite R12 Applications Database Administrator Certified Professional
Oracle WebLogic Server 12c Administrator Certified Associate
Oracle WebLogic Server 12c Administrator Certified Professional
Oracle Autonomous Database Cloud 2019 Certified Specialist
I'm looking for an interesting company in a good place. I would like to participate fast growing team."
data engineer,"During my working time, I have work mostly as a Data Engineer but also as a Data Analyst or a BI project Leader. Here are the technologies that I've used :
- Coding : Python / SQL
- Reporting : Qlikview / Qliksense / Power BI / Tableau Software
- ETL : Talend / Stambia / Semarchy xDi / Airflow / DBT
- DB : Oracle / Postgres / MySQL / Redshift / Snowflake
I'm looking to improve on Python mostly but also on Snowflake and other data tools that I do not know yet"
data engineer,"Experienced, result-oriented, resourceful and problem-solving Data engineer with business management skills.
Adapt and met challenges of release dates. Over 4 years of diverse experience in Data engineering fields, includes development and implementation of various applications in data processing environments.
Willing to provide systematic solutions to complex challenges in competitive environments. Ability to create automated data management systems for recursive data processing operations.
Working with:
- New technologies - especially Cloud Based technologies.
- Large datasets
- International team
- Develop ETLs from Flat files, RDBs, online spreadsheets, online applications, API endpoinds (POS systems), cloud storage systems.
- Interested in IOT systems, real time data processing"
data engineer,"Development of DB Migration tools
Development and design the architecture of projects for acquiring and issuing systems
Investigation and improvement of Database queries on UAT and Prod environments
Usage of Python Pandas and Numpy libraries for Data Engineering (ETL) and creation of statistical and non-statistical reports/rules
Usage of DBA knowledge to create Databases and Schemes with dierent user roles for the whole team
Investigation of data quality in Prod environment and development of scripts for correction of invalid data
Creation of several DataGenerators for testing the entire platform stack
Analyze new/old business requirements and make changes accordingly
Conguration of projects for QA/Python teams and provide help with business logic
Creation and maintenance of Fraud Detection rules
Development of JSON/XML/CSV parsers
Translation of SAS based codes to Python
Usage of the R language for basic statistical analysis and collection of aggregations (for Fraud Rules)
Creation of Python-based scripts to collect Data from various DBs to match PCI DSS standard
Creation and correction of various reports (RTF, PDF, XML) for Banks
Development of Migration Tools for Central Bank of Armenia
BPM (Business Process Management), user information transfer using XML schemas (Account/Customer/Card to BPM transfer)
Loan module, creation of new products related to Loans
Data migration from Central Bank of Armenia to local banks (Customer, Loan, Transaction data migration)
Customer module, creation of customer notication service
A new module in EXE to control bank cash collectors (using Borland C++ Builder)  A new module in DB to control cash collector transactions (using Oracle Types, sequences, packages, procedures, triggers, tables, views, etc.)
Mentoring newcomers for easy integration in a working environment
Participating in logical and architectural decisions
Providing support for clients (remote and locally)"
data engineer,"I've been in the industry for 15+ years, and I have had various experiences since 2006 (except banking software)
Started with PHP websites and e-commerce
Switched to ISP billing systems with Linux and Python
I spent a couple of years as a freelancer (from NodeJS to Golang)
I was working in the biggest CIS gaming/gambling companies with all underlying aspects there
Worked remotely for small startups, leading a small team
Led series A startup from R&D prototype to 30+ developers
Excellent software engineering experience for enterprise automotive
Architect role in world-known venture AI startup with 200M downloads
As my next step, I would like to work with top-notch modern technologies for all possible kinds of data processing.
- open source perfectionist
- committed in Python itself
- maintaining ~20 open source projects
- led a team of 30+ members
- 10K+ r/s systems
- 200M+ downloads apps
- excellent communication skills
- 15+ years in the industry
I am looking for tech-related vacancies closer to development, less bureaucracy, new challenging problems, and nonstandard projects.
As for me, a startup culture and a healthy atmosphere are the best way to go, especially for R&D!
I am interested in crawling, API, data processing, analytics, pipelines, real-time applications, DWH, ETL, and, of course, AI/ML."
data engineer,"Python Developer and Data Engineer.
Have experience in different ETL projects:
- Apache Airflow;
- Scrapy;
- Playwright;
- Zyte;
Data Science and ML:
- Pandas;
- numpy;
- visualization(matplotlib, plotnine, pygal, plotly);
- sklearn;
- Time Series Analysis and Forecasting;
- Computer vision(object detection).
Clouds:
- AWS(Batch, Lambda, Cloudwatch, Secrets Manager, System Manager, DynamoDB, S3);
- GCP(Google Cloud Dataproc, Cloud Storage).
Databases:
- PostgreSQL;
- SQLite."
data engineer,"Mar 2024 - present
- Work in data BackEnd team
Data/Software/ML Engineer
Creating a backend in a microservice architecture using the FastAPI framework
Transform data using Pandas
Data generation for LLM, LLM
Oct 2023 - Feb 2024
Work in data platform BackEnd team
Led the migration from Bottle to Flask, enhancing the application's efficiency and maintainability.
Developed and maintained a CI/CD pipeline, automating the build, test, and deployment processes.
Implemented data integration solutions with Google Drive for streamlined data access and management.
Managed server configurations, including Nginx, to ensure optimal performance and secure application delivery.
Integrated Docker into our development processes
Configured and managed the server settings to optimize
performance.
Jun 2023 - Oct 2023
Work in BackEnd team
Fintech web application for purchase/sale of shares
Python Developer
Creating a backend on Flask
Designing a PostgreSQL database
Writing unittest and end-to-end tests
Documenting REST APIs with Swagger
Writing migrations with Alembic
Data Engineer(June 2022 - May 2023)
Creating Python ETL Pipeline
Manage ETL Pipeline by Apache Airflow
Manage data with DBT and Snowflake
Python Developer - (August 2021 - June 2022)
Working as Backend Engineer in analytical team. Working with AWS services
Python Intern ‚Äì (February 2021-August 2021). Work in Automotive Production team, which deals with compilation map.
Development of internal web and desktop applications for automation process of compilation map and other internal processes. (Web ‚Äì Flask, Django, desktop ‚Äì wxPython)
Preperation environment for compilation maps
Support process of compilation
Visual testing
Report for the customer at the end of the iteration
Startup AllMemes at the Odessa I.I. Mechnikov National University, work within the KeepSolid  laboratoty (September 2020 ‚Äì nowadays): AllMemes ‚Äì social network, which specializes of classification images for  the convenience of users
Database design and support (PorsgreSQL)
Python backend (Django, Django Rest Framework)
Development of parsers from Internet, other social networking (Python, work with outside API, requests, bs4)
Development of neural networks for classification images (Python, Keras)
System programming (Python) Jul 2019 - Sep 2019
Web and desktop application for automation work with database for admins (web ‚Äì Django, Flask, desktop ‚Äì C#, WinForms, database - MySql)"
data engineer,"Looking for Big Data projects.
About myself:
Big Data Engineer with over 10 years of experience designing and implementing robust data solutions. Expertise in Scala, Python, Spark, Polars and large-scale distributed systems. Proficient in creating efficient, scalable architectures that deliver measurable business impact, including cost savings and enhanced data processing performance."
data engineer,"Key Skills:
- Experience hands-on in SQL and NoSQL Databases
- Hands-on experience in AWS Cloud with services such as Redshift, EC2, RDS, EMR, MWAA, S3, IAM, Apache Sqoop, Apache Airflow
- Software development skills in Python (Pandas, Boto3, SQLAlchemy) and a PySpark
- Using Apache Spark for Big Data analysis, processing and computations
- Experience in developing a Data solutions (migration, storage, processing)
- Experience in SQL and Query optimization
- Supporting large-scale systems in a production environment
- Developing, scheduling and monitoring batch-oriented workflows in Apache Airflow
I have about 6 years of hands-on experience in Data Engineering and all the time before starting from 2007 I worked on a Database/ETL Developer positions where I had a deal with a various relational databases such as Oracle, PostgreSQL, MySql, MsSQL and ETL tools as following: IBM DataStage, SQL Server Integration Services (SSIS).
As a Data Engineer I was engaged in data processing of the various data types with their next transformation and delivery to the Data Warehouse and Data Lake. End user reports were built based on these data.
The main technical stack that I‚Äôm working with includes:
Apache Airflow (Amazon Managed Workflows for Apache Airflow) to setup and operate data pipelines, for data orchestration and data validation;
Amazon Elastic MapReduce with the using Apache Spark for Big Data analysis, processing and computations of the large amounts of data;
Amazon Redshift for data warehousing and preparing the end users‚Äô reports;
Amazon S3 as object storage for Data Lake.
Using Apache Sqoop for data import
Using PySpark Job for data processing and computation.
Creating SQL ETL jobs to process data from the staging layer.
Python coding including libraries to work with data and services like Pandas, Beautiful Soup, boto3, SQLAlchemy, PyHive, PySpark.
And last but not least it‚Äôs AWS cost optimisation that includes different approaches to achieve optimal costs of the storage and AWS usage.
Rework the pipeline to incremental way
Optimize the s3 data lake to change the storage class - Intelligent Tiering storage class. Frequent and Infrequent modes
interesting to work with cutting-edge stack that included:
AWS, Python, SQL and noSQL DBs, PySpark, Airflow, Pandas"
data engineer,"Senior Data Analyst/Engineer, 05.2023 ‚Äì to the present
Lead complex data analysis projects and actively contributed to building mature processes in a DA
team
- Analysed omnichannel digital campaigns and created automated reports with suggestions
for improvements(automated runs of calculations and built UI for analysis of matching results);
- Developed proposals for improving the functionality of existing reports;
-Led a classification task involving processing of natural language (English): from idea to
deployment on AWS Sagemaker
Data analyst
10.2019 ‚Äì 04.2023
IT Company, Kharkov (IT)
-Creating and deploy machine learning resume_classifier;
-Collecting and interpreting data, analysing results;
-Identifying patterns and trends in data sets;
- Analyze and assess data using a variety of tools, with the intent of identifying the root cause of issues and proposing their resolution;
-Defining new data collection and analysis processes;
- Analyzing issue resolution data, make recommendations for support process improvement, and assist in the implementation of changes;
- Communicate application problems and issues to key stakeholders, including management and development team members;
Achievements:
-–°reated an NLP (natural language processing) model to extract special information from verifiable Text data. I achieved more than 80% accuracy on an internal dataset.
-Created a python script for daily automatic data uploading and creation of a calculation program in Power Query(Excell)
- Created a dashdoard in PBI for end-to-end analytics
- Created various macros for automation daily data processing
Template Developer
08.2018 ‚Äì 10.2019 (1 year 2 month)
IT Company(Template Developer), Kharkov (IT)
- Economic analysis of bank reports and relationships between individual reporting indicators;
- Analysis of the structure of bank report templates and incoming data required for reporting;
- Development of algorithms for converting the incoming data to obtain a structure suitable for presenting and submitting the report data;
- Configure templates to generate reports.
Achievements:
Created and provided some complex financial reports.
Additional information:
In the preparation of analytical reports used databases and Excel and Excel-like applications.
–î–æ—Å—è–≥–Ω–µ–Ω–Ω—è -–º—ñ–π –¥–æ—Å–≤—ñ–¥ —Ç–∞ –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å –≤–∏—Ä—ñ—à—É–≤–∞—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ –∑–∞–¥–∞—á—ñ –Ω–∞–π–∫—Ä–∞—â–∏–º —á–∏–Ω–æ–º.
–®—É–∫–∞—é —Ä–æ–±–æ—Ç—É –∑ –¥–∞–Ω–∏–º–∏, –¥–µ –º–æ–∂–Ω–∞ –±—É–¥–µ –∑–∞—Å—Ç–æ—Å–æ–≤—É–≤–∞—Ç–∏ —è–∫ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ –Ω–∞–≤–∏—á–∫–∏, —Ç–∞–∫ —ñ —Ç–≤–æ—Ä—á–∏–π –ø—ñ–¥—Ö—ñ–¥ –¥–æ –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—Ö –∑–∞–≤–¥–∞–Ω—å. –•–æ—á—É –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ —Ç–∞ –ø—Ä–∞—Ü—é—é –∑ –ø–µ—Ä–µ–¥–æ–≤–∏–º–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—è–º–∏. –ó–∞–≤–∂–¥–∏ –≤—ñ—Ä—ñ—à—É—é —Å–∫–ª–∞–¥–Ω—ñ –∑–∞–¥–∞—á—ñ  —ñ –º–æ—è —Ä–æ–±–æ—Ç–∞ –ø–æ–≤–∏–Ω–Ω–∞ –≥—ñ–¥–Ω–æ –æ—Ü—ñ–Ω—é–≤–∞—Ç–∏—Å—è."
data engineer,"I have PhD degree in System Analysis.
Proficient in various fields of math.
Experienced with Linux-based systems, able to dive deep into technical details and come to the surface with relevant knowledge and solutions.
I worked for 4 years in a high frequency trading startup. I was responsible for designing database structures for detailed trading history from exchanges, manually loading first batches of data and subsequent automation of updating this data (i.e. implementing an ETL).
Able to put together a big picture of a system, analyze it and come up with relevant strategies, solutions and modifications.
I have scientific publications about innovative methods of discrete linear system control. This methods involves sophisticated math, gives unique results compared to traditional algorithms and are intended for implementation as a controller for a technical system.
I have also developed a small framework in Python to test and apply the developed methods.
Don't ask me to learn a new programming language over night. I can, but I definitely won't be happy.
Don't ask me to do the impossible. I can, but I am not ready to put my health and sanity at stake.
Because of ethical reasons I can't participate in web development if user's privacy and security are not considered or, if I put it straight, neglected.
And definitely not marketing involving big data and not targeted advertisements, also because of ethical reasons."
data engineer,"Developed DWH and client-server information systems based on Firebird, MS SQL Server, ClickHouse and Vertica DBMS.
Implemented and supported SugarCRM system to combine all information about company's clients in one place. Created marketing campaigns to increase the quantity of clients.
Designed and developed automated corporate reporting based on developed DWH using Power BI and Tableau.
Developed automated alerts using Python and Jenkins.
At the moment main responsibility is configuring high-load ETL streaming pipelines in Kafka/Rundeck/Hadoop/Hive/Vertica stack
and Kafka/Iceberg/Alluxio/Hive/Airflow/Vertica stack. Python ETL development and Tableau dashboards development according to the business requirements.
- Participated in the implementation and maintain consolidated data warehouse for machine learning and predictive analytics
- Successfully implemented and maintained the CRM system for corporate business with back-end on MySQL
- Developed databases and client-sever information systems with back-end on Firebird for detailed clients analysis
- Created PHP online-store
- Python (Django) blog was created and deployed on pythonanywhere.com
- Developed Python ETL for the Vertica user_profile table incremental update (without UPDATE operator)
- Developed Tableau dashboard to monitor company business alerts statistics on one page (tables not updated, reports not sent, kafka topics unconvertibles and etc.)
- Developed Power BI dashboards for Supply Chain management, Call Centre, Purchase management, Storage goods monitoring, Orders Status monitoring
I'm looking for an opportunity to join a dynamic growing international company with a good team environment to build a career as a Data developer | Data Analyst | BI developer | Data Engineer"
data engineer,"I have more than 7 years as a Software Developer in C# & other languages
AND 4 years experience as Data engineer
In Summary:
.NET  Experience			5 Years
‚Ä¢	C#, .NET, CORE
‚Ä¢	REST, JSON, XML
‚Ä¢	Unit Tests, Integration Tests, TFS, SVN, GIT, Entity Framework
ETL / BI Experience			4 Years
‚Ä¢	Designing ETL data flows with different data sources
‚Ä¢	Designing and deployment of reports
Cloud Experience			3 Years
‚Ä¢	Azure (DevOps, Event Bus, Data Lake, Blob Storage)
‚Ä¢	AWS (EC2, RDS, SNS, SQS, SES, Glue, Athena, Lambda, API Gateway, S3, CloudFormation, CloudWatch)
T-SQL / Databases Experience	3 Years
‚Ä¢	Installation, Configuration and Updates of SQL Server
‚Ä¢	MSSQL, MySQL
‚Ä¢	T-SQL developer skills including Stored Procedures, Indexed Views etc.
No special needs
No special needs"
data engineer,"With 10+ years of experience in designing and implementing data solutions across banking, software development, and analytics, I specialize in optimizing data architectures, developing scalable solutions, and streamlining data migrations. Proficient in Python, PySpark, SQL, PL/SQL, AWS, Apache Kafka, RDBMS, Docker, Airflow and other technologies, I have a strong focus on performance optimization, query development, and integrating modern tools. Experienced in leading cross-functional teams, collaborating with stakeholders, and driving the development of high-performance data platforms.
-
Expert in building and orchestrating data pipelines using Apache Airflow, Python, and PySpark. Skilled in implementing scalable, real-time data warehousing solutions with AWS services, Kafka, and ClickHouse, enabling near real-time insights for data-driven decision-making.
--
Proven expertise in building data lakes and delta lakes on platforms like Databricks, optimized for analytics and reporting. Comprehensive understanding of Spark for large-scale data processing.
&
Led cloud migrations from on-premises to AWS, designing resilient architectures using RDS, DMS, Glue, Redshift, and S3. Skilled in crafting scalable, high-performance cloud infrastructures.
Extensive experience in database technologies like Redshift, ClickHouse, Oracle, PostgreSQL, and SQL Server. Proficient in ETL tools such as SSIS, Pentaho, and AWS Glue, with expertise in PL/SQL and database architecture.
Solid background in software development, especially in secure, high-performance systems for the banking sector. Skilled in complex PL/SQL applications and database design.
-
Delivered tailored data solutions across banking, finance, and e-commerce, aligning with each domain‚Äôs specific requirements.
&
Proficient in Docker for containerization and Git/GitHub for version control, ensuring efficient and maintainable workflows.
&
Strong communicator and tech lead with a track record of guiding diverse teams. Adept at fostering collaboration across multicultural, distributed teams and delivering results in fast-paced envir
Core bank architecting , DWH from scratch , Migration projects, query optimization,PL SQL architecting, Development of data pipelines with Airflow and  Spark. Migration to AWS .Big Data projects.
Challenging project using : DWH / ETL / Python data engineering / Big data / Lakehouse"
data engineer,"Results-driven professional with extensive expertise in building data pipelines, combined with a strong background in database management, cloud computing, and data analytics.
Proficient in SQL, MS SQL, PostgreSQL, and Oracle, with advanced skills in Python programming. Experienced in utilizing Google Cloud Platform tools such as BigQuery, Dataflow, Cloud Function, and Cloud Run to deliver efficient and scalable solutions. Additionally, skilled in SSIS, SSRS, and Looker Studio for data integration, reporting, and visualization. Currently, I maintain a data science team in leveraging AI and big data to provide companies with the business intelligence needed to make critical decisions. Known for my ability to analyze complex data sets, streamline processes, and drive business outcomes. Seeking opportunities to apply my diverse skill set and industry experience to contribute to innovative projects."
data engineer,"Designing, architecting, implementing, maintaining, migrating from on-premise to cloud Data platforms, Warehouses, Data Lakes, ETLs, Business Intelligence platform.
Public clouds: mostly OCI, also familiar with AWS, Google Cloud and Digital Ocean
–í—Å–µ–≥–¥–∞ —Ö–æ—Ä–æ—à–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è —Å –∑–∞–∫–∞–∑—á–∏–∫–æ–º –∏ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–æ–º.
–•–æ—Ä–æ—à–µ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∏ –ø–æ–∂–µ–ª–∞–Ω–∏–π –±–∏–∑–Ω–µ—Å–∞.
–ù—É –∏ –∫–æ–Ω–µ—á–Ω–æ –∂–µ - –∫—É—á–∞ –∞—Ä—Ç–∏—Ñ–∞–∫—Ç–æ–≤ –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ, –≥–æ–¥–∞–º–∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∏—Ö –±–∏–∑–Ω–µ—Å –∑–∞–∫–∞–∑—á–∏–∫–∞.
–ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä –∏–ª–∏ –≤–µ–¥—É—â–∏–π Data Engineer –≤ –ø—Ä–æ–¥—É–∫—Ç–µ, –∂–∏–≤—É—â–µ–º –∏–ª–∏ –ø–µ—Ä–µ–µ–∑–∂–∞—é—â–µ–º –≤ –æ–±–ª–∞–∫–æ.
–ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞ –º–æ–≥—É –Ω–µ –ø–æ—Ç—è–Ω—É—Ç—å, –ø–æ—Å–∫–æ–ª—å–∫—É –Ω–∏–∫–æ–≥–¥–∞ –Ω–∞ —Ç–∞–∫–æ–π –ø–æ–∑–∏—Ü–∏–∏ –Ω–µ –±—ã–ª, –Ω–æ –≤ —Ç–µ–∫—É—â–∞—è —Ä–æ–ª—å –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç —á–∞—Å—Ç–∏—á–Ω–æ –∑–∞–¥–∞—á–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞, –∏ —Ö–æ—Ç–µ–ª –±—ã –¥–≤–∏–≥–∞—Ç—å –∫–∞—Ä—å–µ—Ä—É –≤ —ç—Ç–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏."
data engineer,"I worked at Ministry of finance as mid economist about 1 year.
Then I moved to State Revenue Committee and worked there as
leading statistical specialist about 3 years. Here I used tools like Eviews and Stata, and also had a little project in ML (classification algorithms, DT and RF).
Currently I am working in Questrade Inc. company as Data Engineer/ Data Analyst and using the related tools like T-SQL,SSRS,MySQL,BigQuery, PowerBI,Python.
I have an experience of using Jira board and Git repository"
data engineer,"In general, my work as ETL Developer consists in moving data from the source to the destination, which can be S3, FTP/SFTP, external DB, API. During this process, the input data is validated and, if necessary, transformed.
A team of analysts at my company faced the problem of validating and analyzing data from large Text files. I implemented a solution that saved 24% of the annual cost of working hours of the analyst team.
Looking for a Data Engineer/ETL Developer role.
I am comfortable working both on monotonous work and multitasking with a variety of tasks. I would like to work in cooperation with other team members rather then individually. Since I like to be the part of the team."
data engineer,"I  am a Security Analyst, specializing in the verification of candidates, counterparties and security intelligence (OSINT). My job is information security and business intelligence through analysis, verification and collection of critically important data.
Basic bindings:
OSINT analytics: search, collection and analysis of information from hidden sites (social networks, registries, forums, databases, etc.);
Verification of candidates and counterparties: risk assessment, reputation analysis, identification of possible connections;
Financial and economic intelligence: analysis of financial indicators, P&L information and potential risks of cooperation with companies;
Process automation: Python, AI tools and Power BI for extensive information processing and feedback generation;
Interaction with other departments: work with legal, financial and IT services for a comprehensive risk assessment.
Technologies used:
Languages ‚Äã‚Äãand analytical tools + AI: Python (pandas, numpy), Excel (VBA, Power Query, DAX), SQL, ChatGPT prompt;
BI and visualization: Power BI, Tableau;
OSINT tools: Google Dorks, PimEyes, X-Ray, getcontact, geoanalytics, registers etc., use as needed.
Additionally, I worked in the finance department, where I monitored contracts, tracked the budget and the impact of invoices on it, maintained the P&L report, and applied my skills to optimize and automate all processes.
Developed a script to save and process the counterparty's tender data. This provided a visually convenient representation of the scope and details of public procurement.
An internal algorithm for candidate verification was developed, efficiently checking more than 100 candidates, and the collected data allowed us to filter out those with critical flags.
Mentored two colleagues who successfully integrated into the work and acquired the skills at a sufficient level to fulfill requests.
What am I looking for?
A job focused on analytics;
Using BI tools, Python, and AI for in-depth data analysis etc (It is focused on providing a clear presentation for the requester and gaining insights);
Continuous improvement with modern technologies.
What am I not interested in?
Purely operational work without the ability to analyze data and influence processes;
Routine checks without analytical or automation tools;
Positions unrelated to data engineering or analytics."
data engineer,"Tech-savvy leader driven by entrepreneurial spirit. Mastering Python, Pandas, Docker..., and more to craft innovative solutions. Experienced in product management, team leadership, and software engineering."
data engineer,"Data Engineer with experience in Data Scraping and automation solutions. Key achievements include:
- Developed a system of two Telegram bots to automate the generation and validation of promo codes, improving interactions between buyers and sellers;
- Extensive experience in developing Python scripts for data scraping using GraphQL and multiprocessing;
- Set up and optimized ETL processes using Apache Airflow, handling data pipelines in cloud environments (Google Cloud, BigQuery);
- Strong knowledge of SQL, NoSQL, and various database systems;
- Focus on ensuring data quality and validation for robust data management solutions.
Additionally, I have worked with Docker, Snowflake, and participated in projects involving both local and cloud-based data processing.
I have completed a Data Engineering Certification from Robot_dreams and hold a Bachelor‚Äôs degree in Economics from Kyiv National Economics University. Additionally, I have taken several courses related to Data Engineering on DataCamp, covering topics such as SQL, PySpark, and Big Data Fundamentals. I have hands-on experience with ETL/ELT tools, Apache Airflow, and cloud platforms such as Google Cloud Platform.
I am seeking a job where I can further develop my skills."
data engineer,"I have significant experience in developing and managing data-driven projects across various industries, including retail, distribution, manufacturing, and security systems. My roles have primarily involved architecting, developing, and deploying scalable data platform parts.
Key Projects and Technologies:
1) Retail & Distribution Buiseness Data Platform (Data Engineer):
Technologies: Spring Boot, Kafka Streams, Spark Streaming, Kubernetes, Azure, Databricks
Tasks: Developed microservices for real-time data processing, built a universal framework for scalable microservices, and facilitated data service migrations to improve efficiency and monitoring.
2)
Technologies: Java, Spring Framework, JUnit, JPA, PostgreSQL
Tasks: Developed APIs for reporting systems and supported digital transformation projects.
Security Systems (Core Platform Developer):
Technologies: Spring Boot, FastAPI, Project Reactor, Kafka, Spark, Milvus, Cassandra, Vast Data
Tasks: Redesigned data platform architecture, developed real-time video stream analytics services, and replaced legacy systems with scalable, extendable solutions.
2) Government program for AI-driven security system development (Senior Data Engineer)
Current Role:
As a Senior Data Engineer, I focus on enhancing data platform architecture and developing real-time analytics services. My goal is to leverage Data and AI practises to help businesses grow and maximize the value derived from their data.
1) Picked up an existing project at the stage of collapse and brought it to the level required by the customer by reworking the architecture and building modern processes in the team.
2) Designed and Developed a universal framework for building scalable, traceable, and fault-tolerance microservices for processing real-time events based on top of Kafka Streams with Scala or Java programming languages.
3)Unified the microservices development process to meet the actual development best practices and fit in the existing git-ops Kubernetes platform. That makes possible the migration of all data services from a legacy environment to a new one saving costs, increasing stability and monitoring and highly decreasing ‚Äútime to market‚Äù.
As an experienced Data Engineer, I aim to build cloud-native data systems and guide enterprise-level, data-driven projects to success.
I am passionate about utilizing my skills and experience in the data engineering field to create scalable, effective solutions and systems that are helping businesses to grow and enhance.
My goal is to work closely with business stakeholders, helping them to generate as much value from their data as possible"
data engineer,"Staff Data Developer (Shopify - 09.2022 to 06.2023):
- Ensured data pipeline integrity and established best practices for data development.
- Mentored engineers and guided the data platform evolution process.
- Led the transition to a cloud-based infrastructure and migrated data resume_classifier.
Principal Data Engineer (SumUp - 05.2021 to 08.2022):
- Developed data platform roadmap and managed risk assessment and mitigation.
- Reviewed design documents and played a key role in shaping team performance.
- Transformed the data platform into a Data Mesh approach.
Software Developer - SRE (Google - 10.2019 to 04.2021):
- Monitored production services and optimized hardware resources.
- Resolved infrastructure issues and deployed code changes.
- Led the migration of monitoring services and implemented database-level isolation.
Big Data Engineer (OLX Group - 01.2018 to 10.2019):
- Built and maintained a data lake, ensuring GDPR compliance.
- Enabled data access democratization and implemented data governance.
- Optimized storage format and developed replication system for RDBMSs.
Data Warehouse Developer (Zalando - 02.2016 to 01.2018):
- Performed data modeling and developed ETL pipelines.
- Conducted performance tuning and mentored colleagues.
- Redesigned core dimension tables and ingested pricing data.
Data Warehouse Developer (Azercell - 01.2011 to 02.2016):
- Created and supported ETL jobs and developed data architecture strategy.
- Calculated data-mart values and ensured data quality.
- Provided online reporting support and facilitated real-time data streaming.
Built data lake ground up
Developed complex database solution in-house - cdc (change data capture) on cloud storage (s3)
Leaded transition from traditional data platform to data mesh
A job with complex challenges in a company which is doing meaningful business"
data engineer,
data engineer,"Skilled software engineer with expertise in MySQL, Python, Jenkins, and AWS. Experienced in updating legacy code, implementing CI/CD pipelines, and managing AWS infrastructure. Proficient in requirements analysis, testing planning, and test case design. Strong analytical and troubleshooting skills in BigData. Specializing in regression testing, functional testing, non-functional testing, and data validation."
data engineer,"Title	Data Engineer
Customer and Customer Domain Description	Business Information & Media
EPAM Project	TKM-UNIV
Project Description	This is a comprehensive Extract, Transform, Load (ETL) project aimed at efficiently collecting and processing data. The data is initially collected from various sources like MongoDB and PostgreSQL. Then data is collecting and aggregating using Kafka and next moves to Clickhouse which is fast open-source column-oriented database management system. Data from Clickhouse is using in reports.
Responsibilities	Preparing data in Clickhouse for reporting
Database	PostgreSQL, MongoDB, Cickhouse
Tools	Studio 3t, Dbeaver, Visual Code
Technologies	PostgreSQL, MongoDB, Cickhouse, Kafka
Title	DWH Developer + DevOps
Project Roles	DWH Developer + DevOps
Customer and Customer Domain Description	Life Sciences & Healthcare.
Project Description	This is a comprehensive Extract, Transform, Load (ETL) project aimed at efficiently collecting and processing data. The data is initially collected from various sources and seamlessly stored in the highly scalable and durable Amazon Web Services (AWS) S3 storage service. Subsequently, leveraging the robust capabilities of Airflow, a powerful data orchestration tool, the collected data is seamlessly moved from AWS S3 to AWS Redshift, a powerful and fully managed data warehousing solution (DWH). This process ensures the smooth and efficient transformation and loading of the data, enabling organizations to gain valuable insights and make data-driven decisions.
Responsibilities	Performing quality check automation using Airflow
Developing stored procedures and views
Creating data quality checks
Creating pipelines, automation using Jenkins
Database	Amazon Redshift
Tools	Visual Code, Dbeaver, AWS console, TortoiseGit, WinSCP
Technologies	Jenkins, Airflow, Redshift, S3, Athena, DBT
UKRPOSHTA
Oct, 2020 ‚Äì May, 2021
Title	Head of Master Data
Project Roles	Lead Software Engineer
Customer and Customer Domain Description	Ukrposhta
Project Description	Address classifier
Responsibilities	¬∑ Performed validation using packages on Oracle side and integration using dblinks from SQL Server to Oracle
¬∑ Optimized queries
¬∑ Oracle database monitoring
¬∑ Took part in creation views for DWH
¬∑ Making reports for business
Database	Oracle, SQL Server
Tools	Toad for Oracle, SQL Server Management Studio"
data engineer,"Hello everyone!
My technical expertise includes 0.5 year of commercial experience as a Technical Support Engineer at Fintech domen.
I have a solid level in:
- Python, Java, SQL (MariaDB, PostgreSQL);
- Understansing and building APIs for automatization processes;
- Experience in working with PowerShell and GIT.
- Experience with such monitoring system as Zabbix and op-src issue tracking tool like Redmine.
I am passionate about staying up-to-date with emerging technologies and learning something new.
My goal is to become a really good specialist (Backend Developer), so I actively find new job opportunities.
Bachelor of Mathematics, Computer Mathematics, 2024.
During studying at faculty of mechanics and mathematics I finished courses at such programming languages as Python, C/C++, R, Java. Also I hace finished course in project management.
At third course (2022) in uiversity I started learning Java and soon was involved in university course project as backend developer (Java/Spring Stack).
Further studying of Java/Spring during the next period was only by myself. I don't have commercial experience at programming, so I can't share with great achievements. But I constantly learning and finding new job opportunities.
Hi everyone!
I am eager to join a dynamic team where I can expand my understanding of data-driven problem solving and refine my machine learning skills. I hope to work with experienced guys who can guide me
I look forward to tackling real-world projects that challenge my analytical thinking, while also learning best practices for code optimization, and reproducible research. It‚Äôs important to me that I contribute meaningful insights to the organization, whether by improving an existing model, building new prediction systems, or supporting data-driven decision-making across various departments.
In addition, I want to deepen my knowledge of tools like Python, scikit-learn, sql, as well as explore cloud platforms for scalable model training. Working in a supportive environment where teamwork, continuous learning, and open communication are encouraged will help me grow not only as a Data Scientist but also as a professional overall. By collaborating with peers, I hope to gain exposure to cutting-edge techniques, stay updated on the latest research, and ultimately deliver impactful solutions for the company‚Äôs data challenges."
data engineer,"Hard skills:
Strong knowledge in Excel.
Building dashboards and visualizations in Power BI using DAX-functions, also has experience with Qlik, Sisense, Tableau and Looker Studio.
SQL and non-SQL DBMS: MySQL, MS SQL, MongoDB.
GCP: collecting data in BigQuery from Google Analytics, Google Ads and other sources using Cloud Functions.
Basic experience in Amplitude.
Development of A/B tests: design, post-analysis.
Soft skills:
Analytical mindset, innovator, ability to be flexible, self-learning and easy to learn, passion and diligence in work, ability to delegate, experience in Scrum/Agile.
5+ years of experience as a data analyst. Data extraction and aggregation. Data analysis, hypothesis testing, identifying correlations and patterns. Processing data-related questions coming from users. Performing research and presenting the findings. Creating standard reports, support and update them.
For me, the company's healthy Ukrainian position and its involvement in supporting the Ukrainian armed forces are important.
I expect that the company will maintain work-life balance, white salaries, and that the company pays taxes."
data engineer,"I have 2 years of experience as a Data Engineer at GlobalLogic. During this time, I went from a Trainee to a Middle Data Rngineer.
During this time, I worked on a project where the main goal was to rewrite the old database using new technologies such as Python and AWS cloud. The client himself works with pharmaceutical data.
Responsibilities on the project
-Building data pipelines using Apache Airflow
- Design DWH on Snowflake and Postgres
-Building ETLs using SQL, Spark(PySpark)
-Cover code base with unit tests using Pytest
I wish to continue working with data, creating and designing databases and etc.
Also worked on various projects at the university. These include: creating a module to do polymorphic code, writing my Rick and Morty Universe google in React.js, AI related projects like writing an Atari boxing bot using RL and a fake news analyzer. Development of a system for working with the WIKI API using Django, Kafka, Spark, etc.
BSc in Computer Science at Ukrainian Catholic University Lviv, Ukraine.
Average rate - 87
Also, for my thesis, I developed a political and social product that showed countries that voted like Russia in the UN with the help of data visualizations"
data engineer,"I have completed various projects and tasks using a range of technologies, including Kafka, Microsoft Azure, Python, MySQL,PostgreSQL, AWS, GCP and Docker. My current role in the team involves creating and managing data pipelines that process large volumes of data to extract meaningful insights for business decision-making.
Some of the notable projects that I have worked on include building a real-time data streaming platform using Kafka and integrating it with Microsoft Azure cloud services to enable efficient data processing and storage. I have also developed data processing pipelines using Python and SQL to extract data from various sources, transform it into a structured format, and load it into data warehouses for analysis.
Successfully designed and implemented a real-time data streaming platform using Kafka, which enabled the efficient processing and storage of large volumes of data. This platform could have helped the organization to obtain real-time insights into business operations and improve decision-making.
Improved the performance and scalability of existing data processing pipelines by optimizing the code and enhancing the infrastructure. This could have resulted in significant cost savings and improved the efficiency of the organization's data processing capabilities.
Developed a custom data visualization tool that helped the business to visualize complex data sets in a more intuitive and accessible way. This could have enabled the business to quickly identify trends and patterns in the data and make more informed decisions.
Professional Growth:
I am keen on continuously advancing my technical skills, particularly in emerging data technologies and cloud services. I value opportunities for certifications and training, and I am eager to take on challenging projects that stretch my current capabilities.
I also look forward to advancing to roles with increased responsibilities over time, such as leading a data engineering team or taking charge of strategic data infrastructure projects.
Collaborative Environment:
I thrive in collaborative, team-oriented settings where open communication and knowledge sharing are encouraged. I look forward to being a part of a supportive team that values each member‚Äôs input in solving complex data challenges.
Impactful Work:
I am motivated by seeing the direct impact of my work on business decisions and operations. I seek to contribute to projects that drive tangible value for the organization and help in making data-driven decisions.
Work-Life Balance:
While I am committed to delivering high-quality work, maintaining a healthy work-life balance is important to me. I appreciate a culture that respects personal time and promotes a balanced approach to managing professional and personal commitments.
Innovative Culture:
I am drawn towards organizations that foster a culture of innovation and encourage the exploration of new technologies and methodologies to solve data-related challenges."
data engineer,"I have over three years of professional experience as a Data Engineer, specializing in designing and developing ETL/ELT pipelines, data modeling, and building scalable data architectures. At my current position with the International Bank of Azerbaijan, I lead the design and implementation of ETL pipelines, optimize Data Marts, and review and refactor code to improve efficiency. My daily responsibilities include working with a robust stack of technologies like Python, PySpark, Airflow, and Kafka to handle complex data processes effectively.
Previously, I worked at Simbrella, where I was involved in end-to-end database design, administration, and ETL pipeline development. My work there expanded my experience with cloud services like AWS (Glue, Kinesis) and reinforced my ability to analyze and optimize data flows for business-critical applications.
I also executed a personal AWS project to deepen my expertise in cloud technologies, focusing on ETL pipelines, logical and physical data modeling, and data warehouse development using AWS Glue, Athena, and PostgreSQL.
At AccessBank, my responsibilities included designing OLAP and OLTP databases, conducting in-depth data analysis, and creating interactive dashboards using tools like Power BI and QlikSense. I gained substantial experience in optimizing complex SQL queries and creating visualizations to support business decisions.
While I am proficient in various tools and technologies such as Apache Spark, Delta Lake, and Azure Synapse, I aim to enhance my expertise in real-time data processing and containerization technologies like Kubernetes. Additionally, I seek opportunities to work on innovative projects that leverage cutting-edge technologies to solve challenging data problems.
From my work, I expect opportunities to engage in challenging projects that involve modern data engineering technologies, allowing me to grow professionally and make a meaningful impact. I value a collaborative team environment, clear communication, and a culture that supports learning and innovation. Working on real-time data processing and cloud-based solutions excites me, as does contributing to efficient and scalable systems."
data engineer,"Hi and welcome on my profile.
I`ve been working as a data specialist for more than 5 years. Though I`ve been working on different positions and various technologies, every project was related to:
1. Data integration.
2. Utilization of cloud services and CI/CD practices.
3. Work with SQL/NoSQL DBs and distributed systems.
4. Incorporation of AI tools in data processing.
5. ETL/ELT
6. OLTP/OLAP systems development maintenance and optimization."
data engineer,"Over 13 years of hands-on experience in software development and architecture for data-driven applications and business intelligence. Expert in Microsoft Data Platform and Azure cloud. Expert in data modelling, optimization and performance tuning. Solid experience in full cycle of data integration from raw source to end user report. Experience in both delivery and pre-sales of data projects. Solid experience with data lakes, warehouses and lakehouses. Result oriented and highly organized while remaining focused on the big picture.
Skills:
¬∑ Azure (SQL DB, Synapse Dedicated/Serverless, Data Lake, Blob, Data Factory, DevOps)
¬∑ AWS (Redshift, DMS)
¬∑ Databricks, MS Fabric
¬∑ MS SQL Server, MySQL, IBM Netezza, SAP HANA
¬∑ Power BI, IBM Cognos, SSRS
¬∑ OLAP, OLTP, Warehouses/Lakehouses
¬∑ SQL/T-SQL, Python
¬∑ CI/CD
¬∑ ETL/ELT, SSIS
¬∑ Git, SVN, TFS
¬∑ MPP, In-Memory, Columnstore
¬∑ Database design and optimization
¬∑ Advanced English
¬∑ Waterfall, Scrum, Kanban
¬∑ Jira, Redmine
Located in Poland.
Microsoft Certified Solutions Expert (MCSE): Data Platform
Microsoft Certified Solutions Expert (MCSE): Data Management and Analytics
Microsoft Certified: Azure Data Engineer Associate
Certified Scrum Master (PSM I)
- A technical role, not people management role;
- Projects with MS Azure stack;
- An opportunity to get familiar with new technologies;
- Projects that are not on the phase of support or testing;
- Not interested in opportunities with on-call support, shifts, etc.
- Remote only;"
data engineer,"Data Analyst with a background in Data Engineering, specialized in automation of data workflows and business intelligence reporting.
Proficient in Python and SQL, with hands-on experience in data manipulation, automation, and integration with reporting tools like Power BI.
Skilled in data visualization, advanced web scraping, and data cleaning, with a foundational understanding of machine learning concepts and applications. A problem-solver with a focus on automation and efficiency.
Interested in AI and machine learning, to further develop skills in ML engineering to create robust and intelligent data-driven solutions. Experienced in working within multilingual teams, and committed to continuous learning and adaptation in the evolving field of data science."
data engineer,"Participated in designing and implementing real-time ELT pipeline to process semi-structured data from Apache Kafka to PostgreSQL using Python and its libraries;
Wrote lots of SQL procedures and functions in Postgresql that process, enrich the data received from Kafka;
Created APIs using PostgREST in order to data in database was accessible from outside;
Developed a generic Python application that loads data from a database and puts it on either Email or SFTP server in CSV format;
Created pipelines that archive data from PostgreSQL to Google Cloud Storage daily;
Utilised Apache Airflow as a tool for triggering data pipelines periodically and continuously;
Implemented real-time ETL pipelines that loads data from Kafka to Google BigQuery;
Participated in designing data warehouse solution in Google BigQuery and wrote lots of SQL procedures and functions that loads data into the DWH;
Implemented a Python application with Pandas that loads data from multiple data sources, compares it and sends the results of comparison to Email;
Created many reports and dashboards using  JasperReports tools which are sent to the clients' emails everyday.
Not sure that statements like ""performance increased by 42%"" are worth to be added here, cause no one knows what and how something was implemented before.
Anyways, I've got some certifications in Python, SQL and Data Engineering fields from DataCamp in my LinkedIn profile.
I want to improve my skills in Data Engineering field, especially focusing on the cloud products. And if your company provides any help with preparing or/and getting any AWS/GCP certifications then I will consider them at first."
data engineer,"A highly skilled and motivated Data Engineer with focus on MS SQL Server and Architecture design offering a combination of cross-functional skills with 16+ years of  experience in the IT sphere as a DBA, DB Engineer, Data Architect, Team Lead.
Key Responsibilities:
Team management and mentoring UP to 11 people (DBA, DBD, QA, BI)
Code review. Unify code style
CICD implementation
Data storage optimisation
Big Query data transfering
GCP data transfering
Creating Reports on SSRS;
SSRS server setting up and management
Creating and supporting dashboards in Qlikview/QlikSense;
Qlikview/QlikSense server management
Configuring ETL process
SQL Query optimization
Data recalculation and migration
MS SQL server replication setup and management
Failover cluster setup
Database BackUp/restoring
2021 - present time, Data Engineer Team lead
Team: Up to 11 people (DBA, DBD, BI, QA)
New data model for analytics:
One data structure for Big Query and MS Sql Server
Increasing performance for data query 10+ times
Decreasing query complexity
Big Query:
Data Uploading
Data Monitoring
2018 - 2021, DBA Team lead
Team: Up to 4 people (DBA, DBD, BI, QA)
Interviewing, mentoring
Code review
DBA Ms Sql Server support:
Up to 20 servers
100+ databases
Setup, back up, restore
Backup automatisation
MS Sql Server Replication:
Setup, support
Up to 7 servers in one system
Hundreds of tables
Single destination DB up to 7 TB
QlikView: Support
QlikSense:
Qlik view to Qlik Sense report migration
Setup
Reports - 2+ Billion Rows
New data model for user filtering:
200+ dimensions, 300 mln rows
30+ tables as datasource
6000 result reports per day (up to 50 mln records each)
Data migration:
For scalability PK was changed in the 20 biggest tables
Full data migration to GCP virtual machines
QlikSense migration to GCP
GDPR implementation
2014 - 2018, DB Developer
MS SSRS:
50+ reports
Version control, CICD implementation
Simplifying subscriptions. 200+ nightly subscriptions
Data model optimisation and refactoring"
data engineer,"–ú—ñ–π –¥–æ—Å–≤—ñ–¥ —É Data Engineering  –≤ –æ—Å–Ω–æ–≤–Ω–æ–º—É —Ü–µ:
- —Å–∞–º–æ–æ—Å–≤—ñ—Ç–∞ —á–µ—Ä–µ–∑ YouTube-–∫—É—Ä—Å–∏ –∑ Data Engineering
—Ç–∞ –ø—Ä–∞–∫—Ç–∏–∫–∞ –∑–∞ –≤—ñ–¥–µ–æ—É—Ä–æ–∫–∞–º–∏: –ø–æ–±—É–¥–æ–≤–∞ –ø—Ä–æ—Å—Ç–∏—Ö –¥–∞—Ç–∞-–ø–∞–π–ø–ª–∞–π–Ω—ñ–≤, –æ–±—Ä–æ–±–∫–∞ —Ç–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö.
- SQL: –∑–∞–ø–∏—Ç–∏ –¥–æ –±–∞–∑ –¥–∞–Ω–∏—Ö —É PostgreSQL —á–µ—Ä–µ–∑ DBeaver.
- Python: –æ–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö, —Ä–æ–±–æ—Ç–∞ –∑ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞–º–∏ (`pandas`, `numpy`)
- Airflow: –∑–Ω–∞–π–æ–º—Å—Ç–≤–æ –∑ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü—ñ—î—é –∑–∞–≤–¥–∞–Ω—å
- Docker: –æ—Å–Ω–æ–≤–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü—ñ—ó.
–ü—Ä–∞—Ü—é—é —ñ–Ω–¥–∏–≤—ñ–¥—É–∞–ª—å–Ω–æ, –Ω–∞–≤—á–∞—é—á–∏—Å—å —ñ –≤–∏–∫–æ–Ω—É—é—á–∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ñ –∑–∞–≤–¥–∞–Ω–Ω—è –¥–ª—è –∑–¥–æ–±—É—Ç—Ç—è –Ω–∞–≤–∏—á–æ–∫ —É Data Engineering.
–•–æ—á—É —Ä–æ–∑–≤–∏–≤–∞—Ç–∏—Å—è —è–∫ Data Engineer, –∑–∞–≥–ª–∏–±–ª—é—é—á–∏—Å—å —É —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—é ETL-–ø—Ä–æ—Ü–µ—Å—ñ–≤, —Ä–æ–±–æ—Ç—É –∑ —Ö–º–∞—Ä–Ω–∏–º–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞–º–∏ (AWS, GCP, Azure), –ø–æ–∫—Ä–∞—â—É—é—á–∏ —Å–≤–æ—ó –Ω–∞–≤–∏—á–∫–∏ –≤ Python —Ç–∞ SQL.
SQL —Ç–∞ —Ä–æ–±–æ—Ç–∞ –∑ –±–∞–∑–∞–º–∏ –¥–∞–Ω–∏—Ö
–û—Å–≤–æ—ó–ª–∞ PostgreSQL, –≤–∏–∫–æ–Ω—É—é—á–∏ —Å–∫–ª–∞–¥–Ω—ñ –∑–∞–ø–∏—Ç–∏, —Å—Ç–≤–æ—Ä—é—é—á–∏ —Ç–∞–±–ª–∏—Ü—ñ, —ñ–Ω–¥–µ–∫—Å–∏ —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑—É—é—á–∏ –∑–∞–ø–∏—Ç–∏.
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é DBeaver –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –±–∞–∑–æ—é, –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏ —Ç–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –¥–∞–Ω–∏—Ö.
–ü–æ–±—É–¥–æ–≤–∞ –¥–∞—Ç–∞-–ø–∞–π–ø–ª–∞–π–Ω—ñ–≤
–†–µ–∞–ª—ñ–∑—É–≤–∞–ª–∞ ETL-–ø—Ä–æ—Ü–µ—Å –¥–ª—è –æ–±—Ä–æ–±–∫–∏ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ CSV —É PostgreSQL.
–ê–≤—Ç–æ–º–∞—Ç–∏–∑—É–≤–∞–ª–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é Python —Ç–∞ Pandas.
–û—Ä–∫–µ—Å—Ç—Ä–∞—Ü—ñ—è –∑–∞–≤–¥–∞–Ω—å
–û—Å–≤–æ—ó–ª–∞ Apache Airflow: –Ω–∞–ø–∏—Å–∞–ª–∞ DAG –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è ETL-—Å–∫—Ä–∏–ø—Ç—ñ–≤.
–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü—ñ—è —Ç–∞ —Ä–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è
–ó–∞–ø—É—Å—Ç–∏–ª–∞ —Å–≤–æ—é –ø–µ—Ä—à—É –±–∞–∑—É –¥–∞–Ω–∏—Ö —É Docker, –Ω–∞–ª–∞—à—Ç—É–≤–∞–ª–∞ –∑–≤‚Äô—è–∑–æ–∫ –º—ñ–∂ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞–º–∏.
–°–∞–º–æ—Å—Ç—ñ–π–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø—Ä–æ–µ–∫—Ç—ñ–≤
–í–∏–≤—á–∏–ª–∞ –æ—Å–Ω–æ–≤–∏ Data Engineering —á–µ—Ä–µ–∑ YouTube-–∫—É—Ä—Å–∏ —Ç–∞ –∑–∞—Å—Ç–æ—Å—É–≤–∞–ª–∞ –∑–Ω–∞–Ω–Ω—è –≤ –ø—Ä–∞–∫—Ç–∏—á–Ω–∏—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö."
data engineer,"Project:
‚Ä¢   Mediation device for data collection and preprocessing  (IN, VLR, SMSC, SGSN, MMSC) and subscribers services control (IN, HLR, VoMS, Voicemail)
‚Ä¢   CRM subsystem for CRM campaigns management, CRM statistics reporting, CRM analytics
‚Ä¢   Hot-billing system for offline VAS charging ‚Äì fully functional convergent billing system optimized for fast and flexible service billing and charging
‚Ä¢   CORBA/SOAP based online charging subsystem
‚Ä¢   DWH and reporting subsystem
‚Ä¢   Subscribers migration module for Prepaid-postpaid migration
‚Ä¢   Subscribers notification module for flexible SMS and USSD subscribers notification and alerts
‚Ä¢   SIM management system
‚Ä¢   Customer care frontends
‚Ä¢   Monitoring subsystem
‚Ä¢   IT infrastructure integration (SAP R/3 and other accounting interfaces)
‚Ä¢   Fraud prevention system
‚Ä¢   Revenue sharing billing system for prepaid subscribers
‚Ä¢   Mobile network configuration DWH
‚Ä¢   IN monitoring\fraud prevention system
‚Ä¢   Campaigns management system for prepaid subscribers
‚Ä¢   Revenue sharing for postpaid subscribers
‚Ä¢   Revenue sharing and dealers management system
‚Ä¢   Data Quality FrameWork
‚Ä¢   Data Quality Process
Relational databases: SQL Oracle (PL/SQL), PostgreSQL, SyBase IQ
JavaScript (React, Redux, Saga), Web and Web Services
Unix, Windows
Jira, Kanban, Agile
Git, GitHub
C/C++
Python
REST, SOAP, XML, JSON, HTML, CSS
Deep knowledge of telecommunications architecture and network elements
Deep knowledge in software design, development, implementation, testing and support
Deep knowledge in DQ process"
data engineer,"Data Engineer with over 6 years of expertise in building and optimizing data solutions. Specialized in designing scalable ETL pipelines, implementing data warehousing solutions, and driving cloud migration initiatives. Proven track record of improving system performance and delivering data-driven solutions that directly impact business outcomes.
Led data engineering team, establishing best practices and coding standards
Architected and executed migration strategy from on-premises to cloud-based data platform
Developed automated reporting system, reducing manual workload by 40%
Optimized infrastructure performance, achieving 20% improvement in processing efficiency"
data engineer,"I'm professional Database developer with 15+ years of experience in IT industry.
All time I'm looking for interesting projects and always ready to non-standard challange/solutions.
I expect the long-term relationship with my clients and I will be glad to provide the qualify services in the term-of.
I'm experienced in:
1. Database development and design:
- MS SQL Server, MySQL, PostgreSQL, Amazon Redshift, MongoDB, Cassandra, SnowFlake, BigQuery;
- Index tunning, query optimization, performance optimization;
- Database design;
- Stored Procedures, Functions, Triggers, Views etc.;
- T-SQL;
2. Python:
- Machine Learning (OpenCV, PyTorch);
- Web scraping (Scrapy, BeautifulSoup, Selenium, XPath);
- Web development (Django Rest Framework, Fast API);
- Data processing/ETL (CSV, Excel, Pandas, NumPy, SciPy, PySpark);
- Other (GZip, Celery, SQLAlchemy, Redis, Multi-Threading, Regular Expression, RESTful, JSON etc.);
3. ETL development:
- Python, Matillion;
- MS Integration Service (SSIS);
- Apache NiFi;
- Talend;
- Airflow;
- Azure Data Factory;
4. BI (Business Intelligence) and tools:
- QlikView, Tableau, Power BI, MS Reporting Service (SSRS);
5. Other:
- Google Cloud Services;
- Amazon Web Services, S3, EC2, AWS RDS;
- Miscrofost Azure;"
data engineer,"Led the design and development of data-driven solutions to business problems. Combined responsibilities as a Data Engineer, Data Scientist, and Product Manager in a crowdsourcing project dedicated to democratizing internet access.  The project's core objective centered around inspiring community collaboration, allowing individuals to share wifi hotspots on a mapped platform, ensuring widespread internet accessibility.
Established and maintained a robust data lakehouse, facilitating the storage of vast data volumes. Engineered and implemented ETL pipelines for continuous big data processing, encompassing data collection, transformation, efficient storage, analysis preparation, and the delivery of informative features across the system to satisfy product needs.
Designed and developed a microservices architecture to process large volumes of data collected from mobile phones in real time, configured logging, testing and resolved issues with deployment.
Conducted comprehensive big data analysis using Apache Spark to derive insights crucial for the modeling process, ensuring data quality, and guiding hypothesis-driven product development across various product-related domains. Created user-friendly and informative dashboards for monitoring the product ecosystem, utilizing Redash, Retool and Databox.
Designed and engineered data-driven (ML) solutions for validating crowdsourced data and ensuring seamless internet connectivity for users. Implemented resume_classifier capable of both real-time and batch inference. Established system monitoring, anomaly detection, and alert systems for prompt issue resolution.
Technologies stack: Python, Rabbit MQ, Linux, Docker, Kibana, Elasticsearch, Logstash, PostgreSQL (AWS RDS), Redis, Apache Spark, Databricks, Autoloader and structured streaming, Data lakes and lake houses, AWS S3, AWS EC2, Google BigQuery, Retool, Cron, AWS SQS, ML Flow, FireBase, Clickhouse, Adapty, GreatExpectations, Redash
Algorithms and resume_classifier: K-d trees, Light GBM, regression analysis, beta distribution, hazard resume_classifier, shap values, Spotify annoy clustering, A\B tests, RFM analysis, etc.
Seeking a stable and enduring collaboration within a project that aligns with my interests and preferences."
data engineer,"Helping people build custom stream and batch data pipelines as well as finding useful, accurate and quality information from their data is not only my job, it is my hobby and my passion. It is your data, you should be able to access it, read it, trust it and make informed decisions based on it."
data engineer,"As a data engineer with 3 years of experience, I've worked extensively
with big data, including organizing data warehouses, optimizing
procedures, and utilizing tools such as SSIS packages. I'm also familiar
with the Azure Cloud platform. My proactive and confident approach to
problem-solving has helped me excel in my field, and I'm excited to
continue growing and exploring new opportunities."
data engineer,"Over 8 years of experience in data warehousing and reporting systems including creating DWH from scratch , custom logics and migrating  to cloud. Huge experience with legacy systems and reverse engineering  of some ancient or not-so-well-documented processes. Good knowledge in domains of medical insurance, Revenue cycle analytics, supply chain and warehouse tracking. C1 English. Lot of experience in explaining tech stuff to non-tech people."
data engineer,"Integration Engineer / Data Engineer (Data-Driven Processes) ‚Äì 2+ years
Developed and optimized data pipelines, ETL processes, and real-time data flows. Worked with production databases and collaborated with teams to build scalable data solutions.
OPS in Site Reliability Team ‚Äì 2+ years
Acted as a subject matter expert in Site Reliability Team, assisting in fast, reactive handling of production incidents.
L3 Technical Support ‚Äì 3 years
Provided advanced troubleshooting and resolved complex system issues. Highlighted responsibility in application support, ensuring the stability of enterprise applications, optimizing performance, and collaborating with engineering teams for escalations and system improvements.
5+ years of experience in Engineering and Data-Driven processes, with a strong foundation in designing, supporting, troubleshooting,and optimizing data solutions.
Commercial experience working in data-driven companies, handling large-scale data processing, analytics, and infrastructure in both B2B and B2C environments.
Hands-on experience with production databases, ensuring data integrity, performance optimization, and scalability in high-traffic applications.
Proven ability to collaborate in cross-functional teams, effectively working with engineers, analysts, and stakeholders to drive data-driven decision-making.
Results-oriented mindset, focused on delivering high-quality solutions that align with business goals and operational efficiency."
data engineer,"Proactive and results-driven software engineer with a deep understanding of the software development lifecycle. Focused on delivering tailored, scalable solutions that efficiently meet business requirements.
Committed to producing idiomatic, maintainable, and self-documented code by applying experience in coding best practices and design patterns.
Proficient with The Twelve-Factor App methodology, Domain-Driven Design (DDD), and Event-Driven Design (EDD) approaches. Hands-on experience with containerization, orchestration systems, cloud computing platforms, and distributed systems.
No legacy code preferable."
data engineer,"Specializing in finding optimal technical solutions for complex business problems, I utilize a comprehensive array of theoretical materials and a wealth of experience to apply best current practices uniquely tailored to each scenario. With a profound background in Python Backend Development, Machine Learning, DevOps, and data engineering, my expertise is well-suited to assist companies in these rapidly advancing sectors.
Skills:
I excel in Python programming and have extensive experience with frameworks and technologies such as FastAPI, Flask, and RESTful API. My proficiency extends to database management with MongoDB and PostgreSQL, enhancing data interaction through tools like Mongo Engine and Pydantic. I adeptly manage task queues with Celery, and utilize Docker for containerization, ensuring scalable and efficient application deployment. My experience with Alembic and Redis, along with messaging systems like RabbitMQ, strengthens backend infrastructures.
In Machine Learning, I specialize in Large Language Models (LLM), using tools such as LangChain and LangGraph to enhance natural language processing capabilities. My data engineering skills are demonstrated through my use of AWS services including Glue, Athena, Aurora, RedShift, and Quicksight, alongside data orchestration with Airflow and data integrity with dbt and Great Expectations. Advanced analytics and big data handling are facilitated through platforms like Spark, Snowflake, and integrations such as AWS Lambda Functions and OpenSearch.
Last Achievements:
- Under my leadership the technical team created an updated core product of the platform, which accelerated the development of client bots by 40 times and also reduced the cost of such a bot by 80 times.
- I took the initiative to develop a transparent and fair system for grading our technical specialists, which also includes an effective wage calculation method. This system ensures that everyone's contributions
are recognized and valued, fostering a positive work environment.
- Thanks to the development standards I set up, deploying updates has accelerated 10 times."
data engineer,"From 2016 November to 2018 July at Alfa Bank Ukraine as specialist of support ABS B2
From 2018 July to November 2019 at Art Consulting as Oracle developer
From 2019 November to 2020 May at Unitask Group as Senior ETL Developer
From May 2020 to December 2021 Ukranian processing Center Teradata developer
From December 2021 to present Emergn, Oracle database developer"
data engineer,I am a Senior BI developer and Data Engineer with around 7 years related experience with Microsoft and Amazon Data Tools and Technologies.
data engineer,"I manage Data Engineering team, that consists of Senior Data Engineers.
I built the team and established processes from the beginning of its existence. I made sure that each team member has enough information, clarity and technical knowledge to perform tasks, meets deadlines and everyone in the team can substitute each other.   I achieved it by having proper communication with the stakeholders and deliver updates asap to the team and everybody individually. By writing and keeping up to date internal Wiki knowledge base, I help newcomers with faster onboarding.
Certificates:
DP-203 Azure Data Engineer Associate (#381-8039)
Snowflake Hands On Essentials - Data Warehouse
Dell Boomi ‚Äì Associate Developer Certification
Dell Boomi ‚Äì Associate Administrator Certification
MongoDB ‚Äì M001: MongoDB Basics
Sololearn - Intermediate Python"
data engineer,"january 2024 - nowadays: Senior Data Engineer in RE Partners
november 2021 - december 2023:  Senior Data Engineer in EPAM
september 2020 - november 2021: Big Data Developer in telecom company Beeline KZ. Technologies: Python, Pyspark, Spark, Scala, Spark Streaming, Flask API, Docker, Parsing.
may 2019 - september 2020: Data Engineer in Kaspi Bank. Technologies: data consolidation, python, Oracle, SQL, parsing.
may 2019 - august 2019: Web Software Developer in Smart Technologies Solutions. Technologies: PHP, Laravel, Java, API.
october 2018 - october 2019: Web Development Trainer in Bitlab Academy.
Data validation system in Switzerland Reinsurance company
Mobile Advertising system in mobile telecom using ClickHouse and Flask API.
Bank notification integration between several banks in Kazakhstan and mobile telecom company.
Scoring system for issuance of credit.
Data consolidation system in Kaspi Bank."
data engineer,"Summary:
Remote developer, I work through monthly or weekly contracts. Located in Kyiv - Ukraine.
Looking for a remote position. (Preferably Data Engineering / ETL / Python / Ruby on rails / BI / ML / Data analysis / DBA / SQL) Background:
Database architect, Data warehouse management, ML, Data science, Data Analysis, BI Business Intelligence, Desktop software development:
Currently, my personal interest area is Python, AI, Fintech & ML & statistics mixture, Machine Learning / Neural networks / Data Science / Big Data and Data Analysis, OpenCV and Object detection/classification. In past , mainly worked on Windows API and DirectShow / DVB / Satellite technologies, Video player applications.
ML:
MLOps, Visual tracking, Outlier detection, Recommendation systems, Visual and Text classification, LLM  fine tuning, RAG implementation, Stable diffusion
Web dev:
When it comes to web, I am a well-experienced back-end programmer (Python, Django, Flask, Node.JS/Express, REST APIs, PHP) and can do some light front-end development too. (React at novice level, CSS, HTML, JS/TS)
Fintech:
Banking systems & broker integration, Automatization of financial strategies/systems, on Futures, Equities, FX, Options, Cryptocurrencies.
Languages I use:
Python
C / C++
SQL
Ruby on rails
JS / JavaScript
Delphi (Object Pascal)
Swift
PHP
Google script
GoLang
CSS/HTML
Tech stack and libraries I usually use and have experience with:
- AWS, Azure and some GCP, Docker, Kubernetes, IaC (terraform, terragrunt, cloudformation, localstack)
- SQL and NoSQL databases (Postgres, MySQL, MSSQL, DynamoDB) Columnar DWHs (Redshift, BigQuery) ETL Tooling DataBricks, DBT,  Spark, Kafka,  RabbitMQ, DMS, Athena
- Sci-kit, SkLearn, Scrapy, Tensorflow, CPython, Flask, Django,  Google App Engine / Firebase, WebSockets,  GStreamer, ffmpeg, Medialive, LightFM, XGBoost/CatBoost/LightGBM
- Windows API, QT, PyQT
- CircleCI, Github actions
Methodologies:
Scrum, Agile, Test-driven development, OOP, MVC, REST APIs, CI/CD, DevOp
Please let me know if you need further information or clarification.
Less meetings, more work"
data engineer,"Data Engineer | Kapital Bank | Baku, Azerbaijan | Jul 2022 - Present
- Redesigned and migrated more than 15 ETL packages from former projects to current projects.
- Designed and implemented 30+ ETL/EtLT pipelines using SQL, SAP BODS and various SQL databases.
- Administered nearly hundred ETL jobs, fixed them when it is broken, monitored and implemented relevant changes to prevent
possible future errors and performance issues.
- Modelling data and building data marts to minimize execution time of analytic queries.
- Tuning poorly performing SQL queries and typing ad-hoc report scripts.
- Rewriting report queries to migrate that operated on prior data warehouse.
- Searching for alternatives to ETL, data cataloging tools and testing their compatibility and harmony with existing architecture.
Business Intelligence Specialist | Veyseloglu Group of Companies| Baku, Azerbaijan | Sep 2021 ‚Äì Jul 2022
- Analyzed, designed and developed new data pipelines based on business requirements, using SSIS, Azure blob storage,
Synapse, Azure DevOps and other tools.
- Monitored performance of data pipelines and made changes to optimize the overall performance.
- Managed and supported 200+ ETL /ELT packages and 10+ SSAS Tabular resume_classifier.
- Developed ETL packages that automated manual tasks and saved 30 minutes of BI specialists daily.
- Analyzed storage and compute costs and reduced them where it is possible.
- Implemented security guidelines by using user filters and row-level security, minimizing private data exposure.
- Writing SQL scripts for ad-hoc reporting, optimizing slowly performing report scripts that runs on ERP applications.
Data analyst | Freelance | New Jersey/ New York, USA| July 2019 ‚Äì September 2021
- Built out the data and reporting infrastructure from ground up using Power BI, Tableau, Quicksight, SQL and Python to provide
near to real-time insights into the products and business KPIs.
- Redesigned and developed ETL processes to optimize data ingestion and check validation rules in various steps of pipelines
ensuring data accuracy.
- Created dashboards and reports to monitor data quality and performance.
- Analyzed business processes and generated data, suggested solutions which reduced the waste."
data engineer,"1. Data Lake for Ministry of agriculture
- Role:- Data Engineer
- Tech Stack:- airflow , docker , aws redshift , grafana
- Key Contributions:- Development of the ELT pipeline , Data visualization
- Duration:- 1.5 years
- Description:- An initiative funded by GIZ, the Ethiopian Ministry of Agriculture boasts numerous ongoing and past projects dedicated to nationwide soil and water conservation efforts. However, the scattered storage of project inputs and outputs across various systems posed a challenge in obtaining a comprehensive overview. To address this issue, the Ministry, supported by GIZ funding, embarked on an initiative to establish a unified data lake, streamlining access to both current and historical project data.
2. Public Auction Statistics
- Role:- Data Engineer
- Tech Stack:- airflow , dbt ,  aws s3 , postgres , grafana
- Key Contributions:- Business requirement gathering through interviews, Data modeling , ELT pipeline development , Data visualization
- Duration:- 4months / ongoing
- Description:- The ongoing project involves the development of a data warehouse tailored for our company's operations. Specifically, it focuses on scraping various Ethiopian auction websites to compile comprehensive auction data. The primary goal is to enable market analysis, providing insights into key metrics such as the distribution of auctions across companies, sectors, and regions. This centralized repository of auction information empowers our business to conduct informed decision-making and strategic planning within the auction sector."
data engineer,"Projects aimed at data processing and analysis in the context of Data Engineering. Python scripts that perform ETL processes (Extract, Transform, Load) using Spark, (Pandas libraries, NumPy), and other data processing tools. Also, unit tests to ensure code quality. skills in automating data processing processes and managing large volumes of data"
data engineer,"My current project stack is Spark Streaming, pySpark, Databricks, pytest, AWS (EKS, S3, Athena, Glue, other), Hudi, Argo CI, Kafka
Few years ago I started with Python and used it as a convenient way to manage different cloud and  bigdata APIs.
Now I improved Python skills much, cause project requires not only write pySpark code, but also cover it with unit/integration tests on pytest for Spark jobs.
Had some experience with Databricks, Snowflake, Cloudera (Hive, Spark), Azure.
Spark processing on java/scala.
Had some experience with Snowflake and Redshift, got SnowPro certificate.
Overall, I spend >10 years as an Oracle DB developer.
ERP for Retail chain, DWH for telecom company,
(PL/Sql, SQL optimisation).
Have some experience in Tableau
and much more hands-on experience with QlikView.
I'd like to know more about future team and tech stack first, while considering a proposition .
I set Salary in net, for gross there should be ~7500$"
data engineer,"As team lead expericenced in build data flow from scratch in  company
Has been involved to the Data Platform migration and support  (distributed data platform in K8s): S3 (MinIO), Apache Hive, Apache Spark
Development of ETL\ELT: dbt, Airflow, Airbyte, DuckDB
SQL/NoSQL: MS SQL, ClickHouse, Hive, PostgreSQL, Mongo, Trino;
Experience in build data catalog from scratch (DataHub)
Experience developing on Python (automatizations, scriptings, apps)
Development and support web service for internal clients (Django);
Some experience with Apache Kafka;
CI/CD tools: TeamCity, GitLab CI/CD
Monitoring tools: Kibana, Grafana, Greylog, OkMeter
BI platforms: Tableau, PowerBI, Apache Superset
Experience with Git, GitLab, TFS;
Supporting end users: troubleshooting, problem solving
Need to further develop as a Data Engineer in a modern team with advanced skills and technologies"
data engineer,"Writing SQL queries, stored procedures
ETL from various connected servers to MS SQL Server
Implementation of SSRS, Tableau, Power BI reports, as well as data visualization
Working with regular data updates
Formation of regular reporting
Development of an ETL system based on Excel calculations
Analysis of financial indicators
Writing SQL queries, stored procedures
Creating different jobs
ETL from different source to Google Big Query
Implementation of Power BI reports, as well as data visualization
Work with regular data updates
Formation of regular reporting on operational and financial efficiency of Business
Performing tasks in Bitrix
Writing Python scripts to check data
Connect google tables and advertising from Google Ads, Facebook Ads
Monitoring the work of subordinates
I have certifications SQL and visualization"
data engineer,"I am currently a data engineer with 4+ year experience, as following:
Designed and built Data Lakehouse on AWS.
Designed and created batch and stream processing data pipelines using various AWS services like Lambda, Glue/Glue Streaming, DMS, Kinesis Data Stream, S3, Redshift, SQS and Step Function.
Skills: SQL, Python, AWS, Airflow, Spark, ETL, Kafka"
data engineer,"- 10+ years of professional experience in the Information Technology industry, 6+ years in database development, 5+ years as Senior Data Engineer;
- Proven ability to translate complex business requirements into efficient, scalable, and high-performance data resume_classifier. Proven record of success in data migration, ETL procedures
development and support, and DB performance optimization;
- Python development experience with the integration of various data sources and API,  built data pipelines, development DB benchmarks, and other helpful tools and scripts for automation processes.
Aim: I wish to develop the area of work concerning the design and development of big data solutions, and get helpful job experience while solving new tasks within large and interesting projects.
Projects: Playtica ‚ÄúSlotomania‚Äù, Wininteractive ""Slots Craze DB, DWH"", Up games DWH, Choreograph. I have experience with the following technologies: Mysql, Postgres, Vertica, Kafka, Google Cloud, Hadoop, Hive, Presto, BQ, Airflow, Talend, Pentaho, Python, and other
Build ETL and DWH projects for Wninteractive and UP Games from zero point"
data engineer,"09/2023 - Now
DevPro / Artory   |    Kyiv / New York
NLP | AI | BACKEND | DATA  ENGINEER
Python, FastAPI, Airflow
Spacy, OpenAI, BERT, RoBERTa
SQL, MySQL, BigQuery, Elk Stack
GCP (certified), AWS, Docker, Terraform
Node, TS/JS
08/2022 - 09/2023
Tech-Azur / Cloudpay  |   Kyiv / Andover
FULL STACK ENGINEER
TypeScript, Node, Express, React, MUI,
SQL(PostgreSQL), AWS, GraphQL, Redis
10/2021 - 07/2022
Fairmart | Singapore
BACKEND | DATA  ENGINEER
Javascript (Node, React), AWS (Amplify, Lambda), Bash, Retool, etc.
04/2021 - 10/2021
Freelance | Beijing
SOFTWARE ENGINEER
AWS, Node (Express), PostgreSQL, Python, etc.
_____________________
02/2016 - 05/2019
""Phinnie"" Education Company | Beijing, China
FOREIGN HEADTEACHER
Managing the Foreign Teaching Department
Training teachers
07/2014 - 12/2015
""Fast Track English Club"" | Kyiv, Ukraine
founder/ ceo
Created a sustainable and useful business
09/2019 - 09/2021
""Better English"" Education | Baotou, China
HEADTEACHER
Teaching Kids and Adults
Training teachers
Certified   Google Cloud Associate Engineer
___
Trained and customized multiple Text Categorization and NER resume_classifier for unstructured data. Built inference and training pipelines around them. Managed team and infrastructure to finalize them as a service. Released and maintain the service in production.
___
Managed two multinational Education teams in China.
___
Created and managed my own business  when I was 20.
Cool team, challenging tasks."
data engineer,"Oracle Data Integrator Engineer:
supported existing data flows (Oracle Data Integrator), optimized the performance,
integrated new sources of the data (includes a Knowledge Module developing),
developed a new functionality (e.g. transferring data between schemas),
conducted code review activities,
enhanced the deployment process,
took part in migration from ODI 11 to ODI 12"
data engineer,"CRM Systems Development:
Developed and implemented CRM systems for business process automation, including customer data management, interaction tracking, and performance analytics.
Microservices Development:
Designed and implemented microservices for data processing and communication between different services. Ensured their interaction via REST API, enabling scalability and system maintenance.
Working with APIs:
Retrieved and processed data from other servers, integrated external services using REST API. Configured data exchange mechanisms between internal and external systems.
Page Layout and Frontend Work:
Created web interfaces and responsive pages using HTML, CSS, JavaScript, and jQuery. Ensured user-friendly UX/UI.
Creating Scripts and Financial Reports:
Wrote SQL queries and PL/SQL scripts to automate financial reporting. Generated analytical reports based on operational data, simplifying financial performance monitoring.
2019 - 2023 Taras Shevchenko National University of Kyiv, bachelor's degree in applied mathematics
2024 English intermediate course at Hillel IT-school
2020-2021, Frontend-basic course at Hillel IT-school
2020 Python Advanced course at Hillel IT-school
I want to find a company where I can develop and learn something new"
data engineer,"Areas of responsibility and achievements:
- Application and ETL pipeline development
- Architecture design, development and tuning
- Infrastructure design, development and tuning
- Migration to microservice architecture
- Microservice configuration tool development
Fixed distributed concurrency issues that led to lower CPU
usage and lower rate of network calls for a set of services
Solved resource usage issues that led up to 50% lower
rate of failure for monolith service
Solved and fixed performance issues that led up to 80%
lower latency and eventually to a higher number of allowed
concurrent processes
Reduced logging solution cost for 40%"
data engineer,"I‚Äôm an experienced Data Engineer / Business Intelligence developer with profound knowledge of tools and technologies such as Data Warehousing, ETL, and Data Modeling. Have extensive experience in designing, building, and optimizing complex ETL pipelines and data-driven applications across a range of industries.
I have technical proficiency in a variety of technologies including Python, Vertica, Single Store, Spark, Apache Iceberg, Jenkins, and Airflow. Recently expanded expertise to include Java and Spring in the development of data applications.
Have a strong database management background, utilizing both SQL (Vertica, Postgres, Oracle 11g/12c) and NoSQL (SingleStore) databases.
Responsibilities:
-	ETL Pipeline Optimization: Created and optimized numerous ETL pipelines to support data transformation and integration processes.
-	PII Database Management: Developed a centralized PII storage system, incorporating Kafka for real-time data processing and SingleStore as the primary database
-	GDPR Compliance Service: Led the design and implementation of a GDPR service that automates the discovery and masking of user data across multiple platforms.
-	Data Retention: Development of a Data Retention Application using Java, Spring, and PostgreSQL to enhance data storage compliance in Vertica/Iceberg tables.
Tools & Technologies: Python, Spark, Airflow, Jenkins, Vertica, PostgreSQL, SingleStore, Java, Spring"
data engineer,"Azure Data Engineer
WKW Webster | September 2024 ‚Äì Presenter
Led the successful migration of FoxPro databases and codebase to Azure using SSIS, redesigning and optimizing legacy FoxPro logic to replicate complex Excel report functionality via custom stored procedures. Leveraged Azure Data Factory to orchestrate automated overnight data extracts, ensuring secure client delivery via encrypted FTP transfers. Implemented CI/CD pipelines with Azure DevOps to streamline database deployment, enhance automation, and improve operational efficiency.
Lead SQL Server Developer
Care Control | April 2024 ‚Äì August 2024
Designed backend for new payment schedule and invoicing system to enhance existing invoicing structures.
Migrated access database legacy system to SQL Server 2019 ready for cloud migration.
Build backend stored procs to deliver JSON payloads to new cloud apps.
Wrap existing legacy stored procedures to enable use in cloud.
Lead Data Engineer
Lloyds Bank | June 2022 ‚Äì March 2024
Lead data engineer developing Performance Reporting, for all Large Corporates.
Successfully Implemented conformed Dimensional Model to allow the business to deep dive into Relationship Managers performance via Power BI
Build out full ETL for Financial data into facts and dimensions using SSIS pipelines.
Translating business requirements into User Stories.
Consolidating disparate source systems into single confirmed Dimensional Model.
Lead SQL Server Developer
HSBC | August 2021 ‚Äì May 2022
Lead the design and development of all Prudential Regulation Authority Outsourcing SS2/21 requirements.
Lead the design and development of TPM breach process, exposing breaches in standard procurement protocols.
Developed end to end automation of monthly data ingestion reducing output time from 5 days down to hours.
Lead SQL Server Developer
Total Care Facilities | December 2019 ‚Äì July 2021
Designed, developed and migrated Excel booking system to SQL Server.
Developed all booking system reporting such as forecasted hours vs actuals using Power BI.
Implemented reporting to understand the impact of Furloughing employees.
Automated importing of monthly employee data.
Developed online forms for sales agents using google forms, importing into SQL Server.
Optimise data flows to ensure reports are timely."
data engineer,"Raiffeisen Bank Ukraine	Lviv
Data Analyst	October 2024 ‚Äì Present
‚Ä¢	Create dynamic and insightful Power BI dashboards, leveraging DAX for advanced calculations and data modeling to support decision-making.
‚Ä¢	Designing and maintaining SQL Server stored procedures, develop and manage ETL SSIS packages, ensuring seamless data integration and transformation processes.
‚Ä¢	Develop stored procedures in Amazon Redshift for automating and managing complex SQL workflows, and use Amazon Athena for querying large-scale datasets efficiently.
PwC Ukraine	Lviv
Financial Security Analyst	May 2024 ‚Äì October 2024
‚Ä¢	Automated manual data tasks by developing VBA macros / Python scripts, resulting in an average of 70% reduction in time consumed.
‚Ä¢	Merging, transforming, cleaning data to meet analytical needs (Alteryx, Power Query, Python)
‚Ä¢	Conducting comprehensive research and analysis on ad-hoc requests from colleagues, thorough quality checks on financial reports submitted to the court to ensure accuracy of calculations, etc.
University machine learning competition focused on fraud detection - 3rd place. Automated a lot of manual data tasks which resulted in a 75%+ reduction in the time required in PwC. Automating reporting in international bank."
data engineer,"Experienced data engineer with certified expertise in AWS cloud software development and architecture, databases and SQL, Spark, Python, CI/CD , Databricks, ETL. I have experience working in team, leading a team, working solo and duo. Had projects were i needed to do architecture, planning, developing and testing myself for Economist,  had a lot of projects working with a team, team of backend developers, or data science team, other data engineers or product managers.
Senior Data Engineer at PartsID, current, (AWS, Databricks, Spark, SQL, Terraform, BI)
Senior Data Engineer at Intellias, 5 month, (AWS, spark, Redshift, Glue, DMS, Postgres, Terraform, Python, sql)
Data Engineer at Zoolatech, over 2 years (AWS, Teradata, kafka, Airflow, spark, presto)
Data Engineer at Genuisee, 1,5 year (Databricks, AWS, Spark, terraform,  kafka, elasticsearch)
QA consultant at Economist, 7 month, (AWS, ts, cicd, cypress)
Development Lead at CFRA, over 2 years, (AWS, Python, SQL, Spark, Terraform, Oracle, RestAPI)
Database developer at SoftServe (SQLServer, SQL, Java)
Achievements that i am personally most proud of is my experience, variety of different projects i participated, roles i had and architectures i seen and helped design. Second is my communication skill, being good team plater and not having any negative feedback ever. Third is aws cloud expertise and solutions architect certification.
I want ability to do my job properly, to discover and to implement myself good solutions, to optimize and improve workflows, i like doing that. What i dont like is bad planning and urgent tasks, being pulled in different directions and poor (not clear) requirements."
data engineer,"Briefly about myself: I have studied World Economy for Bachelors and MBA (Computer Information Systems) for the Master Degree. My career started as an Intern in the field of Insurance first, then I worked at Veyseloglu Group of Companies as HR Business Partner, at MKT Cotton LLC (Joint Venture of Gilan Holding and PMD Group) as a Learning and Development Specialist. In between 2021-2023 I worked as MS Excel Expert Trainer as a second job. Now I am working at Turan Drilling and Engineering Company (JV of KCA Deutag and SOCAR AQS) as a HRIS  & Payroll Coordinator. I got Microsoft Excel Expert MO211 certification and PL-300 Power BI certification in 2024 and 2025 respectively. I daily use Excel, PowerBi and PowerQuery and I have limited proficiency on SQL, Python. In addition, I have experience with ERP systems like SAP, 1C, PRONET and HRB.
Learning environment, money isn't my preference, I could work even for free, I just want to be a part of Data field and learn as much as I can."
data engineer,"- Coding using Java, Python, Groovy
- Designing and implementing ETL processes for data pipelines using Glue, PySpark, Spring Batch
- Building predictive resume_classifier using machine learning techniques (XGBoost, Random Forest, Sagemaker, Deep Learning)
- Design and development of web services / microservices for ETL steps (Spring, Falcon) and ML resume_classifier (scikit-learn, PyTorch) and its CI/CD integration (Docker, Kubernetes, AWS, CircleCI, Git)
- Anomaly detection using SparkML
- Designing and developing various data mining applications for natural language processing: web crawlers (HtmlUnit/HttpClient) and search systems (Lucene)
- Database knowledge: SQL (MySQL, Aurora, Postgres) and NoSQL (OrientDB, DynamoDB)"
data engineer,"Experienced data professional with expertise in data science, quantitative development and data engineering. Skilled in building data pipelines, ML resume_classifier, and implementing quantitative strategies for financial, energy and crypto markets. Proficient Python developer with extensive experience in cloud platforms and big data technologies.
Very experienced, multiple successful projects.
Challenging and interesting projects. Open minded colleagues. Ethical employer."
data engineer,"I am BI developer with about 5 years of experience in the field Business Intelligence.
I have huge experience in SQL Database development using mostly TSQL , PL SQL, MySQL and many others;
ETL work using SSIS and Azure Data Factories mostly ;
Datawarehouse Design , Report writing using different tools like PowerBI and SSRS mostly."
data engineer,"Motivated and detail-oriented Data Engineer with hands-on experience in database development, ETL processes, and data warehousing. Skilled in SQL Server, T-SQL, SSIS, and Python, with a strong foundation in data modeling, transformation, and optimization. Experienced in integrating and processing large datasets, designing relational and warehouse databases, and building reports using Tableau and Power BI. Proven ability to ensure data consistency, automate workflows, and contribute to business intelligence initiatives. A reliable team player with strong analytical and problem-solving skills, eager to apply knowledge in real-world data-driven projects."
data engineer,"Data Engineer / Analytics engineer
Experience:
1. Kimball Data architecture implementation for Mobile games.
a. ELT pipeline design in GCP (Dataform, schedulers, Cloud functions)
b. Data integration (sources: API, S3 bucket, Firebase, BigQuery)
c. Master Data table design
d. Data Marts design
e. OLAP tables for BI instruments (Tableau, Metabase)
- metrics: Retention, Churn, Level progress, Installs, DAU, sessions, traffic source, ROAS, etc.
- cohorts: install dates, ad campaigns, countries, devices, depositors, etc.
2. Install table (master data) for Mobile game with attribution. ELT pipeline based on sources:
a. RAW in-app event data to define Installs
b. User level Attribution data (fetched from Appsflyer by API or Singular)
3. ROAS aggregate table (OLAP) and Visualization. ELT pipeline that includes:
a. API extract of costs from Meta Ads, Google Ads
b. API extract of Ad Revenue from Appodeal, AppLovin
c. in-app purchase revenue based on Firebase in-app event
d. Pipeline design in Dataform to get OLAP data connected to BI
4. Open source BI instrument setup on GCP:
a. Metabase docker container launched in Google Cloud Run and connected with PostgreSQL DB (needed for meta data) using Google Cloud SQL service.
5. Telegram channel parser:
a. Scheduled Python service launched in docker in GCP and loading data to BigQuery.
6. Automatic creation of In-app purchases in Appstoreconnect, through API
a. I have created a Python service that handles a batch creation of in-app purchases in AppStoreConnect through API. Service is managed from Google Sheet (UI created using AppScript)
Tech stack & hard-skills:
- Database design (OLAP), Data modeling (Kimbal, OBT), ELT / ETL, Data integration
- Data analysis
- SQL, Python, AppScript/JS
- BI: Tableau, Metabase
- Google Cloud Platform (BigQuery, Dataform, cloud functions, secret manager, pub/sub, scheduler, cloud run, docker containerization)
- API: S3, Meta ads, Google ads, Exmox, Singular, Applovin, AppsFlyer, Appodeal, AppStoreConnect
- In-app events design (client/server side) for tracking systems
- Jira, Confluence, Google Docs, Notion
Soft-skills:
- 10+ years‚Äô experience of entrepreneurship in software development services.
- 8+ years‚Äô as Project manager, Product owner.
I have a registration as Self entrepreneur in Finland, I pay taxes by myself."
data engineer,"Aspiring Data Engineer with background in Java development and a Bachelor‚Äôs degree with honors in Computer Science. Skilled in Python, Apache Airflow, PostgreSQL, and working with APIs for data processing. Currently transitioning to Data Engineering, focusing on building automated workflows and efficient data pipelines. Pursuing a Master's degree in Information Computer Science.
Academic Project: Autoencoder Based on Convolutional Neural Network
- Developed an autoencoder model for handwritten digit recognition using Convolutional Neural Networks (CNN).
- Technologies used: Python, TensorFlow/Keras, image processing, and data analysis.
- The project involved designing the model architecture, training it on the MNIST dataset, and evaluating accuracy.
Udemy Course: Data Science Fundamentals
- Completed a 3-month course covering machine learning, data analysis, and key libraries such as Pandas, NumPy, and Scikit-learn.
- Worked on several hands-on projects, including data analysis and building machine learning resume_classifier.
Pet-project: Automating Data Collection and Storage.
Developed an automated ETL (Extract, Transform, Load) pipeline that connects to an external API to retrieve data, processes the data as per requirements, and stores it in a PostgreSQL database. The process is implemented using Apache Airflow.
Technology Stack
- Apache Airflow: Workflow orchestration and DAG management.
- PostgreSQL: For storing the processed data.
- Python: For implementing the ETL process.
- Docker: For running the project in an isolated environment.
06.2024 - 'Computer Science' bachelor's degree with honors"
data engineer,"PostgreSQL, Clickhouse, Greenplum, MySQL, Snowflake, BigQuery, Redshift, DBT, Kafka
Modeling relational databases (ER modeling, Relational schema, Normal forms), data modeling 3NF/Dimensional (Star,Snowflake,DataVault)
Python, Django, FastApi, Pandas, Polars
Looker,Metabase,Tableau,Superset
Docker, Git (GitHub, Actions), Airflow, Dagster,
Nginx, ElasticSearch, Terraform,
Anorbank.uz
Data Engineer
Financial Sector - Banking
Building DWH with:
- Airbyte-core
- DQ with soda-core
- Greenplum(GPFDIST) as central DWH for structured data
- Minio as object storage for unstructured data(data lake)
- Working with GCP(BigQuery)
- Applying dbt-core for transformations
- Clickhouse for datamarts(serving layer for BI - apache superset)
- Airflow as the main orchestrator tool
September 2023 - May 2024
Apex Bank
ApexBank.uz
Financial Sector - Banking
Data Engineer
1. Apache Spark(PySpark) on k8s for compute
2. Apache Hudi on S3(MinIO) as datalake
3. Greenplum as DWH
4. Clickhouse as serving layer for BI (still in progress)
5. Airflow for orchestration
September 2022 ‚Äî September 2023
Itransition
www.itransition.ru
Data Engineer
Data engineering in the mobile gaming studio. The goal is to get and
structure the data about players, games, finance and
allow data science team to build forecasts based on
this data.
Main responsibilities:
1. BigQuery migration to Snowflake
2. ELT with Snowflake-tasks and Airflow(DBT/raw-sql transforms)
3. Preparing semantic layer for Looker with LookML
4. Building Looker reports and dashboards boards
Activities includes work with Python, Apache Airflow, GCP/AWS, Snowflake/BigQuery, Looker(LookML), SQL, DBT, etc.
September 2021 ‚Äî September 2022
EPAM Systems Inc
5-month BigDataEngineering project + training on Azure
Spark with Databricks(Azure) + AWS with Terraform
Python Engineer
Extracting and data validation for RSS feeds.
Extending functionalities of existing lambdas.
- Python, AWS(Lambda)
June 2020 ‚Äî September 2021
AlifTech
Data Engineer
- Improved initially working with MySQL Tableau performance by replacing with MongoDB.
- Wrote data pipelines using Apache Airflow (including data retrieving from several MySQL instances, processing them on Pandas and storing into MongoDB as data warehouse).
- Automated daily manual jobs with Airflow including sending daily report to other departments.
- Automated ML model builds by refactoring code into Airflow DAGs and Tasks.
- Airflow, python, pandas, FastAPI, Tableau"
data engineer,"I Interviewer Startup
Data Science Intern | Python, LLM, Docker, AWS Oct 2024 - Jan 2025
‚Ä¢ Designed and implemented a web application as the MVP to simulate job interview processes for technical roles, by
enabling interactive question-and-answer sessions in a chat format to evaluate candidates‚Äô suitability through the
developed system improving evaluation efficiency by 15%.
‚Ä¢ Leveraged real-time speech-to-Text using AWS account with accuracy 99% and LlamaIndex from Hugging Face LLM to
evaluate responses of a candidate with accuracy 85%.
‚Ä¢ Deployed application using Docker on AWS.
CGS-Team
Machine Learning Engineer intern | Python, NumPy, PyTorch, Sklearn, Pandas, Matplotlib Aug - Sep 2024
‚Ä¢ Machine Learning: Conducted Exploratory Data Analysis (EDA), performed Feature Engineering, and implemented
Regression, Classification, and Clustering techniques using Sklearn and Pandas.
‚Ä¢ Deep Learning: Designed and trained resume_classifier for tasks including CNN-based image classification, semantic
segmentation, generative adversarial networks (GANs), and Text classification using PyTorch.
-"
data engineer,"Experienced Engineer with over 8 years in IT, specializing in developing complex data-intensive web applications. Possesses strong theoretical knowledge complemented by practical Python, SQL, Linux, and Google Cloud Platform (GCP) skills. Certified as a Google Cloud Data Engineer. Known for a responsible and proactive approach, coupled with the ability to adapt to rapidly new technologies.
I took an active role in designing and building complex web platforms based on microservice architecture. Also, I build highly intensive streaming data pipelines and complex batch processes. I gained two GCP certifications: GCP Professional Cloud Architect and Data Engineer."
data engineer,"I have more than 6 years of experience in big coportations on Data Engineering field. Solid knowledge in SQL and Python. I have worked on cloud infrastructures such as Amazon AWS (Redshift, Glue, S3, SQS, Glue, EC2, IAM, Matillion, Lambda), IBM Cloud (COS, DB2, IBM Infosphere). Working with large engineering teams in my previous workplaces like HBO MAX, IBM - I have experience with data processing and scheduling tools like Apache Spark, Apache Airflow, DBT, Kafka, Firehose, Cron Jobs. Familiar with database/storage solution like Snowflake, PostgreSQL, DynamoDb and MySQL. Created complex ETL pipelines (both batch and streaming) in above-mentioned tools, also with Python and PySpark using Databricks. For cloud infra, I used terraform, GitHub CI/CD, Kubernetes, Docker. Created dashboards both from scratch and templates in Power BI and Tableau, Grafana. I have experience in modelling and creating DWH. Mentored a small group of data engineer trainees. I am also experienced on PII data management including GDPR regulations. Please refer to my resume for further details."
data engineer,"With over seven years of experience as a Data Engineer and BI Expert across diverse sectors, including banking, government agencies, and dynamic startups, I possess a proven track record in developing and deploying robust data-driven solutions. My expertise spans the entire data lifecycle, from architectural design and ETL pipeline development to data cleansing, BI infrastructure implementation, and impactful data visualization.
Specifically, my skillset aligns perfectly with the development of a sophisticated churn prediction model for Push30. My experience encompasses:
Data Warehouse Architecture: Designing and implementing scalable data warehouses to support complex analytical requirements, ensuring data integrity and accessibility.
ETL Pipeline Development: Constructing efficient and reliable ETL pipelines to extract, transform, and load data from disparate sources, ensuring data quality for model training.
Data Cleansing and Preprocessing: Implementing rigorous data cleaning and preprocessing techniques to address missing values, outliers, and inconsistencies, maximizing model accuracy.
BI Architecture and Data Visualization: Designing and deploying BI solutions that provide actionable insights, enabling data-driven decision-making.
Machine Learning Model Support: Experience with providing cleaned and prepared data to machine learning teams, and understanding the requirements of a machine learning workflow.
My work history has instilled a strong understanding of the importance of clear problem definition, rigorous data analysis, and effective communication. I am adept at translating business objectives into technical solutions, and I am committed to delivering high-quality, impactful results.
Furthermore, I operate with a high degree of flexibility, accommodating remote work and diverse time zones."
data engineer,"Data cleaning using pandas and apache spark. Creating data pipelines using Nifi. Orchestration of ETL processes using Apache Airflow. Creating gitlab ci pipelines for kubernetes deployments. Writing simple data visualization pages on Streamlit. Installation and configuration of services on kubernetes cluster using helm and kustomization (Apache Airflow, Jupyterhub, Grafana + Prometheus, Keycloak). Installation on local cloud using Juju.
Creating  and configuring workspace for data scientists on jupyterhub allowing them to choose their own environment, organizing  all the connection and access to minio, hdfs, and spark cluster.
Creating first version of chatbot using Rasa and telegram bot api.
Installation and configuration of Apache Airflow on kubernetes. Creating first dags in a company for ETL processes.
Collecting all money transfer reports from different transfer services and creating unique board for automating it.
Streamlit app with the functionality of implementing model on datasets, visualization of inputs and results, and automatic plot builder."
data engineer,"Technologies:
Python, Django, django-rest framework, PySpark, ETL,  pandas, scikit-learn, Postgres, MySql
Machine learning algorithms(linear, logistic regression, decision trees, etc.)
Linux, bash, Docker, CI/CD, Jenkins
General responsibilities:
tech-group lead, code-review, knowledge management"
data engineer,"I have over 10 years of hands-on experience in IT projects, having worked in distributed international teams. Interacted and collaborated closely with team members, stakeholders, and solution users across various business domains, including banking, insurance, logistics, and more.
I have extensive experience in data engineering, data pipelines and data model development, with a strong background in system integration, process modeling, and business analysis. Skilled in designing robust, scalable systems and optimizing processes.
Projects:
- implementation of DWH and ETL processes from scratch in a big logistic company as ETL developer/data
engineer
- development of the system designed for IoT data ingestion, transformation, storing and representation
as a Python developer/data engineer
- core bank system implementation and data migration as a technical business analyst
- development and implementation of B2C application for an insurance company in UAE as a system analyst
Key responsibilities:
- ETL pipelines implementation - development and maintenance
- development of APIs, data ingestion and processing services
- data modelling
- data analysis and comprehension, verification, validation, cleaning, formatting, mapping etc.
- system integration (modelling, specification, implementation)
- testing (bug analysis and resolving)
- live support (bug analysis and resolving)
- business and technical requirements elicitation, business process modelling, gap and system
customization analysis at the development and implementation stages"
data engineer,"I worked as a Customer Support Specialist at the domain registrar company Namecheap.
My main responsibilities included:
‚Ä¢	Assisting clients in resolving various issues with DNS records related to Email services
‚Ä¢	Creating tasks in Jira to address specific problems on the company‚Äôs side.
Microsoft Azure fundamentals certification (az-900)
Hi, I want to be a data engineer!"
data engineer,"‚Ä¢ –∏–Ω—Ç–µ—Ä–Ω–∞—Ç—É—Ä–∞ –≤ IT –∫–æ–º–ø–∞–Ω–∏–∏ (Junior Data Analyst)
‚Ä¢ –∫—É—Ä—Å—ã DataQuest: Data Analyst in Python
..................................SQL Fundamentals
..................................Analyzing Data with Power BI
‚Ä¢ –∫—É—Ä—Å—ã DataCamp: Data Analyst in Python
.................................Associate Data Analyst in SQL
.................................Data Engineer in Python
.................................Associate Data Engineer in SQL
‚Ä¢ –∫—É—Ä—Å—ã CS50 Harvard: Introduction to Python
–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π –≤ –¥–∞–Ω–Ω–æ–π –æ—Ç—Ä–∞—Å–ª–∏ –Ω–µ—Ç, –¥–æ —ç—Ç–æ–≥–æ —Ä–∞–±–æ—Ç–∞–ª –ø–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏ 3D Generalist (–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞) –ø–æ—Ä—è–¥–∫–∞ 15-—Ç–∏ –ª–µ—Ç."
data engineer,"Data engineer that switched from .NET. Have expirience in writing Spark jobs, Airflow dags, aws lambda expiriense, SQL databases, non-relational databases. Have knowledge in Python and some knowledge in Scala.
Was  .NET developer with 3+ years expirience.
Developed 2 projects during this time:
Healthcare domain (Microservices, Web
API, PostgreSQL, Angular) and Finance
domain (VB.NET, WebForms, MSSQL,
Javascript). Have expirience in utilizing
technologies such as: Entity Framework,
Angular, Redis, Kafka, JasperStudio,
Azure DevOps .
Participated in mentoring new employees,
was giving lectures in internal school.
Certificates:
AWS Certified Data Engineer ‚Äì Associate
Microsoft Certified: Azure Fundamentals
Had good relationships with client. Mentor a lot of trainees who became a good developer."
data engineer,"Background:
I got interested in data science and AI in 2013. I started working as a software engineer in Gamedev at roughly the same time. I have a lot of academic experience in data and have done many projects, but never worked as a Data Engineer. But I'm sure that my experience amount to something, so I set it as 6 months.
Skills
I have a strong foundation in Python and SQL and experience in ETL/ELT pipelines, PySpark, Airflow, and Databricks.
1. General:
- Python (pandas, NumPy, regex, OOP)
- SQL (PostgreSQL, MYSQL)
- Shell scripting (bash)
2. Data Engineering & Big Data:
- ETL/ELT pipelines
- PySpark
- Apache Airflow
- Databricks
3. Data Manipulation & Analysis:
- pandas
- scikit-learn
4. Cloud & Containers:
- Docker
- AWS services (S3, EC2, Sagemaker, Lambda)
- BigQuery
- I have earned a Data Engineer certification from DataCamp.
Also Data Analyst and Data Scientist certifications.
- Sagemaker deployment
This project is part of Udacity Deep Learning Nanodegree program.
Built and deployed a neural network that predicts the sentiment of a user-provided movie review and uses your deployed model through a simple web app.
- Dog Breed Classifier
This project is part of the Udacity Deep Learning Nanodegree program.
I designed and trained a convolutional neural network to analyze images of dogs and correctly identify their breeds. Used transfer learning and well-known architectures to improve this model.
- Face Generation
This project is part of Udacity Deep Learning Nanodegree program.
I built a Deep Convolutional GAN for generating new, realistic faces. I trained it on a set of celebrity faces.
- Predicting Bike-Sharing Patterns
This project is part of Udacity Deep Learning Nanodegree program.
Build and train neural networks from scratch to predict the number of bike-share users on a given day.
- TV Script Generation
This project is part of Udacity Deep Learning Nanodegree program.
Built a recurrent neural network with pytorch to process Text. Used it to generate new episodes of a TV show, based on old scripts.
- Self Driving Car Engineer projects for computer vision part of the nanodegree
I want to grow my skills in Data Engineering and AI"
data engineer,"Senior Database Engineer with over 14 years experience in development and administration of MS SQL Server. Proven experience working with Azure SQL and DB administration tasks. Proficient in complex stored procedures and useful functions to facilitate efficient data manipulation and consistent data storage using T-SQL.
Hands-on experience in using Microsoft BI studio products like SSIS, SSAS, SSRS, Power BI for implementation of ETL in datawarehouse.
Good experience to automate tasks and performance optimization. Effective team player, punctual and purposeful. Fundamental background in Computer Software Engineering and MBA background.
Experience
Senior Data Engineer
September 2021 ‚Äì Present
Responsibilities:
Worked with customers on projects required data engineering and management
Used MS SQL, Azure and Snowflake
Recent Projects:
Developing custom BI software and DWH (Snowflake)
Developing DWH, ETL and creating reports (Azure Synapse, Power BI, Azure Data Factory)
Administration of DWH in Synapse
Administration of MSSQL servers
Migration of on premises MSQL to Azure (Azure SQL, Azure MI, VM)
Building near-real time DWH staging layer in Azure
Gold Mining Company
Senior Database Administrator
July 2013 ‚Äì August 2021 Largest Gold Mining company.
Supported bunch of systems in One of the largest Gold Mining companies
Worked with MS SQL (2000-2019), SSIS, SSAS, SSRS
Managed Migration to MS Azure projects
Written new solutions with Azure
Built new Transaction Log shipping backup system and standards
Developed standards and check their implementation in T-SQL scripts
Responsible for all production systems and databases
Consulted Developers team in code review and developing new scripts
Migration of all company's products in to newer servers and DBMS
Administration of Servers, managing backups system, Optimization of network load (Veeam
Backup, Office 365, Hyper-V, AD, MS Exchange, Cisco, Fortigate, Juniper)
Managed server loads, memory, CPU, Hard drives, etc.
Tools and technologies used: Azure, T-SQL, MS SQL, SSIS, Always-on, SSAS, SSRS, Power BI, SQL Database, PowerShell, C#, Hyper-V, AD, MS Exchange, Cisco, Fortigate, Juniper."
data engineer,"Results-driven Software Engineering Professional with over 13 years of expertise, specializing in Business Intelligence and Data Management Applications. Adept at leveraging analytical skills, demonstrating hands-on proficiency in MSSQL, Snowflake, Amazon Redshift, and Amazon Athena interacting with S3. Skilled in programming, optimization, clearance, and data integration. Possesses additional non-business experience with MySQL and PostgreSQL. Proven problem solver with a track record in designing and implementing ETL processes using Matillion and Microsoft Integration Services. In addition, non-business experience with applications like Airflow and DBT. Experienced in developing Reporting Systems based on Microsoft Reporting Services. Recognized for strong teamwork and communication skills."
data engineer,"I have 3 projects during my career. Here is a summary of my responsibilities:
Database creation and administration (SQL)
Controlling Database-Server Interaction (C#)
Control over data integrity (SQL)
Efficiency analysis and improvement of the DB functionality (SQL)
Working with Reporting Services (C# + SQL)
Database implementation in Snowflake
Creating RAG for OpenAI via Langchain
Improvement of server functionality (C#)
Working with MVC (C# + JavaScript)
Web API development (C#)
Building web applications with React.js and Angular
Cloud resource management in Microsoft Azure
Working with DevOps Pipelines for CI/CD
I'm looking for a job related to the implementation of interesting systems with complex data storage and management architectures (both in the cloud and on-premise)"
data engineer,"I have extensive experience with BigData and cloud, especially in performance tuning and optimizing data pipelines.
On my current project in last 2 years:
I built the infrastructure for Aiflow(MWAA) + EMR in AWS.
Rewritten and optimized existing data pipelines.
Which resulted in cost savings on AWS infrastructure more than 20k USD per month.
Used technologies:
‚Ä¢ Scala, Python, Java
‚Ä¢ AWS (S3, Redshift, EC2, EMR, MWAA, Lambda, RDS, SQS, Athena, Glue)
‚Ä¢ GCP (BigQuery, GCS, PubSub)
‚Ä¢ Akka, Play
‚Ä¢ Cats, Cats Effect, Shapeless
‚Ä¢ ScalaTest, ScalaMock, Mockito, Cucumber
‚Ä¢ Sbt, Maven, Gradle
‚Ä¢ Postgres, Cassandra, MongoDb, Oracle, SqlLite, MSSQL
‚Ä¢ Spark, DeltaLake, Presto, Druid, Hive, Impala, Zookeeper
‚Ä¢ Etcd, Vault
‚Ä¢ TeamCity, Jenkins
‚Ä¢ Kafka, RabbitMQ
‚Ä¢ ELK, Grafana, Prometheus, Graphite
‚Ä¢ Ansible, Terraform
‚Ä¢ Airflow, Luigi
‚Ä¢ Docker, Kubernetes
For last 4 years working in startups"
data engineer,"Algoseek February 2024 - July 2024
Developing data pipelines was involved in data processing. Working with the AWS platform, implementing DuckDB into the project. As a Salesforce developer  had a lot of communication experience
Developing data pipelines was involved in data processing. Working with the AWS platform, implementing DuckDB into the project. As a Salesforce developer  had a lot of communication experience
Ability to continue grow up like a professional"
data engineer,"RedCloudTechnology Apr 2023 - Nov 2023 - Data Engineer
-Built the Data Engineering foundation for an open commerce company mainly using AWS lambda functions, Snowflake,
DBT, Python, Fivetran, Mage.ai, and AWS Data Engineering tools.
- Migrated legacy SSIS-based procs to a DBT model to Snowflake and optimized the queries to improve performance.
Excellerent - Nov 2021 - Oct 2023 -  Data Engineer
- Worked as a team lead, implemented and managed Airflow workflows within Kubernetes clusters, orchestrating job deployment and scheduling through Azure Pipelines, optimizing data warehousing processes using Snowflake, and wrote different transformation scripts for restaurant systems data ingestion and reporting.
- Developed a reliable method for ingesting large datasets into an enterprise-scale analytical system using Fivetran, Snowflake, and Looker which increased customers by at least 10%.
- Developed ETL processes using SQL Server Integration Services (SSIS) to extract, transform, and load data from various sources into data warehouses/marts.
10 Academy - Web3, Data Engineering, and Machine Learning Tutor (Contract)  Aug 2022 - Dec 2022
Designed curriculums and gave various tutorial sessions for the top selected 50 students from the entire Africa.
Presented 10+ workshops on Data Engineering, Deep Learning, and Machine Learning Operations and how-tos on Python, Git, Docker, CML, DVC, MLflow, GitHub actions, Airflow, DBT, and, Kafka.
Gave sessions on the fundamentals of blockchain, Dapp‚Äôs on Ethereum, and Smart Contract writing using Solidity & Algo.
Addis Software - Oct 2021 - Nov 2021 - Full-Stack Developer
TECHIN - Feb 2021 - Oct 2021 - Web Developer | Data Scientist
AIESEC - May 2020 - Feb 2021 - Node.JS developer | Partner Manager
MN ale Addis - Sep 2020 - Nov 2020 - Node.JS Developer
Certified in different cloud technologies(AWS, Azure) with demonstratable work experience in Node.JS development as well as a full-stack developer."
data engineer,"Experienced Senior Data Engineer with over 6 years of expertise in ETL, cloud-based data solutions, and data-related projects and products from small startups to big enterprises. Proven track record in optimizing data pipelines, building scalable data lakehouses, and implementing near real-time/batch solutions. Having vast domain experience and knowledge for building new products from scratch and/or updating existing solutions according to a main target customer needs and product revenue.
Main achievements:
- Application Analyses and optimization, which resulted in halving the cost of applications (up to 2k for target pipelines) while preserving computational efficiency;
- Migrated from batch to near real-time data processing pipelines that helped the company to decrease reaction on incidents from week to 3 days;
- Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc;
- Developed Data Quality monitoring ETL pipelines from scratch and deployed it on an enterprise level. This system allows managers to react to incidents during logistics on a daily level (instead of a monthly level);
- Develop and maintain data processing, ETLs, and database architectures on a scale;"
data engineer,"As a Junior Data Analyst, I was responsible for extracting, transforming, and loading (ETL) data using a variety of tools, including MS SQL, MS Excel, and Alteryx. I have collaborated with a team to clean, transform, and prepare data for analysis, and used QlikSense to visualize and check our results."
data engineer,"Architect, backend developer, and database administrator with expertise in high-write-intensity environments using AWS/RDS/GCP/MSSQL/PostgreSQL/.NET Core/PHP.
Industry Domains:
Worked in FinTech, Marketing Platforms, Energy Management & Billing Systems, and Hosting Providers.
Key Responsibilities & Expertise:
Participated in the full lifecycle of application development, from inception to implementation.
Developed web interfaces with role-based access control (RBAC) at the action, responsibility object, and user level.
Created various data visualizations using Vector graphics with JavaScript (D3, HighChart, Flexmonster, etc.).
Strong SQL expertise, including vertical, horizontal, and aggregated structures, as well as working with semi-structured data (JSON/JSOB/XML).
Experience in building ETL/ELT pipelines in a microservices ecosystem.
Experience in building datasets (statistical typing) for machine learning and deploying trained resume_classifier in real-time applications.
Looking for creative tasks and challenging positions in structured and architectural programming"
data engineer,"I‚Äôm a data engineer focused on designing and optimizing data pipelines, ensuring data quality, and managing data systems. I collaborate closely with the team to support analytics and reporting. I want to improve my expertise in advanced data processing and automation tools."
data engineer,"Took courses on learning Python and SQL on the Udemy platform, also studied these technologies on ITVDN. Took a pre-internship course on learning Python and the Flask framework at CHI. Also studied JavaScript and NodeJS.
My education: National Technical University of Ukraine ""Kyiv Polytechnic Institute"" Bachelor`s degree and Master's degree, Computer systems and Networks."
data engineer,"Trainee Data Integration Engineer - EPAM Labs (July 2024 - Present)
‚Ä¢ Practical experience in implementing ETL in Azure Data Factory
‚Ä¢ Mastering AWS Lambda Functions, AWS S3, AWS EC2, AWS Step Functions (Practical)
‚Ä¢ Databricks for Data Engineering (theoretical)
Student - EPAM University (April 2024 - July 2024)
Project: Data Analytics and Data Quality Engineering Basics (external course)
Responsibilities:
- Software Development Methodologies (Waterfall, Scrum, Agile, Kanban)
- Business Intelligence and Data Warehouse (Inmon's and Kimball's approaches, Linstedts Data Vault), ETL/ELT theory
- Implementing ETL with SQL Server Integration Services
Other experience
JSC Sense Bank (May 2019 - March 2024)
‚Ä¢ Years of expirience: 5 years in banking anti-fraud
‚Ä¢ Project: Fraud Prevention in banking app and transactional. Textual addresses parsing and geocoding. Ecosystem for Merchants Analytics (database level)
‚Ä¢ Size of team: 4-5 people in department
‚Ä¢ Roles played: Data Analyst, Systems and Processes Support Analyst
‚Ä¢ Responsible for analysis of different fraud logics and writing anti-fraud rules
‚Ä¢ Developed textual addresses geocoding pipeline, Merchants Analytics ecosystem
‚Ä¢ Trained and implemented fraud detection model in banking app
‚Ä¢ Methodologies and project phases: Kanban
1) Developing a database back-end for Merchants Analytics that includes:
‚Ä¢ building complex scripts for counting 50+ aggregations groupped by Merchant_ID + Terminal_ID
‚Ä¢ developing tables, indexes, triggers, procedures, functions that are needed for displaying in the interface
‚Ä¢ developing an alert system when some logics occur to highlight Merchant for Monitoring team
During this project I have learnt more about transactions and isolation levels, improved even more my SQL skills writing complex dynamic queries, developed my problem-solving skills by implementing the processes in the way of high efficiency and performance and at the same time reducing the server load.
2) Developing a geocoding (obtaining latitude and longitude) of textual addresses that includes:
‚Ä¢ Preparing database tables and configuring the updating process
‚Ä¢ Cleansing the parts of addresses (region, district, city, street, house number) and bringing them to a cannonized view, ready for sending to API.
‚Ä¢ Developing the complicated process of sending to API and parsing the obtained data
This project helped me to significantly improve my regular expressions writing skills and leverage them for cleansing which was quite a challenge especially when it was needed the streets data to be cleansed. And also helped me to enhance my Python language skills writing a complex ETL process and dealing with OpenStreetMap API
My experience showed me that I like very much to organize the data in databases, do extracting and transforming (I was very passionate implementing the geocoding). So that I am looking for opportunities to develop as a Data Engineer, dealing with Clouds and Cloud ETL tools combining with Python"
data engineer,"Overall experience in IT and Data Engineering - 12+ years
- Leading Data engineering teams - 4+ years
- Data engineering competency development and conducting technical interviews - 5+ years
- Developing Data integration pipelines(ETL, ELT) and Data Lakes(Delta Lakes) - 7 years
- Data platforms development - 5+ years(using data lake, delta lake, data mesh approaches)
- Cloud providers - GCP, Azure, AWS
I would not like to work on some project where legacy coding needs to be maintained. My preference is to work with up-to-date and new/evolving technologies and approaches;
Please, no thin clients and Citrix remote desktop VDIs."
data engineer,"I am a data engineer with vast experience in building and optimising pipelines. I use tools and technologies such as SQL, Python, Kubernetes, Kubeflow, Docker, Big Query, Amazon Redshift, Postgresql, Numpy, Pandas, Web Scraping and several others.
Built ETL data and machine learning pipelines involving data scraping and collection from a variety of sources for analysis, model training and evaluation.
Built and designed data lakes and data warehouses using GCP Big Query and AWS S3.
Built data processing and transformation pipelines using Apache Spark (PySpark).
Set up Apache Airflow for pipeline orchestration and monitoring.
Led a team that implemented a climate change project from data collection to analysis and model training.
Worked with Cloud services includeing GCP, AWS, and Azure.
Designed and built a logs extraction data pipeline using Python and BigQuery for system performance monitoring.
Optimised data retrieval pipeline between the Bolt Taxi API and BigQuery data warehouse for storage and analytics, improving performance and reliability, and enhancing business decisions.
Designed and implemented robust data pipelines to cleanse, transform, and submit customer data to various credit bureaus across international markets, ensuring compliance with local regulations.
Significantly improved data accessibility and time-to-insights by automating the extraction and loading of critical data from BigQuery, enabling data-driven decision-making.
Significantly reduced data integration costs by 100% by developing a custom data pipeline to extract and load data from Zoho CRM and Zoho Desk into BigQuery.
Adopted CI/CD pipelines using GitHub, DBT, Kestra, and Airflow for deploying and maintaining ETL workflows, minimizing errors and downtime by 90%.
Refactored ETL workflows using optimized SQL queries, indexed key tables in PostgreSQL/MySQL, and batch-processed data, improving data processing efficiency by 50%.
Designed data governance policies, leading to a 25% improvement in data accuracy and compliance.
Automated API integrations, reducing manual intervention by 40% and operational costs significantly.
Integrated the flow of data between BigQuery, Slack and WhatsApp APIs, scheduled on an hourly basis to provide real-time data to relevant stakeholders for insights and improving decision-making by 80%.
Implemented CI/CD across data pipelines using GitHub, GitHub actions, Kestra, and DBT to ensure seamless deployment, automated testing, and consistent integration of updates, reducing downtime and minimising errors in production.
Prioritized data quality by implementing rigorous validation checks and error-handling mechanisms at every stage of the ETL process.
Developed new courses and improved on existing courses
Led the design and optimization of data pipelines leveraging PySpark for multi-horizon time series forecasting, achieving a 20% reduction in model training time.
Developed and optimized ETL workflows to handle large volumes of structured and unstructured data stored in AWS S3 and Redshift, increasing data accessibility across teams by 35%.
Developed geospatial data processing scripts for climate change analysis, enabling more accurate environmental impact predictions."
data engineer,"Experience in database development, data engineering. Throughout my career, I have successfully executed the complete Data Warehouse Lifecycle, encompassing essential aspects like data modelling, performance tuning, ETL architecture. Over the past five years, I have expanded my knowledge and proficiency in software engineering and database services in a cloud platform, further enhancing my ability to contribute to diverse projects."
data engineer,"-- Full stack GIS developer [JSP] Jan 2025
Farmland care automatization.
Responsibilities:
Development of web-based mapping tools for field care management and planning, drone image processing pipelines and GIS RestAPI.
--GIS analyst [zoniq.io] Nov 2023 -  Nov 2024
An antilithic solution for finding the profitable places for EV chargers.
Responsibilities:
Geospatial data collection and processing, ELT pipeline development using python.
Development of the scouting and site evaluation algorithms for charging stations.
Development of map visualization designs.
Optimization of existing infrastructure and services to improve the efficiency of data processing.
-- Backend GIS developer [jito.dev] Nov 2022 -  Nov 2023
Worldwide property databases based on machine vision and deep learning technology.
Responsibilities:
Geospatial data collection and processing, ELT pipeline development using python.
Optimization of existing databases, development of functions and procedures.
Development of geographic data processing services to ensure efficient processing of large amounts of data.
Development of prototypes for geographic web services.
-- Full stack GIS developer [Upwork] Jul 2022 - Nov 2022
Responsibilities:
Ensuring communication with clients, assessing the complexity of tasks, searching for relevant technologies, and estimating deadlines.
Development of serverless geographic systems.
Development of routing services.
Integration of geographic solutions into existing projects .
Development of desktop GIS.
-- GIS analyst/developer [m4u estate] Nov 2020 - Nov 2022
Analytical solutions for real estate developers and sales optimization.
Responsibilities:
Geospatial data collection and processing, ELT pipeline development using python.
Design, development and maintenance of geospatial databases based on postgresql, msql.
Development of spatial and network analysis algorithms for real estate analytics service.
Web map and data visualization development.
Backend development based on FastAPI and Flask frameworks.
Deploying solutions on Azure, DigitalOcean cloud services.
-- Land Surveyor Technician/ Engineer Aug 2017 -  Nov 2020
Surveying services of various levels of complexity and scale.  Field surveying of data using geodetic tools, results processing and project documentation development."
data engineer,"Experienced data engineer with over 5+ years in computer science, specializing in big data, software development, and data warehousing.
‚Ä¢ Worked on various projects in banking, trading, and tech sectors as part of a global team.
‚Ä¢ Skilled in data engineering tools and technologies, including ETL/ELT processing in both cloud (Azure, AWS) and on-premise environments.
‚Ä¢ Proven ability to analyze requirements, design architecture, and write high-quality code.
‚Ä¢ Committed to teamwork, high-quality delivery, and continuous learning and improvement.
Microsoft Certified: Azure Data Engineer Associate (DP-203)"
data engineer,"AWS certified software engineer with 3 years of experience in back-end software development and big data solutions for enterprise-level applications. Having solid engineering knowledge combined with previous experience in business field I create well-architected and effective software products with deep understanding of user expectations and business needs.
- experience in design and development of high-load data intensive applications;
- knowledge of OOP and programming concepts in Python,
- knowledge of SQL and relational databases (PostgreSQL), as well as NoSQL databases like Elasticsearch and Cassandra;
- experience with microservices architecture and building ETL pipelines;
- experience with Django and FastAPI frameworks;
- infrastructure as code with CloudFormation;
- CI/CD.
- Playing a leading role in the development of the customer support system which visualizes and analyzes customers' STB network in order to troubleshoot device connectivity and overall network health.
- Development and maintenance of a high-load big data analytics tool data from 9M devices daily using Elasticsearch and Grafana.
- Created a data warehousing solution using Cassandra for processing and storing of terabytes of data. Created a business intelligence solution with AWS Glue, AWS Athena and Apache Superset which enables leadership to get valuable insights for decision-making."
data engineer,"Last 3 years of commercial experience as a Data Science Engineer
‚Ä¢ Understanding of principals OLAP/OLTP systems.
‚Ä¢ Data cleansing, modeling, and mining
‚Ä¢ Understanding ETL processing in data warehouses;
‚Ä¢ Understanding of DWH principals.
‚Ä¢ 8+ years of overall database development expertise in DBMS (SQL, PL/SQL, T/SQL);
‚Ä¢ 8+ years of overall Data warehouse & Data Mart modeling expertise;
‚Ä¢ Strong expertise in BI stack: Data Warehouse design, implement analytics methods, Data Marts design;
‚Ä¢ With expertise in advanced statistical algorithms, forecasting.
‚Ä¢ Knowledge of cloud databases (Oracle), enterprise data warehouse structures, data pipelines and strong
SQL skills
‚Ä¢ Experience in creating data driven dashboards to answer questions raised by the business
‚Ä¢ Ability to understand and create test cases & strategies
‚Ä¢ Good visualization skills (Power BI, Oracle BI)
‚Ä¢	Transformed reporting system in according to new account matrix. Reviewed packages/procedures and processes ETL.
‚Ä¢	Optimization processes aggregation and preparing  core data to MIS Reporting
‚Ä¢	Redesigned data‚Äôs for process budgeting and forecasting
‚Ä¢	Fixed many bug‚Äôs of MIS reports based on Oracle Analytics Report (OBIEE)
‚Ä¢	More 50 reports was created in according to business during the last year"
data engineer,"Data Engineer -  Data orchestration using Apache Airflow , ETL development (SAP BODS, Pentaho DI, SAP BODS), Data processing using Apache Spark (Python, Scala) | Hive | Impala | Databricks,
Linux | Docker | Git
Data Warehouse Architect - DWH data resume_classifier and OLAP cubes
Database Developer - SQL | PL/SQL | T-SQL programming on Oracle | SAP IQ
BI Developer - Semantic layers and dashboards using OBIEE | Tibco Spotfire | MicroStrategy | SAP BO
Handling large amount of streaming data
Enhancing proficiency in cloud environments and possible transitioning to DataOps"
data engineer,"Nov 2022 ‚Äì Sep 2024, Data Engineer
Designed and implemented data transformation pipelines using Python and Apache Spark.
Orchestrated workflows through Azure Data Factory and advanced data processing via Databricks.
Migrated legacy data orchestration solutions to modern platforms.
Conducted data quality checks, validation, and anomaly detection, ensuring high
reliability.
Visualized data insights using Power BI, enhancing decision-making processes.
Jul 2022 ‚Äì Aug 2023, Back-End Developer
Developed features to manage customer data, including loan applications and transaction histories.
Integrated accounting systems with CRM for seamless financial transaction management.
Conducted rigorous testing to ensure software accuracy and reliability.
Collaborated with stakeholders to gather requirements and deliver tailored solutions.
Jul 2022 ‚Äì Dec 2022, Data Processing Specialist
Processed and analyzed qualitative data for TB research studies, producing actionable insights.
Trained team members on data analysis tools and methodologies.
Delivered comprehensive reports summarizing key findings and recommendations.
May 2021 ‚Äì Jan 2022, Monitoring and Evaluation Specialist
Coordinated 30+ monitoring requirements, ensuring compliance across regional projects.
Conducted research design, data cleaning, and analysis for 4+ studies.
Contributed to 20+ donor reports, providing critical program insights.
Feb 2019 ‚Äì May 2021, Project Manager/Data Analyst
Managed 10+ international and local social research projects, within which trained and directly managed all field staff, data collection and data processing departments (20+ people).
Prepared 25+ commercial proposals, budgets and work schedules with 45% of winning rate (20% - company average).
Developed 25+ research methodologies, designs, samples, M&E indicators, and tools.
Developed and presented 10+ analytic reports according to main findings of M&E.
Trained and mentored 3 specialists from the position of a Junior Analyst to independent project management.
Provided ongoing feedback to projects/programs, and participated in the development and improvement of the company‚Äôs business processes.
- Developed and implemented data transformation pipelines using Python and Apache Spark
- Successfully transferred legacy data orchestration solutions to modern platforms
- Executed data quality procedures, including validation checks and anomaly detection algorithms
- Utilized Power BI for data visualization
- Implemented efficient features for managing customer information within CRM systems
- Seamlessly integrated accounting systems with CRM for financial transaction management
- Processed over 20 transcripts within a TB research study
- Trained employees in qualitative data processing and analysis
- Compiled and coordinated over 30 monitoring and evaluation requirements
- Coordinated and implemented research design, methodology, data cleaning, reconciliation, and analysis of multiple studies
- Prepared over 20 quarterly and annual reports for donors
- Managed over 10 international and local social research projects
- Prepared 25+ commercial proposals with a high success rate
- Developed and presented analytic reports for variety of sectors
- Trained and mentored junior analysts to become independent project managers.
1. Professional Growth and Development
2. Challenging and Meaningful Projects
3. Collaborative and Inclusive Work Environment
4. Recognition and Reward
5. Work-Life Balance (Flexible working hours and remote work)
7. Constructive Feedback and Continuous Improvement
8. Leadership and Guidance"
data engineer,"As a beginner in data engineering and Python programming, I've been focusing on learning the fundamentals of both Python and SQL.
I'm passionate about data. I have deep experience IT project management. My experience has embedded a strong understanding of the software development lifecycle, CI/CD pipelines, and version control systems (Git), which will can help me with to data engineering workflows.
Successfully completing online courses on Python programming"
data engineer,"Data Engineer experienced in ETL development, database modeling, and financial reporting. Proficient in Guidewire DataHub, SAP Data Services, Oracle SQL. Led a successful product migration project for a UK insurance company."
data engineer,"ƒ∞ have worked as an Data Engineer at Prepare.sh. During this session i have created a system that analyzes technologies required in jobs and returns most required technologies based on the job role. I have created payment system, api s, cronjobs and etc.
I have studied my self some programming languages like C/C++ , Python. I have started solving algorithm problems after learning languages. Currently I am studying at course about Data Science and Machine Learning. And i have taken courses from Udemy like Kubernetes, Pyspark, Airflow"
data engineer,"I have a varied professional background. I started my career as an Automation QA Engineer, where I focused on automating backend tests for business. I also contributed to the development of APIs for testing purposes. Currently, I work as a Data Engineer, worked in few projects was responsible for maintaining and improving Data Warehouse. This involves using technologies like Drools, Azure, SQL Server, Azure DevOps, and MongoDB. Also for training purposes and improving technical expertise tried out PySpark and Hadoop
I also have an interest in data science and like staying informed about the latest developments in the field. I find working with complex datasets to extract meaningful insights intriguing.
Furthermore, I'm proficient in utilizing tools like pandas and NumPy for efficient data handling, along with Matplotlib and Seaborn for creating visualizations. These skills allow me to bring value to the projects I'm involved in.
For my training projects, I did telegram bots, did implementation of microservice architecture using REST API. Helping out my younger brother with developing of his snake game for android devices"
data engineer,"Seeking for positions to work with architecture and big workloads, big data, optimizations, devops. Product companies are preferred.
- Spark ETL frameworks from scratch
- Spark Streaming applications
- Data lakes on AWS
- PoCs of different architectures
NO to frontend, full stack, enterprise, legacy"
data engineer,"Middle Data Software Engineer, EPAM Systems (Apr 2024 ‚Äì Present)
Junior Data Software Engineer, EPAM Systems (Oct 2023 ‚Äì Mar 2024)
Database Engineering: Designed and optimized OLTP and OLAP systems with PostgreSQL and BigQuery, delivering high performance for complex workloads.
ETL/ELT Development: Pioneered dbt integration to refactor ELT pipelines with SQLFluff, boosting dependency tracking, documentation, and code quality, and built new pipelines with dbt, Argo Workflows, and Helm for enhanced data processing efficiency
AI & ML Integration: Used LangChain for prompt engineering and Presidio for data anonymization, ensuring compliance.
Embeddings & Similarity Search: Developed a pipeline to generate embeddings and store them in PostgreSQL, enabling millisecond-level candidate similarity searches using FastAPI.
Real-Time Data Processing: Enabled Apache Kafka for real-time data integration, ensuring seamless system connectivity.
Performance Improvements: Slashed pipeline costs by 50%, saving $600 monthly, through optimized SQL queries and optimized weekly pipeline, cutting execution time by 75% (from 4 hours to 1 hour), with parallel SQL query execution.
Leadership & Mentorship: Interviewed 4 engineers and onboarded 2 new engineers, supporting their integration into the team.
25+ industry-recognized certifications across Google Cloud, Azure, AWS, Databricks, Oracle, Salesforce, and TensorFlow."
data engineer,"skilled data engineer with more than 10 years of commercial experience, including projects in automotive (maps assembly, Location insights choice for electric vehicles stations), marketing (mobile attribution, ads extraction + analysis, search analytics), delivery and telecom domains."
data engineer,"Data Engineer & Python Developer | 5+ Years of
Experience
I‚Äôm a hands-on developer who builds reliable backend systems and data solutions. My favorite projects include a CRM for managing distressed assets and a firmware update server for agricultural robots. I‚Äôm strong in Python, ETL pipelines, and cloud setups, and I love using AI and automation to tackle tough problems.
Key Technologies:
Backend: Python, Django, DRF, FastAPI
Data & Automation: PostgreSQL, MongoDB, Celery, Redis, ETL, Web Scraping
Cloud & DevOps: AWS (EC2, S3, RDS, Lambda), Docker, GitLab CI/CD
AI & Analysis: LangChain, RAG, Pandas"
data engineer,"I am a data engineer with a Ph.D. in Applied Physics, bringing extensive experience in data collection, processing, analysis, and visualization. I have a strong background in programming languages such as Python and SQL, with expertise in data visualization and machine learning libraries. Throughout my career, I‚Äôve held roles as a data engineer, process automation specialist, and research fellow.
In my most recent position, I focused on implementing advanced analytics to streamline payment processes and improve financial efficiency. I analyzed data from patients, hospitals, and locations, uncovering critical insights that drive strategic decision-making. My responsibilities included developing automated data pipelines, enhancing data accuracy and reporting, and expanding predictive analytics capabilities to strengthen data-driven strategies."
data engineer,"In my current role, I am involved in different projects and tasks around data processing and pipeline development.
I've also been responsible for designing and maintaining  ETLs. Processing large volumes of data from various sources and loading to the Data Lake. My recent responsibility includes sprint planning as a Scrum Master for a team of 3 developers and 2 data analysts
-Integration of different data sources into the data lake
-Develop forecasting and planning processes that require minimum human interactions
-Create a fully automated project performance reporting system
-Developed pipeline by acquiring third-party data, and uploading it to the data lake
In my new role I'm looking for:
- Challenging projects
- Cutting edge technology
- Friendly and collaborative environment
- Professional growth
I'm excited to bring my skills and experience to contribute to the success of the company and grow in my data engineering career"
data engineer,"I was involved in a project focused on automating processes for ML insights.
Project utilised - GCP, Azure Databricks, Spark, Spark-Streaming, Scala, Java 17, Kafka,PySpark
‚Ä¢I worked on a complex system, gathering data from diverse sources such  as social media ad reporting APIs,
Delta, Kafka, Google Drive, Buckets. I Used Databricks and GCP to aggregate marketing data, including sales
data, for further ML insights implemented in Scala ‚Äì Spark. Based on report provider APIs I built a reporting
delivery system, installed these via GCP Cloud Function written on Java 17.
‚Ä¢I implemented and support bunch of cross-channels Spark, Spark-Kafka applications on Azure Databricks.
There`s jobs help client to share data with partners or help another teams with sources for analysis. Created
own custom Scala-Spark data source library for working with SFTP
‚Ä¢I also develop a wide bunch of analytical and data monitoring reports using Looker ML for monitoring
alerting data quality and marketing's purposes
We increased speed of delivery data to ML resume_classifier approximately in 300% and because of semantic analysis
integration we increased ad effectiveness total ROAS up to 100%.
We helped a customer save a lot of resources for future investment to ads campaigns.
I would like to evolve with python and Azure data processing technologies."
data engineer,"G-Plans company
Strong hands-on experience with SQL, ETL, and data streaming for
cloud-based analytics. Proficient in ETL design. Experienced Linux
user with administration skills. Excellent communication abilities.
Skilled in developing Airflow DAGs, API integration, data processing,
and creating alerts. Crafting queries for marketing, product, or
financial planning, followed by visualization.
Programming languages: Python
SQL: BigQuery, PostgreSQL
Storage: CSV, XML, ORC, JSON
Orchestration tools: Apache Airflow
Cloud providers: GCS
GSP tools: Cloud Storage, BigQuery, Cloud Dataflow
Analysis/visualization: Redash, BigQuery, metabase, superset.
Gain more varied experience working with DWH and developing optimal queries for analytics. Open to learning new technologies and modern methods of data processing, as well as their presentation. Gain experience working with business analytics."
data engineer,"I am a Digital Petroleum Engineer with extensive experience in applying data-driven solutions to engineering challenges. Skilled in performing segmentation and statistical analyses on large datasets using tools like SQL, PySpark, and Python, I specialize in creating efficient algorithms and pipelines for data solutions.
I have successfully deployed 4+ BP projects at the Digital Research Lab, leveraging my expertise in Palantir Foundry, including Workshop, Slate, and other applications. My work includes building scalable workflows, integrating advanced analytics, and delivering impactful insights for the energy industry. I am passionate about advancing data science in specialized fields, turning complex data into actionable knowledge."
data engineer,"Was working as sql developer, mostly. Have had next responsibilities: maintaining the MySQL database, implementing ETL processes( used java connectors for data load and sql for data transformation), query optimisation, modelling data architecture. Currently unemployed due to maternity leave. Would like to develop as a data engineer."
data engineer,"- –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –≤–∏–¥–∂–µ—Ç–æ–≤ –¥–ª—è —Ñ–æ—Ä–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞–ø–∏—Å–∏ / –∑–∞–∫–∞–∑–æ–≤ (js+css+html)
- –í–µ—Ä—Å—Ç–∫–∞ html-—à–∞–±–ª–æ–Ω–æ–≤ –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö –ø–µ—á–∞—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ CRM —Å–∏—Å—Ç–µ–º–µ (html + css).
- –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ google sheet –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö, –ø–æ–ª—É—á–∞–µ–º—ã—Ö –ø–æ API - —Å–¥–µ–ª–∞–Ω–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è google apps script (javascript)
- –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –≤ CRM (–∑–∞–≥—Ä—É–∑–∫–∞ –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö) –ø–æ API (python / apps script / java script)
- web - –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ svg-—Ñ–∞–π–ª–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –ø–æ API –¥–ª—è html-—à–∞–±–ª–æ–Ω–æ–≤ –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö –ø–µ—á–∞—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (python + flask)
- —Å–æ–∑–¥–∞–Ω–æ web-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ gmail –≤ CRM –ø–æ api (appscript / python) / —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤ –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö —ç–∫–∞—É–Ω—Ç–∞—Ö –¥–ª—è —Ä–∞–±–æ—Ç—ã –ø–æ —Ç—Ä–∏–≥–≥–µ—Ä–∞–º
- —Å–æ–∑–¥–∞–Ω –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–π –≤–∏–¥–∂–µ—Ç –ª–∏–¥-—Ñ–æ—Ä–º—ã, –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–∞ –∫–ª–∏–µ–Ω—Å–∫–∏—Ö —Å–∞–π—Ç–∞—Ö (javascript + html+ css)
- –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö (python + selenium / javascript)
- —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ web-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π (python + pytest + selenium)
- –ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è /–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –¥–∞–Ω–Ω—ã—Ö –≤ CRM —Å–∏—Å—Ç–µ–º–µ, —Ç–∞–º –≥–¥–µ –Ω–µ—Ç API. (Python + Selenium) -
- –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø–æ —Å–æ–±—ã—Ç–∏—è–º –≤ CRM (Javascript)
- –†–æ–∑—Ä–æ–±–æ—Ç–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –¥–ª—è Chrome (JavaScript, fetch, messaging, script injection)
‚Ä¢ Python OOP Stepik 2022
‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø–æ–º–æ—â—å—é Selenium –∏ Python Stepik 2022
‚Ä¢ –î–æ–±—Ä—ã–π, –¥–æ–±—Ä—ã–π Python - –æ–±—É—á–∞—é—â–∏–π –∫—É—Ä—Å –æ—Ç –°–µ—Ä–≥–µ—è
–ë–∞–ª–∞–∫–∏—Ä–µ–≤–∞, Stepik 2022
‚Ä¢ SPA —Å–∞–π—Ç –Ω–∞ Django Rest Framework –∏ NuxtJS, Stepik 2022
‚Ä¢ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ç—Ä–µ–Ω–∞–∂–µ—Ä –ø–æ SQL, Stepik 2023
‚Ä¢ HTML, Sololearn 2021
‚Ä¢ –î–æ–¥–∞—Ç–æ–∫ –¥–ª—è —Ö—Ä–æ–º–∞: remonline companion plugin"
data engineer,"June 2024 - Business Intelligence Engineer
Customer: Healthcare Services provider
Project: Development, maintenance and optimization of reporting solutions
Project Role: Data Engineer (PowerBI)
Tasks performed:
Development and maintenance of reporting solutions with the application of Power BI;
bug fixing, performance optimization;
composition of complex queries and procedures within Azure Synapse environment;
June 2022 - 2024 - Maternity Leave
Dec 2017 ‚Äì June 2022 Data Engineer
Customer: Digital Media Agency
Project: Development, maintenance and optimization of reporting solutions and ETL processes
Project Role: Data Engineer (Looker/Talend)
Tasks performed:
- Creating new LookML explores and reporting dashboards;
- Data issues investigation and troubleshooting;
- Support of existing Datalake (Hive, Talend, GCP) solutions;
- Improvements of ETL processes, performance optimization;
- Close communication with the Customer;
- Cooperation with Support team
Environment:
Datalake, Big Data, GCP, Presto, Hive, Looker, Talend Studio, PostgreSQL, MySQL, JSON.
Jul-2017 ‚Äì Dec 2017 - Software Engineer (DB/BI)
Customer: Oil Company
Project: Development and maintenance of ETL processes
Project Role: Software Engineer (DB/BI)
Tasks performed:
- Maintenance of existing ETL jobs;
- Analysis and performance optimization;
- Verification of the data integrity, testing and troubleshooting failed processes
Environment:
MS SQL Server 2014, IBM DataStage, Excel
Feb-2017 - Jul-2017 - SQL/BI developer
Customer: Various middle-size British business companies
Project: Development and maintenance of ERP system
Project Role: SQL/BI developer
Tasks performed:
- design and implementation of a range of BI solutions (reporting, dashboards, KPI);
- development of business reports by means of Pentaho Report Designer and Excel;
- optimization of existing reports;
- work with clients' databases (complex SQL queries, views, triggers, etc.);
- task management (communication with clients, clarification of requirements regarding reports, primary investigation and evaluation);
- testing of new reports
Environment:
PostgreSQL, Pentaho Tools, Excel, XML, Git
Aug-2016 - Dec-2016 - student (EPAM educational program)
Project: Internal/Educational Project (DWBI Lab): Fundamentals of relational databases, MS SQL, building ETL Data Warehouse, designing ETL packages and populating Data Warehouse by means of SSIS, development of SSAS cubes and reports.
- Extensive experience in designing reports and dashboards using such tools like PowerBI, Looker, Pentaho;
- Solid experience of working with relational databases including PostgreSQL / MySQL working environments, advanced SQL-based skills;
- Strong understanding of ETL processes arrangement and tuning;
- Successful implementation of various performance-optimization solutions, extensive analysis of existing solutions for potential enhancements;
- Strong understanding of data warehousing/datalake structures;
- Strong communicational skills, efficient cross-team cooperation, successful communication with the customer;
- Great desire to learn, dilligence and persistence.
With solid experience in Data Engineering, especially Analytics and Reporting, I am looking opportunities of additional employment that would contribute to my personal and proffesional growth. I consider remote work only, part-time with fixed hourly rate."
data engineer,"1. ML Ops platform - unit testing, bug-fixing in Back-End (Python Flask + Scala), managing Kubernetes, CI/CD pipelines
2. Fintech project - Snowflake, Python, optimizing complex SQL queries, creating data pipelines, Data warehousing, Data resume_classifier, writing Python scripts to analyse and monitor data flow
1. Participant of Google Summer of Code 2023, contributed to the development of a plugin for the Scala compiler.
2. Pet-project, worked in a team with a mentor. Contributed to the development of SDK for training and testing resume_classifier for Computer Vision tasks using PyTorch. Trained and tested resume_classifier as well."
data engineer,"DevOps
Designing and Implementing new solutions that meets business requirements
NOC Engineer
Proactive monitoring, making system more reliable.
SysOps
Oncall. Cloud migration to AWS.
System Administrator
Basic Linux server administration.
- Automated software and infrastructure delivery from scratch.
- Implementing terraform modules for needed applications
- Creating secure environment with best SDLC rules and practices."
data engineer,"I like moving data from one place to another. Use all the cool tools while doing it too.
Lots of experience with batch processing and 3rd party tools. Worked across the clouds, so I'm familiar with most of the popular tools. Learn new things really fast; all I need is a bit of trust in me taking responsibility.
Creation of data pipelines/infrastructure that powered web and mobile reports across the departments from scratch.
ETL, report automation in Python/SQL in the cloud from scratch. The clients included a multimillion-user e-commerce B2C web app, B2B wholesale platform, fintech companies, food tech.
Creation of Inbound Marketing from scratch with a stable income of ~10 B2B leads / month.
Looking for streaming/Spark, or at least an opportunity for one of those. Okay with working alone, managing architecture and writing code.
Being a one-man army, in terms of managing projects and also coding, is not ok =/."
data engineer,"Web scraping engineer with experience going around advanced protections using proxies, undetected headless browsers, API reverse engineering, captcha solving tools, etc...
I am also proficient in integrating database systems, and building backend and data systems using Go & Python."
data engineer,"* 21 years experience in commercial software development
* Python, Scaka, Java, Kotlin, JavaScript
* data engineer / back-end engineer
* Machine Learning / BigData / NoSQL / AI
* clouds especially AWS and GCP
* clouds DeevOps including Terragrant, Terraform, AWS CDK, AWS CloudFormation etc.
* CI/CD
no failed projects;
all delivered in time;
continuous learning cutting edge technologies;
a quick learner;
a best team player but working autonomously;
result oriented;
Fully remote role."
data engineer,"–î–∞—Ç–∞-—ñ–Ω–∂–µ–Ω–µ—Ä —ñ–∑ –¥–æ—Å–≤—ñ–¥–æ–º —Ä–æ–±–æ—Ç–∏ —É —Å—Ñ–µ—Ä—ñ Big Data, ETL-–ø—Ä–æ—Ü–µ—Å–∞–º–∏ —Ç–∞ —ñ–Ω–∂–µ—Å—Ç–æ–º –≤–µ–ª–∏–∫–∏—Ö –æ–±—Å—è–≥—ñ–≤ –¥–∞–Ω–∏—Ö —ñ–∑ —Ä—ñ–∑–Ω–∏—Ö –¥–∂–µ—Ä–µ–ª, —Ç–∞–∫–∏—Ö —è–∫ MSSQL, Google Sheets, AWS, MinIO, SharePoint —Ç–∞ API. –ü—Ä–∞—Ü—é—é –∑ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü—ñ—î—é –∫–æ–Ω–≤–µ—î—Ä—ñ–≤ –¥–∞–Ω–∏—Ö, —Ö–º–∞—Ä–Ω–∏–º–∏ —Å–µ—Ä–≤—ñ—Å–∞–º–∏ —Ç–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü—ñ—î—é, –æ–ø—Ç–∏–º—ñ–∑—É—é—á–∏ –æ–±—Ä–æ–±–∫—É –≤–µ–ª–∏–∫–∏—Ö –º–∞—Å–∏–≤—ñ–≤ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó.
–î–æ—Å–≤—ñ–¥:
–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ ETL-–ø—Ä–æ—Ü–µ—Å—ñ–≤ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –≤–µ–ª–∏–∫–∏—Ö –æ–±—Å—è–≥—ñ–≤ –¥–∞–Ω–∏—Ö.
–ü—ñ–¥—Ç—Ä–∏–º–∫–∞ —Ç–∞ –≤–¥–æ—Å–∫–æ–Ω–∞–ª–µ–Ω–Ω—è Python-—Ä—ñ—à–µ–Ω—å.
–†–æ–±–æ—Ç–∞ –∑ —Å–µ—Ä–≤—ñ—Å–∞–º–∏ AWS, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—è –∑–∞–¥–∞—á –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é AWS CLI —Ç–∞ Boto3.
–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –¥–∞—Ç–∞-–ø–∞–π–ø–ª–∞–π–Ω—ñ–≤ —Ç–∞ —Å—Ö–æ–≤–∏—â –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É –¥–∞–Ω–∏—Ö.
–ù–∞–ø–∏—Å–∞–Ω–Ω—è —Ç–∞ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è Python-—Å–∫—Ä–∏–ø—Ç—ñ–≤, spark-sql, sql –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –≤–µ–ª–∏–∫–∏–º–∏ –æ–±—Å—è–≥–∞–º–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó.
–£—á–∞—Å—Ç—å —É –∫–æ–¥-—Ä–µ–≤‚Äô—é —Ç–∞ –º–µ–Ω—Ç–æ—Ä—ñ–Ω–≥ –≤–µ–ª–∏–∫–æ—ó –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–æ—ó –∫–æ–º–∞–Ω–¥–∏ –≤ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—ñ –∫–æ–¥—É.
–£—á–∞—Å—Ç—å —É –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∏—Ö –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–∞—Ö, –∑ —Ç–∞–∫–∏—Ö —Ç–µ–º —è–∫ ETL-–ø—Ä–æ—Ü–µ—Å–∏ —Ç–∞ –æ—Å–Ω–æ–≤–∏ ClickHouse.
–Ü–Ω—ñ—Ü—ñ–∞—Ç–∏–≤–Ω—ñ—Å—Ç—å —É –ø–æ–∫—Ä–∞—â–µ–Ω—ñ —ñ—Å–Ω—É—é—á–∏—Ö –ï–¢–õ —Ä—ñ—à–µ–Ω—å —Ç–∞ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö –º–µ—Ç–æ–¥—ñ–≤ –Ω–∞ –∫—Ä–æ—Å-–∫–æ–º–∞–Ω–¥–Ω–æ–º—É —Ä—ñ–≤–Ω—ñ —Ç–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó –ø—Ä–æ—Ü–µ—Å—ñ–≤ –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ –∫–æ–º–∞–Ω–¥–∏.
–Ü–¢ —Å—Ñ–µ—Ä–∞ –¥—É–∂–µ –¥–∏–Ω–∞–º—ñ—á–Ω–∞, —Ü—ñ–∫–∞–≤–ª—è—Ç—å —Ä–æ–∑–≤–∏—Ç–æ–∫ —É –Ω–æ–≤–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—è—Ö, –Ω–µ –∑–∞—Ü–∏–∫–ª—é–≤–∞—Ç–∏—Å—è –Ω–∞ —è–∫–∏—Ö–æ—Å—å ""—ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö"" —Ä—ñ—à–µ–Ω–Ω—è—Ö. –•–æ—á—É –∑–≤—ñ—Å–Ω–æ –≤–¥–æ—Å–∫–æ–Ω–∞–ª—é–≤–∞—Ç–∏ –ø–æ—Ç–æ—á–Ω—ñ –Ω–∞–≤–∏—á–∫–∏ —É —Ü—ñ–∫–∞–≤–∏—Ö —Ç–∞ –≥–æ–ª–æ–≤–Ω–µ –∫–æ—Ä–∏—Å–Ω–∏—Ö –¥–ª—è –∫–æ–º–ø–∞–Ω—ñ—ó –∑–∞–¥–∞—á–∞—Ö. –ë–∞–∂–∞–Ω–Ω—è –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –≤ –∫–æ–º–∞–Ω–¥—ñ, –∞–¥–∂–µ —Ç—ñ–ª—å–∫–∏ –ø—ñ–¥ —Ä—ñ–∑–Ω–∏–º–∏ –∫—É—Ç–∞–º–∏ –º–æ–∂–Ω–∞ —Ä–æ–±–∏—Ç–∏ –¥—ñ–π—Å–Ω–æ –≥–∞—Ä–Ω—ñ –ø—Ä–æ–¥—É–∫—Ç–∏.
–ù–µ —Ö–æ—á—É –±—É—Ç–∏ —É—á–∞—Å–Ω–∏–∫–æ–º –∫–æ–º–∞–Ω–¥–∏ —è–∫–∞ –ø—Ä–∞—Ü—é—î —Ö–∞–æ—Ç–∏—á–Ω–æ –±–µ–∑ —Ü—ñ–ª–µ–π —Ç–∞ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–∏—Ö –º–µ—Ç–æ–¥–æ–ª–æ–≥—ñ–π, –±–µ–∑ —á—ñ—Ç–∫–∏—Ö –∑–æ–Ω –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω–æ—Å—Ç—ñ —É –ø—Ä–æ—Ü–µ—Å–∞—Ö."
data engineer,"Experienced in Agile frameworks and passionate about advancing research in adversarial machine learning and AI safety and security.
Experienced Data Engineer specialized in impactful digital health solutions through data collection optimization and cloud-based pipeline architecture."
data engineer,"Software Enginer, Treeum, Finance direction
12 months (November 2023 - Now)
Project description: The first online platform in Ukraine, which implements:
-Increase and sales of financial products
-Promotion of online currency exchange Maidan
-Reliable ratings and statistics
-Current news, comments and professional analytics
It is the owner of the most famous brands in the B2C segment (Finance.ua, Minfin.com.ua) and
B2B segment (Finline, Bank Online).
Responsibilities: Create and modify integrations for loading and transforming data,
support for existing integrations, working with data analytics, loading historical data and
changing existing ones, writing documentation.
Technologies: Python, MySQL, Airflow, GCP, Dataform, BQ, PowerBI, Dataedo, Docker, Git
Software Enginer, Digicode, project ""Elsevier""
11 months (November 2022 - September 2023)
Project description: The company publishes and distributes literature covering a wide range of scientific research. It publishes many standard works that define specialties. It also publishes a wide range of primary and review journals.
Responsibilities: Designing and creating new features, code review, writing documentation, researching the technologies that are most suitable for the project, and discussing them with the team and documenting them.
Technologies: Python, PySpark, MySQL, Airflow, GCP, Kafka, Docker, Jenkins, Git
Full-stack Developer, Product company ""MobilAuto"", project ""Cabinet24""
7 months (March 2022 - November 2022)
Project description: A website with an administrative panel, an order map, as well as a panel for managing drivers and passengers for a taxi aggregator and cargo delivery.
Responsibilities: Creating applications on ReactJS, estimating features, designing features, fixing bugs, —Åreating documentation, interacting with beck-end.
Technologies: React, Redux, Material-UI, Typescript, Next.js, Postman, Docker, Python, REST API, MySQL, Storybook.
2 months (December 2021 - February 2022)
Project description: Platform that features verified medical information and the current situation of the COVID-19 pandemic. The data posted is based on evidence obtained from qualitative clinical trials or approved by doctors, scientists, and UNICEF experts. The project includes recommendations by specialists in various fields of science and medicine.
Responsibilities: Creating applications on ReactJS, estimating features, designing features, fixing bugs.
Technologies: React, Redux, Redux-Saga"
data engineer,"Azure Synapse analytics, Databrics, MS Fabric, Azure Data Factory, Python, SQL, PySpark, DBT, Snowflake, Airflow
Microsoft certified data engineer. DP 203,  Databricks, DBT, DP 600, PowerBI, Pandas, Open AI, Pinecone
Microsoft certified data engineer. DP 203,  Databricks, DBT, DP 600, DP 700"
data engineer,"20+ years experience in IT:
Oracle DBA (15 years)
DevOps /SRE/configuration engineer (7 years)
Oracle Developer (PL/SQL, 5 years)
IT trainer (Oracle, 3 years)
Certifications:
Oracle Certified Professional DBA (9i/10g/11g/12c/19c),Oracle SQL certified expert,Oracle PL/SQL developer;
KCNA, Google Cloud Engineer, Azure Fundamentals, Oracle Cloud Infrastructure Architect Associate
Deployment and support experience of CI/CD infrastructure using Agile/Scrum.
Workgroups management experience.
Working experience:
Kubernetes/Helm, Terraform,Doker;
Clouds: GCP, Azure, Oracle Cloud
Project/task tracking systems -  Jira/Confluence,
Version control systems: SVN, Git, Github
Continuous integration: Team–°ity,Jenkins,Github Actions
Programming Languages: Golang, Python, bash scripting
Detailed CV is available
Professional growth. Challenging tasks."
data engineer,"Experienced Data Engineer with a proven track record of 5+ years in designing and
implementing robust data solutions. Strong knowledge of T-SQL, Snowflake, Teradata, and Azure
Data Factory. Adept at optimizing data infrastructure and pipelines to enable seamless
data processing and analysis, improving data accessibility and actionable insights.
With a keen eye for detail, good analytical skills, exceptional problem-solving abilities,
and expertise in diverse data tools and technologies, I am committed to driving data
excellence and empowering organizations to make data-driven decisions."
data engineer,"‚Ä¢	Created procedures and functions in the database
‚Ä¢	Calculation of various KPIs
‚Ä¢	Created pipelines for the ETL/ELT process
‚Ä¢	Development of dashboards
‚Ä¢      Development of DWH
‚Ä¢      Optimization of scripts
I worked with Telecom project and Financial project"
data engineer,"CTO/Te—Åh Lead/Data Architect with more than 15 years of experience, specializing in designing and implementing data management strategies. Proven ability to select technology stacks and design scalable architectures, aligning projects with technical specifications and business goals. Skilled in recruiting and managing teams across multiple concurrent projects. Extensive experience in data design, SQL/PL-SQL development, ETL processes, and performance tuning. Lead Data Science teams, managing ML development from hypothesis to production. Strong problem-solving, and communication skills, customer-facing experience, and additional knowledge in AI, blockchain, and financial risk management.
- designed 10+ OLTP/OLAP Databases
- 9 successfully finished projects
- managed team 50+ developers + QA
- managed Data Science Team"
data engineer,"Data Analyst / Junior+ Data Engineer with over 3 years of experience in data analytics, reporting, and data engineering. I‚Äôve grown from a Business Analyst in the retail sector to a Data Analyst role in an international tech company, and since 2024 have transitioned into a Junior Data Engineer role. My expertise lies in building data products, automating ETL pipelines, and delivering actionable insights through data visualization tools.
At Monster LG, I‚Äôve been responsible for developing and maintaining internal and client-facing reports in Looker, and starting from 2024, I've taken on responsibilities in data engineering: designing functions and procedures in Snowflake (SQL and Python), building ETL pipelines with dbt, and orchestrating them using Dagster. I also work closely with stakeholders to align data workflows with business goals.
Previously, at Intellias and GAIN Systems, I focused on internal reporting, data visualization (Power BI), and communication with international clients, working on requirement gathering, forecasting, and anomaly detection. My early experience in retail at Banyan Bay Trading helped shape my business acumen through hands-on work in process optimization, ERP improvements, and marketing analytics.
Tech stack & tools I work with:
SQL (Snowflake, MS SQL, PostgreSQL)
Looker, Power BI
Python
dbt, Dagster, Rivery
Jira, Confluence, TestRail
Google Analytics, Hotjar
I thrive in cross-functional teams, enjoy solving complex problems with clean data pipelines, and continuously seek ways to grow as a data engineer.
At Monster LG, I significantly improved the reporting process by optimizing Looker dashboards and automating repetitive reporting tasks, which reduced manual work and improved report delivery time.
Starting 2024, I actively contributed to the company‚Äôs data platform by developing robust functions and procedures in Snowflake using both SQL and Python, improving the maintainability and performance of key ETL workflows. I also built reusable dbt macros and modular resume_classifier, which standardized logic across projects and reduced onboarding time for new team members.
Additionally, I helped implement and orchestrate DAGs in Dagster, allowing the team to gain better visibility into data pipeline execution and simplify error handling with clear retry logic and failure notifications.
I worked closely with stakeholders to define and document data logic in Confluence, improving cross-team collaboration and enabling smoother handoffs between analytics and engineering teams. I also improved pipeline monitoring by integrating Rivery with our internal alerting system, which helped identify and resolve data quality issues proactively.
At Intellias, I redesigned legacy Power BI dashboards, optimizing SQL queries and visuals to cut load. I also introduced naming conventions and version control practices for dataset development, which improved collaboration between analysts.
At GAIN Systems, I created reusable SQL templates to detect anomalies and outliers, which were later adopted across other customer projects.
I also facilitated UAT (user acceptance testing) with clients and created test cases in TestRail, ensuring smoother delivery and faster feedback cycles.
I'm looking for a position that allows me to grow further as a Data Engineer while leveraging my solid background in Data Analytics. Ideally, I want to work on impactful projects that require building scalable and well-documented data pipelines, modern data platforms, and reliable reporting layers that support decision-making across the business.
Since 2024, I‚Äôve been actively working in a data engineering role‚Äîcreating ETL pipelines in dbt, writing functions and procedures in Snowflake (SQL + Python), and orchestrating workflows using Dagster. This hands-on experience reinforced my desire to continue evolving in this direction, combining clean architecture, performance optimization, and automation in data workflows.
I‚Äôve also completed Machine Learning and Deep Learning courses, which sparked my interest in applying ML resume_classifier in real-world scenarios‚Äîespecially around predictive analytics, anomaly detection, and intelligent alerting. I‚Äôm eager to join a team where ML components can be integrated into the data stack or product pipeline.
I value clean, well-organized processes, clear expectations, and continuous feedback. I‚Äôm not interested in legacy-only environments or multi-project overloads where there's no room for deep focus or skill development. I prefer working with modern tools in a remote-friendly, asynchronous team culture where autonomy and impact are encouraged.
My ideal environment uses technologies like Snowflake, dbt, Dagster, Looker, Python, and is open to experimentation with cloud platforms (GCP, AWS), version control, CI/CD practices, and machine learning integration when applicable."
data engineer,"I worked for a US-based travel booking company that specializes in hotels, flights, cars, and cruises. In my role as a Data Engineer, I played a pivotal part in an AB testing project for the company's website. Our main goal was to migrate ETL processes from Composer to DBT using StarBurst for seamless data integration and performance optimization. Simultaneously, I developed PySpark jobs for data transformation.
Technologies and Environments: I worked with Python, Google Cloud Platform (GCP), BigQuery, Google Composer, DataProc, Cloud Storage, MySQL, Apache Airflow, Apache Spark, DBT, and StarBurst.
I was responsible for designing, developing, and maintaining data pipelines, ensuring the smooth flow of data for the AB testing project. I collaborated with data scientists and analysts to provide valuable data insights.
I aim to stay updated on the latest data engineering and cloud technologies, particularly in optimizing data processing pipelines and performance tuning. I'm eager to take on challenging projects involving big data, machine learning, and AI to further enhance my expertise in the field.
What I Want from Work:
I'm passionate about cloud-based technologies, particularly AWS and Azure, and I'm eager to work in a new environment to expand my skill set.
I prefer roles that allow me to leverage Pyspark for data transformation and analysis.
I thrive in collaborative team environments and look forward to contributing to a positive team culture.
Continuous learning and career growth opportunities are important to me.
Competitive compensation and comprehensive benefits are priorities.
I value work-life balance and am open to flexible work arrangements.
What I Don't Want from Work (What to Avoid)
I seek to avoid roles with limited growth opportunities or those that hinder innovation.
Positions offering inadequate compensation and benefits do not align with my expectations.
I'd rather not work in environments with inflexible or excessive working hours.
Toxic workplace cultures and a lack of transparency are aspects I'd like to avoid.
Personal Traits:
I am determined and persistent, never giving up in the face of challenges.
I'm committed to delivering results and contributing positively to my team."
data engineer,"I worked for the Government of Saskatchewan as an ETL Pipeline developer. I used IBM DataStage to build data pipelines for Slowly changing dimension of type 2 for a very large data warehouse. I also wrote python scripts for automation of certain in house tasks.
Top of my classes in mathematics and computer science. I had won multiple awards in school for my academic achievements.
During my time as a data engineer, I progressed to the skill level of an intermediate data engineer in 1 months time using IBM DataStage.
A friendly environment with good mentorship in order to progress my career."
data engineer,"Data Engineer experienced in developing, supporting, and optimizing data systems (OLTP, DW, BI, ETL) for different domains.
- Got multicultural working experience with clients (USA, EU, UK, Ukraine) and was a part of local and abroad development teams (out staff and outsource).
- Got solid experience in working with Microsoft-developed data products (Azure, MS SQL Server, SSIS, SSRS, Power BI) and Azure Databricks.
- Worked side-by-side with Business Analytics, Product Owners, Product Managers, and Delivery Managers with software development teams or as a separate and independent developer on delivering quality data products.
- Worked both in Waterfall and Agile methodologies.
- Worked in such domains as healthcare, agribusiness, cybersecurity, oil & gas, and others.
- Passed certifications 70-461, 70-462 needed for MCSA: SQL Server 2012/2014.
- Passed certification for Databricks Certified Associate Developer for Apache Spark 3.0.
I'm looking for a nice, friendly, and healthy working environment. Teammates who you can call friends. And a company that values the great sacrifice of the AFU."
data engineer,"15+ years of overall experience developing enterprise software (like ERP, CRM);
9 years experience as Siebel CRM developer;
2-year experience in ETL-processes development;
Data Engineer:
Data warehousing support and development (commercial bank, insurance company).
ETL processes design, development, and maintenance.
Analysis of business requirements and process implementation.
Process failure analysis and correction.
Projects:
* Developed automation of financial monitoring of customer activity as a demand government regulator.
* Migration of DWH and BI from Teradata to Oracle DB.
BI: Oracle BI Dashboards, Oracle BI Publisher
ETL: Informatica Power Center, Oracle Data Integrator
RDBM: Oracle Database, MS SQL Server, Teradata, MySQL (DML, DDL, PL/SQL, T-SQL)
Siebel CRM:
Automation by means of Oracle Siebel CRM for commercial banks, telecom operators, etc. (Ukrainian and foreign companies).
High proficiency in various Siebel tools and technologies: eScript, Workflow, Open UI, Runtime Events, Workflow Policies, Data Validation, ADM, etc.
Experienced in supporting and modifying legacy functionality, developing new ones, and integrating with third systems using various technology (Siebel/non-Siebel).
Applications: Siebel Sales, Siebel Marketing, Siebel Field Service, Siebel Financial Services.
Deployed to production:
Automation of financial monitoring of customers activity in commercial bank as government regulator requires (ETL, Informatica)
Integration of dictionary with third system by means of stored procedure (PL/SQL, Siebel)
Implementation of mechanism of remote signing of loan agreement by means of one-time password in SMS (Siebel, Web-Services)
Developing non-vanilla mechanism of auto assigning (Siebel)
Improvement of automation of lead management (PL/SQL, Siebel)
Design and maintaine Dev-Test and Test-Prod deployment (Siebel)
It would be greate to expand and deep my expertize in Data engineering area with a team of professionals. In Siebel area I can work with same effectiveness like a single developer or as a team-player."
data engineer,"I have various experience in the field of data engineering:
- Implementation, development, updating of ETL workflows
- Collection and analysis of data from various sources (DB, .csv and .excel files, REST API)
- Data quality control and data preprocessing
- Automation of data processing procedures
- Development and support of queries in the database
- Technological stack: Python, Apache Nifi, Apache Airflow, Apache Kafka, Redis, Mongodb, RabbitMQ, Oracle PL\SQL GCP, Docker, Redshift. Open to any new technology, if it is needed, and can quickly understand it."
data engineer,"Senior Data engineer, Data Modeler, Data Analyst, Power Bi Developer with 7+ year experience in Banking, Gambling, Fintech, Financial and Public Sectors. Highly skilled Senior level Professional with experience in building data systems, data modeling, and data analysis. Creating Data Platforms from scratch. Proficient in a variety of tools and languages, including Python, SQL. Strong problem-solving and analytical skills, with a focus on delivering valuable insights from data. Excellent communication and interpersonal abilities."
data engineer,"AWS-certified Data Architect / Data Engineer with 5 years of experience in solution design and development, with focusing on BigData and BI. Committed to helping companies resolve business challenges.
DWH: Redshift, PostgreSQL, MySQL
Data Lake: S3, AWS Lake Formation, Snowflake
Data integration: AWS Glue, Lambda, AirFlow, Talend
Programming: SQL, PySpark, Python
Data Visualization: Tableau, Power BI, Data Studio, AWS QuickSight"
